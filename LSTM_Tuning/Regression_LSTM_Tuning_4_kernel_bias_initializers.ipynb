{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parameter Tuning\n",
    "    \n",
    "    Look for the best kernel initializer and bias initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\maria\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "\n",
    "# parameters\n",
    "steps = 48\n",
    "n_hidden = 2\n",
    "units = 100\n",
    "batch_size = 48\n",
    "features_num = 14\n",
    "\n",
    "# months to evaluate model on\n",
    "date = 2018110000\n",
    "\n",
    "# lists to append results\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "y_pred_list = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data; set X and y; fill nan values and split in test and training  data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)\n",
    "\n",
    "# data\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# fill nan values\n",
    "data.fillna(data.mean(), inplace = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "data_train, data_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data scaling  (including offer (y))\n",
    "sc_X = MinMaxScaler()\n",
    "data_train = sc_X.fit_transform(data_train)\n",
    "data_test = sc_X.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Keras libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to process data with spike occurences the same way as features and offers:\n",
    "(Required to evaluate predictions in both normal regions and spike regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data for shaded area\n",
    "data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# make sure shaded area will correspond to values outputed by LSTM\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# fill_nan is already made - so lets split data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "shade_train, shade_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "# reset index of testing data\n",
    "shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(shade_test, steps):\n",
    "    y_spike_occ = list()\n",
    "    upper_lim = list()\n",
    "    lower_lim = list()\n",
    "    for i in range(steps, len(shade_test.index)):\n",
    "        y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "        upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "        lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "    return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "y_spike_occ, spike_upperlim, spike_lowerlim = split_data(shade_test, steps)\n",
    "y_spike_occ = cut_data(y_spike_occ, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/43 [==============================] - 21s 497ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0382 - val_loss: 6.3266e-04 - val_mse: 6.3266e-04 - val_mae: 0.0217\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 21s 498ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0278 - val_loss: 7.0121e-04 - val_mse: 7.0121e-04 - val_mae: 0.0234\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 19s 433ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0234 - val_loss: 8.1707e-04 - val_mse: 8.1707e-04 - val_mae: 0.0256\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 16s 373ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0223 - val_loss: 5.7312e-04 - val_mse: 5.7312e-04 - val_mae: 0.0203\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 21s 488ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0210 - val_loss: 8.1341e-04 - val_mse: 8.1341e-04 - val_mae: 0.0257\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 21s 489ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0198 - val_loss: 5.0047e-04 - val_mse: 5.0047e-04 - val_mae: 0.0180\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 20s 475ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0192 - val_loss: 4.2336e-04 - val_mse: 4.2336e-04 - val_mae: 0.0143\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 21s 499ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0187 - val_loss: 4.7780e-04 - val_mse: 4.7780e-04 - val_mae: 0.0174\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 21s 480ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0184 - val_loss: 4.1311e-04 - val_mse: 4.1311e-04 - val_mae: 0.0143\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 21s 493ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0180 - val_loss: 4.4316e-04 - val_mse: 4.4316e-04 - val_mae: 0.0161\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 20s 466ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0179 - val_loss: 6.1998e-04 - val_mse: 6.1998e-04 - val_mae: 0.0215\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 22s 516ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0174 - val_loss: 3.8647e-04 - val_mse: 3.8647e-04 - val_mae: 0.0119\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 20s 455ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0172 - val_loss: 3.6839e-04 - val_mse: 3.6839e-04 - val_mae: 0.0114\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 22s 501ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0172 - val_loss: 4.0708e-04 - val_mse: 4.0708e-04 - val_mae: 0.0124\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 20s 473ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168 - val_loss: 3.7488e-04 - val_mse: 3.7488e-04 - val_mae: 0.0121\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 20s 457ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0166 - val_loss: 3.8245e-04 - val_mse: 3.8245e-04 - val_mae: 0.0122\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 21s 493ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0164 - val_loss: 3.8516e-04 - val_mse: 3.8516e-04 - val_mae: 0.0121\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 22s 517ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0163 - val_loss: 3.8155e-04 - val_mse: 3.8155e-04 - val_mae: 0.0118\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 22s 514ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0161 - val_loss: 3.6609e-04 - val_mse: 3.6609e-04 - val_mae: 0.0119\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 22s 515ms/step - loss: 9.7789e-04 - mse: 9.7789e-04 - mae: 0.0160 - val_loss: 3.7799e-04 - val_mse: 3.7799e-04 - val_mae: 0.0120\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 21s 493ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0160 - val_loss: 4.3252e-04 - val_mse: 4.3252e-04 - val_mae: 0.0137\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 20s 454ms/step - loss: 9.6825e-04 - mse: 9.6825e-04 - mae: 0.0157 - val_loss: 4.2040e-04 - val_mse: 4.2040e-04 - val_mae: 0.0124\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 20s 454ms/step - loss: 9.4020e-04 - mse: 9.4020e-04 - mae: 0.0154 - val_loss: 4.1484e-04 - val_mse: 4.1484e-04 - val_mae: 0.0141\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 21s 478ms/step - loss: 9.3740e-04 - mse: 9.3740e-04 - mae: 0.0153 - val_loss: 4.0658e-04 - val_mse: 4.0658e-04 - val_mae: 0.0125\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 20s 456ms/step - loss: 9.1581e-04 - mse: 9.1581e-04 - mae: 0.0153 - val_loss: 3.7924e-04 - val_mse: 3.7924e-04 - val_mae: 0.0117\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 20s 455ms/step - loss: 9.4426e-04 - mse: 9.4426e-04 - mae: 0.0154 - val_loss: 3.7201e-04 - val_mse: 3.7201e-04 - val_mae: 0.0116\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 21s 496ms/step - loss: 9.3144e-04 - mse: 9.3144e-04 - mae: 0.0155 - val_loss: 3.7081e-04 - val_mse: 3.7081e-04 - val_mae: 0.0118\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 21s 488ms/step - loss: 9.1558e-04 - mse: 9.1558e-04 - mae: 0.0152 - val_loss: 3.4358e-04 - val_mse: 3.4358e-04 - val_mae: 0.0115\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 17s 404ms/step - loss: 9.1364e-04 - mse: 9.1364e-04 - mae: 0.0150 - val_loss: 4.5757e-04 - val_mse: 4.5757e-04 - val_mae: 0.0142\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 22s 517ms/step - loss: 8.9502e-04 - mse: 8.9502e-04 - mae: 0.0146 - val_loss: 3.8859e-04 - val_mse: 3.8859e-04 - val_mae: 0.0136\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 22s 518ms/step - loss: 9.1398e-04 - mse: 9.1398e-04 - mae: 0.0149 - val_loss: 3.3645e-04 - val_mse: 3.3645e-04 - val_mae: 0.0122\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 22s 518ms/step - loss: 8.8791e-04 - mse: 8.8791e-04 - mae: 0.0146 - val_loss: 4.7124e-04 - val_mse: 4.7124e-04 - val_mae: 0.0141\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 8.7460e-04 - mse: 8.7460e-04 - mae: 0.0145 - val_loss: 4.4354e-04 - val_mse: 4.4354e-04 - val_mae: 0.0142\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 22s 522ms/step - loss: 9.1430e-04 - mse: 9.1430e-04 - mae: 0.0148 - val_loss: 2.9674e-04 - val_mse: 2.9674e-04 - val_mae: 0.0109\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 9.2408e-04 - mse: 9.2408e-04 - mae: 0.0148 - val_loss: 3.2752e-04 - val_mse: 3.2752e-04 - val_mae: 0.0108\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 22s 516ms/step - loss: 8.8654e-04 - mse: 8.8654e-04 - mae: 0.0145 - val_loss: 3.4336e-04 - val_mse: 3.4336e-04 - val_mae: 0.0108\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 20s 458ms/step - loss: 9.1192e-04 - mse: 9.1192e-04 - mae: 0.0145 - val_loss: 2.7458e-04 - val_mse: 2.7458e-04 - val_mae: 0.0104\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 23s 524ms/step - loss: 9.0138e-04 - mse: 9.0138e-04 - mae: 0.0141 - val_loss: 2.8351e-04 - val_mse: 2.8351e-04 - val_mae: 0.0103\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 18s 417ms/step - loss: 8.5952e-04 - mse: 8.5952e-04 - mae: 0.0139 - val_loss: 3.2574e-04 - val_mse: 3.2574e-04 - val_mae: 0.0104\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 24s 559ms/step - loss: 8.7257e-04 - mse: 8.7257e-04 - mae: 0.0139 - val_loss: 2.9210e-04 - val_mse: 2.9210e-04 - val_mae: 0.0104\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 22s 504ms/step - loss: 8.6662e-04 - mse: 8.6662e-04 - mae: 0.0140 - val_loss: 2.8369e-04 - val_mse: 2.8369e-04 - val_mae: 0.0106\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 22s 500ms/step - loss: 8.5887e-04 - mse: 8.5887e-04 - mae: 0.0137 - val_loss: 2.9167e-04 - val_mse: 2.9167e-04 - val_mae: 0.0103\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 22s 520ms/step - loss: 8.3096e-04 - mse: 8.3096e-04 - mae: 0.0136 - val_loss: 3.3528e-04 - val_mse: 3.3528e-04 - val_mae: 0.0110\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 21s 500ms/step - loss: 8.5790e-04 - mse: 8.5790e-04 - mae: 0.0140 - val_loss: 2.7666e-04 - val_mse: 2.7666e-04 - val_mae: 0.0102\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 21s 484ms/step - loss: 8.2760e-04 - mse: 8.2760e-04 - mae: 0.0132 - val_loss: 3.4490e-04 - val_mse: 3.4490e-04 - val_mae: 0.0116\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 20s 458ms/step - loss: 8.2763e-04 - mse: 8.2763e-04 - mae: 0.0135 - val_loss: 3.3112e-04 - val_mse: 3.3112e-04 - val_mae: 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "43/43 [==============================] - 22s 511ms/step - loss: 8.8001e-04 - mse: 8.8001e-04 - mae: 0.0138 - val_loss: 2.5237e-04 - val_mse: 2.5237e-04 - val_mae: 0.0096\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 22s 514ms/step - loss: 8.5674e-04 - mse: 8.5674e-04 - mae: 0.0136 - val_loss: 2.9128e-04 - val_mse: 2.9128e-04 - val_mae: 0.0105\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 22s 500ms/step - loss: 8.3678e-04 - mse: 8.3678e-04 - mae: 0.0135 - val_loss: 3.0509e-04 - val_mse: 3.0509e-04 - val_mae: 0.0107\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 20s 459ms/step - loss: 8.2067e-04 - mse: 8.2067e-04 - mae: 0.0132 - val_loss: 3.3342e-04 - val_mse: 3.3342e-04 - val_mae: 0.0109\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 20s 472ms/step - loss: 8.2764e-04 - mse: 8.2764e-04 - mae: 0.0135 - val_loss: 3.2319e-04 - val_mse: 3.2319e-04 - val_mae: 0.0109\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 24s 564ms/step - loss: 8.5192e-04 - mse: 8.5192e-04 - mae: 0.0136 - val_loss: 2.5092e-04 - val_mse: 2.5092e-04 - val_mae: 0.0094\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 22s 507ms/step - loss: 8.3012e-04 - mse: 8.3012e-04 - mae: 0.0132 - val_loss: 3.3978e-04 - val_mse: 3.3978e-04 - val_mae: 0.0114\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 21s 495ms/step - loss: 8.3216e-04 - mse: 8.3216e-04 - mae: 0.0136 - val_loss: 3.0510e-04 - val_mse: 3.0510e-04 - val_mae: 0.0107\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 23s 539ms/step - loss: 8.1779e-04 - mse: 8.1779e-04 - mae: 0.0132 - val_loss: 3.0011e-04 - val_mse: 3.0011e-04 - val_mae: 0.0105\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 26s 595ms/step - loss: 8.2006e-04 - mse: 8.2006e-04 - mae: 0.0134 - val_loss: 3.3274e-04 - val_mse: 3.3274e-04 - val_mae: 0.0118\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 8.5027e-04 - mse: 8.5027e-04 - mae: 0.0134 - val_loss: 3.1763e-04 - val_mse: 3.1763e-04 - val_mae: 0.0110\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 8.3677e-04 - mse: 8.3677e-04 - mae: 0.0134 - val_loss: 2.5672e-04 - val_mse: 2.5672e-04 - val_mae: 0.0098\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 22s 523ms/step - loss: 8.2687e-04 - mse: 8.2687e-04 - mae: 0.0133 - val_loss: 3.3856e-04 - val_mse: 3.3856e-04 - val_mae: 0.0116\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 22s 512ms/step - loss: 8.0188e-04 - mse: 8.0188e-04 - mae: 0.0133 - val_loss: 3.2326e-04 - val_mse: 3.2326e-04 - val_mae: 0.0106\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 23s 541ms/step - loss: 8.1622e-04 - mse: 8.1622e-04 - mae: 0.0133 - val_loss: 2.5197e-04 - val_mse: 2.5197e-04 - val_mae: 0.0099\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 22s 500ms/step - loss: 8.1539e-04 - mse: 8.1539e-04 - mae: 0.0131 - val_loss: 3.0663e-04 - val_mse: 3.0663e-04 - val_mae: 0.0114\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 23s 537ms/step - loss: 7.7111e-04 - mse: 7.7111e-04 - mae: 0.0126 - val_loss: 3.8528e-04 - val_mse: 3.8528e-04 - val_mae: 0.0146\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 22s 520ms/step - loss: 8.1347e-04 - mse: 8.1347e-04 - mae: 0.0134 - val_loss: 2.8346e-04 - val_mse: 2.8346e-04 - val_mae: 0.0100\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 22s 511ms/step - loss: 8.0595e-04 - mse: 8.0595e-04 - mae: 0.0132 - val_loss: 2.6538e-04 - val_mse: 2.6538e-04 - val_mae: 0.0099\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 22s 503ms/step - loss: 7.7758e-04 - mse: 7.7758e-04 - mae: 0.0129 - val_loss: 3.4224e-04 - val_mse: 3.4224e-04 - val_mae: 0.0132\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 22s 513ms/step - loss: 8.0748e-04 - mse: 8.0748e-04 - mae: 0.0131 - val_loss: 2.6529e-04 - val_mse: 2.6529e-04 - val_mae: 0.0096\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 22s 519ms/step - loss: 8.0341e-04 - mse: 8.0341e-04 - mae: 0.0129 - val_loss: 4.0959e-04 - val_mse: 4.0959e-04 - val_mae: 0.0143\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 7.9793e-04 - mse: 7.9793e-04 - mae: 0.0131 - val_loss: 2.9056e-04 - val_mse: 2.9056e-04 - val_mae: 0.0103\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 24s 569ms/step - loss: 7.6208e-04 - mse: 7.6208e-04 - mae: 0.0125 - val_loss: 4.4958e-04 - val_mse: 4.4958e-04 - val_mae: 0.0163\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 22s 505ms/step - loss: 7.9354e-04 - mse: 7.9354e-04 - mae: 0.0131 - val_loss: 2.8742e-04 - val_mse: 2.8742e-04 - val_mae: 0.0096\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 22s 519ms/step - loss: 7.9209e-04 - mse: 7.9209e-04 - mae: 0.0129 - val_loss: 2.7150e-04 - val_mse: 2.7150e-04 - val_mae: 0.0099\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 7.6783e-04 - mse: 7.6783e-04 - mae: 0.0127 - val_loss: 4.1926e-04 - val_mse: 4.1926e-04 - val_mae: 0.0148\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 19s 431ms/step - loss: 7.5167e-04 - mse: 7.5167e-04 - mae: 0.0128 - val_loss: 3.9716e-04 - val_mse: 3.9716e-04 - val_mae: 0.0144\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 21s 497ms/step - loss: 7.8414e-04 - mse: 7.8414e-04 - mae: 0.0129 - val_loss: 2.7973e-04 - val_mse: 2.7973e-04 - val_mae: 0.0104\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 7.6986e-04 - mse: 7.6986e-04 - mae: 0.0128 - val_loss: 3.5622e-04 - val_mse: 3.5622e-04 - val_mae: 0.0135\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 23s 537ms/step - loss: 7.5528e-04 - mse: 7.5528e-04 - mae: 0.0129 - val_loss: 3.1502e-04 - val_mse: 3.1502e-04 - val_mae: 0.0104\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 7.6817e-04 - mse: 7.6817e-04 - mae: 0.0130 - val_loss: 2.8190e-04 - val_mse: 2.8190e-04 - val_mae: 0.0101\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 21s 483ms/step - loss: 7.6220e-04 - mse: 7.6220e-04 - mae: 0.0130 - val_loss: 3.2365e-04 - val_mse: 3.2365e-04 - val_mae: 0.0107\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 7.3141e-04 - mse: 7.3141e-04 - mae: 0.0126 - val_loss: 3.2561e-04 - val_mse: 3.2561e-04 - val_mae: 0.0110\n",
      "Epoch 81/100\n",
      "43/43 [==============================] - 22s 503ms/step - loss: 7.4759e-04 - mse: 7.4759e-04 - mae: 0.0130 - val_loss: 3.2441e-04 - val_mse: 3.2441e-04 - val_mae: 0.0105\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 23s 545ms/step - loss: 7.2898e-04 - mse: 7.2898e-04 - mae: 0.0125 - val_loss: 2.9499e-04 - val_mse: 2.9499e-04 - val_mae: 0.0108\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 24s 557ms/step - loss: 7.4611e-04 - mse: 7.4611e-04 - mae: 0.0129 - val_loss: 2.9779e-04 - val_mse: 2.9779e-04 - val_mae: 0.0114\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 24s 547ms/step - loss: 7.1649e-04 - mse: 7.1649e-04 - mae: 0.0124 - val_loss: 2.9613e-04 - val_mse: 2.9613e-04 - val_mae: 0.0112\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 23s 538ms/step - loss: 9.0803e-04 - mse: 9.0803e-04 - mae: 0.0137 - val_loss: 2.9924e-04 - val_mse: 2.9924e-04 - val_mae: 0.0113\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 24s 547ms/step - loss: 7.0613e-04 - mse: 7.0613e-04 - mae: 0.0124 - val_loss: 2.8344e-04 - val_mse: 2.8344e-04 - val_mae: 0.0105\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 23s 542ms/step - loss: 6.9384e-04 - mse: 6.9384e-04 - mae: 0.0127 - val_loss: 2.8979e-04 - val_mse: 2.8979e-04 - val_mae: 0.0109\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 24s 554ms/step - loss: 6.9530e-04 - mse: 6.9530e-04 - mae: 0.0127 - val_loss: 3.0144e-04 - val_mse: 3.0144e-04 - val_mae: 0.0119\n",
      "Epoch 89/100\n",
      "21/43 [=============>................] - ETA: 2:27 - loss: 8.0234e-04 - mse: 8.0234e-04 - mae: 0.0130"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Dictionary to include the parameters\n",
    "parameters = {'bias_initializer':[initializers.Zeros(),\n",
    "                                 initializers.Ones()],\n",
    "              'kernel_initializer': ['glorot_uniform',\n",
    "                                     'he_normal',\n",
    "                                     'he_uniform']\n",
    "               }\n",
    "\n",
    "all_param = ParameterGrid(parameters)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# divide features and labels\n",
    "X_train = data_train[:, 0:14] \n",
    "y_train = data_train[:, -1]\n",
    "X_test = data_test[:, 0:14] \n",
    "y_test = data_test[:, -1] \n",
    "\n",
    "# divide data into train and test \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "         X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "# put data into correct shape\n",
    "X_train, y_train = split_data(X_train, y_train, steps)\n",
    "X_test, y_test = split_data(X_test, y_test, steps)\n",
    "X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "X_train = cut_data(X_train, batch_size)\n",
    "y_train = cut_data(y_train, batch_size)\n",
    "X_test = cut_data(X_test, batch_size)\n",
    "y_test = cut_data(y_test, batch_size)\n",
    "X_val = cut_data(X_val, batch_size)\n",
    "y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "# inverse of test set should not be inside the loop \n",
    "y_test = (y_test * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "for i in range(len(all_param)):\n",
    "    \n",
    "    bias_initializer = all_param[i]['bias_initializer']\n",
    "    kernel_initializer = all_param[i]['kernel_initializer']\n",
    "\n",
    "    # design the LSTM\n",
    "    def regressor_tunning(kernel_initializer = 'he_uniform',\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        if n_hidden == 0:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        else:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           return_sequences = True,\n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(LSTM(units = units, \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        optimizer = optimizers.RMSprop()\n",
    "        model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "        return model\n",
    "\n",
    "    model = regressor_tunning(bias_initializer, kernel_initializer)\n",
    "\n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = 100,\n",
    "                        shuffle = False, \n",
    "                        validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "    \n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Need to process data with spike occurences the same way as features\n",
    "    data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "    # set predictive window according with tuning best results\n",
    "    data = data.loc[data.index > date, :]\n",
    "\n",
    "    # make sure shaded area will correspond to values outputed by LSTM\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # fill_nan is already made - so lets split data into test and train\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    shade_train, shade_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "    # reset index of testing data\n",
    "    shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # function to split data into correct shape for RNN\n",
    "    def split_data_shade(shade_test, steps):\n",
    "        y_spike_occ = list()\n",
    "        upper_lim = list()\n",
    "        lower_lim = list()\n",
    "        for i in range(steps, len(shade_test.index)):\n",
    "            y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "            upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "            lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "        return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "    \n",
    "    # function to cut data set so it can be divisible by the batch_size\n",
    "    def cut_data_shade(data, batch_size):\n",
    "         # see if it is divisivel\n",
    "        condition = data.shape[0] % batch_size\n",
    "        if condition == 0:\n",
    "            return data\n",
    "        else:\n",
    "            return data[: -condition]\n",
    "    \n",
    "    # shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "    y_spike_occ, spike_upperlim, spike_lowerlim = split_data_shade(shade_test, steps)\n",
    "    y_spike_occ = cut_data_shade(y_spike_occ, batch_size)\n",
    "        \n",
    "    # continue\n",
    "    \n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'all_param':all_param,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "results.to_csv('Results_LSTM_5_kernel_bias.csv')\n",
    "\n",
    "y_pred = pd.DataFrame({'all_param': all_param,\n",
    "                       'Predicitons': y_pred_list})\n",
    "\n",
    "y_pred.to_csv('Pedictions_LSTM_4_kernel_bias.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
