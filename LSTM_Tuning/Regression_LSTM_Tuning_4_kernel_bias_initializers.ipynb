{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parameter Tuning\n",
    "    \n",
    "    Look for the best kernel initializer and bias initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\maria\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "\n",
    "# parameters\n",
    "steps = 48\n",
    "n_hidden = 2\n",
    "units = 100\n",
    "batch_size = 48\n",
    "features_num = 14\n",
    "\n",
    "# months to evaluate model on\n",
    "date = 2018090000\n",
    "\n",
    "# lists to append results\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "y_pred_list = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data; set X and y; fill nan values and split in test and training  data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)\n",
    "\n",
    "# data\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# fill nan values\n",
    "data.fillna(data.mean(), inplace = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "data_train, data_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data scaling  (including offer (y))\n",
    "sc_X = MinMaxScaler()\n",
    "data_train = sc_X.fit_transform(data_train)\n",
    "data_test = sc_X.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Keras libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to process data with spike occurences the same way as features and offers:\n",
    "(Required to evaluate predictions in both normal regions and spike regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data for shaded area\n",
    "data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# make sure shaded area will correspond to values outputed by LSTM\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# fill_nan is already made - so lets split data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "shade_train, shade_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "# reset index of testing data\n",
    "shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(shade_test, steps):\n",
    "    y_spike_occ = list()\n",
    "    upper_lim = list()\n",
    "    lower_lim = list()\n",
    "    for i in range(steps, len(shade_test.index)):\n",
    "        y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "        upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "        lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "    return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "y_spike_occ, spike_upperlim, spike_lowerlim = split_data(shade_test, steps)\n",
    "y_spike_occ = cut_data(y_spike_occ, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0310 - val_loss: 5.6415e-04 - val_mse: 5.6415e-04 - val_mae: 0.0126\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 10s 118ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0224 - val_loss: 5.0598e-04 - val_mse: 5.0598e-04 - val_mae: 0.0116\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 10s 110ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0203 - val_loss: 4.0239e-04 - val_mse: 4.0239e-04 - val_mae: 0.0120\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0191 - val_loss: 3.9946e-04 - val_mse: 3.9946e-04 - val_mae: 0.0122\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0184 - val_loss: 4.0671e-04 - val_mse: 4.0671e-04 - val_mae: 0.0125\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0181 - val_loss: 4.0339e-04 - val_mse: 4.0339e-04 - val_mae: 0.0125 - ETA: 0s - loss: 0.0011 - mse: 0.0011 - mae: 0.018\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0174 - val_loss: 3.9098e-04 - val_mse: 3.9098e-04 - val_mae: 0.0127\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0172 - val_loss: 3.6822e-04 - val_mse: 3.6822e-04 - val_mae: 0.0120\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0170 - val_loss: 3.6382e-04 - val_mse: 3.6382e-04 - val_mae: 0.0118\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0165 - val_loss: 3.4973e-04 - val_mse: 3.4973e-04 - val_mae: 0.0118\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0163 - val_loss: 3.4737e-04 - val_mse: 3.4737e-04 - val_mae: 0.0117\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0161 - val_loss: 3.5169e-04 - val_mse: 3.5169e-04 - val_mae: 0.0122\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0161 - val_loss: 3.4548e-04 - val_mse: 3.4548e-04 - val_mae: 0.0120\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 9.9550e-04 - mse: 9.9550e-04 - mae: 0.0159 - val_loss: 3.4796e-04 - val_mse: 3.4796e-04 - val_mae: 0.0121\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 9s 105ms/step - loss: 9.8756e-04 - mse: 9.8756e-04 - mae: 0.0158 - val_loss: 3.3728e-04 - val_mse: 3.3728e-04 - val_mae: 0.0115\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 9s 106ms/step - loss: 9.8495e-04 - mse: 9.8495e-04 - mae: 0.0157 - val_loss: 3.4193e-04 - val_mse: 3.4193e-04 - val_mae: 0.0121\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 9s 106ms/step - loss: 9.8004e-04 - mse: 9.8004e-04 - mae: 0.0157 - val_loss: 3.3412e-04 - val_mse: 3.3412e-04 - val_mae: 0.0117\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 9.6443e-04 - mse: 9.6443e-04 - mae: 0.0154 - val_loss: 3.3606e-04 - val_mse: 3.3606e-04 - val_mae: 0.0117\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 9.6987e-04 - mse: 9.6987e-04 - mae: 0.0155 - val_loss: 3.5575e-04 - val_mse: 3.5575e-04 - val_mae: 0.0129\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 9.6349e-04 - mse: 9.6349e-04 - mae: 0.0152 - val_loss: 3.2870e-04 - val_mse: 3.2870e-04 - val_mae: 0.0113\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 9.5802e-04 - mse: 9.5802e-04 - mae: 0.0153 - val_loss: 3.2329e-04 - val_mse: 3.2329e-04 - val_mae: 0.0114\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 9.3238e-04 - mse: 9.3238e-04 - mae: 0.0150 - val_loss: 3.3558e-04 - val_mse: 3.3558e-04 - val_mae: 0.0118\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 9.1902e-04 - mse: 9.1902e-04 - mae: 0.0149 - val_loss: 3.1049e-04 - val_mse: 3.1049e-04 - val_mae: 0.0114s: 7.2344e-04 - mse: 7.2344e-04 - mae: - ETA: 1s - loss: 9.3427e-04 - mse: 9.3427e-0\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 9.2175e-04 - mse: 9.2175e-04 - mae: 0.0148 - val_loss: 2.9219e-04 - val_mse: 2.9219e-04 - val_mae: 0.0109\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 9.1453e-04 - mse: 9.1453e-04 - mae: 0.0146 - val_loss: 2.7421e-04 - val_mse: 2.7421e-04 - val_mae: 0.0103\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 9.0291e-04 - mse: 9.0291e-04 - mae: 0.0143 - val_loss: 2.7946e-04 - val_mse: 2.7946e-04 - val_mae: 0.0107\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.5465e-04 - mse: 8.5465e-04 - mae: 0.0137 - val_loss: 2.9372e-04 - val_mse: 2.9372e-04 - val_mae: 0.0116\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 9s 103ms/step - loss: 8.4976e-04 - mse: 8.4976e-04 - mae: 0.0136 - val_loss: 2.9556e-04 - val_mse: 2.9556e-04 - val_mae: 0.0108\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.5270e-04 - mse: 8.5270e-04 - mae: 0.0137 - val_loss: 2.7272e-04 - val_mse: 2.7272e-04 - val_mae: 0.0101\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 8.8313e-04 - mse: 8.8313e-04 - mae: 0.0139 - val_loss: 2.7865e-04 - val_mse: 2.7865e-04 - val_mae: 0.0103\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 8.8581e-04 - mse: 8.8581e-04 - mae: 0.0139 - val_loss: 2.7985e-04 - val_mse: 2.7985e-04 - val_mae: 0.0102mse: 6.9399e-04 - mae:  - ETA: 1s - loss: 6.6607e-04 - mse: 6\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.6783e-04 - mse: 8.6783e-04 - mae: 0.0136 - val_loss: 2.9290e-04 - val_mse: 2.9290e-04 - val_mae: 0.0106\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.3578e-04 - mse: 8.3578e-04 - mae: 0.0134 - val_loss: 2.9418e-04 - val_mse: 2.9418e-04 - val_mae: 0.0107\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.6391e-04 - mse: 8.6391e-04 - mae: 0.0136 - val_loss: 2.8405e-04 - val_mse: 2.8405e-04 - val_mae: 0.0105\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.4452e-04 - mse: 8.4452e-04 - mae: 0.0134 - val_loss: 2.9552e-04 - val_mse: 2.9552e-04 - val_mae: 0.0108\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.0709e-04 - mse: 8.0709e-04 - mae: 0.0130 - val_loss: 2.9028e-04 - val_mse: 2.9028e-04 - val_mae: 0.0113\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.3117e-04 - mse: 8.3117e-04 - mae: 0.0133 - val_loss: 2.8546e-04 - val_mse: 2.8546e-04 - val_mae: 0.0100\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.2272e-04 - mse: 8.2272e-04 - mae: 0.0133 - val_loss: 2.9951e-04 - val_mse: 2.9951e-04 - val_mae: 0.0106\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.3024e-04 - mse: 8.3024e-04 - mae: 0.0134 - val_loss: 2.6245e-04 - val_mse: 2.6245e-04 - val_mae: 0.0098\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.9887e-04 - mse: 7.9887e-04 - mae: 0.0130 - val_loss: 2.9678e-04 - val_mse: 2.9678e-04 - val_mae: 0.0113\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.0598e-04 - mse: 8.0598e-04 - mae: 0.0131 - val_loss: 2.8753e-04 - val_mse: 2.8753e-04 - val_mae: 0.0103se: 6.\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.1351e-04 - mse: 8.1351e-04 - mae: 0.0131 - val_loss: 2.9230e-04 - val_mse: 2.9230e-04 - val_mae: 0.0100\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.3601e-04 - mse: 8.3601e-04 - mae: 0.0133 - val_loss: 2.6220e-04 - val_mse: 2.6220e-04 - val_mae: 0.0100\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.1107e-04 - mse: 8.1107e-04 - mae: 0.0132 - val_loss: 2.7914e-04 - val_mse: 2.7914e-04 - val_mae: 0.0101- loss: 8.5483e-04 - mse: 8.5483e-0\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 95ms/step - loss: 7.8724e-04 - mse: 7.8724e-04 - mae: 0.0130 - val_loss: 3.0705e-04 - val_mse: 3.0705e-04 - val_mae: 0.0111\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 8.0937e-04 - mse: 8.0937e-04 - mae: 0.0131 - val_loss: 2.8286e-04 - val_mse: 2.8286e-04 - val_mae: 0.0103\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 8.1231e-04 - mse: 8.1231e-04 - mae: 0.0132 - val_loss: 2.5436e-04 - val_mse: 2.5436e-04 - val_mae: 0.0097\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.1387e-04 - mse: 8.1387e-04 - mae: 0.0132 - val_loss: 2.8957e-04 - val_mse: 2.8957e-04 - val_mae: 0.0104\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.9188e-04 - mse: 7.9188e-04 - mae: 0.0129 - val_loss: 3.0028e-04 - val_mse: 3.0028e-04 - val_mae: 0.0107\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 8.0246e-04 - mse: 8.0246e-04 - mae: 0.0131 - val_loss: 2.6302e-04 - val_mse: 2.6302e-04 - val_mae: 0.0098\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.8281e-04 - mse: 7.8281e-04 - mae: 0.0129 - val_loss: 3.0977e-04 - val_mse: 3.0977e-04 - val_mae: 0.01100\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 7.7623e-04 - mse: 7.7623e-04 - mae: 0.0128 - val_loss: 3.7152e-04 - val_mse: 3.7152e-04 - val_mae: 0.0124\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.0521e-04 - mse: 8.0521e-04 - mae: 0.0132 - val_loss: 3.1066e-04 - val_mse: 3.1066e-04 - val_mae: 0.0110\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 9s 101ms/step - loss: 8.3188e-04 - mse: 8.3188e-04 - mae: 0.0129 - val_loss: 2.8659e-04 - val_mse: 2.8659e-04 - val_mae: 0.0100\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.8722e-04 - mse: 7.8722e-04 - mae: 0.0127 - val_loss: 2.8436e-04 - val_mse: 2.8436e-04 - val_mae: 0.0103\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.7558e-04 - mse: 7.7558e-04 - mae: 0.0128 - val_loss: 3.3646e-04 - val_mse: 3.3646e-04 - val_mae: 0.0112\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 8.0604e-04 - mse: 8.0604e-04 - mae: 0.0130 - val_loss: 2.8226e-04 - val_mse: 2.8226e-04 - val_mae: 0.0103\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.3594e-04 - mse: 8.3594e-04 - mae: 0.0131 - val_loss: 2.9912e-04 - val_mse: 2.9912e-04 - val_mae: 0.0104\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 7.9111e-04 - mse: 7.9111e-04 - mae: 0.0128 - val_loss: 2.6357e-04 - val_mse: 2.6357e-04 - val_mae: 0.0099\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.8327e-04 - mse: 7.8327e-04 - mae: 0.0128 - val_loss: 3.4216e-04 - val_mse: 3.4216e-04 - val_mae: 0.0113\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 8.0934e-04 - mse: 8.0934e-04 - mae: 0.0129 - val_loss: 3.1366e-04 - val_mse: 3.1366e-04 - val_mae: 0.01110691e\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.8299e-04 - mse: 7.8299e-04 - mae: 0.0127 - val_loss: 3.3479e-04 - val_mse: 3.3479e-04 - val_mae: 0.0114\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 7.8437e-04 - mse: 7.8437e-04 - mae: 0.0128 - val_loss: 3.0096e-04 - val_mse: 3.0096e-04 - val_mae: 0.0106\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.7416e-04 - mse: 7.7416e-04 - mae: 0.0127 - val_loss: 2.7964e-04 - val_mse: 2.7964e-04 - val_mae: 0.0107\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 7.6089e-04 - mse: 7.6089e-04 - mae: 0.0125 - val_loss: 3.5332e-04 - val_mse: 3.5332e-04 - val_mae: 0.0123\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 8.1416e-04 - mse: 8.1416e-04 - mae: 0.0129 - val_loss: 3.3253e-04 - val_mse: 3.3253e-04 - val_mae: 0.0111\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 7.7327e-04 - mse: 7.7326e-04 - mae: 0.0126 - val_loss: 2.8710e-04 - val_mse: 2.8710e-04 - val_mae: 0.0110\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.5067e-04 - mse: 7.5067e-04 - mae: 0.0125 - val_loss: 3.0165e-04 - val_mse: 3.0165e-04 - val_mae: 0.0108\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.4529e-04 - mse: 7.4529e-04 - mae: 0.0123 - val_loss: 3.1839e-04 - val_mse: 3.1839e-04 - val_mae: 0.0114\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 7.5602e-04 - mse: 7.5602e-04 - mae: 0.0126 - val_loss: 3.2682e-04 - val_mse: 3.2682e-04 - val_mae: 0.0113\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 7.7409e-04 - mse: 7.7409e-04 - mae: 0.0126 - val_loss: 3.1275e-04 - val_mse: 3.1275e-04 - val_mae: 0.0113\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.5613e-04 - mse: 7.5613e-04 - mae: 0.0126 - val_loss: 3.0957e-04 - val_mse: 3.0957e-04 - val_mae: 0.0109\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.5504e-04 - mse: 7.5504e-04 - mae: 0.0126 - val_loss: 4.1339e-04 - val_mse: 4.1339e-04 - val_mae: 0.0129\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.0596e-04 - mse: 8.0596e-04 - mae: 0.0126 - val_loss: 3.7188e-04 - val_mse: 3.7188e-04 - val_mae: 0.0124\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.7334e-04 - mse: 7.7334e-04 - mae: 0.0124 - val_loss: 3.2022e-04 - val_mse: 3.2022e-04 - val_mae: 0.0112\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 9s 101ms/step - loss: 7.9965e-04 - mse: 7.9965e-04 - mae: 0.0126 - val_loss: 3.6763e-04 - val_mse: 3.6763e-04 - val_mae: 0.0125\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.5294e-04 - mse: 7.5294e-04 - mae: 0.0123 - val_loss: 3.1479e-04 - val_mse: 3.1479e-04 - val_mae: 0.0113e-04 - mse\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 8.2587e-04 - mse: 8.2587e-04 - mae: 0.0129 - val_loss: 3.3369e-04 - val_mse: 3.3369e-04 - val_mae: 0.0117\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.6109e-04 - mse: 7.6109e-04 - mae: 0.0124 - val_loss: 4.2173e-04 - val_mse: 4.2173e-04 - val_mae: 0.0130\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.5521e-04 - mse: 7.5521e-04 - mae: 0.0125 - val_loss: 3.2061e-04 - val_mse: 3.2061e-04 - val_mae: 0.0114\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.5657e-04 - mse: 7.5657e-04 - mae: 0.0122 - val_loss: 3.3550e-04 - val_mse: 3.3550e-04 - val_mae: 0.0118\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.2032e-04 - mse: 7.2032e-04 - mae: 0.0122 - val_loss: 4.1395e-04 - val_mse: 4.1395e-04 - val_mae: 0.0125\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.7209e-04 - mse: 7.7209e-04 - mae: 0.0124 - val_loss: 3.7254e-04 - val_mse: 3.7254e-04 - val_mae: 0.0123\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 7.3273e-04 - mse: 7.3273e-04 - mae: 0.0122 - val_loss: 3.4605e-04 - val_mse: 3.4605e-04 - val_mae: 0.0114\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.2309e-04 - mse: 7.2309e-04 - mae: 0.0123 - val_loss: 3.6872e-04 - val_mse: 3.6872e-04 - val_mae: 0.0120\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 7.6283e-04 - mse: 7.6283e-04 - mae: 0.0125 - val_loss: 3.5943e-04 - val_mse: 3.5943e-04 - val_mae: 0.0119\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 9s 101ms/step - loss: 7.2707e-04 - mse: 7.2707e-04 - mae: 0.0121 - val_loss: 3.6778e-04 - val_mse: 3.6778e-04 - val_mae: 0.0115\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 7.2395e-04 - mse: 7.2395e-04 - mae: 0.0122 - val_loss: 3.9495e-04 - val_mse: 3.9495e-04 - val_mae: 0.0126\n",
      "Epoch 89/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 7.1220e-04 - mse: 7.1220e-04 - mae: 0.0123 - val_loss: 3.3088e-04 - val_mse: 3.3088e-04 - val_mae: 0.0118\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 92ms/step - loss: 6.9970e-04 - mse: 6.9970e-04 - mae: 0.0121 - val_loss: 3.6862e-04 - val_mse: 3.6862e-04 - val_mae: 0.0121\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 6.9271e-04 - mse: 6.9271e-04 - mae: 0.0120 - val_loss: 3.7993e-04 - val_mse: 3.7993e-04 - val_mae: 0.0118\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.1011e-04 - mse: 7.1011e-04 - mae: 0.0121 - val_loss: 4.3992e-04 - val_mse: 4.3992e-04 - val_mae: 0.0138\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 6.9597e-04 - mse: 6.9597e-04 - mae: 0.0123 - val_loss: 4.1083e-04 - val_mse: 4.1083e-04 - val_mae: 0.0118\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 6.8899e-04 - mse: 6.8899e-04 - mae: 0.0121 - val_loss: 5.3930e-04 - val_mse: 5.3930e-04 - val_mae: 0.0141\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 6.8044e-04 - mse: 6.8044e-04 - mae: 0.0122 - val_loss: 3.9381e-04 - val_mse: 3.9381e-04 - val_mae: 0.0114\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 6.1888e-04 - mse: 6.1888e-04 - mae: 0.0121 - val_loss: 4.8031e-04 - val_mse: 4.8031e-04 - val_mae: 0.0114\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 6.1459e-04 - mse: 6.1459e-04 - mae: 0.0119 - val_loss: 3.8273e-04 - val_mse: 3.8273e-04 - val_mae: 0.0126\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 7.5713e-04 - mse: 7.5713e-04 - mae: 0.0119 - val_loss: 3.3352e-04 - val_mse: 3.3352e-04 - val_mae: 0.0113\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 6.7283e-04 - mse: 6.7283e-04 - mae: 0.0121 - val_loss: 4.9631e-04 - val_mse: 4.9631e-04 - val_mae: 0.0143\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 6.5929e-04 - mse: 6.5929e-04 - mae: 0.0117 - val_loss: 3.7951e-04 - val_mse: 3.7951e-04 - val_mae: 0.0124\n",
      "Epoch 1/100\n",
      "87/87 [==============================] - 9s 105ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0382 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0249\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0242 - val_loss: 6.9970e-04 - val_mse: 6.9970e-04 - val_mae: 0.0162\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0216 - val_loss: 5.6667e-04 - val_mse: 5.6667e-04 - val_mae: 0.0129\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0201 - val_loss: 4.2819e-04 - val_mse: 4.2819e-04 - val_mae: 0.0114\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0190 - val_loss: 4.1121e-04 - val_mse: 4.1121e-04 - val_mae: 0.0117\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0186 - val_loss: 3.8925e-04 - val_mse: 3.8925e-04 - val_mae: 0.0118\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 8s 98ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0180 - val_loss: 3.8244e-04 - val_mse: 3.8244e-04 - val_mae: 0.0117\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0178 - val_loss: 3.7003e-04 - val_mse: 3.7003e-04 - val_mae: 0.0121\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0174 - val_loss: 3.8113e-04 - val_mse: 3.8113e-04 - val_mae: 0.0122\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0171 - val_loss: 3.8027e-04 - val_mse: 3.8027e-04 - val_mae: 0.0124\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0171 - val_loss: 3.6289e-04 - val_mse: 3.6289e-04 - val_mae: 0.0122\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168 - val_loss: 3.5957e-04 - val_mse: 3.5957e-04 - val_mae: 0.0119\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0165 - val_loss: 3.7027e-04 - val_mse: 3.7027e-04 - val_mae: 0.0126\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0163 - val_loss: 3.4066e-04 - val_mse: 3.4066e-04 - val_mae: 0.0118\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0160 - val_loss: 3.4221e-04 - val_mse: 3.4221e-04 - val_mae: 0.0121\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 9.9491e-04 - mse: 9.9491e-04 - mae: 0.0159 - val_loss: 3.3115e-04 - val_mse: 3.3115e-04 - val_mae: 0.0117\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.9210e-04 - mse: 9.9210e-04 - mae: 0.0158 - val_loss: 3.3604e-04 - val_mse: 3.3604e-04 - val_mae: 0.0117\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 9.7966e-04 - mse: 9.7966e-04 - mae: 0.0156 - val_loss: 3.2966e-04 - val_mse: 3.2966e-04 - val_mae: 0.0115\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.5838e-04 - mse: 9.5838e-04 - mae: 0.0151 - val_loss: 3.3026e-04 - val_mse: 3.3026e-04 - val_mae: 0.0121\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.5191e-04 - mse: 9.5191e-04 - mae: 0.0150 - val_loss: 2.9665e-04 - val_mse: 2.9665e-04 - val_mae: 0.0108\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 9.3246e-04 - mse: 9.3246e-04 - mae: 0.0148 - val_loss: 2.9916e-04 - val_mse: 2.9916e-04 - val_mae: 0.0110\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.1242e-04 - mse: 9.1242e-04 - mae: 0.0144 - val_loss: 2.8213e-04 - val_mse: 2.8213e-04 - val_mae: 0.0103\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.1270e-04 - mse: 9.1270e-04 - mae: 0.0143 - val_loss: 2.6048e-04 - val_mse: 2.6048e-04 - val_mae: 0.0098\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.0358e-04 - mse: 9.0358e-04 - mae: 0.0142 - val_loss: 2.5773e-04 - val_mse: 2.5773e-04 - val_mae: 0.0100\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.8793e-04 - mse: 8.8793e-04 - mae: 0.0140 - val_loss: 2.6519e-04 - val_mse: 2.6519e-04 - val_mae: 0.0097\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.1345e-04 - mse: 9.1345e-04 - mae: 0.0143 - val_loss: 2.7869e-04 - val_mse: 2.7869e-04 - val_mae: 0.0099137e-04 - mse: - ETA: 0s - loss: 8.7372e-04 - mse: 8.7372e-04 -\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 9.0216e-04 - mse: 9.0216e-04 - mae: 0.0140 - val_loss: 2.5750e-04 - val_mse: 2.5750e-04 - val_mae: 0.0099\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.9497e-04 - mse: 8.9497e-04 - mae: 0.0140 - val_loss: 2.5652e-04 - val_mse: 2.5652e-04 - val_mae: 0.0097\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.4661e-04 - mse: 8.4661e-04 - mae: 0.0135 - val_loss: 2.6579e-04 - val_mse: 2.6579e-04 - val_mae: 0.0102\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.7838e-04 - mse: 8.7838e-04 - mae: 0.0138 - val_loss: 2.6934e-04 - val_mse: 2.6934e-04 - val_mae: 0.0097\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.3386e-04 - mse: 8.3386e-04 - mae: 0.0134 - val_loss: 2.6734e-04 - val_mse: 2.6734e-04 - val_mae: 0.0106\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.5774e-04 - mse: 8.5774e-04 - mae: 0.0138 - val_loss: 2.6188e-04 - val_mse: 2.6188e-04 - val_mae: 0.0098\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.5920e-04 - mse: 8.5920e-04 - mae: 0.0135 - val_loss: 2.7381e-04 - val_mse: 2.7381e-04 - val_mae: 0.00998.6834e-04 - mse: 8.6834e-0\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.6122e-04 - mse: 8.6122e-04 - mae: 0.0135 - val_loss: 2.7926e-04 - val_mse: 2.7926e-04 - val_mae: 0.0101\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 97ms/step - loss: 8.0999e-04 - mse: 8.0999e-04 - mae: 0.0131 - val_loss: 2.8623e-04 - val_mse: 2.8623e-04 - val_mae: 0.0106\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.2787e-04 - mse: 8.2787e-04 - mae: 0.0135 - val_loss: 2.9983e-04 - val_mse: 2.9983e-04 - val_mae: 0.0114\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 8.4051e-04 - mse: 8.4051e-04 - mae: 0.0134 - val_loss: 2.8835e-04 - val_mse: 2.8835e-04 - val_mae: 0.0102\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 8.0355e-04 - mse: 8.0355e-04 - mae: 0.0129 - val_loss: 2.8985e-04 - val_mse: 2.8985e-04 - val_mae: 0.0108\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.3656e-04 - mse: 8.3656e-04 - mae: 0.0134 - val_loss: 2.6287e-04 - val_mse: 2.6287e-04 - val_mae: 0.0104\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.0920e-04 - mse: 8.0920e-04 - mae: 0.0132 - val_loss: 2.9579e-04 - val_mse: 2.9579e-04 - val_mae: 0.0108\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.0164e-04 - mse: 8.0164e-04 - mae: 0.0130 - val_loss: 3.1501e-04 - val_mse: 3.1501e-04 - val_mae: 0.0107\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.2966e-04 - mse: 8.2966e-04 - mae: 0.0133 - val_loss: 2.7182e-04 - val_mse: 2.7182e-04 - val_mae: 0.0104\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.3910e-04 - mse: 8.3910e-04 - mae: 0.0134 - val_loss: 3.1179e-04 - val_mse: 3.1179e-04 - val_mae: 0.0109.3743e-04 - mse: 6.\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.0642e-04 - mse: 8.0642e-04 - mae: 0.0130 - val_loss: 3.4014e-04 - val_mse: 3.4014e-04 - val_mae: 0.0117\n",
      "Epoch 45/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.1177e-04 - mse: 8.1177e-04 - mae: 0.0130 - val_loss: 2.7174e-04 - val_mse: 2.7174e-04 - val_mae: 0.0101\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 8s 98ms/step - loss: 8.1053e-04 - mse: 8.1053e-04 - mae: 0.0132 - val_loss: 2.9887e-04 - val_mse: 2.9887e-04 - val_mae: 0.0106\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.9584e-04 - mse: 7.9584e-04 - mae: 0.0128 - val_loss: 2.9712e-04 - val_mse: 2.9712e-04 - val_mae: 0.0108\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.9950e-04 - mse: 7.9950e-04 - mae: 0.0131 - val_loss: 3.4519e-04 - val_mse: 3.4519e-04 - val_mae: 0.0116\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.7508e-04 - mse: 7.7508e-04 - mae: 0.0128 - val_loss: 3.7906e-04 - val_mse: 3.7906e-04 - val_mae: 0.0122\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 8.0836e-04 - mse: 8.0836e-04 - mae: 0.0129 - val_loss: 2.9698e-04 - val_mse: 2.9698e-04 - val_mae: 0.0107\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 7.7429e-04 - mse: 7.7429e-04 - mae: 0.0128 - val_loss: 2.9489e-04 - val_mse: 2.9489e-04 - val_mae: 0.0101\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 7.7817e-04 - mse: 7.7817e-04 - mae: 0.0128 - val_loss: 2.9517e-04 - val_mse: 2.9517e-04 - val_mae: 0.0107\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 7.9288e-04 - mse: 7.9288e-04 - mae: 0.0129 - val_loss: 2.9171e-04 - val_mse: 2.9171e-04 - val_mae: 0.0108\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 7.8243e-04 - mse: 7.8243e-04 - mae: 0.0129 - val_loss: 2.9803e-04 - val_mse: 2.9803e-04 - val_mae: 0.0106\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.6817e-04 - mse: 7.6817e-04 - mae: 0.0126 - val_loss: 3.6396e-04 - val_mse: 3.6396e-04 - val_mae: 0.0117: 8.6443e-04 - mse: 8.6443e-04 - mae: 0 - ETA: 1s - loss: 8.3219e-04 - mse: 8.3219e-0\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 9s 102ms/step - loss: 8.1468e-04 - mse: 8.1468e-04 - mae: 0.0130 - val_loss: 3.1415e-04 - val_mse: 3.1415e-04 - val_mae: 0.0106\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 9s 104ms/step - loss: 8.0209e-04 - mse: 8.0209e-04 - mae: 0.0129 - val_loss: 2.8477e-04 - val_mse: 2.8477e-04 - val_mae: 0.0103.3119e-04 - mse: 8.3119e-04 - mae: 0\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 9s 102ms/step - loss: 7.7418e-04 - mse: 7.7418e-04 - mae: 0.0128 - val_loss: 2.9327e-04 - val_mse: 2.9327e-04 - val_mae: 0.0107\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 9s 102ms/step - loss: 8.1346e-04 - mse: 8.1346e-04 - mae: 0.0127 - val_loss: 3.2893e-04 - val_mse: 3.2893e-04 - val_mae: 0.0108\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 7.7697e-04 - mse: 7.7697e-04 - mae: 0.0128 - val_loss: 4.4148e-04 - val_mse: 4.4148e-04 - val_mae: 0.0133\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.3853e-04 - mse: 8.3853e-04 - mae: 0.0131 - val_loss: 3.1016e-04 - val_mse: 3.1016e-04 - val_mae: 0.0105\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.5570e-04 - mse: 7.5570e-04 - mae: 0.0126 - val_loss: 3.1627e-04 - val_mse: 3.1627e-04 - val_mae: 0.0112oss: 6.5529e-04 - ms\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.9023e-04 - mse: 7.9023e-04 - mae: 0.0127 - val_loss: 3.1160e-04 - val_mse: 3.1160e-04 - val_mae: 0.0110\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.9179e-04 - mse: 7.9179e-04 - mae: 0.0128 - val_loss: 3.1865e-04 - val_mse: 3.1865e-04 - val_mae: 0.0109\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.3364e-04 - mse: 8.3364e-04 - mae: 0.0129 - val_loss: 3.0612e-04 - val_mse: 3.0612e-04 - val_mae: 0.01089821e-04 - mse: - ETA: 2s - loss: 6.4719e-04 - ms\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.5883e-04 - mse: 7.5883e-04 - mae: 0.0124 - val_loss: 3.1895e-04 - val_mse: 3.1895e-04 - val_mae: 0.0106\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.3425e-04 - mse: 7.3425e-04 - mae: 0.0122 - val_loss: 3.1333e-04 - val_mse: 3.1333e-04 - val_mae: 0.0111\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.9436e-04 - mse: 7.9436e-04 - mae: 0.0128 - val_loss: 3.4330e-04 - val_mse: 3.4330e-04 - val_mae: 0.0114\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.7558e-04 - mse: 7.7558e-04 - mae: 0.0126 - val_loss: 3.5008e-04 - val_mse: 3.5008e-04 - val_mae: 0.0114\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.8379e-04 - mse: 7.8379e-04 - mae: 0.0127 - val_loss: 3.1951e-04 - val_mse: 3.1951e-04 - val_mae: 0.0112\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.2912e-04 - mse: 8.2912e-04 - mae: 0.0129 - val_loss: 2.9870e-04 - val_mse: 2.9870e-04 - val_mae: 0.0108\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.4245e-04 - mse: 7.4245e-04 - mae: 0.0125 - val_loss: 3.6775e-04 - val_mse: 3.6775e-04 - val_mae: 0.0120\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.4401e-04 - mse: 7.4401e-04 - mae: 0.0126 - val_loss: 3.1756e-04 - val_mse: 3.1756e-04 - val_mae: 0.0109\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 9s 106ms/step - loss: 7.3578e-04 - mse: 7.3578e-04 - mae: 0.0124 - val_loss: 3.5873e-04 - val_mse: 3.5873e-04 - val_mae: 0.0114\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 9s 103ms/step - loss: 7.4630e-04 - mse: 7.4630e-04 - mae: 0.0125 - val_loss: 3.3234e-04 - val_mse: 3.3234e-04 - val_mae: 0.0121\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 9s 103ms/step - loss: 7.7031e-04 - mse: 7.7031e-04 - mae: 0.0126 - val_loss: 3.3367e-04 - val_mse: 3.3367e-04 - val_mae: 0.0119\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 9s 103ms/step - loss: 7.2250e-04 - mse: 7.2250e-04 - mae: 0.0121 - val_loss: 3.5621e-04 - val_mse: 3.5621e-04 - val_mae: 0.0120\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 9s 101ms/step - loss: 7.4821e-04 - mse: 7.4821e-04 - mae: 0.0126 - val_loss: 3.1027e-04 - val_mse: 3.1027e-04 - val_mae: 0.0111\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 96ms/step - loss: 7.0642e-04 - mse: 7.0642e-04 - mae: 0.0124 - val_loss: 6.0526e-04 - val_mse: 6.0526e-04 - val_mae: 0.0131\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 6.8537e-04 - mse: 6.8537e-04 - mae: 0.0125 - val_loss: 4.0560e-04 - val_mse: 4.0560e-04 - val_mae: 0.0125\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 7.0156e-04 - mse: 7.0156e-04 - mae: 0.0126 - val_loss: 3.7074e-04 - val_mse: 3.7074e-04 - val_mae: 0.0112\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 7.5150e-04 - mse: 7.5150e-04 - mae: 0.0127 - val_loss: 9.4315e-04 - val_mse: 9.4315e-04 - val_mae: 0.0125\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.0157e-04 - mse: 7.0157e-04 - mae: 0.0124 - val_loss: 4.9103e-04 - val_mse: 4.9103e-04 - val_mae: 0.0145\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 7.2871e-04 - mse: 7.2871e-04 - mae: 0.0125 - val_loss: 3.0516e-04 - val_mse: 3.0516e-04 - val_mae: 0.0109\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.0086e-04 - mse: 7.0086e-04 - mae: 0.0123 - val_loss: 3.4683e-04 - val_mse: 3.4683e-04 - val_mae: 0.0113\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.0635e-04 - mse: 7.0635e-04 - mae: 0.0122 - val_loss: 3.6141e-04 - val_mse: 3.6141e-04 - val_mae: 0.0123\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 6.9272e-04 - mse: 6.9272e-04 - mae: 0.0124 - val_loss: 4.6325e-04 - val_mse: 4.6325e-04 - val_mae: 0.0141\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 6.0086e-04 - mse: 6.0086e-04 - mae: 0.0120 - val_loss: 4.1421e-04 - val_mse: 4.1421e-04 - val_mae: 0.0125ss\n",
      "Epoch 89/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 6.0997e-04 - mse: 6.0997e-04 - mae: 0.0121 - val_loss: 3.8463e-04 - val_mse: 3.8463e-04 - val_mae: 0.0123\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 6.6796e-04 - mse: 6.6796e-04 - mae: 0.0122 - val_loss: 4.0127e-04 - val_mse: 4.0127e-04 - val_mae: 0.0118\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 6.0142e-04 - mse: 6.0142e-04 - mae: 0.0120 - val_loss: 5.0858e-04 - val_mse: 5.0858e-04 - val_mae: 0.0141\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 6.9480e-04 - mse: 6.9480e-04 - mae: 0.0121 - val_loss: 3.5231e-04 - val_mse: 3.5231e-04 - val_mae: 0.0116\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 6.8042e-04 - mse: 6.8042e-04 - mae: 0.0121 - val_loss: 3.1243e-04 - val_mse: 3.1243e-04 - val_mae: 0.0107\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 7.3541e-04 - mse: 7.3541e-04 - mae: 0.0117 - val_loss: 4.0996e-04 - val_mse: 4.0996e-04 - val_mae: 0.0127\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 6.4873e-04 - mse: 6.4873e-04 - mae: 0.0118 - val_loss: 4.0670e-04 - val_mse: 4.0670e-04 - val_mae: 0.0130\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 6.7867e-04 - mse: 6.7867e-04 - mae: 0.0120 - val_loss: 3.6477e-04 - val_mse: 3.6477e-04 - val_mae: 0.0126\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 6.8595e-04 - mse: 6.8595e-04 - mae: 0.0120 - val_loss: 3.3073e-04 - val_mse: 3.3073e-04 - val_mae: 0.0111\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 6.3610e-04 - mse: 6.3610e-04 - mae: 0.0115 - val_loss: 3.9453e-04 - val_mse: 3.9453e-04 - val_mae: 0.0121\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 6.3416e-04 - mse: 6.3416e-04 - mae: 0.0117 - val_loss: 3.9208e-04 - val_mse: 3.9208e-04 - val_mae: 0.0120\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 7.0371e-04 - mse: 7.0371e-04 - mae: 0.0117 - val_loss: 3.7824e-04 - val_mse: 3.7824e-04 - val_mae: 0.0123\n",
      "Epoch 1/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0369 - val_loss: 7.6894e-04 - val_mse: 7.6894e-04 - val_mae: 0.0179\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0239 - val_loss: 5.2596e-04 - val_mse: 5.2596e-04 - val_mae: 0.0120\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0212 - val_loss: 4.8580e-04 - val_mse: 4.8580e-04 - val_mae: 0.0114\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0195 - val_loss: 4.1422e-04 - val_mse: 4.1422e-04 - val_mae: 0.0119\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0189 - val_loss: 4.0011e-04 - val_mse: 4.0011e-04 - val_mae: 0.0123\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0184 - val_loss: 4.0406e-04 - val_mse: 4.0406e-04 - val_mae: 0.0125\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0178 - val_loss: 3.9353e-04 - val_mse: 3.9353e-04 - val_mae: 0.0125 ETA: 0s - loss: 0.0010 - mse: 0.0010 - m\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0174 - val_loss: 4.0548e-04 - val_mse: 4.0548e-04 - val_mae: 0.0126\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0170 - val_loss: 3.9245e-04 - val_mse: 3.9245e-04 - val_mae: 0.0127\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168 - val_loss: 3.7351e-04 - val_mse: 3.7351e-04 - val_mae: 0.0124\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0166 - val_loss: 3.6573e-04 - val_mse: 3.6573e-04 - val_mae: 0.0120\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0163 - val_loss: 3.7244e-04 - val_mse: 3.7244e-04 - val_mae: 0.0121\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0162 - val_loss: 3.6892e-04 - val_mse: 3.6892e-04 - val_mae: 0.0119\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0161 - val_loss: 3.6342e-04 - val_mse: 3.6342e-04 - val_mae: 0.0120\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 9.9445e-04 - mse: 9.9445e-04 - mae: 0.0157 - val_loss: 3.5556e-04 - val_mse: 3.5556e-04 - val_mae: 0.0118\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 9.9261e-04 - mse: 9.9261e-04 - mae: 0.0159 - val_loss: 3.5486e-04 - val_mse: 3.5486e-04 - val_mae: 0.0116\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 9.8000e-04 - mse: 9.8000e-04 - mae: 0.0156 - val_loss: 3.5397e-04 - val_mse: 3.5397e-04 - val_mae: 0.0120\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 9.6400e-04 - mse: 9.6400e-04 - mae: 0.0155 - val_loss: 3.4547e-04 - val_mse: 3.4547e-04 - val_mae: 0.0115\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 9.6596e-04 - mse: 9.6596e-04 - mae: 0.0155 - val_loss: 3.5070e-04 - val_mse: 3.5070e-04 - val_mae: 0.0116\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 9.7826e-04 - mse: 9.7826e-04 - mae: 0.0155 - val_loss: 3.3640e-04 - val_mse: 3.3640e-04 - val_mae: 0.0114\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 9.4106e-04 - mse: 9.4106e-04 - mae: 0.0151 - val_loss: 3.5051e-04 - val_mse: 3.5051e-04 - val_mae: 0.0116\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 9.5537e-04 - mse: 9.5537e-04 - mae: 0.0152 - val_loss: 3.4927e-04 - val_mse: 3.4927e-04 - val_mae: 0.0118\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 9.5760e-04 - mse: 9.5760e-04 - mae: 0.0152 - val_loss: 3.3757e-04 - val_mse: 3.3757e-04 - val_mae: 0.0116\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 9.2789e-04 - mse: 9.2789e-04 - mae: 0.0150 - val_loss: 3.5199e-04 - val_mse: 3.5199e-04 - val_mae: 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 9.3727e-04 - mse: 9.3727e-04 - mae: 0.0150 - val_loss: 3.2777e-04 - val_mse: 3.2777e-04 - val_mae: 0.0111\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 9.1138e-04 - mse: 9.1138e-04 - mae: 0.0148 - val_loss: 3.4880e-04 - val_mse: 3.4880e-04 - val_mae: 0.0117\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 8.9395e-04 - mse: 8.9395e-04 - mae: 0.0145 - val_loss: 3.4316e-04 - val_mse: 3.4316e-04 - val_mae: 0.0114\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 8.8357e-04 - mse: 8.8357e-04 - mae: 0.0143 - val_loss: 3.2505e-04 - val_mse: 3.2505e-04 - val_mae: 0.0112\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 9s 104ms/step - loss: 8.7920e-04 - mse: 8.7920e-04 - mae: 0.0142 - val_loss: 3.1600e-04 - val_mse: 3.1600e-04 - val_mae: 0.0114\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 8.5704e-04 - mse: 8.5704e-04 - mae: 0.0138 - val_loss: 2.9357e-04 - val_mse: 2.9357e-04 - val_mae: 0.0108\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 8.6583e-04 - mse: 8.6583e-04 - mae: 0.0137 - val_loss: 2.6654e-04 - val_mse: 2.6654e-04 - val_mae: 0.0104\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 13s 151ms/step - loss: 8.4803e-04 - mse: 8.4803e-04 - mae: 0.0137 - val_loss: 2.8599e-04 - val_mse: 2.8599e-04 - val_mae: 0.0107\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 8.3929e-04 - mse: 8.3929e-04 - mae: 0.0135 - val_loss: 2.6669e-04 - val_mse: 2.6669e-04 - val_mae: 0.0102\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 8.1708e-04 - mse: 8.1708e-04 - mae: 0.0134 - val_loss: 2.6959e-04 - val_mse: 2.6959e-04 - val_mae: 0.0099\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 8.3181e-04 - mse: 8.3181e-04 - mae: 0.0135 - val_loss: 2.6573e-04 - val_mse: 2.6573e-04 - val_mae: 0.0107\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 8.0800e-04 - mse: 8.0800e-04 - mae: 0.0133 - val_loss: 2.8094e-04 - val_mse: 2.8094e-04 - val_mae: 0.0102\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 8.2860e-04 - mse: 8.2860e-04 - mae: 0.0134 - val_loss: 2.7360e-04 - val_mse: 2.7360e-04 - val_mae: 0.0105\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 11s 124ms/step - loss: 8.2524e-04 - mse: 8.2524e-04 - mae: 0.0133 - val_loss: 3.0781e-04 - val_mse: 3.0781e-04 - val_mae: 0.0114\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 11s 131ms/step - loss: 8.2749e-04 - mse: 8.2749e-04 - mae: 0.0133 - val_loss: 2.7289e-04 - val_mse: 2.7289e-04 - val_mae: 0.0105\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 11s 122ms/step - loss: 8.2600e-04 - mse: 8.2600e-04 - mae: 0.0134 - val_loss: 3.0101e-04 - val_mse: 3.0101e-04 - val_mae: 0.0113\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 8.2104e-04 - mse: 8.2104e-04 - mae: 0.0132 - val_loss: 2.7642e-04 - val_mse: 2.7642e-04 - val_mae: 0.0102\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 7.9552e-04 - mse: 7.9552e-04 - mae: 0.0131 - val_loss: 3.2058e-04 - val_mse: 3.2058e-04 - val_mae: 0.0120\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 12s 132ms/step - loss: 7.8422e-04 - mse: 7.8422e-04 - mae: 0.0128 - val_loss: 3.1749e-04 - val_mse: 3.1749e-04 - val_mae: 0.0115\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 8.0504e-04 - mse: 8.0504e-04 - mae: 0.0131 - val_loss: 2.8499e-04 - val_mse: 2.8499e-04 - val_mae: 0.0111\n",
      "Epoch 45/100\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 8.0408e-04 - mse: 8.0408e-04 - mae: 0.0130 - val_loss: 2.7046e-04 - val_mse: 2.7046e-04 - val_mae: 0.0103\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 10s 114ms/step - loss: 8.0133e-04 - mse: 8.0133e-04 - mae: 0.0132 - val_loss: 3.0931e-04 - val_mse: 3.0931e-04 - val_mae: 0.0111\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 7.8551e-04 - mse: 7.8551e-04 - mae: 0.0128 - val_loss: 3.4378e-04 - val_mse: 3.4378e-04 - val_mae: 0.0118\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 12s 134ms/step - loss: 8.6723e-04 - mse: 8.6723e-04 - mae: 0.0132 - val_loss: 2.8621e-04 - val_mse: 2.8621e-04 - val_mae: 0.0109\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 11s 128ms/step - loss: 7.9686e-04 - mse: 7.9686e-04 - mae: 0.0129 - val_loss: 2.7248e-04 - val_mse: 2.7248e-04 - val_mae: 0.0103\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 12s 143ms/step - loss: 7.8196e-04 - mse: 7.8196e-04 - mae: 0.0128 - val_loss: 2.7971e-04 - val_mse: 2.7971e-04 - val_mae: 0.0101\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 7.8303e-04 - mse: 7.8303e-04 - mae: 0.0127 - val_loss: 2.9029e-04 - val_mse: 2.9029e-04 - val_mae: 0.0110\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 10s 119ms/step - loss: 7.8633e-04 - mse: 7.8633e-04 - mae: 0.0127 - val_loss: 3.4513e-04 - val_mse: 3.4513e-04 - val_mae: 0.0124\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 13s 147ms/step - loss: 8.1625e-04 - mse: 8.1625e-04 - mae: 0.0131 - val_loss: 3.2172e-04 - val_mse: 3.2172e-04 - val_mae: 0.0121\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 13s 150ms/step - loss: 8.1913e-04 - mse: 8.1913e-04 - mae: 0.0129 - val_loss: 3.1311e-04 - val_mse: 3.1311e-04 - val_mae: 0.0113\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 11s 126ms/step - loss: 8.1919e-04 - mse: 8.1919e-04 - mae: 0.0129 - val_loss: 3.2415e-04 - val_mse: 3.2415e-04 - val_mae: 0.0119\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 12s 143ms/step - loss: 8.1983e-04 - mse: 8.1983e-04 - mae: 0.0130 - val_loss: 2.8843e-04 - val_mse: 2.8843e-04 - val_mae: 0.0104\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 7.6529e-04 - mse: 7.6529e-04 - mae: 0.0125 - val_loss: 3.5786e-04 - val_mse: 3.5786e-04 - val_mae: 0.0132\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 10s 110ms/step - loss: 8.1498e-04 - mse: 8.1498e-04 - mae: 0.0131 - val_loss: 2.9704e-04 - val_mse: 2.9704e-04 - val_mae: 0.0109\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 8.0427e-04 - mse: 8.0427e-04 - mae: 0.0129 - val_loss: 3.0491e-04 - val_mse: 3.0491e-04 - val_mae: 0.0113\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 7.6478e-04 - mse: 7.6478e-04 - mae: 0.0127 - val_loss: 3.4164e-04 - val_mse: 3.4164e-04 - val_mae: 0.0128\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 11s 130ms/step - loss: 8.0542e-04 - mse: 8.0542e-04 - mae: 0.0128 - val_loss: 3.3048e-04 - val_mse: 3.3048e-04 - val_mae: 0.0117\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 13s 152ms/step - loss: 7.4070e-04 - mse: 7.4070e-04 - mae: 0.0127 - val_loss: 3.0814e-04 - val_mse: 3.0814e-04 - val_mae: 0.0112\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 7.5682e-04 - mse: 7.5682e-04 - mae: 0.0127 - val_loss: 3.1848e-04 - val_mse: 3.1848e-04 - val_mae: 0.0110\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 12s 133ms/step - loss: 7.3247e-04 - mse: 7.3247e-04 - mae: 0.0124 - val_loss: 4.1865e-04 - val_mse: 4.1865e-04 - val_mae: 0.0132\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 7.6361e-04 - mse: 7.6361e-04 - mae: 0.0129 - val_loss: 3.2831e-04 - val_mse: 3.2831e-04 - val_mae: 0.0120\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 7.3953e-04 - mse: 7.3953e-04 - mae: 0.0127 - val_loss: 2.8490e-04 - val_mse: 2.8490e-04 - val_mae: 0.0106\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 7.8317e-04 - mse: 7.8317e-04 - mae: 0.0128 - val_loss: 4.1685e-04 - val_mse: 4.1685e-04 - val_mae: 0.0129\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 7.7751e-04 - mse: 7.7751e-04 - mae: 0.0128 - val_loss: 3.4834e-04 - val_mse: 3.4834e-04 - val_mae: 0.0122\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 7.2434e-04 - mse: 7.2434e-04 - mae: 0.0126 - val_loss: 3.3213e-04 - val_mse: 3.3213e-04 - val_mae: 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 7.0087e-04 - mse: 7.0087e-04 - mae: 0.0123 - val_loss: 5.2488e-04 - val_mse: 5.2488e-04 - val_mae: 0.0130\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 12s 134ms/step - loss: 7.6921e-04 - mse: 7.6921e-04 - mae: 0.0129 - val_loss: 4.3564e-04 - val_mse: 4.3564e-04 - val_mae: 0.0126\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 6.9209e-04 - mse: 6.9209e-04 - mae: 0.0126 - val_loss: 3.7271e-04 - val_mse: 3.7271e-04 - val_mae: 0.0121\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.2208e-04 - mse: 7.2208e-04 - mae: 0.0124 - val_loss: 7.9403e-04 - val_mse: 7.9403e-04 - val_mae: 0.0151\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 6.6323e-04 - mse: 6.6323e-04 - mae: 0.0125 - val_loss: 4.1309e-04 - val_mse: 4.1309e-04 - val_mae: 0.0129\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 11s 131ms/step - loss: 7.3741e-04 - mse: 7.3741e-04 - mae: 0.0126 - val_loss: 7.9555e-04 - val_mse: 7.9555e-04 - val_mae: 0.0156\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 11s 129ms/step - loss: 6.8054e-04 - mse: 6.8054e-04 - mae: 0.0125 - val_loss: 4.4411e-04 - val_mse: 4.4411e-04 - val_mae: 0.0127\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 6.8513e-04 - mse: 6.8513e-04 - mae: 0.0124 - val_loss: 6.5460e-04 - val_mse: 6.5460e-04 - val_mae: 0.0128\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 6.7330e-04 - mse: 6.7330e-04 - mae: 0.0122 - val_loss: 3.5005e-04 - val_mse: 3.5005e-04 - val_mae: 0.0125\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 11s 122ms/step - loss: 6.8470e-04 - mse: 6.8470e-04 - mae: 0.0124 - val_loss: 3.1705e-04 - val_mse: 3.1705e-04 - val_mae: 0.0119\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 6.4990e-04 - mse: 6.4990e-04 - mae: 0.0122 - val_loss: 4.8324e-04 - val_mse: 4.8324e-04 - val_mae: 0.0140\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 6.6871e-04 - mse: 6.6871e-04 - mae: 0.0123 - val_loss: 4.3375e-04 - val_mse: 4.3375e-04 - val_mae: 0.0122\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 11s 128ms/step - loss: 6.2954e-04 - mse: 6.2954e-04 - mae: 0.0123 - val_loss: 3.7179e-04 - val_mse: 3.7179e-04 - val_mae: 0.0130\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 6.7869e-04 - mse: 6.7869e-04 - mae: 0.0123 - val_loss: 3.2002e-04 - val_mse: 3.2002e-04 - val_mae: 0.0111\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 13s 150ms/step - loss: 6.9574e-04 - mse: 6.9574e-04 - mae: 0.0120 - val_loss: 4.2783e-04 - val_mse: 4.2783e-04 - val_mae: 0.0134\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 11s 122ms/step - loss: 6.3220e-04 - mse: 6.3220e-04 - mae: 0.0122 - val_loss: 3.7070e-04 - val_mse: 3.7070e-04 - val_mae: 0.0125\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 13s 147ms/step - loss: 7.2648e-04 - mse: 7.2648e-04 - mae: 0.0122 - val_loss: 3.8903e-04 - val_mse: 3.8903e-04 - val_mae: 0.0127\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 6.1785e-04 - mse: 6.1785e-04 - mae: 0.0122 - val_loss: 0.0010 - val_mse: 0.0010 - val_mae: 0.0129\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 10s 118ms/step - loss: 5.9264e-04 - mse: 5.9264e-04 - mae: 0.0120 - val_loss: 3.2658e-04 - val_mse: 3.2658e-04 - val_mae: 0.0122\n",
      "Epoch 89/100\n",
      "87/87 [==============================] - 13s 146ms/step - loss: 6.6715e-04 - mse: 6.6715e-04 - mae: 0.0119 - val_loss: 3.5275e-04 - val_mse: 3.5275e-04 - val_mae: 0.0124\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 6.7432e-04 - mse: 6.7432e-04 - mae: 0.0123 - val_loss: 4.0141e-04 - val_mse: 4.0141e-04 - val_mae: 0.0120\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 6.7697e-04 - mse: 6.7697e-04 - mae: 0.0119 - val_loss: 4.5331e-04 - val_mse: 4.5331e-04 - val_mae: 0.0138\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 6.2646e-04 - mse: 6.2646e-04 - mae: 0.0120 - val_loss: 4.0269e-04 - val_mse: 4.0269e-04 - val_mae: 0.0129\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 6.9148e-04 - mse: 6.9148e-04 - mae: 0.0121 - val_loss: 4.0160e-04 - val_mse: 4.0160e-04 - val_mae: 0.0134\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 5.8488e-04 - mse: 5.8488e-04 - mae: 0.0118 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0141\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 12s 133ms/step - loss: 6.6684e-04 - mse: 6.6684e-04 - mae: 0.0118 - val_loss: 3.6012e-04 - val_mse: 3.6012e-04 - val_mae: 0.0125\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 11s 131ms/step - loss: 6.1624e-04 - mse: 6.1624e-04 - mae: 0.0117 - val_loss: 8.8533e-04 - val_mse: 8.8533e-04 - val_mae: 0.0135\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 13s 147ms/step - loss: 5.8719e-04 - mse: 5.8719e-04 - mae: 0.0120 - val_loss: 6.4253e-04 - val_mse: 6.4253e-04 - val_mae: 0.0145\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 11s 129ms/step - loss: 7.1814e-04 - mse: 7.1814e-04 - mae: 0.0122 - val_loss: 3.2938e-04 - val_mse: 3.2938e-04 - val_mae: 0.0123\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 6.5956e-04 - mse: 6.5956e-04 - mae: 0.0121 - val_loss: 4.7237e-04 - val_mse: 4.7237e-04 - val_mae: 0.0133\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 5.8296e-04 - mse: 5.8296e-04 - mae: 0.0116 - val_loss: 3.8669e-04 - val_mse: 3.8669e-04 - val_mae: 0.0130\n",
      "Epoch 1/100\n",
      "87/87 [==============================] - 13s 147ms/step - loss: 0.8597 - mse: 0.8597 - mae: 0.7034 - val_loss: 0.0206 - val_mse: 0.0206 - val_mae: 0.1419\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 11s 126ms/step - loss: 0.3509 - mse: 0.3509 - mae: 0.4734 - val_loss: 5.4547e-04 - val_mse: 5.4547e-04 - val_mae: 0.0123\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 10s 119ms/step - loss: 0.2464 - mse: 0.2464 - mae: 0.3941 - val_loss: 0.0139 - val_mse: 0.0139 - val_mae: 0.1160\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 13s 150ms/step - loss: 0.1723 - mse: 0.1723 - mae: 0.3322 - val_loss: 8.7616e-04 - val_mse: 8.7616e-04 - val_mae: 0.0267\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.1083 - mse: 0.1083 - mae: 0.2623 - val_loss: 0.0019 - val_mse: 0.0019 - val_mae: 0.0377\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0633 - mse: 0.0633 - mae: 0.2007 - val_loss: 6.1576e-04 - val_mse: 6.1576e-04 - val_mae: 0.0216\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0335 - mse: 0.0335 - mae: 0.1457 - val_loss: 0.0010 - val_mse: 0.0010 - val_mae: 0.0235\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.1078 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0964\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0811 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0540\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 12s 133ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0630 - val_loss: 0.0024 - val_mse: 0.0024 - val_mae: 0.0467\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0554 - val_loss: 5.3225e-04 - val_mse: 5.3225e-04 - val_mae: 0.0190\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0493 - val_loss: 5.1533e-04 - val_mse: 5.1533e-04 - val_mae: 0.0119\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0462 - val_loss: 5.6570e-04 - val_mse: 5.6570e-04 - val_mae: 0.0201\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0474 - val_loss: 4.6975e-04 - val_mse: 4.6975e-04 - val_mae: 0.0151\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0458 - val_loss: 6.4736e-04 - val_mse: 6.4736e-04 - val_mae: 0.0148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0439 - val_loss: 4.7623e-04 - val_mse: 4.7623e-04 - val_mae: 0.0129\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0445 - val_loss: 5.1114e-04 - val_mse: 5.1114e-04 - val_mae: 0.0181\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0472 - val_loss: 5.7299e-04 - val_mse: 5.7299e-04 - val_mae: 0.0128\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0465 - val_loss: 6.2503e-04 - val_mse: 6.2503e-04 - val_mae: 0.0142\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0446 - val_loss: 4.6800e-04 - val_mse: 4.6800e-04 - val_mae: 0.0146\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 12s 132ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0606\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 11s 127ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0437 - val_loss: 4.9464e-04 - val_mse: 4.9464e-04 - val_mae: 0.0121\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0460 - val_loss: 6.5285e-04 - val_mse: 6.5285e-04 - val_mae: 0.0150\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0474 - val_loss: 8.6741e-04 - val_mse: 8.6741e-04 - val_mae: 0.0202\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0477 - val_loss: 4.7348e-04 - val_mse: 4.7348e-04 - val_mae: 0.0131\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0443 - val_loss: 4.9575e-04 - val_mse: 4.9575e-04 - val_mae: 0.0173\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0452 - val_loss: 5.0831e-04 - val_mse: 5.0831e-04 - val_mae: 0.0119\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0476 - val_loss: 8.5103e-04 - val_mse: 8.5103e-04 - val_mae: 0.0199\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 11s 130ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0472 - val_loss: 4.7652e-04 - val_mse: 4.7652e-04 - val_mae: 0.0129\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 11s 129ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 4.9433e-04 - val_mse: 4.9433e-04 - val_mae: 0.0121\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 11s 128ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 5.2723e-04 - val_mse: 5.2723e-04 - val_mae: 0.0120\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 12s 134ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 5.8492e-04 - val_mse: 5.8492e-04 - val_mae: 0.0131\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0462 - val_loss: 6.7346e-04 - val_mse: 6.7346e-04 - val_mae: 0.0155\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0462 - val_loss: 4.8830e-04 - val_mse: 4.8830e-04 - val_mae: 0.0169\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0466 - val_loss: 5.1366e-04 - val_mse: 5.1366e-04 - val_mae: 0.0119\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 11s 126ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0452 - val_loss: 5.7767e-04 - val_mse: 5.7767e-04 - val_mae: 0.0129\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0446 - val_loss: 4.8083e-04 - val_mse: 4.8083e-04 - val_mae: 0.0126\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0445 - val_loss: 4.7047e-04 - val_mse: 4.7047e-04 - val_mae: 0.0135\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0461 - val_loss: 4.6777e-04 - val_mse: 4.6777e-04 - val_mae: 0.0145\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0444 - val_loss: 6.3074e-04 - val_mse: 6.3074e-04 - val_mae: 0.0143\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 5.7265e-04 - val_mse: 5.7265e-04 - val_mae: 0.0203\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 11s 124ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0468 - val_loss: 5.8796e-04 - val_mse: 5.8796e-04 - val_mae: 0.0132\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0446 - val_loss: 4.9683e-04 - val_mse: 4.9683e-04 - val_mae: 0.0120\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0447 - val_loss: 6.0065e-04 - val_mse: 6.0065e-04 - val_mae: 0.0212\n",
      "Epoch 45/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0467 - val_loss: 4.9241e-04 - val_mse: 4.9241e-04 - val_mae: 0.0122\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0449 - val_loss: 4.6830e-04 - val_mse: 4.6830e-04 - val_mae: 0.0147\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 12s 133ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0447 - val_loss: 5.8538e-04 - val_mse: 5.8538e-04 - val_mae: 0.0131\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 5.4382e-04 - val_mse: 5.4382e-04 - val_mae: 0.0123\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0457 - val_loss: 4.8054e-04 - val_mse: 4.8054e-04 - val_mae: 0.0127\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0456 - val_loss: 6.2234e-04 - val_mse: 6.2234e-04 - val_mae: 0.0141\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 12s 133ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0452 - val_loss: 5.0739e-04 - val_mse: 5.0739e-04 - val_mae: 0.0119\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0455 - val_loss: 5.2903e-04 - val_mse: 5.2903e-04 - val_mae: 0.0121\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 5.2622e-04 - val_mse: 5.2622e-04 - val_mae: 0.0120\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 5.0025e-04 - val_mse: 5.0025e-04 - val_mae: 0.0120\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 4.6796e-04 - val_mse: 4.6796e-04 - val_mae: 0.0146\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0446 - val_loss: 4.7906e-04 - val_mse: 4.7906e-04 - val_mae: 0.0162\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0468 - val_loss: 4.9666e-04 - val_mse: 4.9666e-04 - val_mae: 0.0174\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0461 - val_loss: 4.6909e-04 - val_mse: 4.6909e-04 - val_mae: 0.0137\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0446 - val_loss: 6.0489e-04 - val_mse: 6.0489e-04 - val_mae: 0.0136\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0450 - val_loss: 4.6784e-04 - val_mse: 4.6784e-04 - val_mae: 0.0141\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 11s 131ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0478 - val_loss: 5.0875e-04 - val_mse: 5.0875e-04 - val_mae: 0.0119\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0443 - val_loss: 5.2955e-04 - val_mse: 5.2955e-04 - val_mae: 0.0121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 4.9328e-04 - val_mse: 4.9328e-04 - val_mae: 0.0121\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0463 - val_loss: 6.5773e-04 - val_mse: 6.5773e-04 - val_mae: 0.0226\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0474 - val_loss: 4.8766e-04 - val_mse: 4.8766e-04 - val_mae: 0.0123\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 12s 143ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0444 - val_loss: 4.8332e-04 - val_mse: 4.8332e-04 - val_mae: 0.0125\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 4.7823e-04 - val_mse: 4.7823e-04 - val_mae: 0.0128\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 11s 122ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0447 - val_loss: 4.7330e-04 - val_mse: 4.7330e-04 - val_mae: 0.0131\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 6.0420e-04 - val_mse: 6.0420e-04 - val_mae: 0.0136\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0445 - val_loss: 5.8022e-04 - val_mse: 5.8022e-04 - val_mae: 0.0130\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 4.7534e-04 - val_mse: 4.7534e-04 - val_mae: 0.0159\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0476 - val_loss: 4.7701e-04 - val_mse: 4.7701e-04 - val_mae: 0.0129\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 13s 144ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0463 - val_loss: 6.1601e-04 - val_mse: 6.1601e-04 - val_mae: 0.0139\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0461 - val_loss: 5.0198e-04 - val_mse: 5.0198e-04 - val_mae: 0.0119\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 13s 146ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0443 - val_loss: 5.1753e-04 - val_mse: 5.1753e-04 - val_mae: 0.0120\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0450 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0567\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 13s 144ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0431 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0878\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 12s 134ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0432 - val_loss: 4.9117e-04 - val_mse: 4.9117e-04 - val_mae: 0.0171\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0445 - val_loss: 4.7194e-04 - val_mse: 4.7194e-04 - val_mae: 0.0133\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0458 - val_loss: 4.8697e-04 - val_mse: 4.8697e-04 - val_mae: 0.0124\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 12s 144ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 5.0198e-04 - val_mse: 5.0198e-04 - val_mae: 0.0119\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0453 - val_loss: 6.7025e-04 - val_mse: 6.7025e-04 - val_mae: 0.0154\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 13s 144ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0450 - val_loss: 5.3889e-04 - val_mse: 5.3889e-04 - val_mae: 0.0122\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0458 - val_loss: 5.0943e-04 - val_mse: 5.0943e-04 - val_mae: 0.0119\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 12s 144ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0461 - val_loss: 5.1307e-04 - val_mse: 5.1307e-04 - val_mae: 0.0119\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 13s 146ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0441 - val_loss: 5.3865e-04 - val_mse: 5.3865e-04 - val_mae: 0.0122\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 13s 144ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0462 - val_loss: 4.7768e-04 - val_mse: 4.7768e-04 - val_mae: 0.0128\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 13s 144ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0449 - val_loss: 7.0742e-04 - val_mse: 7.0742e-04 - val_mae: 0.0164\n",
      "Epoch 89/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0475 - val_loss: 5.1132e-04 - val_mse: 5.1132e-04 - val_mae: 0.0119\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 13s 144ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 4.9911e-04 - val_mse: 4.9911e-04 - val_mae: 0.0120\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 21s 237ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0467 - val_loss: 4.8164e-04 - val_mse: 4.8164e-04 - val_mae: 0.0126\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 418s 5s/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0308\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 11s 132ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0476 - val_loss: 5.0206e-04 - val_mse: 5.0206e-04 - val_mae: 0.0177\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0465 - val_loss: 5.6217e-04 - val_mse: 5.6217e-04 - val_mae: 0.0126\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 12s 143ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 6.0057e-04 - val_mse: 6.0057e-04 - val_mae: 0.0135\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0470 - val_loss: 9.0016e-04 - val_mse: 9.0016e-04 - val_mae: 0.0210\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0472 - val_loss: 4.6802e-04 - val_mse: 4.6802e-04 - val_mae: 0.0140\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 12s 139ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0471 - val_loss: 5.5007e-04 - val_mse: 5.5007e-04 - val_mae: 0.0124\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 10s 119ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 5.0345e-04 - val_mse: 5.0345e-04 - val_mae: 0.0119\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 11s 128ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 6.0265e-04 - val_mse: 6.0265e-04 - val_mae: 0.0136\n",
      "Epoch 1/100\n",
      "87/87 [==============================] - 15s 167ms/step - loss: 0.4676 - mse: 0.4676 - mae: 0.5423 - val_loss: 0.0018 - val_mse: 0.0018 - val_mae: 0.0399\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 13s 155ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4695 - val_loss: 0.0016 - val_mse: 0.0016 - val_mae: 0.0374\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 13s 148ms/step - loss: 0.2486 - mse: 0.2486 - mae: 0.3987 - val_loss: 8.2164e-04 - val_mse: 8.2164e-04 - val_mae: 0.0192\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 13s 155ms/step - loss: 0.1588 - mse: 0.1588 - mae: 0.3174 - val_loss: 4.6780e-04 - val_mse: 4.6780e-04 - val_mae: 0.0142\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.0950 - mse: 0.0950 - mae: 0.2447 - val_loss: 0.0016 - val_mse: 0.0016 - val_mae: 0.0333\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 0.0517 - mse: 0.0517 - mae: 0.1803 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0609\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 0.0285 - mse: 0.0285 - mae: 0.1332 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0314\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 13s 151ms/step - loss: 0.0155 - mse: 0.0155 - mae: 0.0976 - val_loss: 4.6769e-04 - val_mse: 4.6769e-04 - val_mae: 0.0143\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 13s 145ms/step - loss: 0.0093 - mse: 0.0093 - mae: 0.0754 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0810\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 13s 147ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0604 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0616\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 13s 150ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0535 - val_loss: 0.0019 - val_mse: 0.0019 - val_mae: 0.0385\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0494 - val_loss: 7.6923e-04 - val_mse: 7.6923e-04 - val_mae: 0.0179\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 13s 147ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0491 - val_loss: 6.4719e-04 - val_mse: 6.4719e-04 - val_mae: 0.0223\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0485 - val_loss: 4.8720e-04 - val_mse: 4.8720e-04 - val_mae: 0.0123\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0459 - val_loss: 6.1399e-04 - val_mse: 6.1399e-04 - val_mae: 0.0139\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0441 - val_loss: 4.7408e-04 - val_mse: 4.7408e-04 - val_mae: 0.0157\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 13s 151ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0485 - val_loss: 5.1505e-04 - val_mse: 5.1505e-04 - val_mae: 0.0119\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 4.8626e-04 - val_mse: 4.8626e-04 - val_mae: 0.0124\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 5.1906e-04 - val_mse: 5.1906e-04 - val_mae: 0.0120\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 13s 144ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0460 - val_loss: 5.2443e-04 - val_mse: 5.2443e-04 - val_mae: 0.0120\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0461 - val_loss: 9.7218e-04 - val_mse: 9.7218e-04 - val_mae: 0.0225\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0481 - val_loss: 5.7189e-04 - val_mse: 5.7189e-04 - val_mae: 0.0128\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 11s 131ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0415 - val_loss: 4.6832e-04 - val_mse: 4.6832e-04 - val_mae: 0.0139\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0314\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 13s 146ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 4.8767e-04 - val_mse: 4.8767e-04 - val_mae: 0.0123\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0444 - val_loss: 4.7739e-04 - val_mse: 4.7739e-04 - val_mae: 0.0161\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0466 - val_loss: 5.3450e-04 - val_mse: 5.3450e-04 - val_mae: 0.0121\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 13s 147ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0467 - val_loss: 5.2540e-04 - val_mse: 5.2540e-04 - val_mae: 0.0120\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 5.8447e-04 - val_mse: 5.8447e-04 - val_mae: 0.0131\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 12s 135ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0443 - val_loss: 5.7830e-04 - val_mse: 5.7830e-04 - val_mae: 0.0129\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 5.6003e-04 - val_mse: 5.6003e-04 - val_mae: 0.0200\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0467 - val_loss: 7.9023e-04 - val_mse: 7.9023e-04 - val_mae: 0.0184\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0468 - val_loss: 4.7258e-04 - val_mse: 4.7258e-04 - val_mae: 0.0132\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 12s 138ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 5.6825e-04 - val_mse: 5.6825e-04 - val_mae: 0.0127\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 5.0161e-04 - val_mse: 5.0161e-04 - val_mae: 0.0119\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0475 - val_loss: 4.9841e-04 - val_mse: 4.9841e-04 - val_mae: 0.0120\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 12s 136ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 5.8668e-04 - val_mse: 5.8668e-04 - val_mae: 0.0132\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0467 - val_loss: 6.0478e-04 - val_mse: 6.0478e-04 - val_mae: 0.0136\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 14s 156ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0439 - val_loss: 4.7148e-04 - val_mse: 4.7148e-04 - val_mae: 0.0154\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 13s 155ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0466 - val_loss: 4.7757e-04 - val_mse: 4.7757e-04 - val_mae: 0.0161\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 4.7320e-04 - val_mse: 4.7320e-04 - val_mae: 0.0156\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 14s 156ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0462 - val_loss: 4.8887e-04 - val_mse: 4.8887e-04 - val_mae: 0.0123\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 14s 156ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0457 - val_loss: 5.2840e-04 - val_mse: 5.2840e-04 - val_mae: 0.0121\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 11s 132ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0462 - val_loss: 5.3224e-04 - val_mse: 5.3224e-04 - val_mae: 0.0121\n",
      "Epoch 45/100\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0447 - val_loss: 5.6175e-04 - val_mse: 5.6175e-04 - val_mae: 0.0126\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0457 - val_loss: 4.6852e-04 - val_mse: 4.6852e-04 - val_mae: 0.0138\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 11s 128ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 5.0192e-04 - val_mse: 5.0192e-04 - val_mae: 0.0119\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 5.1163e-04 - val_mse: 5.1163e-04 - val_mae: 0.0119\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 13s 155ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0463 - val_loss: 7.2476e-04 - val_mse: 7.2476e-04 - val_mae: 0.0168\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 14s 155ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0465 - val_loss: 5.3997e-04 - val_mse: 5.3997e-04 - val_mae: 0.0122\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 14s 156ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0447 - val_loss: 5.7112e-04 - val_mse: 5.7112e-04 - val_mae: 0.0128\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 14s 157ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 5.0488e-04 - val_mse: 5.0488e-04 - val_mae: 0.0119\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 14s 156ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 5.4754e-04 - val_mse: 5.4754e-04 - val_mae: 0.0123\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 14s 156ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 4.8843e-04 - val_mse: 4.8843e-04 - val_mae: 0.0123\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 14s 155ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 4.8699e-04 - val_mse: 4.8699e-04 - val_mae: 0.0124\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 14s 159ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 4.9519e-04 - val_mse: 4.9519e-04 - val_mae: 0.0121\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 11s 132ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0448 - val_loss: 4.6770e-04 - val_mse: 4.6770e-04 - val_mae: 0.0143\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0460 - val_loss: 5.1513e-04 - val_mse: 5.1513e-04 - val_mae: 0.0119\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0462 - val_loss: 5.2584e-04 - val_mse: 5.2584e-04 - val_mae: 0.0120\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 13s 147ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0457 - val_loss: 5.5412e-04 - val_mse: 5.5412e-04 - val_mae: 0.0124\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0443 - val_loss: 4.6849e-04 - val_mse: 4.6849e-04 - val_mae: 0.0139\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 13s 154ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0475 - val_loss: 4.7146e-04 - val_mse: 4.7146e-04 - val_mae: 0.0133\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0457 - val_loss: 4.9609e-04 - val_mse: 4.9609e-04 - val_mae: 0.0121\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 12s 133ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 5.6715e-04 - val_mse: 5.6715e-04 - val_mae: 0.0202\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 13s 149ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0474 - val_loss: 6.2706e-04 - val_mse: 6.2706e-04 - val_mae: 0.0142\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 12s 132ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0461 - val_loss: 6.6202e-04 - val_mse: 6.6202e-04 - val_mae: 0.0152\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0462 - val_loss: 4.7101e-04 - val_mse: 4.7101e-04 - val_mae: 0.0134\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 12s 137ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0457 - val_loss: 6.8874e-04 - val_mse: 6.8874e-04 - val_mae: 0.0159\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 15s 168ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0477 - val_loss: 9.2414e-04 - val_mse: 9.2414e-04 - val_mae: 0.0215\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 16s 186ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0468 - val_loss: 5.0318e-04 - val_mse: 5.0318e-04 - val_mae: 0.0119\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 16s 186ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0469 - val_loss: 6.5658e-04 - val_mse: 6.5658e-04 - val_mae: 0.0151\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 15s 177ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0461 - val_loss: 4.9327e-04 - val_mse: 4.9327e-04 - val_mae: 0.0121\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 15s 170ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 5.7900e-04 - val_mse: 5.7900e-04 - val_mae: 0.0130\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 18s 202ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 6.4665e-04 - val_mse: 6.4665e-04 - val_mae: 0.0148\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 16s 186ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0440 - val_loss: 5.1230e-04 - val_mse: 5.1230e-04 - val_mae: 0.0182\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 16s 181ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0468 - val_loss: 4.6923e-04 - val_mse: 4.6923e-04 - val_mae: 0.0137\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 16s 187ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0448 - val_loss: 4.6812e-04 - val_mse: 4.6812e-04 - val_mae: 0.0140\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 16s 184ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0442 - val_loss: 4.9792e-04 - val_mse: 4.9792e-04 - val_mae: 0.0120\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 16s 185ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0434 - val_loss: 4.6922e-04 - val_mse: 4.6922e-04 - val_mae: 0.0137\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 14s 164ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0477 - val_loss: 5.5683e-04 - val_mse: 5.5683e-04 - val_mae: 0.0125\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 15s 173ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 4.6772e-04 - val_mse: 4.6772e-04 - val_mae: 0.0142mse: 0.0036 - mae: 0.045\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 17s 198ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0471 - val_loss: 5.3696e-04 - val_mse: 5.3696e-04 - val_mae: 0.0122\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 18s 211ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 4.9267e-04 - val_mse: 4.9267e-04 - val_mae: 0.0122\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0441 - val_loss: 9.4481e-04 - val_mse: 9.4481e-04 - val_mae: 0.0279\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 16s 187ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0462 - val_loss: 4.7756e-04 - val_mse: 4.7756e-04 - val_mae: 0.0128\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 16s 182ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0466 - val_loss: 4.9382e-04 - val_mse: 4.9382e-04 - val_mae: 0.0121\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 16s 182ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 4.9797e-04 - val_mse: 4.9797e-04 - val_mae: 0.0120\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 16s 184ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 7.4391e-04 - val_mse: 7.4391e-04 - val_mae: 0.0173\n",
      "Epoch 89/100\n",
      "87/87 [==============================] - 16s 186ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0477 - val_loss: 5.1994e-04 - val_mse: 5.1994e-04 - val_mae: 0.0120\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 5.4396e-04 - val_mse: 5.4396e-04 - val_mae: 0.0123\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 15s 173ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 5.1786e-04 - val_mse: 5.1786e-04 - val_mae: 0.0120\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 15s 176ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0456 - val_loss: 4.9253e-04 - val_mse: 4.9253e-04 - val_mae: 0.0122\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 18s 206ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 5.6216e-04 - val_mse: 5.6216e-04 - val_mae: 0.0126\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 15s 176ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0449 - val_loss: 5.6553e-04 - val_mse: 5.6553e-04 - val_mae: 0.0126\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0439 - val_loss: 4.7823e-04 - val_mse: 4.7823e-04 - val_mae: 0.0128\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 13s 150ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0470 - val_loss: 5.6853e-04 - val_mse: 5.6853e-04 - val_mae: 0.0127\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 13s 155ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0465 - val_loss: 6.2436e-04 - val_mse: 6.2436e-04 - val_mae: 0.0142\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 14s 162ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0445 - val_loss: 5.1272e-04 - val_mse: 5.1272e-04 - val_mae: 0.0119\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 14s 160ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 4.9510e-04 - val_mse: 4.9510e-04 - val_mae: 0.0121\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 15s 172ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0442 - val_loss: 5.0267e-04 - val_mse: 5.0267e-04 - val_mae: 0.0177\n",
      "Epoch 1/100\n",
      "87/87 [==============================] - 17s 195ms/step - loss: 0.4942 - mse: 0.4942 - mae: 0.5585 - val_loss: 4.9544e-04 - val_mse: 4.9544e-04 - val_mae: 0.0173\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 15s 171ms/step - loss: 0.3413 - mse: 0.3413 - mae: 0.4654 - val_loss: 0.0019 - val_mse: 0.0019 - val_mae: 0.0421\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 15s 167ms/step - loss: 0.2338 - mse: 0.2338 - mae: 0.3844 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "87/87 [==============================] - 17s 199ms/step - loss: 0.1442 - mse: 0.1442 - mae: 0.3030 - val_loss: 4.7326e-04 - val_mse: 4.7326e-04 - val_mae: 0.0131\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 17s 197ms/step - loss: 0.0883 - mse: 0.0883 - mae: 0.2367 - val_loss: 0.0023 - val_mse: 0.0023 - val_mae: 0.0429\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0494 - mse: 0.0494 - mae: 0.1773 - val_loss: 0.0016 - val_mse: 0.0016 - val_mae: 0.0374\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 17s 198ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1281 - val_loss: 0.0027 - val_mse: 0.0027 - val_mae: 0.0500\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 18s 203ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0960 - val_loss: 0.0023 - val_mse: 0.0023 - val_mae: 0.0423\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 18s 204ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0706 - val_loss: 0.0015 - val_mse: 0.0015 - val_mae: 0.0369\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0586 - val_loss: 0.0023 - val_mse: 0.0023 - val_mae: 0.0427\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0507 - val_loss: 8.8496e-04 - val_mse: 8.8496e-04 - val_mae: 0.0269\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0478 - val_loss: 0.0028 - val_mse: 0.0028 - val_mae: 0.0507 ETA: 9s - loss: 0.0042 - mse: 0.0042 -  - ETA: 7s - loss - ETA: 0s - loss: 0.0039 - mse: 0.0039 - mae: 0.\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0483 - val_loss: 4.6795e-04 - val_mse: 4.6795e-04 - val_mae: 0.0146\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 17s 199ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 4.8407e-04 - val_mse: 4.8407e-04 - val_mae: 0.0125\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0470 - val_loss: 5.5633e-04 - val_mse: 5.5633e-04 - val_mae: 0.0125\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 17s 192ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0443 - val_loss: 5.1587e-04 - val_mse: 5.1587e-04 - val_mae: 0.0119\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 17s 198ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 7.2604e-04 - val_mse: 7.2604e-04 - val_mae: 0.0168\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 17s 190ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0468 - val_loss: 5.1683e-04 - val_mse: 5.1683e-04 - val_mae: 0.0120\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 12s 141ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0462 - val_loss: 4.9799e-04 - val_mse: 4.9799e-04 - val_mae: 0.0120\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 12s 140ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 4.7217e-04 - val_mse: 4.7217e-04 - val_mae: 0.0155\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 18s 211ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 5.2569e-04 - val_mse: 5.2569e-04 - val_mae: 0.0120\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 18s 211ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 5.2477e-04 - val_mse: 5.2477e-04 - val_mae: 0.0120\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 18s 203ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 5.3032e-04 - val_mse: 5.3032e-04 - val_mae: 0.0121\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 4.6906e-04 - val_mse: 4.6906e-04 - val_mae: 0.0137\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0466 - val_loss: 4.8046e-04 - val_mse: 4.8046e-04 - val_mae: 0.0127\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0440 - val_loss: 4.7107e-04 - val_mse: 4.7107e-04 - val_mae: 0.0134\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 17s 199ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 4.7571e-04 - val_mse: 4.7571e-04 - val_mae: 0.0130\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 17s 191ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 6.5683e-04 - val_mse: 6.5683e-04 - val_mae: 0.0151\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 17s 192ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0441 - val_loss: 4.9992e-04 - val_mse: 4.9992e-04 - val_mae: 0.0176\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 16s 181ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 5.2672e-04 - val_mse: 5.2672e-04 - val_mae: 0.0120\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 17s 192ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0468 - val_loss: 5.6213e-04 - val_mse: 5.6213e-04 - val_mae: 0.0126\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 17s 198ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0447 - val_loss: 8.9220e-04 - val_mse: 8.9220e-04 - val_mae: 0.0208\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 17s 198ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0476 - val_loss: 4.8552e-04 - val_mse: 4.8552e-04 - val_mae: 0.0124\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 19s 214ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 4.7250e-04 - val_mse: 4.7250e-04 - val_mae: 0.0132\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 18s 212ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 4.8717e-04 - val_mse: 4.8717e-04 - val_mae: 0.0124\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 18s 210ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 4.8097e-04 - val_mse: 4.8097e-04 - val_mae: 0.0164\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 19s 213ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0443 - val_loss: 8.6735e-04 - val_mse: 8.6735e-04 - val_mae: 0.0266\n",
      "Epoch 45/100\n",
      "87/87 [==============================] - 15s 174ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0468 - val_loss: 5.9622e-04 - val_mse: 5.9622e-04 - val_mae: 0.0134\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 16s 180ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 5.1700e-04 - val_mse: 5.1700e-04 - val_mae: 0.0120\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 15s 174ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0444 - val_loss: 5.5720e-04 - val_mse: 5.5720e-04 - val_mae: 0.0125\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 20s 232ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0448 - val_loss: 5.1555e-04 - val_mse: 5.1555e-04 - val_mae: 0.0119\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 18s 211ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 4.6791e-04 - val_mse: 4.6791e-04 - val_mae: 0.0146\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 20s 228ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 4.6833e-04 - val_mse: 4.6833e-04 - val_mae: 0.0147\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 19s 213ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0449 - val_loss: 4.8696e-04 - val_mse: 4.8696e-04 - val_mae: 0.0168\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 18s 211ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0465 - val_loss: 6.0940e-04 - val_mse: 6.0940e-04 - val_mae: 0.0138\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 16s 182ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0462 - val_loss: 7.1670e-04 - val_mse: 7.1670e-04 - val_mae: 0.0166\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 15s 168ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0471 - val_loss: 4.7356e-04 - val_mse: 4.7356e-04 - val_mae: 0.0131\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 15s 175ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0464 - val_loss: 4.9177e-04 - val_mse: 4.9177e-04 - val_mae: 0.0122\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 15s 175ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0434 - val_loss: 8.2762e-04 - val_mse: 8.2762e-04 - val_mae: 0.0193\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 11s 128ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0472 - val_loss: 6.0203e-04 - val_mse: 6.0203e-04 - val_mae: 0.0136\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 18s 212ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0447 - val_loss: 6.9242e-04 - val_mse: 6.9242e-04 - val_mae: 0.0160\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 15s 168ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0464 - val_loss: 5.3141e-04 - val_mse: 5.3141e-04 - val_mae: 0.0121\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 14s 163ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 4.9248e-04 - val_mse: 4.9248e-04 - val_mae: 0.0122\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 11s 123ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0462 - val_loss: 7.1010e-04 - val_mse: 7.1010e-04 - val_mae: 0.0164\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 13s 150ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0464 - val_loss: 4.6785e-04 - val_mse: 4.6785e-04 - val_mae: 0.0141\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 14s 163ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0483 - val_loss: 5.4676e-04 - val_mse: 5.4676e-04 - val_mae: 0.0123\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 14s 166ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 7.2097e-04 - val_mse: 7.2097e-04 - val_mae: 0.0167\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 11s 130ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0463 - val_loss: 5.7687e-04 - val_mse: 5.7687e-04 - val_mae: 0.0129\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 12s 142ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0452 - val_loss: 5.0650e-04 - val_mse: 5.0650e-04 - val_mae: 0.0119\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 10s 110ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0449 - val_loss: 4.6832e-04 - val_mse: 4.6832e-04 - val_mae: 0.0147\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 9s 109ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0457 - val_loss: 6.2303e-04 - val_mse: 6.2303e-04 - val_mae: 0.0141\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 13s 153ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0446 - val_loss: 6.4398e-04 - val_mse: 6.4398e-04 - val_mae: 0.0147\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 14s 159ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0456 - val_loss: 6.2247e-04 - val_mse: 6.2247e-04 - val_mae: 0.0141\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 17s 191ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0449 - val_loss: 5.7440e-04 - val_mse: 5.7440e-04 - val_mae: 0.0128\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 17s 201ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0446 - val_loss: 4.6786e-04 - val_mse: 4.6786e-04 - val_mae: 0.0145\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 5.2652e-04 - val_mse: 5.2652e-04 - val_mae: 0.0120\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 17s 195ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 4.7452e-04 - val_mse: 4.7452e-04 - val_mae: 0.0130\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 18s 202ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0478 - val_loss: 4.9514e-04 - val_mse: 4.9514e-04 - val_mae: 0.0121\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 18s 203ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0459 - val_loss: 5.0962e-04 - val_mse: 5.0962e-04 - val_mae: 0.0119\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 18s 203ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0456 - val_loss: 6.2161e-04 - val_mse: 6.2161e-04 - val_mae: 0.0141\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 18s 205ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 5.7659e-04 - val_mse: 5.7659e-04 - val_mae: 0.0129\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 18s 203ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0451 - val_loss: 4.6918e-04 - val_mse: 4.6918e-04 - val_mae: 0.0137\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 18s 203ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 4.7302e-04 - val_mse: 4.7302e-04 - val_mae: 0.0132\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 18s 204ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 4.7592e-04 - val_mse: 4.7592e-04 - val_mae: 0.0159\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0454 - val_loss: 4.9646e-04 - val_mse: 4.9646e-04 - val_mae: 0.0121\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 17s 200ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0463 - val_loss: 5.5458e-04 - val_mse: 5.5458e-04 - val_mae: 0.0198\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 17s 192ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0473 - val_loss: 6.0999e-04 - val_mse: 6.0999e-04 - val_mae: 0.0138\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 15s 167ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0450 - val_loss: 5.4172e-04 - val_mse: 5.4172e-04 - val_mae: 0.0122\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 13s 152ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0455 - val_loss: 6.7759e-04 - val_mse: 6.7759e-04 - val_mae: 0.0156\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 13s 155ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0459 - val_loss: 6.1825e-04 - val_mse: 6.1825e-04 - val_mae: 0.0140\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 17s 197ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0446 - val_loss: 4.6844e-04 - val_mse: 4.6844e-04 - val_mae: 0.0139\n",
      "Epoch 89/100\n",
      "87/87 [==============================] - 18s 201ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0850\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 18s 202ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0442 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0706\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 17s 197ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0435 - val_loss: 9.0164e-04 - val_mse: 9.0164e-04 - val_mae: 0.0272\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 18s 202ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0467 - val_loss: 4.6773e-04 - val_mse: 4.6773e-04 - val_mae: 0.0142\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 19s 215ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0461 - val_loss: 4.7214e-04 - val_mse: 4.7214e-04 - val_mae: 0.0132\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 16s 180ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0458 - val_loss: 5.1981e-04 - val_mse: 5.1981e-04 - val_mae: 0.0120\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 14s 161ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0456 - val_loss: 5.9202e-04 - val_mse: 5.9202e-04 - val_mae: 0.0133\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 17s 196ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0453 - val_loss: 6.0969e-04 - val_mse: 6.0969e-04 - val_mae: 0.0138\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 17s 191ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0445 - val_loss: 6.5339e-04 - val_mse: 6.5339e-04 - val_mae: 0.0150\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 15s 178ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0453 - val_loss: 6.6913e-04 - val_mse: 6.6913e-04 - val_mae: 0.0154\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 19s 221ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0477 - val_loss: 6.5049e-04 - val_mse: 6.5049e-04 - val_mae: 0.0149\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 18s 208ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0470 - val_loss: 4.7117e-04 - val_mse: 4.7117e-04 - val_mae: 0.0153\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Dictionary to include the parameters\n",
    "parameters = {'bias_initializer':[initializers.Zeros(),\n",
    "                                 initializers.Ones()],\n",
    "              'kernel_initializer': ['glorot_uniform',\n",
    "                                     'he_normal',\n",
    "                                     'he_uniform']\n",
    "               }\n",
    "\n",
    "all_param = ParameterGrid(parameters)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# divide features and labels\n",
    "X_train = data_train[:, 0:14] \n",
    "y_train = data_train[:, -1]\n",
    "X_test = data_test[:, 0:14] \n",
    "y_test = data_test[:, -1] \n",
    "\n",
    "# divide data into train and test \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "         X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "# put data into correct shape\n",
    "X_train, y_train = split_data(X_train, y_train, steps)\n",
    "X_test, y_test = split_data(X_test, y_test, steps)\n",
    "X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "X_train = cut_data(X_train, batch_size)\n",
    "y_train = cut_data(y_train, batch_size)\n",
    "X_test = cut_data(X_test, batch_size)\n",
    "y_test = cut_data(y_test, batch_size)\n",
    "X_val = cut_data(X_val, batch_size)\n",
    "y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "# inverse of test set should not be inside the loop \n",
    "y_test = (y_test * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "for i in range(len(all_param)):\n",
    "    \n",
    "    bias_initializer = all_param[i]['bias_initializer']\n",
    "    kernel_initializer = all_param[i]['kernel_initializer']\n",
    "\n",
    "    # design the LSTM\n",
    "    def regressor_tunning(kernel_initializer = 'he_uniform',\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        if n_hidden == 0:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        else:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           return_sequences = True,\n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(LSTM(units = units, \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        optimizer = optimizers.RMSprop()\n",
    "        model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "        return model\n",
    "\n",
    "    model = regressor_tunning(bias_initializer, kernel_initializer)\n",
    "\n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = 100,\n",
    "                        shuffle = False, \n",
    "                        validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "    \n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Need to process data with spike occurences the same way as features\n",
    "    data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "    # set predictive window according with tuning best results\n",
    "    data = data.loc[data.index > date, :]\n",
    "\n",
    "    # make sure shaded area will correspond to values outputed by LSTM\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # fill_nan is already made - so lets split data into test and train\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    shade_train, shade_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "    # reset index of testing data\n",
    "    shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # function to split data into correct shape for RNN\n",
    "    def split_data_shade(shade_test, steps):\n",
    "        y_spike_occ = list()\n",
    "        upper_lim = list()\n",
    "        lower_lim = list()\n",
    "        for i in range(steps, len(shade_test.index)):\n",
    "            y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "            upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "            lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "        return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "    \n",
    "    # function to cut data set so it can be divisible by the batch_size\n",
    "    def cut_data_shade(data, batch_size):\n",
    "         # see if it is divisivel\n",
    "        condition = data.shape[0] % batch_size\n",
    "        if condition == 0:\n",
    "            return data\n",
    "        else:\n",
    "            return data[: -condition]\n",
    "    \n",
    "    # shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "    y_spike_occ, spike_upperlim, spike_lowerlim = split_data_shade(shade_test, steps)\n",
    "    y_spike_occ = cut_data_shade(y_spike_occ, batch_size)\n",
    "        \n",
    "    # continue\n",
    "    \n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_param</th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>27.071469</td>\n",
       "      <td>16.745480</td>\n",
       "      <td>34.813916</td>\n",
       "      <td>22.288862</td>\n",
       "      <td>25.716341</td>\n",
       "      <td>15.917876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>27.602370</td>\n",
       "      <td>16.700069</td>\n",
       "      <td>37.918931</td>\n",
       "      <td>24.655460</td>\n",
       "      <td>25.709408</td>\n",
       "      <td>15.512362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>26.672001</td>\n",
       "      <td>17.646845</td>\n",
       "      <td>36.419015</td>\n",
       "      <td>23.513701</td>\n",
       "      <td>24.891491</td>\n",
       "      <td>16.770948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>32.908293</td>\n",
       "      <td>17.340064</td>\n",
       "      <td>50.084267</td>\n",
       "      <td>30.180382</td>\n",
       "      <td>29.498100</td>\n",
       "      <td>15.423059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>32.101319</td>\n",
       "      <td>25.473410</td>\n",
       "      <td>44.418363</td>\n",
       "      <td>34.767499</td>\n",
       "      <td>29.829247</td>\n",
       "      <td>24.085842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>30.327591</td>\n",
       "      <td>21.020342</td>\n",
       "      <td>44.428027</td>\n",
       "      <td>32.026841</td>\n",
       "      <td>27.611449</td>\n",
       "      <td>19.377118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           all_param  rmse_general  \\\n",
       "0  {'bias_initializer': <tensorflow.python.ops.in...     27.071469   \n",
       "1  {'bias_initializer': <tensorflow.python.ops.in...     27.602370   \n",
       "2  {'bias_initializer': <tensorflow.python.ops.in...     26.672001   \n",
       "3  {'bias_initializer': <tensorflow.python.ops.in...     32.908293   \n",
       "4  {'bias_initializer': <tensorflow.python.ops.in...     32.101319   \n",
       "5  {'bias_initializer': <tensorflow.python.ops.in...     30.327591   \n",
       "\n",
       "   mae_general  rmse_spike  mae_spike  rmse_normal  mae_normal  \n",
       "0    16.745480   34.813916  22.288862    25.716341   15.917876  \n",
       "1    16.700069   37.918931  24.655460    25.709408   15.512362  \n",
       "2    17.646845   36.419015  23.513701    24.891491   16.770948  \n",
       "3    17.340064   50.084267  30.180382    29.498100   15.423059  \n",
       "4    25.473410   44.418363  34.767499    29.829247   24.085842  \n",
       "5    21.020342   44.428027  32.026841    27.611449   19.377118  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'all_param':all_param,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "results.to_csv('Results_LSTM_5_kernel_bias.csv')\n",
    "\n",
    "y_pred = pd.DataFrame({'all_param': all_param,\n",
    "                       'Predicitons': y_pred_list})\n",
    "\n",
    "y_pred.to_csv('Pedictions_LSTM_4_kernel_bias.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col2 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col3 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_8172c486_ebae_11ea_9822_7cb27da2bf47row1_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col0 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col4 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_8172c486_ebae_11ea_9822_7cb27da2bf47row3_col5 {\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rmse_general</th>        <th class=\"col_heading level0 col1\" >mae_general</th>        <th class=\"col_heading level0 col2\" >rmse_spike</th>        <th class=\"col_heading level0 col3\" >mae_spike</th>        <th class=\"col_heading level0 col4\" >rmse_normal</th>        <th class=\"col_heading level0 col5\" >mae_normal</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col0\" class=\"data row0 col0\" >27.071469</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col1\" class=\"data row0 col1\" >16.745480</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col2\" class=\"data row0 col2\" >34.813916</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col3\" class=\"data row0 col3\" >22.288862</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col4\" class=\"data row0 col4\" >25.716341</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row0_col5\" class=\"data row0 col5\" >15.917876</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row1_col0\" class=\"data row1 col0\" >27.602370</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row1_col1\" class=\"data row1 col1\" >16.700069</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row1_col2\" class=\"data row1 col2\" >37.918931</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row1_col3\" class=\"data row1 col3\" >24.655460</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row1_col4\" class=\"data row1 col4\" >25.709408</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row1_col5\" class=\"data row1 col5\" >15.512362</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col0\" class=\"data row2 col0\" >26.672001</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col1\" class=\"data row2 col1\" >17.646845</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col2\" class=\"data row2 col2\" >36.419015</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col3\" class=\"data row2 col3\" >23.513701</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col4\" class=\"data row2 col4\" >24.891491</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row2_col5\" class=\"data row2 col5\" >16.770948</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row3_col0\" class=\"data row3 col0\" >32.908293</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row3_col1\" class=\"data row3 col1\" >17.340064</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row3_col2\" class=\"data row3 col2\" >50.084267</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row3_col3\" class=\"data row3 col3\" >30.180382</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row3_col4\" class=\"data row3 col4\" >29.498100</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row3_col5\" class=\"data row3 col5\" >15.423059</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row4_col0\" class=\"data row4 col0\" >32.101319</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row4_col1\" class=\"data row4 col1\" >25.473410</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row4_col2\" class=\"data row4 col2\" >44.418363</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row4_col3\" class=\"data row4 col3\" >34.767499</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row4_col4\" class=\"data row4 col4\" >29.829247</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row4_col5\" class=\"data row4 col5\" >24.085842</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row5_col0\" class=\"data row5 col0\" >30.327591</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row5_col1\" class=\"data row5 col1\" >21.020342</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row5_col2\" class=\"data row5 col2\" >44.428027</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row5_col3\" class=\"data row5 col3\" >32.026841</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row5_col4\" class=\"data row5 col4\" >27.611449</td>\n",
       "                        <td id=\"T_8172c486_ebae_11ea_9822_7cb27da2bf47row5_col5\" class=\"data row5 col5\" >19.377118</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18655c9f388>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (144,) and (0, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-cb7949454acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m144\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m226\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m82\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Real values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'steelblue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m144\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m226\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m82\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Predicted values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'deepskyblue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m#plt.plot(np.arange(0, 144), Residual[-226:-82], label = 'Residual error', linewidth = 1, color = 'slategrey')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_between\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m144\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spike_lowerlim'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m226\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m82\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spike_upperlim'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m226\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m82\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'skyblue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Spike delimitator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2794\u001b[0m     return gca().plot(\n\u001b[0;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2796\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \"\"\"\n\u001b[0;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1665\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 270\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (144,) and (0, 2)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_plot = 144 # 3 days\n",
    "fontsize = 13\n",
    "\n",
    "#y_pred = y_pred.reshape(len(y_pred))\n",
    "\n",
    "#Residual = list(y_test) - y_pred\n",
    "\n",
    "plt.figure(figsize=(12.5,4))\n",
    "plt.plot(np.arange(0, 144), y_test[-226:-82], label = 'Real values', linewidth = 2, color = 'steelblue')\n",
    "plt.plot(np.arange(0, 144), y_pred[-226:-82], label = 'Predicted values', linewidth = 1.8, color= 'deepskyblue')\n",
    "#plt.plot(np.arange(0, 144), Residual[-226:-82], label = 'Residual error', linewidth = 1, color = 'slategrey')\n",
    "plt.fill_between(np.arange(0, 144),  data['spike_lowerlim'][-226:-82], data['spike_upperlim'][-226:-82], facecolor='skyblue', alpha=0.5, label = 'Spike delimitator')\n",
    "plt.ylim(-100, 260)\n",
    "plt.xlim(0, 144 - 1)\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.xlabel('Accumulated SP', fontsize = fontsize)\n",
    "plt.ylabel('RMSE (£/MWh)', fontsize = fontsize)\n",
    "plt.xticks(fontsize = fontsize)\n",
    "plt.yticks([-100, -50, 0, 50,100, 150, 200, 250],[-100, -50, 0, 50, 100, 150, 200, 250],  fontsize = fontsize)\n",
    "plt.title('LSTM predictions', fontsize = fontsize + 2)\n",
    "plt.legend(loc = 'lower right', fontsize = fontsize - 2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_param</th>\n",
       "      <th>Predicitons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>[[115.80347], [130.11774], [109.788895], [106....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>[[115.00474], [134.26479], [112.73726], [106.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>[[117.79856], [127.007164], [109.93257], [107....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>[[91.58166], [91.58166], [91.58166], [91.58166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>[[117.8779], [117.8779], [117.8779], [117.8779...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>[[111.802284], [111.802284], [111.802284], [11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           all_param  \\\n",
       "0  {'bias_initializer': <tensorflow.python.ops.in...   \n",
       "1  {'bias_initializer': <tensorflow.python.ops.in...   \n",
       "2  {'bias_initializer': <tensorflow.python.ops.in...   \n",
       "3  {'bias_initializer': <tensorflow.python.ops.in...   \n",
       "4  {'bias_initializer': <tensorflow.python.ops.in...   \n",
       "5  {'bias_initializer': <tensorflow.python.ops.in...   \n",
       "\n",
       "                                         Predicitons  \n",
       "0  [[115.80347], [130.11774], [109.788895], [106....  \n",
       "1  [[115.00474], [134.26479], [112.73726], [106.8...  \n",
       "2  [[117.79856], [127.007164], [109.93257], [107....  \n",
       "3  [[91.58166], [91.58166], [91.58166], [91.58166...  \n",
       "4  [[117.8779], [117.8779], [117.8779], [117.8779...  \n",
       "5  [[111.802284], [111.802284], [111.802284], [11...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
