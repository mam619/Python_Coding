{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parameter Tunning \n",
    "    \n",
    "    Look at different steps & batch sizes \n",
    "    (6 months of data used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n",
      "Requirement already satisfied: six in c:\\users\\maria\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# months to evaluate model on\n",
    "date = 2018090000\n",
    "\n",
    "# for later use\n",
    "features_num = 14\n",
    "\n",
    "# lists to append results\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "y_pred_list = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data; fill nan values; split data intro train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# fill nan values in the whole data set\n",
    "data.fillna(data.mean(), inplace = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "data_train, data_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data scaling\n",
    "sc_X = MinMaxScaler()\n",
    "data_train = sc_X.fit_transform(data_train)\n",
    "data_test = sc_X.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Keras libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to process data with spike occurences in the same way as features and offers:\n",
    "(Required to evaluate predictions in both normal regions and spike regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data for shaded area\n",
    "data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# make sure shaded area will correspond to values outputed by LSTM\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# fill_nan is already made - so lets split data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "shade_train, shade_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "# reset index of testing data\n",
    "shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data_shade(shade_test, steps):\n",
    "    y_spike_occ = list()\n",
    "    upper_lim = list()\n",
    "    lower_lim = list()\n",
    "    for i in range(steps, len(shade_test.index)):\n",
    "        y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "        upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "        lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "    return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data_shade(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare of data according to LSTM needs,  create regressor & tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/87 [==============================] - 6s 69ms/step - loss: 0.1656 - mse: 0.1656 - mae: 0.2987 - val_loss: 0.0010 - val_mse: 0.0010 - val_mae: 0.0285\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 5s 62ms/step - loss: 0.0266 - mse: 0.0266 - mae: 0.1280 - val_loss: 0.0104 - val_mse: 0.0104 - val_mae: 0.0996\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 5s 63ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0645 - val_loss: 7.8415e-04 - val_mse: 7.8415e-04 - val_mae: 0.0187\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 6s 64ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0386 - val_loss: 8.5004e-04 - val_mse: 8.5004e-04 - val_mae: 0.0200\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 6s 64ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0266 - val_loss: 8.3182e-04 - val_mse: 8.3182e-04 - val_mae: 0.0196\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0231 - val_loss: 4.9676e-04 - val_mse: 4.9676e-04 - val_mae: 0.0116\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 6s 66ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0212 - val_loss: 5.2848e-04 - val_mse: 5.2848e-04 - val_mae: 0.0119\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0202 - val_loss: 4.6465e-04 - val_mse: 4.6465e-04 - val_mae: 0.0118\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0197 - val_loss: 4.6067e-04 - val_mse: 4.6067e-04 - val_mae: 0.0117\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 5s 59ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0191 - val_loss: 4.2909e-04 - val_mse: 4.2909e-04 - val_mae: 0.0123\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0185 - val_loss: 4.2818e-04 - val_mse: 4.2818e-04 - val_mae: 0.0119\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 5s 58ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0184 - val_loss: 4.1070e-04 - val_mse: 4.1070e-04 - val_mae: 0.0118\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 5s 58ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0179 - val_loss: 4.0721e-04 - val_mse: 4.0721e-04 - val_mae: 0.0121\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 5s 59ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0177 - val_loss: 3.8778e-04 - val_mse: 3.8778e-04 - val_mae: 0.0125\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 5s 59ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0174 - val_loss: 3.7422e-04 - val_mse: 3.7422e-04 - val_mae: 0.0114\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0166 - val_loss: 3.7090e-04 - val_mse: 3.7090e-04 - val_mae: 0.0116\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 5s 62ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0169 - val_loss: 3.5302e-04 - val_mse: 3.5302e-04 - val_mae: 0.0113\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 6s 69ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168 - val_loss: 3.7568e-04 - val_mse: 3.7568e-04 - val_mae: 0.0108\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 6s 72ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0162 - val_loss: 3.4691e-04 - val_mse: 3.4691e-04 - val_mae: 0.0120\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 6s 75ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0166 - val_loss: 3.5719e-04 - val_mse: 3.5719e-04 - val_mae: 0.0106\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0160 - val_loss: 3.3550e-04 - val_mse: 3.3550e-04 - val_mae: 0.0112\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0164 - val_loss: 3.2688e-04 - val_mse: 3.2688e-04 - val_mae: 0.0109\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 9.8680e-04 - mse: 9.8680e-04 - mae: 0.0154 - val_loss: 3.2975e-04 - val_mse: 3.2975e-04 - val_mae: 0.0113\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0162 - val_loss: 3.3369e-04 - val_mse: 3.3369e-04 - val_mae: 0.0111\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 6s 73ms/step - loss: 9.9206e-04 - mse: 9.9206e-04 - mae: 0.0156 - val_loss: 3.3339e-04 - val_mse: 3.3339e-04 - val_mae: 0.0104\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 6s 65ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0159 - val_loss: 3.1829e-04 - val_mse: 3.1829e-04 - val_mae: 0.0105\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 9.7197e-04 - mse: 9.7197e-04 - mae: 0.0152 - val_loss: 3.1493e-04 - val_mse: 3.1493e-04 - val_mae: 0.0101\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0158 - val_loss: 3.1346e-04 - val_mse: 3.1346e-04 - val_mae: 0.0101\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 9.5188e-04 - mse: 9.5188e-04 - mae: 0.0150 - val_loss: 3.2708e-04 - val_mse: 3.2708e-04 - val_mae: 0.0102\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0158 - val_loss: 2.9800e-04 - val_mse: 2.9800e-04 - val_mae: 0.0102\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 9.5605e-04 - mse: 9.5605e-04 - mae: 0.0152 - val_loss: 3.0887e-04 - val_mse: 3.0887e-04 - val_mae: 0.0104\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 9.8863e-04 - mse: 9.8863e-04 - mae: 0.0153 - val_loss: 3.0555e-04 - val_mse: 3.0555e-04 - val_mae: 0.0105\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 5s 63ms/step - loss: 9.8943e-04 - mse: 9.8943e-04 - mae: 0.0153 - val_loss: 3.0606e-04 - val_mse: 3.0606e-04 - val_mae: 0.0111\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 6s 66ms/step - loss: 9.9603e-04 - mse: 9.9603e-04 - mae: 0.0153 - val_loss: 2.9912e-04 - val_mse: 2.9912e-04 - val_mae: 0.0105\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 9.4471e-04 - mse: 9.4471e-04 - mae: 0.0148 - val_loss: 2.8998e-04 - val_mse: 2.8998e-04 - val_mae: 0.0097\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 6s 66ms/step - loss: 9.5319e-04 - mse: 9.5319e-04 - mae: 0.0149 - val_loss: 2.9653e-04 - val_mse: 2.9653e-04 - val_mae: 0.0109\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 9.2847e-04 - mse: 9.2847e-04 - mae: 0.0147 - val_loss: 3.0779e-04 - val_mse: 3.0779e-04 - val_mae: 0.0104\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 5s 57ms/step - loss: 9.3404e-04 - mse: 9.3404e-04 - mae: 0.0149 - val_loss: 2.9535e-04 - val_mse: 2.9535e-04 - val_mae: 0.0101\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 5s 59ms/step - loss: 9.4131e-04 - mse: 9.4131e-04 - mae: 0.0148 - val_loss: 2.8876e-04 - val_mse: 2.8876e-04 - val_mae: 0.0104\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 5s 62ms/step - loss: 9.3730e-04 - mse: 9.3730e-04 - mae: 0.0148 - val_loss: 2.7128e-04 - val_mse: 2.7128e-04 - val_mae: 0.0103\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 9.1344e-04 - mse: 9.1344e-04 - mae: 0.0142 - val_loss: 3.0822e-04 - val_mse: 3.0822e-04 - val_mae: 0.0106\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 9.5416e-04 - mse: 9.5416e-04 - mae: 0.0148 - val_loss: 3.0467e-04 - val_mse: 3.0467e-04 - val_mae: 0.0102\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 5s 63ms/step - loss: 9.3676e-04 - mse: 9.3676e-04 - mae: 0.0145 - val_loss: 3.0169e-04 - val_mse: 3.0169e-04 - val_mae: 0.0107\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 6s 64ms/step - loss: 9.0636e-04 - mse: 9.0636e-04 - mae: 0.0144 - val_loss: 2.7560e-04 - val_mse: 2.7560e-04 - val_mae: 0.0098\n",
      "Epoch 45/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 9.3139e-04 - mse: 9.3139e-04 - mae: 0.0146 - val_loss: 2.8558e-04 - val_mse: 2.8558e-04 - val_mae: 0.0099\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 5s 59ms/step - loss: 9.1365e-04 - mse: 9.1365e-04 - mae: 0.0145 - val_loss: 3.0863e-04 - val_mse: 3.0863e-04 - val_mae: 0.0107\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 5s 59ms/step - loss: 9.3957e-04 - mse: 9.3957e-04 - mae: 0.0145 - val_loss: 2.9280e-04 - val_mse: 2.9280e-04 - val_mae: 0.0109\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 9.1062e-04 - mse: 9.1062e-04 - mae: 0.0142 - val_loss: 2.7053e-04 - val_mse: 2.7053e-04 - val_mae: 0.0097\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 6s 65ms/step - loss: 9.1535e-04 - mse: 9.1535e-04 - mae: 0.0142 - val_loss: 2.8731e-04 - val_mse: 2.8731e-04 - val_mae: 0.0109\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 8.7470e-04 - mse: 8.7470e-04 - mae: 0.0139 - val_loss: 2.9724e-04 - val_mse: 2.9724e-04 - val_mae: 0.0108\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 9.0593e-04 - mse: 9.0593e-04 - mae: 0.0144 - val_loss: 4.7120e-04 - val_mse: 4.7120e-04 - val_mae: 0.0128\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 9.0084e-04 - mse: 9.0084e-04 - mae: 0.0144 - val_loss: 2.7089e-04 - val_mse: 2.7089e-04 - val_mae: 0.0104\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 9.2634e-04 - mse: 9.2634e-04 - mae: 0.0143 - val_loss: 2.7948e-04 - val_mse: 2.7948e-04 - val_mae: 0.0108\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 8.7248e-04 - mse: 8.7248e-04 - mae: 0.0140 - val_loss: 3.1177e-04 - val_mse: 3.1177e-04 - val_mae: 0.0106\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 9.3996e-04 - mse: 9.3996e-04 - mae: 0.0143 - val_loss: 2.7263e-04 - val_mse: 2.7263e-04 - val_mae: 0.0109\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 6s 70ms/step - loss: 8.9790e-04 - mse: 8.9790e-04 - mae: 0.0141 - val_loss: 2.9531e-04 - val_mse: 2.9531e-04 - val_mae: 0.0112\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 6s 64ms/step - loss: 8.8188e-04 - mse: 8.8188e-04 - mae: 0.0139 - val_loss: 2.8407e-04 - val_mse: 2.8407e-04 - val_mae: 0.0097\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 8.9483e-04 - mse: 8.9483e-04 - mae: 0.0139 - val_loss: 3.1380e-04 - val_mse: 3.1380e-04 - val_mae: 0.0119\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 6s 66ms/step - loss: 8.4972e-04 - mse: 8.4972e-04 - mae: 0.0136 - val_loss: 2.8041e-04 - val_mse: 2.8041e-04 - val_mae: 0.0101\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 5s 61ms/step - loss: 9.3555e-04 - mse: 9.3555e-04 - mae: 0.0141 - val_loss: 3.1035e-04 - val_mse: 3.1035e-04 - val_mae: 0.0119\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 9.3074e-04 - mse: 9.3074e-04 - mae: 0.0139 - val_loss: 3.1004e-04 - val_mse: 3.1004e-04 - val_mae: 0.0115\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 8.2617e-04 - mse: 8.2617e-04 - mae: 0.0133 - val_loss: 2.8554e-04 - val_mse: 2.8554e-04 - val_mae: 0.0104\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 6s 73ms/step - loss: 8.7950e-04 - mse: 8.7950e-04 - mae: 0.0140 - val_loss: 2.9134e-04 - val_mse: 2.9134e-04 - val_mae: 0.0107\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 8.1867e-04 - mse: 8.1867e-04 - mae: 0.0132 - val_loss: 2.7467e-04 - val_mse: 2.7467e-04 - val_mae: 0.0102\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 5s 63ms/step - loss: 8.6339e-04 - mse: 8.6339e-04 - mae: 0.0135 - val_loss: 2.5957e-04 - val_mse: 2.5957e-04 - val_mae: 0.0097\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 9.0179e-04 - mse: 9.0179e-04 - mae: 0.0135 - val_loss: 2.8920e-04 - val_mse: 2.8920e-04 - val_mae: 0.0116\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 8.2933e-04 - mse: 8.2933e-04 - mae: 0.0133 - val_loss: 2.7766e-04 - val_mse: 2.7766e-04 - val_mae: 0.0093\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 5s 60ms/step - loss: 8.2579e-04 - mse: 8.2579e-04 - mae: 0.0130 - val_loss: 2.5497e-04 - val_mse: 2.5497e-04 - val_mae: 0.0095\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 5s 58ms/step - loss: 8.8564e-04 - mse: 8.8564e-04 - mae: 0.0138 - val_loss: 2.6841e-04 - val_mse: 2.6841e-04 - val_mae: 0.0107\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 5s 59ms/step - loss: 8.1846e-04 - mse: 8.1846e-04 - mae: 0.0131 - val_loss: 2.6748e-04 - val_mse: 2.6748e-04 - val_mae: 0.0098\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 5s 59ms/step - loss: 8.9345e-04 - mse: 8.9345e-04 - mae: 0.0135 - val_loss: 2.8626e-04 - val_mse: 2.8626e-04 - val_mae: 0.0111\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 5s 63ms/step - loss: 8.0272e-04 - mse: 8.0272e-04 - mae: 0.0130 - val_loss: 2.8029e-04 - val_mse: 2.8029e-04 - val_mae: 0.0101\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 6s 72ms/step - loss: 8.9709e-04 - mse: 8.9709e-04 - mae: 0.0137 - val_loss: 2.6889e-04 - val_mse: 2.6889e-04 - val_mae: 0.0104\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 8.0301e-04 - mse: 8.0301e-04 - mae: 0.0129 - val_loss: 2.8699e-04 - val_mse: 2.8699e-04 - val_mae: 0.0102\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 5s 63ms/step - loss: 8.6615e-04 - mse: 8.6615e-04 - mae: 0.0134 - val_loss: 2.5379e-04 - val_mse: 2.5379e-04 - val_mae: 0.0103\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 6s 72ms/step - loss: 8.7649e-04 - mse: 8.7649e-04 - mae: 0.0133 - val_loss: 2.5357e-04 - val_mse: 2.5357e-04 - val_mae: 0.0103\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 6s 69ms/step - loss: 7.9830e-04 - mse: 7.9830e-04 - mae: 0.0127 - val_loss: 2.6669e-04 - val_mse: 2.6669e-04 - val_mae: 0.0101\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 7.9713e-04 - mse: 7.9713e-04 - mae: 0.0129 - val_loss: 2.8730e-04 - val_mse: 2.8730e-04 - val_mae: 0.0104\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 6s 70ms/step - loss: 8.4008e-04 - mse: 8.4008e-04 - mae: 0.0131 - val_loss: 2.8879e-04 - val_mse: 2.8879e-04 - val_mae: 0.0106\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 6s 67ms/step - loss: 7.8837e-04 - mse: 7.8837e-04 - mae: 0.0127 - val_loss: 2.4878e-04 - val_mse: 2.4878e-04 - val_mae: 0.0100\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 5s 61ms/step - loss: 7.9774e-04 - mse: 7.9774e-04 - mae: 0.0128 - val_loss: 2.5192e-04 - val_mse: 2.5192e-04 - val_mae: 0.0097\n",
      "Epoch 82/100\n",
      "53/87 [=================>............] - ETA: 1s - loss: 7.3020e-04 - mse: 7.3020e-04 - mae: 0.0130"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "# steps = 96\n",
    "n_hidden = 2\n",
    "units = 100\n",
    "# batch_size = 96\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# 48 - day; 96 - 2 days; 336 - 1 week\n",
    "parameters = {'steps': [48, 96, 336],\n",
    "              'batch_size': [48, 96, 336]}\n",
    "\n",
    "all_param = ParameterGrid(parameters)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "for i in range(len(all_param)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # set parameters right\n",
    "    steps = all_param[i]['steps']\n",
    "    batch_size = all_param[i]['batch_size']\n",
    "    \n",
    "    # divide features and labels\n",
    "    X_train = data_train[:, 0:14] \n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, 0:14] \n",
    "    y_test = data_test[:, -1] \n",
    "\n",
    "    # divide data into train and test \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "    # put data into correct shape\n",
    "    X_train, y_train = split_data(X_train, y_train, steps)\n",
    "    X_test, y_test = split_data(X_test, y_test, steps)\n",
    "    X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "    X_train = cut_data(X_train, batch_size)\n",
    "    y_train = cut_data(y_train, batch_size)\n",
    "    X_test = cut_data(X_test, batch_size)\n",
    "    y_test = cut_data(y_test, batch_size)\n",
    "    X_val = cut_data(X_val, batch_size)\n",
    "    y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "    def regressor_tunning(kernel_initializer = 'he_uniform',\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        if n_hidden == 0:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        else:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           return_sequences = True,\n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(LSTM(units = units, \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        optimizer = optimizers.RMSprop()\n",
    "        model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "        return model\n",
    "\n",
    "    model = regressor_tunning()\n",
    "\n",
    "    # fitting the LSTM to the training set\n",
    "    model.fit(X_train,\n",
    "              y_train, \n",
    "              batch_size = batch_size, \n",
    "              epochs = 100,\n",
    "              shuffle = False, \n",
    "              validation_data = (X_val, y_val))\n",
    "\n",
    "    # reset states to have suitable predictions\n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15\n",
    "    y_pred = (y_pred * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "    y_test = (y_test * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "    \n",
    "    # Reshaping\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0]))\n",
    "    \n",
    "    y_pred_list.append(y_pred)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "\n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "    y_spike_occ, spike_upperlim, spike_lowerlim = split_data_shade(shade_test, steps)\n",
    "    y_spike_occ = cut_data_shade(y_spike_occ, batch_size)\n",
    "    \n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "\n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,\n",
    "                       \n",
    "                        'time': time_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,\n",
    "                       \n",
    "                        'time': time_count}, index = all_param)\n",
    "\n",
    "results.to_csv('Results_LSTM_steps_batch.csv')\n",
    "\n",
    "y_pred_list = pd.Series(y_pred_list)\n",
    "y_pred_list.to_csv('Predictions_LSTM_steps_batch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "w_plot = 144 # 3 days\n",
    "fontsize = 13\n",
    "\n",
    "y_pred = y_pred.reshape(len(y_pred))\n",
    "\n",
    "Residual = list(y_test) - y_pred\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(np.arange(0, (w_plot)), y_test[-w_plot:], label = 'Real values', linewidth = 1.5, color = 'steelblue')\n",
    "plt.plot(np.arange(0, (w_plot)), y_pred[-w_plot:], label = 'Predicted values', linewidth = 1.2, color= 'deepskyblue')\n",
    "plt.plot(np.arange(0, (w_plot)), Residual[-w_plot:], label = 'Residual error', linewidth = 0.8, color = 'slategrey')\n",
    "plt.fill_between(np.arange(0, (w_plot)),  data['spike_lowerlim'][-w_plot:],data['spike_upperlim'][-w_plot:], facecolor='skyblue', alpha=0.5, label = 'Not spike regions')\n",
    "plt.xlim(0, w_plot - 1)\n",
    "plt.ylim(-100, 260)\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.xlabel('Accumulated SP', fontsize = fontsize)\n",
    "plt.ylabel('RMSE (£/MWh)', fontsize = fontsize)\n",
    "plt.xticks(fontsize = fontsize)\n",
    "plt.yticks([-100, -50, 0, 50,100, 150, 200, 250],[-100, -50, 0, 50, 100, 150, 200, 250],  fontsize = fontsize)\n",
    "plt.title('Linear Regression predictions', fontsize = fontsize + 2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('LSTM_1_hidden_150_nerons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
