{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression ANN with best parameters\n",
    "    find the best prediction window to apply to 2 years data with a rolling Nested CV technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-b4de82005957>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-b4de82005957>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    dates = [2017000000, 2017030000, 2017060000, 2017090000 2018000000, 2018030000, 2018060000, 2018090000]\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict;\n",
    "from sklearn.preprocessing import MinMaxScaler;\n",
    "from sklearn import metrics;\n",
    "from sklearn.model_selection import TimeSeriesSplit;\n",
    "\n",
    "dates = [2017000000, 2017030000, 2017060000, 2017090000, 2018000000, 2018030000, 2018060000, 2018090000]\n",
    "dates_labels = ['2 years', '1 year and a half', '1 year', '6 months']\n",
    "\n",
    "mae_cv = []\n",
    "mse_cv = []\n",
    "mae_gen = []\n",
    "mse_gen  =[]\n",
    "rmse_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "mse_nor = []\n",
    "mse_spi = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop with model for 4 different data ranges (6 months intervals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "3507/3507 [==============================] - 1s 399us/step - loss: 9354.2266 - mse: 9354.2246 - mae: 78.5116\n",
      "Epoch 2/80\n",
      "3507/3507 [==============================] - 1s 159us/step - loss: 2365.8247 - mse: 2365.8247 - mae: 33.1048\n",
      "Epoch 3/80\n",
      "3507/3507 [==============================] - 1s 158us/step - loss: 2325.2546 - mse: 2325.2544 - mae: 33.0188\n",
      "Epoch 4/80\n",
      "3507/3507 [==============================] - 1s 148us/step - loss: 2258.5323 - mse: 2258.5320 - mae: 32.8474\n",
      "Epoch 5/80\n",
      "3507/3507 [==============================] - 1s 156us/step - loss: 2235.4748 - mse: 2235.4753 - mae: 32.7039\n",
      "Epoch 6/80\n",
      "3507/3507 [==============================] - 1s 155us/step - loss: 2211.1425 - mse: 2211.1428 - mae: 32.5188\n",
      "Epoch 7/80\n",
      "3507/3507 [==============================] - 1s 158us/step - loss: 2233.0266 - mse: 2233.0271 - mae: 32.7050\n",
      "Epoch 8/80\n",
      "3507/3507 [==============================] - 1s 153us/step - loss: 2233.4681 - mse: 2233.4683 - mae: 32.3103\n",
      "Epoch 9/80\n",
      "3507/3507 [==============================] - 1s 156us/step - loss: 2167.7234 - mse: 2167.7234 - mae: 31.9941\n",
      "Epoch 10/80\n",
      "3507/3507 [==============================] - 1s 148us/step - loss: 2146.7370 - mse: 2146.7366 - mae: 31.8076\n",
      "Epoch 11/80\n",
      "3507/3507 [==============================] - 1s 145us/step - loss: 2177.5447 - mse: 2177.5452 - mae: 32.0877\n",
      "Epoch 12/80\n",
      "3507/3507 [==============================] - 1s 150us/step - loss: 2204.7525 - mse: 2204.7524 - mae: 32.4798\n",
      "Epoch 13/80\n",
      "3507/3507 [==============================] - 1s 153us/step - loss: 2137.8395 - mse: 2137.8396 - mae: 31.8734\n",
      "Epoch 14/80\n",
      "3507/3507 [==============================] - 1s 151us/step - loss: 2163.4800 - mse: 2163.4795 - mae: 31.9504\n",
      "Epoch 15/80\n",
      "3507/3507 [==============================] - 0s 141us/step - loss: 2136.8452 - mse: 2136.8447 - mae: 31.8269\n",
      "Epoch 16/80\n",
      "3507/3507 [==============================] - 1s 167us/step - loss: 2162.8645 - mse: 2162.8640 - mae: 32.0591\n",
      "Epoch 17/80\n",
      "3507/3507 [==============================] - 1s 156us/step - loss: 2098.0317 - mse: 2098.0312 - mae: 31.4140\n",
      "Epoch 18/80\n",
      "3507/3507 [==============================] - 1s 163us/step - loss: 2139.3824 - mse: 2139.3818 - mae: 31.8749\n",
      "Epoch 19/80\n",
      "3507/3507 [==============================] - 1s 145us/step - loss: 2118.5308 - mse: 2118.5312 - mae: 31.6121\n",
      "Epoch 20/80\n",
      "3507/3507 [==============================] - 1s 166us/step - loss: 2138.3972 - mse: 2138.3979 - mae: 31.7574\n",
      "Epoch 21/80\n",
      "3507/3507 [==============================] - 1s 174us/step - loss: 2135.1330 - mse: 2135.1333 - mae: 32.0048\n",
      "Epoch 22/80\n",
      "3507/3507 [==============================] - 1s 157us/step - loss: 2109.8287 - mse: 2109.8286 - mae: 31.4823\n",
      "Epoch 23/80\n",
      "3507/3507 [==============================] - 1s 207us/step - loss: 2127.8546 - mse: 2127.8545 - mae: 31.6906\n",
      "Epoch 24/80\n",
      "3507/3507 [==============================] - 1s 171us/step - loss: 2093.9360 - mse: 2093.9358 - mae: 31.3536\n",
      "Epoch 25/80\n",
      "3507/3507 [==============================] - 1s 148us/step - loss: 2106.3319 - mse: 2106.3325 - mae: 31.5128\n",
      "Epoch 26/80\n",
      "3507/3507 [==============================] - 1s 172us/step - loss: 2097.5976 - mse: 2097.5981 - mae: 31.4511\n",
      "Epoch 27/80\n",
      "3507/3507 [==============================] - 1s 181us/step - loss: 2152.6784 - mse: 2152.6782 - mae: 31.7621\n",
      "Epoch 28/80\n",
      "2220/3507 [=================>............] - ETA: 0s - loss: 2227.5540 - mse: 2227.5544 - mae: 32.1067"
     ]
    }
   ],
   "source": [
    "for i in dates:\n",
    "    count = 0\n",
    "    # data\n",
    "    data = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "    data = data.loc[data.index > i, :]\n",
    "    \n",
    "    # reset index\n",
    "    data.reset_index(inplace = True)\n",
    "    data.drop('index', axis = 1, inplace = True)\n",
    "    \n",
    "    # Divide features and labels\n",
    "    X = data.iloc[:, 0:15]\n",
    "    y = data.loc[:, 'Offers']\n",
    "    \n",
    "    X.fillna(X.mean(), inplace = True)\n",
    "    y.fillna(y.mean(), inplace = True)\n",
    "    \n",
    "    X = X.astype('float64')\n",
    "    X = X.round(20)\n",
    "    \n",
    "    # divide data into train and test with 20% test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # feature scaling\n",
    "    sc_X = MinMaxScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "    \n",
    "    import keras\n",
    "    from keras.models import Sequential # to initialise the NN\n",
    "    from keras.layers import Dense # to create layers\n",
    "    from keras.layers import Dropout\n",
    "    from keras import initializers\n",
    "    import keras.optimizers\n",
    "    from keras.wrappers.scikit_learn import KerasRegressor\n",
    "    \n",
    "    # possible debug\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "    \n",
    "    def regressor_tunning(n_hidden = 2, \n",
    "                          n_neurons = 30, \n",
    "                          optimizer = 'Adamax', \n",
    "                          kernel_initializer = \"he_normal\",\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = n_neurons, input_dim = 15))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.1))\n",
    "        for layer in range(n_hidden):\n",
    "            model.add(Dense(n_neurons))\n",
    "            model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(rate = 0.1))\n",
    "        model.add(Dense(units = 1, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "        return model\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits = 7)\n",
    "    \n",
    "    hist_list = pd.DataFrame()\n",
    "    count = 1\n",
    "    \n",
    "    regressor = regressor_tunning()\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "          X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "          y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "          hist = regressor.fit(X_train_split, y_train_split, batch_size = 15, epochs = 80)\n",
    "          hist_list = hist_list.append(hist.history, ignore_index = True)\n",
    "          print(count)\n",
    "          count = count + 1\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(len(hist_list.mse)):\n",
    "        a.append(np.mean(hist_list.mse[i]))\n",
    "        b.append(np.mean(hist_list.mae[i]))\n",
    "      \n",
    "    mse_cv.append(np.mean(a))\n",
    "    mae_cv.append(np.mean(b))\n",
    "    \n",
    "    # predict for X_test  \n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mse_error = mse(y_test, y_pred) # 1479.61335\n",
    "    mae_error = mae(y_test, y_pred) # 23.1525\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mse_gen.append(mse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "    \n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "    \n",
    "    \n",
    "    # smal adjustment\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "    \n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mse_spike = mse(y_test_spike, y_pred_spike)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mse_spi.append(mse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mse_normal = mse(y_test_normal, y_pred_normal)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mse_nor.append(mse_normal)\n",
    "    mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
