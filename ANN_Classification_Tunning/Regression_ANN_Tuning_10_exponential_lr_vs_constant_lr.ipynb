{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression ANN with best parameters\n",
    "    find the best approach for learning rate (exponential schedueling - or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict;\n",
    "from sklearn.preprocessing import MinMaxScaler;\n",
    "from sklearn import metrics;\n",
    "from sklearn.model_selection import TimeSeriesSplit;\n",
    "\n",
    "mae_cv = []\n",
    "mse_cv = []\n",
    "mae_gen = []\n",
    "mse_gen  =[]\n",
    "rmse_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "mse_nor = []\n",
    "mse_spi = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for lr = 0.001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 17405.4434 - mse: 17405.4434 - mae: 113.1076\n",
      "Epoch 2/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 14586.1201 - mse: 14586.1201 - mae: 99.5965\n",
      "Epoch 3/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 6413.8477 - mse: 6413.8477 - mae: 44.7643\n",
      "Epoch 4/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4982.5356 - mse: 4982.5356 - mae: 30.2355\n",
      "Epoch 5/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 5022.3843 - mse: 5022.3843 - mae: 31.3917\n",
      "Epoch 6/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 5037.6724 - mse: 5037.6724 - mae: 31.4848\n",
      "Epoch 7/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 5036.8940 - mse: 5036.8936 - mae: 30.6263\n",
      "Epoch 8/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4922.5034 - mse: 4922.5034 - mae: 29.2518\n",
      "Epoch 9/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4837.8628 - mse: 4837.8628 - mae: 28.8177\n",
      "Epoch 10/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4832.9883 - mse: 4832.9883 - mae: 31.0541\n",
      "Epoch 11/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 5061.2173 - mse: 5061.2173 - mae: 31.7796\n",
      "Epoch 12/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4972.0117 - mse: 4972.0117 - mae: 30.6461\n",
      "Epoch 13/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4891.9365 - mse: 4891.9365 - mae: 28.7543\n",
      "Epoch 14/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4917.6797 - mse: 4917.6797 - mae: 30.0461\n",
      "Epoch 15/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4965.2168 - mse: 4965.2168 - mae: 29.6633\n",
      "Epoch 16/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 5020.3579 - mse: 5020.3579 - mae: 30.7350\n",
      "Epoch 17/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4843.5303 - mse: 4843.5303 - mae: 29.6426\n",
      "Epoch 18/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4895.1904 - mse: 4895.1904 - mae: 30.2948\n",
      "Epoch 19/80\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 5221.9985 - mse: 5221.9985 - mae: 31.0772\n",
      "Epoch 20/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4811.0903 - mse: 4811.0903 - mae: 30.2382\n",
      "Epoch 21/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 5169.7241 - mse: 5169.7246 - mae: 30.3213\n",
      "Epoch 22/80\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4979.7104 - mse: 4979.7104 - mae: 30.8594\n",
      "Epoch 23/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4967.1162 - mse: 4967.1162 - mae: 29.9344\n",
      "Epoch 24/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4748.2246 - mse: 4748.2246 - mae: 29.1893\n",
      "Epoch 25/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4921.8945 - mse: 4921.8945 - mae: 29.7423\n",
      "Epoch 26/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4805.3667 - mse: 4805.3667 - mae: 29.4779\n",
      "Epoch 27/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 5127.6885 - mse: 5127.6885 - mae: 30.3126\n",
      "Epoch 28/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4747.3833 - mse: 4747.3833 - mae: 29.7030\n",
      "Epoch 29/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4805.2466 - mse: 4805.2466 - mae: 29.2296\n",
      "Epoch 30/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4997.0986 - mse: 4997.0986 - mae: 29.9638\n",
      "Epoch 31/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4863.4468 - mse: 4863.4468 - mae: 29.1186\n",
      "Epoch 32/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4925.6309 - mse: 4925.6309 - mae: 29.7621\n",
      "Epoch 33/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4872.5723 - mse: 4872.5723 - mae: 30.0842\n",
      "Epoch 34/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4869.5269 - mse: 4869.5269 - mae: 29.4111\n",
      "Epoch 35/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4934.7471 - mse: 4934.7471 - mae: 29.6981\n",
      "Epoch 36/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4808.0044 - mse: 4808.0044 - mae: 30.6892\n",
      "Epoch 37/80\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4854.2612 - mse: 4854.2612 - mae: 29.5647\n",
      "Epoch 38/80\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4907.7036 - mse: 4907.7036 - mae: 30.5090\n",
      "Epoch 39/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4858.7988 - mse: 4858.7988 - mae: 30.6044\n",
      "Epoch 40/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4836.3936 - mse: 4836.3936 - mae: 28.8457\n",
      "Epoch 41/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4952.9409 - mse: 4952.9409 - mae: 31.7666\n",
      "Epoch 42/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4899.6006 - mse: 4899.6006 - mae: 29.5933\n",
      "Epoch 43/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4780.4844 - mse: 4780.4844 - mae: 30.8479\n",
      "Epoch 44/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4903.9492 - mse: 4903.9492 - mae: 28.5716\n",
      "Epoch 45/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4920.4365 - mse: 4920.4365 - mae: 28.9189\n",
      "Epoch 46/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 5019.6279 - mse: 5019.6279 - mae: 29.7877\n",
      "Epoch 47/80\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4893.2861 - mse: 4893.2861 - mae: 29.0479\n",
      "Epoch 48/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4837.5649 - mse: 4837.5649 - mae: 29.8482\n",
      "Epoch 49/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4988.3169 - mse: 4988.3169 - mae: 29.5090\n",
      "Epoch 50/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4850.8975 - mse: 4850.8975 - mae: 29.3598\n",
      "Epoch 51/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 5039.0742 - mse: 5039.0742 - mae: 29.3349\n",
      "Epoch 52/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4732.5171 - mse: 4732.5171 - mae: 29.2305\n",
      "Epoch 53/80\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4936.0850 - mse: 4936.0850 - mae: 30.0887\n",
      "Epoch 54/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 5101.1084 - mse: 5101.1084 - mae: 29.0427\n",
      "Epoch 55/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4775.9072 - mse: 4775.9072 - mae: 29.6821\n",
      "Epoch 56/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4738.6978 - mse: 4738.6978 - mae: 29.0932\n",
      "Epoch 57/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4941.6416 - mse: 4941.6416 - mae: 28.9099\n",
      "Epoch 58/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4762.4258 - mse: 4762.4258 - mae: 28.9850\n",
      "Epoch 59/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 5047.3369 - mse: 5047.3369 - mae: 29.6209\n",
      "Epoch 60/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4828.3257 - mse: 4828.3257 - mae: 29.3647\n",
      "Epoch 61/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 5007.1440 - mse: 5007.1440 - mae: 29.1104\n",
      "Epoch 62/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4728.0269 - mse: 4728.0269 - mae: 29.2902\n",
      "Epoch 63/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4910.1118 - mse: 4910.1118 - mae: 29.1226\n",
      "Epoch 64/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4860.3765 - mse: 4860.3765 - mae: 29.5633\n",
      "Epoch 65/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4963.4224 - mse: 4963.4224 - mae: 29.4515\n",
      "Epoch 66/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4861.1377 - mse: 4861.1377 - mae: 29.0869\n",
      "Epoch 67/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4903.1104 - mse: 4903.1104 - mae: 29.2062\n",
      "Epoch 68/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4580.2500 - mse: 4580.2500 - mae: 28.6015\n",
      "Epoch 69/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4716.9570 - mse: 4716.9570 - mae: 29.1410\n",
      "Epoch 70/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4887.5571 - mse: 4887.5571 - mae: 29.3972\n",
      "Epoch 71/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4772.1982 - mse: 4772.1982 - mae: 29.1779\n",
      "Epoch 72/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 2ms/step - loss: 5066.1079 - mse: 5066.1079 - mae: 29.2030\n",
      "Epoch 73/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4937.0806 - mse: 4937.0806 - mae: 29.4870\n",
      "Epoch 74/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4795.5947 - mse: 4795.5947 - mae: 29.0222\n",
      "Epoch 75/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4908.7134 - mse: 4908.7134 - mae: 28.9514\n",
      "Epoch 76/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4819.6855 - mse: 4819.6855 - mae: 29.2689\n",
      "Epoch 77/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 5033.4722 - mse: 5033.4722 - mae: 29.9331\n",
      "Epoch 78/80\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 4855.7129 - mse: 4855.7129 - mae: 28.8137\n",
      "Epoch 79/80\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4819.1343 - mse: 4819.1343 - mae: 28.4469\n",
      "Epoch 80/80\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4653.2676 - mse: 4653.2676 - mae: 28.6943\n",
      "1\n",
      "Epoch 1/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3649.3022 - mse: 3649.3022 - mae: 31.8861\n",
      "Epoch 2/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3601.4456 - mse: 3601.4451 - mae: 31.4829\n",
      "Epoch 3/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3562.5967 - mse: 3562.5967 - mae: 32.4919\n",
      "Epoch 4/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3583.7610 - mse: 3583.7610 - mae: 31.9935\n",
      "Epoch 5/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3591.6853 - mse: 3591.6853 - mae: 31.0659\n",
      "Epoch 6/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3652.5254 - mse: 3652.5254 - mae: 32.0326\n",
      "Epoch 7/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3615.4204 - mse: 3615.4204 - mae: 31.8038\n",
      "Epoch 8/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3618.2014 - mse: 3618.2014 - mae: 31.5638\n",
      "Epoch 9/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3508.9575 - mse: 3508.9575 - mae: 30.9483\n",
      "Epoch 10/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3654.2886 - mse: 3654.2886 - mae: 31.7400\n",
      "Epoch 11/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3601.9548 - mse: 3601.9548 - mae: 32.2031\n",
      "Epoch 12/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3596.9346 - mse: 3596.9346 - mae: 31.9123\n",
      "Epoch 13/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3595.5400 - mse: 3595.5400 - mae: 31.6815\n",
      "Epoch 14/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3546.7976 - mse: 3546.7976 - mae: 31.2636\n",
      "Epoch 15/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3505.7720 - mse: 3505.7720 - mae: 30.9535\n",
      "Epoch 16/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3644.0876 - mse: 3644.0876 - mae: 31.2538\n",
      "Epoch 17/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3716.8923 - mse: 3716.8923 - mae: 32.2912\n",
      "Epoch 18/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3578.5234 - mse: 3578.5234 - mae: 30.7649\n",
      "Epoch 19/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3539.8833 - mse: 3539.8833 - mae: 30.9247\n",
      "Epoch 20/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3527.6443 - mse: 3527.6443 - mae: 30.7652\n",
      "Epoch 21/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3509.2000 - mse: 3509.2000 - mae: 31.3472\n",
      "Epoch 22/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3617.1785 - mse: 3617.1785 - mae: 31.7588\n",
      "Epoch 23/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3544.2471 - mse: 3544.2471 - mae: 31.4712\n",
      "Epoch 24/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3506.6765 - mse: 3506.6765 - mae: 31.0521\n",
      "Epoch 25/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3452.8052 - mse: 3452.8052 - mae: 30.8752\n",
      "Epoch 26/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3620.2368 - mse: 3620.2368 - mae: 31.3843\n",
      "Epoch 27/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3512.0784 - mse: 3512.0786 - mae: 30.4980\n",
      "Epoch 28/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3400.6191 - mse: 3400.6191 - mae: 30.5641\n",
      "Epoch 29/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3539.6973 - mse: 3539.6973 - mae: 31.1926\n",
      "Epoch 30/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3607.2625 - mse: 3607.2625 - mae: 31.2603\n",
      "Epoch 31/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3483.4336 - mse: 3483.4336 - mae: 30.8406\n",
      "Epoch 32/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3534.4431 - mse: 3534.4431 - mae: 30.9771\n",
      "Epoch 33/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3507.0884 - mse: 3507.0884 - mae: 31.3693\n",
      "Epoch 34/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3422.4106 - mse: 3422.4106 - mae: 30.9961\n",
      "Epoch 35/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3522.9133 - mse: 3522.9133 - mae: 30.7340\n",
      "Epoch 36/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3426.4924 - mse: 3426.4924 - mae: 30.7594\n",
      "Epoch 37/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3543.8396 - mse: 3543.8396 - mae: 31.0801\n",
      "Epoch 38/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3492.0164 - mse: 3492.0164 - mae: 30.8681\n",
      "Epoch 39/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3509.2083 - mse: 3509.2083 - mae: 30.7303\n",
      "Epoch 40/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3431.6653 - mse: 3431.6653 - mae: 30.1073\n",
      "Epoch 41/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3368.0420 - mse: 3368.0420 - mae: 30.2708\n",
      "Epoch 42/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3429.9761 - mse: 3429.9761 - mae: 30.5248\n",
      "Epoch 43/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3415.1538 - mse: 3415.1538 - mae: 29.9336\n",
      "Epoch 44/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3469.3850 - mse: 3469.3850 - mae: 30.5377\n",
      "Epoch 45/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3377.3740 - mse: 3377.3740 - mae: 29.9167\n",
      "Epoch 46/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3558.5647 - mse: 3558.5647 - mae: 30.8464\n",
      "Epoch 47/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3404.5693 - mse: 3404.5693 - mae: 30.0172\n",
      "Epoch 48/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3462.1147 - mse: 3462.1147 - mae: 30.5924\n",
      "Epoch 49/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3456.3535 - mse: 3456.3535 - mae: 30.5212\n",
      "Epoch 50/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3351.4998 - mse: 3351.5000 - mae: 30.0862\n",
      "Epoch 51/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3438.7524 - mse: 3438.7524 - mae: 30.2982\n",
      "Epoch 52/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3397.1555 - mse: 3397.1555 - mae: 30.1054\n",
      "Epoch 53/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3490.3010 - mse: 3490.3010 - mae: 30.6509\n",
      "Epoch 54/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3391.2598 - mse: 3391.2598 - mae: 30.3692\n",
      "Epoch 55/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3465.3835 - mse: 3465.3835 - mae: 31.1501\n",
      "Epoch 56/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3399.9812 - mse: 3399.9812 - mae: 30.6708\n",
      "Epoch 57/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3415.8708 - mse: 3415.8708 - mae: 30.2862\n",
      "Epoch 58/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3500.1472 - mse: 3500.1472 - mae: 29.7685\n",
      "Epoch 59/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3503.1558 - mse: 3503.1558 - mae: 31.0197\n",
      "Epoch 60/80\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 3425.9919 - mse: 3425.9919 - mae: 30.4980\n",
      "Epoch 61/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3425.3450 - mse: 3425.3450 - mae: 30.2109\n",
      "Epoch 62/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3505.8721 - mse: 3505.8721 - mae: 30.7074\n",
      "Epoch 63/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 3ms/step - loss: 3402.1643 - mse: 3402.1643 - mae: 30.5494\n",
      "Epoch 64/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3498.1089 - mse: 3498.1089 - mae: 30.6818\n",
      "Epoch 65/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3471.5337 - mse: 3471.5337 - mae: 31.0931\n",
      "Epoch 66/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3493.7056 - mse: 3493.7056 - mae: 30.3355\n",
      "Epoch 67/80\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 3397.8958 - mse: 3397.8958 - mae: 30.0990\n",
      "Epoch 68/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3581.5837 - mse: 3581.5837 - mae: 30.5266\n",
      "Epoch 69/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3403.4346 - mse: 3403.4346 - mae: 29.5355\n",
      "Epoch 70/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3374.0320 - mse: 3374.0320 - mae: 29.3623\n",
      "Epoch 71/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3438.7505 - mse: 3438.7505 - mae: 30.0409\n",
      "Epoch 72/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3421.6458 - mse: 3421.6458 - mae: 31.0545\n",
      "Epoch 73/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3309.3115 - mse: 3309.3115 - mae: 29.5088\n",
      "Epoch 74/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3371.3052 - mse: 3371.3052 - mae: 30.4709\n",
      "Epoch 75/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3443.6982 - mse: 3443.6982 - mae: 30.0015\n",
      "Epoch 76/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3435.3750 - mse: 3435.3750 - mae: 30.2973\n",
      "Epoch 77/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3553.0459 - mse: 3553.0459 - mae: 30.4736\n",
      "Epoch 78/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3484.3154 - mse: 3484.3154 - mae: 30.4568\n",
      "Epoch 79/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3355.8782 - mse: 3355.8782 - mae: 29.6411\n",
      "Epoch 80/80\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3367.5078 - mse: 3367.5078 - mae: 30.1796\n",
      "2\n",
      "Epoch 1/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2699.8918 - mse: 2699.8918 - mae: 28.5325\n",
      "Epoch 2/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2807.5378 - mse: 2807.5378 - mae: 29.3353\n",
      "Epoch 3/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2760.1970 - mse: 2760.1970 - mae: 28.5633\n",
      "Epoch 4/80\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 2706.7026 - mse: 2706.7026 - mae: 28.5361\n",
      "Epoch 5/80\n",
      "118/118 [==============================] - 0s 1ms/step - loss: 2770.9524 - mse: 2770.9524 - mae: 28.3697\n",
      "Epoch 6/80\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 2704.3779 - mse: 2704.3779 - mae: 28.2872\n",
      "Epoch 7/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2755.8828 - mse: 2755.8828 - mae: 28.6276: 0s - loss: 1504.1124 - mse: 1504.1124 - ma\n",
      "Epoch 8/80\n",
      "118/118 [==============================] - ETA: 0s - loss: 2871.4373 - mse: 2871.4373 - mae: 28.41 - 0s 3ms/step - loss: 2805.1230 - mse: 2805.1230 - mae: 28.5305\n",
      "Epoch 9/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2718.7085 - mse: 2718.7085 - mae: 28.3661\n",
      "Epoch 10/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2812.8921 - mse: 2812.8921 - mae: 28.9717\n",
      "Epoch 11/80\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 2753.2202 - mse: 2753.2202 - mae: 28.5728\n",
      "Epoch 12/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2755.8291 - mse: 2755.8291 - mae: 28.1147\n",
      "Epoch 13/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2725.7522 - mse: 2725.7522 - mae: 28.8167\n",
      "Epoch 14/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2716.8381 - mse: 2716.8381 - mae: 27.7025\n",
      "Epoch 15/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2681.1895 - mse: 2681.1895 - mae: 28.1709\n",
      "Epoch 16/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2684.6418 - mse: 2684.6418 - mae: 28.3843\n",
      "Epoch 17/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2650.9558 - mse: 2650.9558 - mae: 27.5861\n",
      "Epoch 18/80\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 2698.6243 - mse: 2698.6243 - mae: 28.1780\n",
      "Epoch 19/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2744.8364 - mse: 2744.8364 - mae: 28.5012\n",
      "Epoch 20/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2706.0623 - mse: 2706.0623 - mae: 27.8454\n",
      "Epoch 21/80\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 2737.6575 - mse: 2737.6575 - mae: 28.4548\n",
      "Epoch 22/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2746.7729 - mse: 2746.7729 - mae: 28.3884\n",
      "Epoch 23/80\n",
      "118/118 [==============================] - 0s 1ms/step - loss: 2675.3608 - mse: 2675.3608 - mae: 28.1657\n",
      "Epoch 24/80\n",
      "118/118 [==============================] - 0s 1ms/step - loss: 2746.3635 - mse: 2746.3635 - mae: 28.7174\n",
      "Epoch 25/80\n",
      "118/118 [==============================] - 0s 1ms/step - loss: 2696.0754 - mse: 2696.0754 - mae: 28.0461\n",
      "Epoch 26/80\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 2711.5608 - mse: 2711.5608 - mae: 28.0697\n",
      "Epoch 27/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2695.3298 - mse: 2695.3298 - mae: 28.3226\n",
      "Epoch 28/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2742.7964 - mse: 2742.7964 - mae: 28.6349\n",
      "Epoch 29/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2765.6477 - mse: 2765.6477 - mae: 28.5186\n",
      "Epoch 30/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2789.1125 - mse: 2789.1125 - mae: 28.3559\n",
      "Epoch 31/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2788.9697 - mse: 2788.9697 - mae: 28.4082\n",
      "Epoch 32/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2657.4797 - mse: 2657.4797 - mae: 28.4574\n",
      "Epoch 33/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2713.9209 - mse: 2713.9209 - mae: 28.2532\n",
      "Epoch 34/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2660.3691 - mse: 2660.3691 - mae: 28.3718\n",
      "Epoch 35/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2669.2119 - mse: 2669.2119 - mae: 28.6196\n",
      "Epoch 36/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2636.7383 - mse: 2636.7383 - mae: 28.0814\n",
      "Epoch 37/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2704.3018 - mse: 2704.3015 - mae: 27.9570\n",
      "Epoch 38/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2711.4021 - mse: 2711.4021 - mae: 27.9243\n",
      "Epoch 39/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2733.5913 - mse: 2733.5913 - mae: 28.6425\n",
      "Epoch 40/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2726.8770 - mse: 2726.8770 - mae: 28.0784\n",
      "Epoch 41/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2702.9536 - mse: 2702.9536 - mae: 27.8751\n",
      "Epoch 42/80\n",
      "118/118 [==============================] - 0s 1ms/step - loss: 2700.6653 - mse: 2700.6653 - mae: 28.2853\n",
      "Epoch 43/80\n",
      "118/118 [==============================] - 0s 1ms/step - loss: 2729.9495 - mse: 2729.9495 - mae: 28.0385\n",
      "Epoch 44/80\n",
      "118/118 [==============================] - 0s 1ms/step - loss: 2670.2227 - mse: 2670.2227 - mae: 27.9686\n",
      "Epoch 45/80\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 2663.0542 - mse: 2663.0542 - mae: 27.7287\n",
      "Epoch 46/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2616.7654 - mse: 2616.7654 - mae: 28.0252: 0s - loss: 3618.5610 - mse: 3618.5610 - mae: 2\n",
      "Epoch 47/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2650.9441 - mse: 2650.9441 - mae: 27.5859\n",
      "Epoch 48/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2667.5303 - mse: 2667.5303 - mae: 27.6892\n",
      "Epoch 49/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2678.2996 - mse: 2678.2996 - mae: 28.1630\n",
      "Epoch 50/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2644.3003 - mse: 2644.3003 - mae: 27.8603\n",
      "Epoch 51/80\n",
      "118/118 [==============================] - 0s 3ms/step - loss: 2717.5366 - mse: 2717.5366 - mae: 28.3296\n",
      "Epoch 52/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 0s 2ms/step - loss: 2631.8508 - mse: 2631.8508 - mae: 27.1297\n",
      "Epoch 53/80\n",
      " 68/118 [================>.............] - ETA: 0s - loss: 3337.3911 - mse: 3337.3911 - mae: 29.4504"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# data\n",
    "data = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "\n",
    "# COMPLETE\n",
    "data = data.loc[data.index > 2018090000, :]\n",
    "    \n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "    \n",
    "# Divide features and labels\n",
    "X = data.iloc[:, 0:15]\n",
    "y = data.loc[:, 'Offers']\n",
    "\n",
    "X.fillna(method = 'ffill', inplace = True)\n",
    "y.fillna(method = 'ffill', inplace = True)\n",
    "    \n",
    "X = X.astype('float64')\n",
    "X = X.round(20)\n",
    "    \n",
    "# divide data into train and test with 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "# feature scaling\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "    \n",
    "import keras\n",
    "from keras.models import Sequential # to initialise the NN\n",
    "from keras.layers import Dense # to create layers\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "    \n",
    "# possible debug\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "    \n",
    "def regressor_tunning(n_hidden = 5, \n",
    "                      n_neurons = 40,  \n",
    "                      kernel_initializer = \"he_normal\",\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = n_neurons, input_dim = 15))        \n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dropout(rate = 0.1))        \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.1))\n",
    "    model.add(Dense(units = 1, activation = 'linear'))\n",
    "    optimizer = optimizers.Adamax(lr = 0.001)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 7)\n",
    "\n",
    "hist_list = pd.DataFrame()\n",
    "count = 1\n",
    "    \n",
    "regressor = regressor_tunning()\n",
    "    \n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "    y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "    hist = regressor.fit(X_train_split, y_train_split, batch_size = 15, epochs = 80)\n",
    "    hist_list = hist_list.append(hist.history, ignore_index = True)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for i in range(len(hist_list.mse)):\n",
    "    a.append(np.mean(hist_list.mse[i]))\n",
    "    b.append(np.mean(hist_list.mae[i]))\n",
    "\n",
    "mse_cv.append(np.mean(a))\n",
    "mae_cv.append(np.mean(b))\n",
    "\n",
    "# predict for X_test  \n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mse_error = mse(y_test, y_pred) # 1479.61335\n",
    "mae_error = mae(y_test, y_pred) # 23.1525\n",
    "\n",
    "rmse_gen.append(rmse_error)\n",
    "mse_gen.append(mse_error)\n",
    "mae_gen.append(mae_error)\n",
    "\n",
    "# =============================================================================\n",
    "# Metrics evaluation on spike regions\n",
    "# =============================================================================\n",
    "\n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "\n",
    "# smal adjustment\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mse_spike = mse(y_test_spike, y_pred_spike)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "rmse_spi.append(rmse_spike)\n",
    "mse_spi.append(mse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "\n",
    "# =============================================================================\n",
    "# Metric evaluation on normal regions\n",
    "# =============================================================================\n",
    "\n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mse_normal = mse(y_test_normal, y_pred_normal)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "rmse_nor.append(rmse_normal)\n",
    "mse_nor.append(mse_normal)\n",
    "mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv = []\n",
    "for i in mse_cv:\n",
    "    rmse_cv.append(i ** 0.5)\n",
    "    \n",
    "results = pd.DataFrame({'rmse_cv':rmse_cv,\n",
    "              \n",
    "                        'mae_cv': mae_cv,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for exponential schedueling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "# data\n",
    "data = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "data = data.loc[data.index > 2018090000, :]\n",
    "    \n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "    \n",
    "# Divide features and labels\n",
    "X = data.iloc[:, 0:15]\n",
    "y = data.loc[:, 'Offers']\n",
    "\n",
    "X.fillna(method = 'ffill', inplace = True)\n",
    "y.fillna(method = 'ffill', inplace = True)\n",
    "    \n",
    "X = X.astype('float64')\n",
    "X = X.round(20)\n",
    "    \n",
    "# divide data into train and test with 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "# feature scaling\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "    \n",
    "import keras\n",
    "from keras.models import Sequential # to initialise the NN\n",
    "from keras.layers import Dense # to create layers\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "    \n",
    "# possible debug\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "    \n",
    "def regressor_tunning(n_hidden = 5, \n",
    "                      n_neurons = 40,  \n",
    "                      kernel_initializer = \"he_normal\",\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = n_neurons, input_dim = 15))        \n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dropout(rate = 0.1))        \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.1))\n",
    "    model.add(Dense(units = 1, activation = 'linear'))\n",
    "    optimizer = optimizers.Adamax(lr = 0.001)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.001 * 0.1 ** (epoch / 20)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 7)\n",
    "    \n",
    "hist_list = pd.DataFrame()\n",
    "count = 1\n",
    "    \n",
    "regressor = regressor_tunning()\n",
    "    \n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "    y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "    hist = regressor.fit(X_train_split, y_train_split, batch_size = 15, epochs = 80, callbacks = [lr_scheduler])\n",
    "    hist_list = hist_list.append(hist.history, ignore_index = True)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for i in range(len(hist_list.mse)):\n",
    "    a.append(np.mean(hist_list.mse[i]))\n",
    "    b.append(np.mean(hist_list.mae[i]))\n",
    "\n",
    "mse_cv.append(np.mean(a))\n",
    "mae_cv.append(np.mean(b))\n",
    "\n",
    "# predict for X_test  \n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mse_error = mse(y_test, y_pred) # 1479.61335\n",
    "mae_error = mae(y_test, y_pred) # 23.1525\n",
    "\n",
    "rmse_gen.append(rmse_error)\n",
    "mse_gen.append(mse_error)\n",
    "mae_gen.append(mae_error)\n",
    "\n",
    "# =============================================================================\n",
    "# Metrics evaluation on spike regions\n",
    "# =============================================================================\n",
    "\n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "\n",
    "# smal adjustment\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mse_spike = mse(y_test_spike, y_pred_spike)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "rmse_spi.append(rmse_spike)\n",
    "mse_spi.append(mse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "\n",
    "# =============================================================================\n",
    "# Metric evaluation on normal regions\n",
    "# =============================================================================\n",
    "\n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mse_normal = mse(y_test_normal, y_pred_normal)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "rmse_nor.append(rmse_normal)\n",
    "mse_nor.append(mse_normal)\n",
    "mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv = []\n",
    "for i in mse_cv:\n",
    "    rmse_cv.append(i ** 0.5)\n",
    "    \n",
    "results = pd.DataFrame({'rmse_cv':rmse_cv,\n",
    "              \n",
    "                        'mae_cv': mae_cv,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor}, index = ['lr = 0.001', 'exp schedueling'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
