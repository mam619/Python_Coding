{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression LSTM with best parameters\n",
    "    find the best prediction window to apply w/ lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 67.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.0 pytz-2020.1\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 62.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=b33676890d2101374fffcffac005742c87f644b21d52e2fed36b8c91c450a2cd\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.16.0 scikit-learn-0.23.1 sklearn-0.0 threadpoolctl-2.1.0\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.0-1-cp37-cp37m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Installing collected packages: cycler, kiwisolver, pyparsing, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.0 pyparsing-2.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "dates = [2018010000, \n",
    "         2018030000, \n",
    "         2018050000,\n",
    "         2018070000, \n",
    "         2018090000, \n",
    "         2018110000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import keras libraries, packages and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "mae_gen = []\n",
    "rmse_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "hist_list = []\n",
    "y_pred_list = []\n",
    "prediction_list = []\n",
    "time_count = []\n",
    "\n",
    "# import data\n",
    "data_full = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "\n",
    "# parameters\n",
    "features_num = 15\n",
    "steps = 96\n",
    "n_hidden = 1\n",
    "units = 150\n",
    "batch_size = 96\n",
    "epochs = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create loop for different dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6240 samples, validate on 960 samples\n",
      "Epoch 1/180\n",
      "6240/6240 [==============================] - 64s 10ms/step - loss: 4541328929020116992000.0000 - mse: 4541328251773815619584.0000 - mae: 40589611008.0000 - val_loss: 2101813479807672909824.0000 - val_mse: 2101813479807672909824.0000 - val_mae: 45845565440.0000\n",
      "Epoch 2/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3495672798890966384640.0000 - mse: 3495673201616702406656.0000 - mae: 58674622464.0000 - val_loss: 2947353473059880173568.0000 - val_mse: 2947353473059880173568.0000 - val_mae: 54289510400.0000\n",
      "Epoch 3/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3454446648704340852736.0000 - mse: 3454447250627752886272.0000 - mae: 58550603776.0000 - val_loss: 4089396814241790427136.0000 - val_mse: 4089397377191743848448.0000 - val_mae: 63948410880.0000\n",
      "Epoch 4/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3480814852109549174784.0000 - mse: 3480814419071123587072.0000 - mae: 58690727936.0000 - val_loss: 3214593415773348167680.0000 - val_mse: 3214593415773348167680.0000 - val_mae: 56697409536.0000\n",
      "Epoch 5/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3503505541640190689280.0000 - mse: 3503505524318653120512.0000 - mae: 58848714752.0000 - val_loss: 2992254924294717440000.0000 - val_mse: 2992254924294717440000.0000 - val_mae: 54701506560.0000\n",
      "Epoch 6/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3448218096803155804160.0000 - mse: 3448218209393146068992.0000 - mae: 58465488896.0000 - val_loss: 4145884056993015005184.0000 - val_mse: 4145884338467991715840.0000 - val_mae: 64388550656.0000\n",
      "Epoch 7/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3477140464772415750144.0000 - mse: 3477139481775189262336.0000 - mae: 58748809216.0000 - val_loss: 3432831379791258058752.0000 - val_mse: 3432831098316281348096.0000 - val_mae: 58590363648.0000\n",
      "Epoch 8/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3465135518042248183808.0000 - mse: 3465134855493456494592.0000 - mae: 58614636544.0000 - val_loss: 4048480204252245917696.0000 - val_mse: 4048480485727222628352.0000 - val_mae: 63627677696.0000\n",
      "Epoch 9/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3476292870001007263744.0000 - mse: 3476292242095290187776.0000 - mae: 58579881984.0000 - val_loss: 3478081297047263117312.0000 - val_mse: 3478081578522239827968.0000 - val_mae: 58975272960.0000\n",
      "Epoch 10/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3498244977853528014848.0000 - mse: 3498244757003930959872.0000 - mae: 58874257408.0000 - val_loss: 3007643442746465714176.0000 - val_mse: 3007643161271489003520.0000 - val_mae: 54841970688.0000\n",
      "Epoch 11/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3487825345826421473280.0000 - mse: 3487825960740986028032.0000 - mae: 58817896448.0000 - val_loss: 3507280385231319728128.0000 - val_mse: 3507280385231319728128.0000 - val_mae: 59222274048.0000\n",
      "Epoch 12/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3487623801082328121344.0000 - mse: 3487624143182684487680.0000 - mae: 58800381952.0000 - val_loss: 3361365164679399211008.0000 - val_mse: 3361365446154375921664.0000 - val_mae: 57977290752.0000\n",
      "Epoch 13/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3454271155551938805760.0000 - mse: 3454271047292332015616.0000 - mae: 58471346176.0000 - val_loss: 4969227355794285002752.0000 - val_mse: 4969227355794285002752.0000 - val_mae: 70492749824.0000\n",
      "Epoch 14/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3472904824992489799680.0000 - mse: 3472904690750577442816.0000 - mae: 58577670144.0000 - val_loss: 4046613180731724136448.0000 - val_mse: 4046613180731724136448.0000 - val_mae: 63612997632.0000\n",
      "Epoch 15/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3479801204423418970112.0000 - mse: 3479801390629941936128.0000 - mae: 58728132608.0000 - val_loss: 4186936900871287472128.0000 - val_mse: 4186936900871287472128.0000 - val_mae: 64706572288.0000\n",
      "Epoch 16/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3482333639119269265408.0000 - mse: 3482333539520430997504.0000 - mae: 58714980352.0000 - val_loss: 3417343782147707633664.0000 - val_mse: 3417343782147707633664.0000 - val_mae: 58458062848.0000\n",
      "Epoch 17/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3465759478779467005952.0000 - mse: 3465759729941754150912.0000 - mae: 58575224832.0000 - val_loss: 3921328950072787861504.0000 - val_mse: 3921328950072787861504.0000 - val_mae: 62620520448.0000\n",
      "Epoch 18/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3497642772966816088064.0000 - mse: 3497642400553770156032.0000 - mae: 58862981120.0000 - val_loss: 2729149848989128982528.0000 - val_mse: 2729149567514152271872.0000 - val_mae: 52241248256.0000\n",
      "Epoch 19/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3472484132492297633792.0000 - mse: 3472484167135371722752.0000 - mae: 58556469248.0000 - val_loss: 3602141955982438170624.0000 - val_mse: 3602141393032484749312.0000 - val_mae: 60017864704.0000\n",
      "Epoch 20/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3491974518658694643712.0000 - mse: 3491975183372677808128.0000 - mae: 58685259776.0000 - val_loss: 3132371760326398443520.0000 - val_mse: 3132371760326398443520.0000 - val_mae: 55967608832.0000\n",
      "Epoch 21/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3457349370227630014464.0000 - mse: 3457349257637639749632.0000 - mae: 58558451712.0000 - val_loss: 3340620458895823863808.0000 - val_mse: 3340620458895823863808.0000 - val_mae: 57798082560.0000\n",
      "Epoch 22/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3470569180278818865152.0000 - mse: 3470569011393832419328.0000 - mae: 58640793600.0000 - val_loss: 3595981313167172042752.0000 - val_mse: 3595981031692195332096.0000 - val_mae: 59966492672.0000\n",
      "Epoch 23/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3498149999535617736704.0000 - mse: 3498149618461802758144.0000 - mae: 58909499392.0000 - val_loss: 3704344675601147232256.0000 - val_mse: 3704344957076123942912.0000 - val_mae: 60863324160.0000\n",
      "Epoch 24/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3484659700692578992128.0000 - mse: 3484660211677921280000.0000 - mae: 58794860544.0000 - val_loss: 3235787355619753721856.0000 - val_mse: 3235787355619753721856.0000 - val_mae: 56883982336.0000\n",
      "Epoch 25/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3465900082025910501376.0000 - mse: 3465900185955132768256.0000 - mae: 58628444160.0000 - val_loss: 3543053602971430420480.0000 - val_mse: 3543053602971430420480.0000 - val_mae: 59523547136.0000\n",
      "Epoch 26/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3487675752702260805632.0000 - mse: 3487675371628445827072.0000 - mae: 58789347328.0000 - val_loss: 3117415305963901026304.0000 - val_mse: 3117415305963901026304.0000 - val_mae: 55833800704.0000\n",
      "Epoch 27/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3426744838511399534592.0000 - mse: 3426745046369843544064.0000 - mae: 58232926208.0000 - val_loss: 3242624382804055556096.0000 - val_mse: 3242624382804055556096.0000 - val_mae: 56944054272.0000\n",
      "Epoch 28/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3475791320565967486976.0000 - mse: 3475791779586698641408.0000 - mae: 58634256384.0000 - val_loss: 3664993066482113970176.0000 - val_mse: 3664993066482113970176.0000 - val_mae: 60539183104.0000\n",
      "Epoch 29/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3516566595474129158144.0000 - mse: 3516566526187980980224.0000 - mae: 59008118784.0000 - val_loss: 2781888126525544333312.0000 - val_mse: 2781888126525544333312.0000 - val_mae: 52743630848.0000\n",
      "Epoch 30/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3447051846375805288448.0000 - mse: 3447051777089657110528.0000 - mae: 58414522368.0000 - val_loss: 4490205359128742723584.0000 - val_mse: 4490205640603719434240.0000 - val_mae: 67008987136.0000\n",
      "Epoch 31/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3515884360755962183680.0000 - mse: 3515883949369457639424.0000 - mae: 58937171968.0000 - val_loss: 2666839451919714353152.0000 - val_mse: 2666839170444737642496.0000 - val_mae: 51641454592.0000\n",
      "Epoch 32/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3463714320572145991680.0000 - mse: 3463714251285997813760.0000 - mae: 58371747840.0000 - val_loss: 2988313711670814834688.0000 - val_mse: 2988313711670814834688.0000 - val_mae: 54665461760.0000\n",
      "Epoch 33/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3489179556582450003968.0000 - mse: 3489179855378964283392.0000 - mae: 58776965120.0000 - val_loss: 3321171100955070955520.0000 - val_mse: 3321170819480094244864.0000 - val_mae: 57629605888.0000\n",
      "Epoch 34/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3467124104109634355200.0000 - mse: 3467124039153870700544.0000 - mae: 58569936896.0000 - val_loss: 3310011181078446866432.0000 - val_mse: 3310010899603470155776.0000 - val_mae: 57532694528.0000\n",
      "Epoch 35/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3461187052024714756096.0000 - mse: 3461186043045182701568.0000 - mae: 58548154368.0000 - val_loss: 3555492263667251019776.0000 - val_mse: 3555492545142227730432.0000 - val_mae: 59627933696.0000\n",
      "Epoch 36/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3483646369124499062784.0000 - mse: 3483646057336832786432.0000 - mae: 58770968576.0000 - val_loss: 3052873375279124316160.0000 - val_mse: 3052873375279124316160.0000 - val_mae: 55252828160.0000\n",
      "Epoch 37/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3475433639487100420096.0000 - mse: 3475433743416322686976.0000 - mae: 58734317568.0000 - val_loss: 3597206292265816817664.0000 - val_mse: 3597206010790840107008.0000 - val_mae: 59976716288.0000\n",
      "Epoch 38/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3506618698186452631552.0000 - mse: 3506618637561072975872.0000 - mae: 58979516416.0000 - val_loss: 2867140423046737690624.0000 - val_mse: 2867140423046737690624.0000 - val_mae: 53545676800.0000\n",
      "Epoch 39/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3441986097916101001216.0000 - mse: 3441986071933795434496.0000 - mae: 58357846016.0000 - val_loss: 3302143674004407320576.0000 - val_mse: 3302143955479384031232.0000 - val_mae: 57464279040.0000\n",
      "Epoch 40/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3487532313054128635904.0000 - mse: 3487532382340276813824.0000 - mae: 58811547648.0000 - val_loss: 4131999459341831766016.0000 - val_mse: 4131998896391878344704.0000 - val_mae: 64280633344.0000\n",
      "Epoch 41/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3471908810631054098432.0000 - mse: 3471908269333021720576.0000 - mae: 58565980160.0000 - val_loss: 3518508985002261217280.0000 - val_mse: 3518508985002261217280.0000 - val_mae: 59317022720.0000\n",
      "Epoch 42/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3493062625136933339136.0000 - mse: 3493062802682687782912.0000 - mae: 58840084480.0000 - val_loss: 3697026889156623597568.0000 - val_mse: 3697026326206670176256.0000 - val_mae: 60803203072.0000\n",
      "Epoch 43/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3489975359958143729664.0000 - mse: 3489974740713195175936.0000 - mae: 58740867072.0000 - val_loss: 3793550288170241753088.0000 - val_mse: 3793550006695265042432.0000 - val_mae: 61591785472.0000\n",
      "Epoch 44/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3475340605511721025536.0000 - mse: 3475340012249078038528.0000 - mae: 58656464896.0000 - val_loss: 2393250278156584091648.0000 - val_mse: 2393250278156584091648.0000 - val_mae: 48920862720.0000\n",
      "Epoch 45/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3462407246048755449856.0000 - mse: 3462406800019176816640.0000 - mae: 58544197632.0000 - val_loss: 3276147771055317975040.0000 - val_mse: 3276147489580341264384.0000 - val_mae: 57237651456.0000\n",
      "Epoch 46/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3492465495460570202112.0000 - mse: 3492465794257084481536.0000 - mae: 58783633408.0000 - val_loss: 2959946944992891633664.0000 - val_mse: 2959946944992891633664.0000 - val_mae: 54405378048.0000\n",
      "Epoch 47/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3471461923636491321344.0000 - mse: 3471461850019958620160.0000 - mae: 58543771648.0000 - val_loss: 3763175760483441442816.0000 - val_mse: 3763175197533488021504.0000 - val_mae: 61344714752.0000\n",
      "Epoch 48/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3468674000599856971776.0000 - mse: 3468674403325592993792.0000 - mae: 58534465536.0000 - val_loss: 4228251797952877559808.0000 - val_mse: 4228252360902830981120.0000 - val_mae: 65024995328.0000\n",
      "Epoch 49/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3454527839078777356288.0000 - mse: 3454528315421045555200.0000 - mae: 58475204608.0000 - val_loss: 3367727625052966879232.0000 - val_mse: 3367727906527943589888.0000 - val_mae: 58032123904.0000\n",
      "Epoch 50/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3460153389302547480576.0000 - mse: 3460153592830608015360.0000 - mae: 58400210944.0000 - val_loss: 3834804949606815760384.0000 - val_mse: 3834805231081792471040.0000 - val_mae: 61925793792.0000\n",
      "Epoch 51/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3521649990249680994304.0000 - mse: 3521649964267375427584.0000 - mae: 59068973056.0000 - val_loss: 2865495201807863906304.0000 - val_mse: 2865495201807863906304.0000 - val_mae: 53530324992.0000\n",
      "Epoch 52/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3501170918897203806208.0000 - mse: 3501170689386838228992.0000 - mae: 58936279040.0000 - val_loss: 3270792991098374455296.0000 - val_mse: 3270792991098374455296.0000 - val_mae: 57190858752.0000\n",
      "Epoch 53/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3463956445347112288256.0000 - mse: 3463956038290992267264.0000 - mae: 58442121216.0000 - val_loss: 3614421020366463827968.0000 - val_mse: 3614421020366463827968.0000 - val_mae: 60120055808.0000\n",
      "Epoch 54/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3494487182985218490368.0000 - mse: 3494487066064843702272.0000 - mae: 58856017920.0000 - val_loss: 3550672849116011167744.0000 - val_mse: 3550673130590987878400.0000 - val_mae: 59587518464.0000\n",
      "Epoch 55/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3459460558134103769088.0000 - mse: 3459461164387899801600.0000 - mae: 58580799488.0000 - val_loss: 3656967933421116456960.0000 - val_mse: 3656967933421116456960.0000 - val_mae: 60472868864.0000\n",
      "Epoch 56/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3480113290886442909696.0000 - mse: 3480113264904137342976.0000 - mae: 58716606464.0000 - val_loss: 2729183063036380839936.0000 - val_mse: 2729182781561404129280.0000 - val_mae: 52241584128.0000\n",
      "Epoch 57/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3458644423604323745792.0000 - mse: 3458644324005485477888.0000 - mae: 58521473024.0000 - val_loss: 3651794423349174599680.0000 - val_mse: 3651794423349174599680.0000 - val_mae: 60430086144.0000\n",
      "Epoch 58/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3513338817986965798912.0000 - mse: 3513338289680086466560.0000 - mae: 59049201664.0000 - val_loss: 4227222443963046690816.0000 - val_mse: 4227222443963046690816.0000 - val_mae: 65017085952.0000\n",
      "Epoch 59/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3452399160750289977344.0000 - mse: 3452398675747252731904.0000 - mae: 58455707648.0000 - val_loss: 3424092426189322321920.0000 - val_mse: 3424092426189322321920.0000 - val_mae: 58515771392.0000\n",
      "Epoch 60/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3463156372212541816832.0000 - mse: 3463156086407180582912.0000 - mae: 58581241856.0000 - val_loss: 3429135050397093724160.0000 - val_mse: 3429134768922117013504.0000 - val_mae: 58558816256.0000\n",
      "Epoch 61/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3478150448953464717312.0000 - mse: 3478150258416557228032.0000 - mae: 58706747392.0000 - val_loss: 3277075512578556297216.0000 - val_mse: 3277075231103579586560.0000 - val_mae: 57245745152.0000\n",
      "Epoch 62/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3471095235358368989184.0000 - mse: 3471094806650327924736.0000 - mae: 58679840768.0000 - val_loss: 3158217355637924298752.0000 - val_mse: 3158217355637924298752.0000 - val_mae: 56198033408.0000\n",
      "Epoch 63/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3487661717926883360768.0000 - mse: 3487661860829563715584.0000 - mae: 58833137664.0000 - val_loss: 3514946919171987865600.0000 - val_mse: 3514946637697011154944.0000 - val_mae: 59287007232.0000\n",
      "Epoch 64/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3443246057858774597632.0000 - mse: 3443245953929552330752.0000 - mae: 58417479680.0000 - val_loss: 3544124615257814466560.0000 - val_mse: 3544124896732791177216.0000 - val_mae: 59532521472.0000\n",
      "Epoch 65/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3467621604635394441216.0000 - mse: 3467621405437718429696.0000 - mae: 58482507776.0000 - val_loss: 3795898352425962045440.0000 - val_mse: 3795898352425962045440.0000 - val_mae: 61610872832.0000\n",
      "Epoch 66/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3530119186894706180096.0000 - mse: 3530118983366645645312.0000 - mae: 59047706624.0000 - val_loss: 3450887155122316509184.0000 - val_mse: 3450886592172363087872.0000 - val_mae: 58744246272.0000\n",
      "Epoch 67/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3446464551002090504192.0000 - mse: 3446464057338285260800.0000 - mae: 58409758720.0000 - val_loss: 3838727303407278751744.0000 - val_mse: 3838727303407278751744.0000 - val_mae: 61957468160.0000\n",
      "Epoch 68/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3479648211947616206848.0000 - mse: 3479649112667541471232.0000 - mae: 58752245760.0000 - val_loss: 3285772807883938856960.0000 - val_mse: 3285772526408962146304.0000 - val_mae: 57321668608.0000\n",
      "Epoch 69/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3484497376238702166016.0000 - mse: 3484497519141382520832.0000 - mae: 58791186432.0000 - val_loss: 4256189315291317010432.0000 - val_mse: 4256189596766293721088.0000 - val_mae: 65239474176.0000\n",
      "Epoch 70/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3482244835929308528640.0000 - mse: 3482244593427790430208.0000 - mae: 58717573120.0000 - val_loss: 3042249383758157316096.0000 - val_mse: 3042249383758157316096.0000 - val_mae: 55156588544.0000\n",
      "Epoch 71/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3446949341850055475200.0000 - mse: 3446949038723157721088.0000 - mae: 58373140480.0000 - val_loss: 2802936543808990478336.0000 - val_mse: 2802936543808990478336.0000 - val_mae: 52942745600.0000\n",
      "Epoch 72/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3488100177993681862656.0000 - mse: 3488099835893325496320.0000 - mae: 58760880128.0000 - val_loss: 4656094606978004811776.0000 - val_mse: 4656094606978004811776.0000 - val_mae: 68235563008.0000\n",
      "Epoch 73/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3520781406105779044352.0000 - mse: 3520781051014269632512.0000 - mae: 58984697856.0000 - val_loss: 3528764807253690679296.0000 - val_mse: 3528765088728667389952.0000 - val_mae: 59403407360.0000\n",
      "Epoch 74/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3440768255290711736320.0000 - mse: 3440767848234591715328.0000 - mae: 58408800256.0000 - val_loss: 3345292943509220753408.0000 - val_mse: 3345292662034244042752.0000 - val_mae: 57838530560.0000\n",
      "Epoch 75/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3485510244455666417664.0000 - mse: 3485510547582564171776.0000 - mae: 58787782656.0000 - val_loss: 3685902998077018472448.0000 - val_mse: 3685902435127065051136.0000 - val_mae: 60711636992.0000\n",
      "Epoch 76/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3462349097648951132160.0000 - mse: 3462349660598904553472.0000 - mae: 58582806528.0000 - val_loss: 4165123716076118474752.0000 - val_mse: 4165123716076118474752.0000 - val_mae: 64537759744.0000\n",
      "Epoch 77/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3476841464730616332288.0000 - mse: 3476841118299875966976.0000 - mae: 58568585216.0000 - val_loss: 3875653442651996291072.0000 - val_mse: 3875653442651996291072.0000 - val_mae: 62254739456.0000\n",
      "Epoch 78/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3488595249173873819648.0000 - mse: 3488595513327312961536.0000 - mae: 58735673344.0000 - val_loss: 3010737415690469244928.0000 - val_mse: 3010737134215492534272.0000 - val_mae: 54870192128.0000\n",
      "Epoch 79/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3454612459117544472576.0000 - mse: 3454612194964105330688.0000 - mae: 58523930624.0000 - val_loss: 3667181815901016031232.0000 - val_mse: 3667181815901016031232.0000 - val_mae: 60557234176.0000\n",
      "Epoch 80/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3487708486076859416576.0000 - mse: 3487709148625651105792.0000 - mae: 58766442496.0000 - val_loss: 3498852180003672555520.0000 - val_mse: 3498852180003672555520.0000 - val_mae: 59151093760.0000\n",
      "Epoch 81/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3508250495234129002496.0000 - mse: 3508250348001064648704.0000 - mae: 58952081408.0000 - val_loss: 3286309299189549367296.0000 - val_mse: 3286309299189549367296.0000 - val_mae: 57326350336.0000\n",
      "Epoch 82/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3477807885246038802432.0000 - mse: 3477807421894923649024.0000 - mae: 58621530112.0000 - val_loss: 3504849285857469792256.0000 - val_mse: 3504848722907516370944.0000 - val_mae: 59201773568.0000\n",
      "Epoch 83/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3438435962294456287232.0000 - mse: 3438435828052543930368.0000 - mae: 58344882176.0000 - val_loss: 3454733229204090912768.0000 - val_mse: 3454732666254137491456.0000 - val_mae: 58776969216.0000\n",
      "Epoch 84/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3449443409341387898880.0000 - mse: 3449442907016814133248.0000 - mae: 58415980544.0000 - val_loss: 3621800168355910385664.0000 - val_mse: 3621800449830887096320.0000 - val_mae: 60181397504.0000\n",
      "Epoch 85/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3510237067672836636672.0000 - mse: 3510236998386688458752.0000 - mae: 58928349184.0000 - val_loss: 3117193503682253029376.0000 - val_mse: 3117193222207276318720.0000 - val_mae: 55831826432.0000\n",
      "Epoch 86/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3462198300677966921728.0000 - mse: 3462198508536410931200.0000 - mae: 58598023168.0000 - val_loss: 3384950234452961787904.0000 - val_mse: 3384950234452961787904.0000 - val_mae: 58180321280.0000\n",
      "Epoch 87/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3493502505569768439808.0000 - mse: 3493502466596309827584.0000 - mae: 58817155072.0000 - val_loss: 3326507022088574861312.0000 - val_mse: 3326506740613598150656.0000 - val_mae: 57675874304.0000\n",
      "Epoch 88/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3464375934000480911360.0000 - mse: 3464375717481267855360.0000 - mae: 58617307136.0000 - val_loss: 3011625187767014653952.0000 - val_mse: 3011624906292037943296.0000 - val_mae: 54878277632.0000\n",
      "Epoch 89/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3452089642205129998336.0000 - mse: 3452089334747847720960.0000 - mae: 58435960832.0000 - val_loss: 3485963440820091617280.0000 - val_mse: 3485963159345114906624.0000 - val_mae: 59042041856.0000\n",
      "Epoch 90/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3502444103833398280192.0000 - mse: 3502444082181477236736.0000 - mae: 58946387968.0000 - val_loss: 2942454401090231205888.0000 - val_mse: 2942454119615254495232.0000 - val_mae: 54244405248.0000\n",
      "Epoch 91/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3462006624879594962944.0000 - mse: 3462006261127317553152.0000 - mae: 58566627328.0000 - val_loss: 3805515507955235028992.0000 - val_mse: 3805514945005281607680.0000 - val_mae: 61688864768.0000\n",
      "Epoch 92/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3445981247641117786112.0000 - mse: 3445981046278249775104.0000 - mae: 58329690112.0000 - val_loss: 3312297320839290814464.0000 - val_mse: 3312297602314267525120.0000 - val_mae: 57552539648.0000\n",
      "Epoch 93/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3510079554275868934144.0000 - mse: 3510079372399730491392.0000 - mae: 58936053760.0000 - val_loss: 3619921885836320178176.0000 - val_mse: 3619921604361343467520.0000 - val_mae: 60165799936.0000\n",
      "Epoch 94/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3473178474964847689728.0000 - mse: 3473178565902916911104.0000 - mae: 58663989248.0000 - val_loss: 2936424644139135533056.0000 - val_mse: 2936424644139135533056.0000 - val_mae: 54188789760.0000\n",
      "Epoch 95/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3469753002445195182080.0000 - mse: 3469753859861278359552.0000 - mae: 58611249152.0000 - val_loss: 4293711055586754297856.0000 - val_mse: 4293710774111777587200.0000 - val_mae: 65526427648.0000\n",
      "Epoch 96/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3491930082420640448512.0000 - mse: 3491929865901427392512.0000 - mae: 58794582016.0000 - val_loss: 3272126056588076122112.0000 - val_mse: 3272126056588076122112.0000 - val_mae: 57202491392.0000\n",
      "Epoch 97/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3455011750858738106368.0000 - mse: 3455011607956057751552.0000 - mae: 58490449920.0000 - val_loss: 4314312772082160631808.0000 - val_mse: 4314313053557137342464.0000 - val_mae: 65683415040.0000\n",
      "Epoch 98/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3493524187803743158272.0000 - mse: 3493524421644493258752.0000 - mae: 58789343232.0000 - val_loss: 4000928666111677825024.0000 - val_mse: 4000928666111677825024.0000 - val_mae: 63252897792.0000\n",
      "Epoch 99/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3491678967767955931136.0000 - mse: 3491678227272248066048.0000 - mae: 58845806592.0000 - val_loss: 3329982956575974752256.0000 - val_mse: 3329982956575974752256.0000 - val_mae: 57706016768.0000\n",
      "Epoch 100/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3462650492393244590080.0000 - mse: 3462650838823984955392.0000 - mae: 58548584448.0000 - val_loss: 3244144347678293098496.0000 - val_mse: 3244144347678293098496.0000 - val_mae: 56957382656.0000\n",
      "Epoch 101/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3471674558494666653696.0000 - mse: 3471673800677421744128.0000 - mae: 58626445312.0000 - val_loss: 2968944292623447752704.0000 - val_mse: 2968944292623447752704.0000 - val_mae: 54488023040.0000\n",
      "Epoch 102/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3485450957164802211840.0000 - mse: 3485450874887501512704.0000 - mae: 58789466112.0000 - val_loss: 3925754862606586216448.0000 - val_mse: 3925754862606586216448.0000 - val_mae: 62655827968.0000\n",
      "Epoch 103/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3481878901138048155648.0000 - mse: 3481878394483089866752.0000 - mae: 58757144576.0000 - val_loss: 4020991358026683252736.0000 - val_mse: 4020991076551706542080.0000 - val_mae: 63411281920.0000\n",
      "Epoch 104/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3469443609481179561984.0000 - mse: 3469443392961966505984.0000 - mae: 58633641984.0000 - val_loss: 3587345660881689116672.0000 - val_mse: 3587345660881689116672.0000 - val_mae: 59894448128.0000\n",
      "Epoch 105/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3490879306019889479680.0000 - mse: 3490880527188250066944.0000 - mae: 58836643840.0000 - val_loss: 3099098321854455087104.0000 - val_mse: 3099098321854455087104.0000 - val_mae: 55669551104.0000\n",
      "Epoch 106/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3473020450582538223616.0000 - mse: 3473020376966005522432.0000 - mae: 58456145920.0000 - val_loss: 2838142589471029198848.0000 - val_mse: 2838142589471029198848.0000 - val_mae: 53274218496.0000\n",
      "Epoch 107/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3449206442054071615488.0000 - mse: 3449205905086423760896.0000 - mae: 58473639936.0000 - val_loss: 3491650924199507132416.0000 - val_mse: 3491650642724530421760.0000 - val_mae: 59090190336.0000\n",
      "Epoch 108/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3448446264749661356032.0000 - mse: 3448446204124281700352.0000 - mae: 58388942848.0000 - val_loss: 3019839753487338438656.0000 - val_mse: 3019839472012361728000.0000 - val_mae: 54953070592.0000\n",
      "Epoch 109/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3510022956153629048832.0000 - mse: 3510023077404388360192.0000 - mae: 58989379584.0000 - val_loss: 3302313403415363846144.0000 - val_mse: 3302312840465410424832.0000 - val_mae: 57465761792.0000\n",
      "Epoch 110/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3455697566804686143488.0000 - mse: 3455696718049371488256.0000 - mae: 58394423296.0000 - val_loss: 3328868315668200554496.0000 - val_mse: 3328868315668200554496.0000 - val_mae: 57696333824.0000\n",
      "Epoch 111/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3492862223614283350016.0000 - mse: 3492861829549316374528.0000 - mae: 58829463552.0000 - val_loss: 3212279128514833154048.0000 - val_mse: 3212279128514833154048.0000 - val_mae: 56676974592.0000\n",
      "Epoch 112/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3476569884681933684736.0000 - mse: 3476570057897303605248.0000 - mae: 58713927680.0000 - val_loss: 3291905866151487340544.0000 - val_mse: 3291905866151487340544.0000 - val_mae: 57375145984.0000\n",
      "Epoch 113/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3483458084017001201664.0000 - mse: 3483458595002343489536.0000 - mae: 58749095936.0000 - val_loss: 3935500933675192680448.0000 - val_mse: 3935500933675192680448.0000 - val_mae: 62733553664.0000\n",
      "Epoch 114/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3441706861748051509248.0000 - mse: 3441707130231875174400.0000 - mae: 58253418496.0000 - val_loss: 4004349712978619138048.0000 - val_mse: 4004349431503642427392.0000 - val_mae: 63279943680.0000\n",
      "Epoch 115/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3497609524276490141696.0000 - mse: 3497609467981495009280.0000 - mae: 58858364928.0000 - val_loss: 3875266977508972560384.0000 - val_mse: 3875266696033995849728.0000 - val_mae: 62251642880.0000\n",
      "Epoch 116/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3474525791749139333120.0000 - mse: 3474526549566384242688.0000 - mae: 58708803584.0000 - val_loss: 3009088253801921511424.0000 - val_mse: 3009088253801921511424.0000 - val_mae: 54855155712.0000\n",
      "Epoch 117/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3489027291611202584576.0000 - mse: 3489027295941587107840.0000 - mae: 58805796864.0000 - val_loss: 3594863013084700606464.0000 - val_mse: 3594863013084700606464.0000 - val_mae: 59957178368.0000\n",
      "Epoch 118/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3479018872873148743680.0000 - mse: 3479018890194686312448.0000 - mae: 58708205568.0000 - val_loss: 3054970645330595414016.0000 - val_mse: 3054970645330595414016.0000 - val_mae: 55271755776.0000\n",
      "Epoch 119/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3479613417310110351360.0000 - mse: 3479613083870522507264.0000 - mae: 58710798336.0000 - val_loss: 4036140622748227469312.0000 - val_mse: 4036140622748227469312.0000 - val_mae: 63530602496.0000\n",
      "Epoch 120/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3463783576407569334272.0000 - mse: 3463783494130268635136.0000 - mae: 58545319936.0000 - val_loss: 3570075200735653396480.0000 - val_mse: 3570075200735653396480.0000 - val_mae: 59750092800.0000\n",
      "Epoch 121/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3493534316572520742912.0000 - mse: 3493534273268678131712.0000 - mae: 58860752896.0000 - val_loss: 3072028028919261167616.0000 - val_mse: 3072028028919261167616.0000 - val_mae: 55425888256.0000\n",
      "Epoch 122/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3466560508928958398464.0000 - mse: 3466559681825565835264.0000 - mae: 58637148160.0000 - val_loss: 3709254443619911204864.0000 - val_mse: 3709254443619911204864.0000 - val_mae: 60903649280.0000\n",
      "Epoch 123/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3482814259807194185728.0000 - mse: 3482814017305676087296.0000 - mae: 58781859840.0000 - val_loss: 3487379822902899638272.0000 - val_mse: 3487380104377876348928.0000 - val_mae: 59054047232.0000\n",
      "Epoch 124/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3474195872763744354304.0000 - mse: 3474195816468749221888.0000 - mae: 58696847360.0000 - val_loss: 3796039652864270794752.0000 - val_mse: 3796039371389294084096.0000 - val_mae: 61612007424.0000\n",
      "Epoch 125/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3483687745946075529216.0000 - mse: 3483687434158409252864.0000 - mae: 58791419904.0000 - val_loss: 3355630956453849726976.0000 - val_mse: 3355631237928826437632.0000 - val_mae: 57927798784.0000\n",
      "Epoch 126/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3475942381690391887872.0000 - mse: 3475942368699238842368.0000 - mae: 58726793216.0000 - val_loss: 3524585466799490859008.0000 - val_mse: 3524585748274467569664.0000 - val_mae: 59368226816.0000\n",
      "Epoch 127/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3456895000329071362048.0000 - mse: 3456894675550252040192.0000 - mae: 58411253760.0000 - val_loss: 3571420932599307042816.0000 - val_mse: 3571420932599307042816.0000 - val_mae: 59761364992.0000\n",
      "Epoch 128/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3513575962820037050368.0000 - mse: 3513576698985360392192.0000 - mae: 59039936512.0000 - val_loss: 2961915018030052540416.0000 - val_mse: 2961915018030052540416.0000 - val_mae: 54423465984.0000\n",
      "Epoch 129/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3481261072224936787968.0000 - mse: 3481261964284093530112.0000 - mae: 58606784512.0000 - val_loss: 3947996170841332121600.0000 - val_mse: 3947996170841332121600.0000 - val_mae: 62833078272.0000\n",
      "Epoch 130/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3441673669352720695296.0000 - mse: 3441673071759693185024.0000 - mae: 58373455872.0000 - val_loss: 3964771797978310508544.0000 - val_mse: 3964771235028357087232.0000 - val_mae: 62966460416.0000\n",
      "Epoch 131/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3488932971511699472384.0000 - mse: 3488934127724295880704.0000 - mae: 58810576896.0000 - val_loss: 3601726498916813242368.0000 - val_mse: 3601726498916813242368.0000 - val_mae: 60014379008.0000\n",
      "Epoch 132/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3450899184929782366208.0000 - mse: 3450898695596361646080.0000 - mae: 58511560704.0000 - val_loss: 3502999432310527361024.0000 - val_mse: 3502999150835550650368.0000 - val_mae: 59186135040.0000\n",
      "Epoch 133/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3490429617266712641536.0000 - mse: 3490429604275559596032.0000 - mae: 58750189568.0000 - val_loss: 3181717138493543546880.0000 - val_mse: 3181717138493543546880.0000 - val_mae: 56406708224.0000\n",
      "Epoch 134/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3476860236946371248128.0000 - mse: 3476859414173362159616.0000 - mae: 58531799040.0000 - val_loss: 3594791799915592810496.0000 - val_mse: 3594791518440616099840.0000 - val_mae: 59956576256.0000\n",
      "Epoch 135/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3504505968663183491072.0000 - mse: 3504506167860859502592.0000 - mae: 58972569600.0000 - val_loss: 3879734548339324092416.0000 - val_mse: 3879734266864347381760.0000 - val_mae: 62287511552.0000\n",
      "Epoch 136/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3476408655815273611264.0000 - mse: 3476408772735648399360.0000 - mae: 58718027776.0000 - val_loss: 3180505107243827462144.0000 - val_mse: 3180505107243827462144.0000 - val_mae: 56395960320.0000\n",
      "Epoch 137/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3461413383888295690240.0000 - mse: 3461413756301341622272.0000 - mae: 58560483328.0000 - val_loss: 3101813992429759496192.0000 - val_mse: 3101813992429759496192.0000 - val_mae: 55693930496.0000\n",
      "Epoch 138/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3463993296917140340736.0000 - mse: 3463994037412848205824.0000 - mae: 58587398144.0000 - val_loss: 3371125028021864497152.0000 - val_mse: 3371124746546887786496.0000 - val_mae: 58061398016.0000\n",
      "Epoch 139/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3459829130129376804864.0000 - mse: 3459828770707483918336.0000 - mae: 58586488832.0000 - val_loss: 3424600769997261766656.0000 - val_mse: 3424600488522285056000.0000 - val_mae: 58520104960.0000\n",
      "Epoch 140/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3499298032036397711360.0000 - mse: 3499297754891805523968.0000 - mae: 58880208896.0000 - val_loss: 3463511026352812720128.0000 - val_mse: 3463511026352812720128.0000 - val_mae: 58851594240.0000\n",
      "Epoch 141/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3478824226431177326592.0000 - mse: 3478824109510802538496.0000 - mae: 58649276416.0000 - val_loss: 2686064474304028868608.0000 - val_mse: 2686064192829052157952.0000 - val_mae: 51827261440.0000\n",
      "Epoch 142/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3471846479080057733120.0000 - mse: 3471846344838145376256.0000 - mae: 58692120576.0000 - val_loss: 3488101806218162470912.0000 - val_mse: 3488101243268209049600.0000 - val_mae: 59060137984.0000\n",
      "Epoch 143/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3473640154552032821248.0000 - mse: 3473640184864722386944.0000 - mae: 58699759616.0000 - val_loss: 3644072720313071173632.0000 - val_mse: 3644072720313071173632.0000 - val_mae: 60366131200.0000\n",
      "Epoch 144/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3491627479499139448832.0000 - mse: 3491627280301463437312.0000 - mae: 58819801088.0000 - val_loss: 3579379919040777551872.0000 - val_mse: 3579379919040777551872.0000 - val_mae: 59827941376.0000\n",
      "Epoch 145/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3456297151808921927680.0000 - mse: 3456297948599625449472.0000 - mae: 58516836352.0000 - val_loss: 2698809379774510661632.0000 - val_mse: 2698809379774510661632.0000 - val_mae: 51950075904.0000\n",
      "Epoch 146/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3444837105990592102400.0000 - mse: 3444837413447874379776.0000 - mae: 58316877824.0000 - val_loss: 3636121615170948562944.0000 - val_mse: 3636121896645925273600.0000 - val_mae: 60300247040.0000\n",
      "Epoch 147/180\n",
      "6240/6240 [==============================] - 55s 9ms/step - loss: 3524187279306283548672.0000 - mse: 3524187179707445280768.0000 - mae: 59005882368.0000 - val_loss: 3846422547795571376128.0000 - val_mse: 3846422266320594665472.0000 - val_mae: 62019514368.0000\n",
      "Epoch 148/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3422974307982989328384.0000 - mse: 3422974407581827596288.0000 - mae: 58237468672.0000 - val_loss: 3589179470354959040512.0000 - val_mse: 3589179470354959040512.0000 - val_mae: 59909738496.0000\n",
      "Epoch 149/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3485451753955505733632.0000 - mse: 3485452000787408355328.0000 - mae: 58735251456.0000 - val_loss: 3958439455427250880512.0000 - val_mse: 3958439173952274169856.0000 - val_mae: 62916132864.0000\n",
      "Epoch 150/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3481398531612409331712.0000 - mse: 3481397916697844776960.0000 - mae: 58723524608.0000 - val_loss: 4024692472495451668480.0000 - val_mse: 4024692472495451668480.0000 - val_mae: 63440461824.0000\n",
      "Epoch 151/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3484932242086567084032.0000 - mse: 3484932960930353905664.0000 - mae: 58718552064.0000 - val_loss: 3795798991759183183872.0000 - val_mse: 3795798991759183183872.0000 - val_mae: 61610045440.0000\n",
      "Epoch 152/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3488483386687743852544.0000 - mse: 3488483204811605409792.0000 - mae: 58856640512.0000 - val_loss: 3724299844075049189376.0000 - val_mse: 3724299844075049189376.0000 - val_mae: 61027033088.0000\n",
      "Epoch 153/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3521790571844202921984.0000 - mse: 3521790420280754044928.0000 - mae: 59016687616.0000 - val_loss: 3245680919576156569600.0000 - val_mse: 3245680919576156569600.0000 - val_mae: 56970883072.0000\n",
      "Epoch 154/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3422621043896065458176.0000 - mse: 3422620875011079012352.0000 - mae: 58288816128.0000 - val_loss: 3038953030305898823680.0000 - val_mse: 3038952748830922113024.0000 - val_mae: 55126704128.0000\n",
      "Epoch 155/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3471864216333974503424.0000 - mse: 3471863796286701436928.0000 - mae: 58538233856.0000 - val_loss: 3062034541346126036992.0000 - val_mse: 3062034541346126036992.0000 - val_mae: 55335665664.0000\n",
      "Epoch 156/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3493567712495911567360.0000 - mse: 3493567487315929989120.0000 - mae: 58859040768.0000 - val_loss: 3482542956903103725568.0000 - val_mse: 3482543238378080436224.0000 - val_mae: 59013066752.0000\n",
      "Epoch 157/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3487777191953482383360.0000 - mse: 3487776702620061663232.0000 - mae: 58781347840.0000 - val_loss: 2513167625434578288640.0000 - val_mse: 2513167625434578288640.0000 - val_mae: 50131505152.0000\n",
      "Epoch 158/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3415249768545197752320.0000 - mse: 3415249889795957063680.0000 - mae: 58077851648.0000 - val_loss: 3813417354976433274880.0000 - val_mse: 3813417354976433274880.0000 - val_mae: 61752872960.0000\n",
      "Epoch 159/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3490238885492109082624.0000 - mse: 3490238201291396349952.0000 - mae: 58781040640.0000 - val_loss: 3352610167003790966784.0000 - val_mse: 3352610448478767677440.0000 - val_mae: 57901715456.0000\n",
      "Epoch 160/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3451591842882856681472.0000 - mse: 3451591405514046570496.0000 - mae: 58478858240.0000 - val_loss: 4239929350311672545280.0000 - val_mse: 4239929068836695834624.0000 - val_mae: 65114738688.0000\n",
      "Epoch 161/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3502313347880755658752.0000 - mse: 3502314040742236913664.0000 - mae: 58800861184.0000 - val_loss: 3250710595934999281664.0000 - val_mse: 3250710595934999281664.0000 - val_mae: 57015005184.0000\n",
      "Epoch 162/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3480602230242526363648.0000 - mse: 3480601905463707041792.0000 - mae: 58726268928.0000 - val_loss: 3334457282805767340032.0000 - val_mse: 3334457001330790629376.0000 - val_mae: 57744769024.0000\n",
      "Epoch 163/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3479957734823159660544.0000 - mse: 3479957609242016350208.0000 - mae: 58696364032.0000 - val_loss: 3479142739184439001088.0000 - val_mse: 3479142176234485579776.0000 - val_mae: 58984239104.0000\n",
      "Epoch 164/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3442788544101244469248.0000 - mse: 3442787712667467382784.0000 - mae: 58358157312.0000 - val_loss: 3124007449918464589824.0000 - val_mse: 3124007731393441300480.0000 - val_mae: 55892819968.0000\n",
      "Epoch 165/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3479817391399771832320.0000 - mse: 3479817997653567864832.0000 - mae: 58711355392.0000 - val_loss: 3791490172815696461824.0000 - val_mse: 3791490172815696461824.0000 - val_mae: 61575053312.0000\n",
      "Epoch 166/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3494176512557846626304.0000 - mse: 3494177443590461980672.0000 - mae: 58832207872.0000 - val_loss: 3683258822145798569984.0000 - val_mse: 3683259103620775280640.0000 - val_mae: 60689870848.0000\n",
      "Epoch 167/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3493328030057665396736.0000 - mse: 3493328233585725931520.0000 - mae: 58765725696.0000 - val_loss: 3169525049877321482240.0000 - val_mse: 3169525049877321482240.0000 - val_mae: 56298524672.0000\n",
      "Epoch 168/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3455095387900279259136.0000 - mse: 3455095768974094237696.0000 - mae: 58535796736.0000 - val_loss: 4162956077280469712896.0000 - val_mse: 4162956077280469712896.0000 - val_mae: 64520974336.0000\n",
      "Epoch 169/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3503175414796351176704.0000 - mse: 3503175635645948231680.0000 - mae: 58941935616.0000 - val_loss: 3371561032760789303296.0000 - val_mse: 3371560751285812592640.0000 - val_mae: 58065154048.0000\n",
      "Epoch 170/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3476576847939818749952.0000 - mse: 3476576813296744660992.0000 - mae: 58719309824.0000 - val_loss: 2822117374621961420800.0000 - val_mse: 2822117374621961420800.0000 - val_mae: 53123608576.0000\n",
      "Epoch 171/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3466012598400062586880.0000 - mse: 3466012494470840320000.0000 - mae: 58531827712.0000 - val_loss: 4096312654419571245056.0000 - val_mse: 4096312372944594534400.0000 - val_mae: 64002453504.0000\n",
      "Epoch 172/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3462297189332861583360.0000 - mse: 3462296743303282950144.0000 - mae: 58481913856.0000 - val_loss: 3514803648408842141696.0000 - val_mse: 3514803929883818852352.0000 - val_mae: 59285782528.0000\n",
      "Epoch 173/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3492317344354367700992.0000 - mse: 3492317175469381255168.0000 - mae: 58867310592.0000 - val_loss: 3734327108645389598720.0000 - val_mse: 3734327390120366309376.0000 - val_mae: 61109133312.0000\n",
      "Epoch 174/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3465047524634143490048.0000 - mse: 3465047879725652901888.0000 - mae: 58594209792.0000 - val_loss: 4028740927085481033728.0000 - val_mse: 4028740364135527612416.0000 - val_mae: 63472398336.0000\n",
      "Epoch 175/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3473230738372446650368.0000 - mse: 3473230357298631671808.0000 - mae: 58677317632.0000 - val_loss: 3562864656257256521728.0000 - val_mse: 3562864093307303100416.0000 - val_mae: 59689746432.0000\n",
      "Epoch 176/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3517393166229969043456.0000 - mse: 3517393781144533598208.0000 - mae: 59073740800.0000 - val_loss: 3114572408699123400704.0000 - val_mse: 3114572127224146690048.0000 - val_mae: 55808356352.0000\n",
      "Epoch 177/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3462367125038613659648.0000 - mse: 3462366549097507192832.0000 - mae: 58567733248.0000 - val_loss: 3104348111645085532160.0000 - val_mse: 3104348111645085532160.0000 - val_mae: 55716659200.0000\n",
      "Epoch 178/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3475543410397633052672.0000 - mse: 3475544363082169974784.0000 - mae: 58744119296.0000 - val_loss: 3096706347502367932416.0000 - val_mse: 3096706347502367932416.0000 - val_mae: 55648055296.0000\n",
      "Epoch 179/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3464609294077711089664.0000 - mse: 3464609623186914410496.0000 - mae: 58580844544.0000 - val_loss: 2886063985730995093504.0000 - val_mse: 2886063985730995093504.0000 - val_mae: 53722091520.0000\n",
      "Epoch 180/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3480860126276957503488.0000 - mse: 3480860299492327424000.0000 - mae: 58709139456.0000 - val_loss: 3405365333038808956928.0000 - val_mse: 3405365051563832246272.0000 - val_mae: 58355486720.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6240 samples, validate on 960 samples\n",
      "Epoch 1/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 5226266278588511682560.0000 - mse: 5226265800926828691456.0000 - mae: 48009019392.0000 - val_loss: 4096457051082623811584.0000 - val_mse: 4096456488132670390272.0000 - val_mae: 64003592192.0000\n",
      "Epoch 2/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4079448613827216146432.0000 - mse: 4079448925614882422784.0000 - mae: 63486201856.0000 - val_loss: 3599397293484532563968.0000 - val_mse: 3599397293484532563968.0000 - val_mae: 59994972160.0000\n",
      "Epoch 3/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4003482514557679173632.0000 - mse: 4003481925625420185600.0000 - mae: 62883053568.0000 - val_loss: 4661336796944264069120.0000 - val_mse: 4661336796944264069120.0000 - val_mae: 68273967104.0000\n",
      "Epoch 4/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4039719174351367438336.0000 - mse: 4039719858552080171008.0000 - mae: 63227363328.0000 - val_loss: 4488870041839227371520.0000 - val_mse: 4488869760364250660864.0000 - val_mae: 66999025664.0000\n",
      "Epoch 5/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3992926276228798742528.0000 - mse: 3992925488098863742976.0000 - mae: 62872825856.0000 - val_loss: 4378612915761942888448.0000 - val_mse: 4378612634286966177792.0000 - val_mae: 66171047936.0000\n",
      "Epoch 6/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3990425080924979331072.0000 - mse: 3990425708830696407040.0000 - mae: 62858129408.0000 - val_loss: 3665098619598380466176.0000 - val_mse: 3665098338123403755520.0000 - val_mae: 60540059648.0000\n",
      "Epoch 7/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4035273926651861270528.0000 - mse: 4035273961294935359488.0000 - mae: 63265890304.0000 - val_loss: 3511479991883842715648.0000 - val_mse: 3511480273358819426304.0000 - val_mae: 59257741312.0000\n",
      "Epoch 8/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3992257829454106066944.0000 - mse: 3992256703554199224320.0000 - mae: 62850379776.0000 - val_loss: 4152597798137517572096.0000 - val_mse: 4152597798137517572096.0000 - val_mae: 64440635392.0000\n",
      "Epoch 9/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4055068433539441098752.0000 - mse: 4055068689032112242688.0000 - mae: 63447547904.0000 - val_loss: 3377366172680469872640.0000 - val_mse: 3377366172680469872640.0000 - val_mae: 58115100672.0000\n",
      "Epoch 10/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3989320798296347770880.0000 - mse: 3989320919547107082240.0000 - mae: 62917750784.0000 - val_loss: 4350660480249737773056.0000 - val_mse: 4350660480249737773056.0000 - val_mae: 65959538688.0000\n",
      "Epoch 11/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4021746122350772355072.0000 - mse: 4021745992439244521472.0000 - mae: 63161487360.0000 - val_loss: 3928795918254968143872.0000 - val_mse: 3928795918254968143872.0000 - val_mae: 62680076288.0000\n",
      "Epoch 12/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3994326324102189023232.0000 - mse: 3994325826107999256576.0000 - mae: 62879391744.0000 - val_loss: 4881332290716755296256.0000 - val_mse: 4881331727766801874944.0000 - val_mae: 69866528768.0000\n",
      "Epoch 13/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3988578384228159848448.0000 - mse: 3988578388558544371712.0000 - mae: 62816116736.0000 - val_loss: 4707117857481322135552.0000 - val_mse: 4707117576006345424896.0000 - val_mae: 68608454656.0000\n",
      "Epoch 14/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4025233857135273050112.0000 - mse: 4025234311825619681280.0000 - mae: 63136542720.0000 - val_loss: 4213403148506460323840.0000 - val_mse: 4213402867031483613184.0000 - val_mae: 64910741504.0000\n",
      "Epoch 15/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4051522576337442177024.0000 - mse: 4051522385800534687744.0000 - mae: 63418896384.0000 - val_loss: 4263872737730587787264.0000 - val_mse: 4263872737730587787264.0000 - val_mae: 65298325504.0000\n",
      "Epoch 16/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3993623961757995237376.0000 - mse: 3993623546041106169856.0000 - mae: 62875688960.0000 - val_loss: 4417428596525319061504.0000 - val_mse: 4417428033575365640192.0000 - val_mae: 66463731712.0000\n",
      "Epoch 17/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4040991662095697182720.0000 - mse: 4040991281021882204160.0000 - mae: 63243849728.0000 - val_loss: 4102000982223916892160.0000 - val_mse: 4102001263698893602816.0000 - val_mae: 64046886912.0000\n",
      "Epoch 18/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4011227696937224372224.0000 - mse: 4011226709609613885440.0000 - mae: 63131389952.0000 - val_loss: 4342251133845530214400.0000 - val_mse: 4342250852370553503744.0000 - val_mae: 65895751680.0000\n",
      "Epoch 19/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4061885718501914574848.0000 - mse: 4061884605593160777728.0000 - mae: 63431614464.0000 - val_loss: 3520301417653954674688.0000 - val_mse: 3520301417653954674688.0000 - val_mae: 59332104192.0000\n",
      "Epoch 20/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3990122300457723363328.0000 - mse: 3990122841755755741184.0000 - mae: 62850408448.0000 - val_loss: 3373484069801676505088.0000 - val_mse: 3373484069801676505088.0000 - val_mae: 58081677312.0000\n",
      "Epoch 21/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3987414372609471021056.0000 - mse: 3987414208054869098496.0000 - mae: 62844772352.0000 - val_loss: 4200399286057404727296.0000 - val_mse: 4200399286057404727296.0000 - val_mae: 64810512384.0000\n",
      "Epoch 22/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3990087258988315410432.0000 - mse: 3990086812958736777216.0000 - mae: 62908751872.0000 - val_loss: 3109830118291502268416.0000 - val_mse: 3109829836816525557760.0000 - val_mae: 55765848064.0000\n",
      "Epoch 23/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4027451282358725509120.0000 - mse: 4027451490217169518592.0000 - mae: 63191662592.0000 - val_loss: 3987195220522988208128.0000 - val_mse: 3987195501997964918784.0000 - val_mae: 63144239104.0000\n",
      "Epoch 24/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4013022433353342648320.0000 - mse: 4013022238486051160064.0000 - mae: 63064834048.0000 - val_loss: 4220924159884169052160.0000 - val_mse: 4220923878409192341504.0000 - val_mae: 64968634368.0000\n",
      "Epoch 25/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3982951223101445636096.0000 - mse: 3982950296399214804992.0000 - mae: 62839255040.0000 - val_loss: 3517704810993798873088.0000 - val_mse: 3517704529518822162432.0000 - val_mae: 59310223360.0000\n",
      "Epoch 26/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4033252953640615280640.0000 - mse: 4033253533912106270720.0000 - mae: 63092666368.0000 - val_loss: 3483019775513651576832.0000 - val_mse: 3483020056988628287488.0000 - val_mae: 59017129984.0000\n",
      "Epoch 27/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4053955161033091842048.0000 - mse: 4053955455499221598208.0000 - mae: 63381979136.0000 - val_loss: 3655279927985782652928.0000 - val_mse: 3655280209460759363584.0000 - val_mae: 60458921984.0000\n",
      "Epoch 28/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3971146725528154144768.0000 - mse: 3971145798825923313664.0000 - mae: 62700290048.0000 - val_loss: 3764972133784808849408.0000 - val_mse: 3764972415259785560064.0000 - val_mae: 61359386624.0000\n",
      "Epoch 29/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4009304387082128982016.0000 - mse: 4009304798468633526272.0000 - mae: 63083827200.0000 - val_loss: 4053040943299888676864.0000 - val_mse: 4053041224774865387520.0000 - val_mae: 63663501312.0000\n",
      "Epoch 30/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4024619081143062888448.0000 - mse: 4024619007526530187264.0000 - mae: 63154311168.0000 - val_loss: 3431894912543741706240.0000 - val_mse: 3431895194018718416896.0000 - val_mae: 58582372352.0000\n",
      "Epoch 31/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3991278300865537507328.0000 - mse: 3991278296535152984064.0000 - mae: 62758903808.0000 - val_loss: 4777487727558892978176.0000 - val_mse: 4777487727558892978176.0000 - val_mae: 69119369216.0000\n",
      "Epoch 32/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4053758496632056840192.0000 - mse: 4053758985965477560320.0000 - mae: 63391383552.0000 - val_loss: 3286439903578743111680.0000 - val_mse: 3286439622103766401024.0000 - val_mae: 57327489024.0000\n",
      "Epoch 33/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4010159183282557157376.0000 - mse: 4010158230598020235264.0000 - mae: 63047389184.0000 - val_loss: 4178970596080422486016.0000 - val_mse: 4178970877555399196672.0000 - val_mae: 64644968448.0000\n",
      "Epoch 34/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3971697528753734811648.0000 - mse: 3971696082405392646144.0000 - mae: 62763991040.0000 - val_loss: 3858647850459045298176.0000 - val_mse: 3858647850459045298176.0000 - val_mae: 62118031360.0000\n",
      "Epoch 35/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4035021123149316489216.0000 - mse: 4035020633815895769088.0000 - mae: 63127588864.0000 - val_loss: 3592629227669524840448.0000 - val_mse: 3592629227669524840448.0000 - val_mae: 59938566144.0000\n",
      "Epoch 36/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3983181391685478776832.0000 - mse: 3983181105880117542912.0000 - mae: 62822989824.0000 - val_loss: 5110021139144955133952.0000 - val_mse: 5110020576195001712640.0000 - val_mae: 71484432384.0000\n",
      "Epoch 37/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4029125629762111799296.0000 - mse: 4029125984853621211136.0000 - mae: 63205998592.0000 - val_loss: 4674456908558701166592.0000 - val_mse: 4674456908558701166592.0000 - val_mae: 68369997824.0000\n",
      "Epoch 38/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4019952143751898988544.0000 - mse: 4019951589462714089472.0000 - mae: 63186857984.0000 - val_loss: 3984413121853180084224.0000 - val_mse: 3984412840378203373568.0000 - val_mae: 63122182144.0000\n",
      "Epoch 39/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4025657814745194430464.0000 - mse: 4025658213140545929216.0000 - mae: 63187668992.0000 - val_loss: 3369287559373897334784.0000 - val_mse: 3369287277898920624128.0000 - val_mae: 58045579264.0000\n",
      "Epoch 40/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4005148984992102612992.0000 - mse: 4005149664862430822400.0000 - mae: 63005835264.0000 - val_loss: 4587186154979515695104.0000 - val_mse: 4587186154979515695104.0000 - val_mae: 67728789504.0000\n",
      "Epoch 41/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4022495378426086555648.0000 - mse: 4022496123252178419712.0000 - mae: 63041351680.0000 - val_loss: 3739445731096872878080.0000 - val_mse: 3739446012571849588736.0000 - val_mae: 61150990336.0000\n",
      "Epoch 42/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4022358334755502555136.0000 - mse: 4022358481988566908928.0000 - mae: 63079129088.0000 - val_loss: 3515618799941396201472.0000 - val_mse: 3515618236991442780160.0000 - val_mae: 59292651520.0000\n",
      "Epoch 43/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3988500952627259375616.0000 - mse: 3988499294090088677376.0000 - mae: 62889578496.0000 - val_loss: 4233944910831827288064.0000 - val_mse: 4233944910831827288064.0000 - val_mae: 65068769280.0000\n",
      "Epoch 44/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4016330617415392034816.0000 - mse: 4016331258312261632000.0000 - mae: 63092703232.0000 - val_loss: 4526997516284545990656.0000 - val_mse: 4526997234809569280000.0000 - val_mae: 67282931712.0000\n",
      "Epoch 45/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3991569471572599635968.0000 - mse: 3991570186086001934336.0000 - mae: 62919032832.0000 - val_loss: 4727707752027706621952.0000 - val_mse: 4727707189077753200640.0000 - val_mae: 68758315008.0000\n",
      "Epoch 46/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4000783173861407981568.0000 - mse: 4000783143548718415872.0000 - mae: 62872416256.0000 - val_loss: 4041161573382792151040.0000 - val_mse: 4041161291907815440384.0000 - val_mae: 63570137088.0000\n",
      "Epoch 47/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 4088799121615474393088.0000 - mse: 4088799242866233704448.0000 - mae: 63665922048.0000 - val_loss: 3703820569194511990784.0000 - val_mse: 3703820287719535280128.0000 - val_mae: 60859006976.0000\n",
      "Epoch 48/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4019162745014521954304.0000 - mse: 4019162333628017410048.0000 - mae: 63005413376.0000 - val_loss: 3958007109863023312896.0000 - val_mse: 3958006828388046602240.0000 - val_mae: 62912720896.0000\n",
      "Epoch 49/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3987711714114099216384.0000 - mse: 3987712008580228972544.0000 - mae: 62846877696.0000 - val_loss: 3537412000013218742272.0000 - val_mse: 3537412000013218742272.0000 - val_mae: 59476148224.0000\n",
      "Epoch 50/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4003156380328125267968.0000 - mse: 4003157103502296088576.0000 - mae: 63043186688.0000 - val_loss: 3945432215278474756096.0000 - val_mse: 3945432215278474756096.0000 - val_mae: 62812672000.0000\n",
      "Epoch 51/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4016938261264730161152.0000 - mse: 4016937836887073095680.0000 - mae: 63152369664.0000 - val_loss: 4204902604209798512640.0000 - val_mse: 4204902604209798512640.0000 - val_mae: 64845209600.0000\n",
      "Epoch 52/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4027899238958199996416.0000 - mse: 4027899316905116172288.0000 - mae: 63154184192.0000 - val_loss: 4429164695679269863424.0000 - val_mse: 4429164414204293152768.0000 - val_mae: 66551943168.0000\n",
      "Epoch 53/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3989585839464802942976.0000 - mse: 3989585787500191809536.0000 - mae: 62868230144.0000 - val_loss: 3642301398284631015424.0000 - val_mse: 3642301116809654304768.0000 - val_mae: 60351488000.0000\n",
      "Epoch 54/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3999894453430710173696.0000 - mse: 3999893401147336032256.0000 - mae: 62974246912.0000 - val_loss: 4066125589067260231680.0000 - val_mse: 4066125589067260231680.0000 - val_mae: 63766175744.0000\n",
      "Epoch 55/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3998776205312850395136.0000 - mse: 3998776226964771438592.0000 - mae: 62914469888.0000 - val_loss: 4158084308383561678848.0000 - val_mse: 4158083745433608257536.0000 - val_mae: 64483225600.0000\n",
      "Epoch 56/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4011261222772142505984.0000 - mse: 4011261049556772585472.0000 - mae: 63101702144.0000 - val_loss: 4468778639476597456896.0000 - val_mse: 4468778076526644035584.0000 - val_mae: 66848944128.0000\n",
      "Epoch 57/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3994937748376983699456.0000 - mse: 3994937189757414801408.0000 - mae: 62930104320.0000 - val_loss: 4027382810322852118528.0000 - val_mse: 4027382810322852118528.0000 - val_mae: 63461658624.0000\n",
      "Epoch 58/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4036452986355843596288.0000 - mse: 4036453341447353008128.0000 - mae: 63164907520.0000 - val_loss: 4117277192159957614592.0000 - val_mse: 4117277473634934325248.0000 - val_mae: 64166031360.0000\n",
      "Epoch 59/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4014089821108103544832.0000 - mse: 4014089028647784546304.0000 - mae: 63008018432.0000 - val_loss: 3458394092751189704704.0000 - val_mse: 3458394374226166415360.0000 - val_mae: 58808078336.0000\n",
      "Epoch 60/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4039519032651773116416.0000 - mse: 4039519166893685473280.0000 - mae: 63295401984.0000 - val_loss: 2935991735624954544128.0000 - val_mse: 2935991454149977833472.0000 - val_mae: 54184783872.0000\n",
      "Epoch 61/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4008565190489444712448.0000 - mse: 4008565363704814632960.0000 - mae: 63005200384.0000 - val_loss: 4352036329935899459584.0000 - val_mse: 4352035766985946038272.0000 - val_mae: 65969975296.0000\n",
      "Epoch 62/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4000135469627307196416.0000 - mse: 4000135469627307196416.0000 - mae: 62961577984.0000 - val_loss: 3523889660657062117376.0000 - val_mse: 3523889660657062117376.0000 - val_mae: 59362373632.0000\n",
      "Epoch 63/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4037760359675748417536.0000 - mse: 4037760229764220583936.0000 - mae: 63299530752.0000 - val_loss: 4302313212350008655872.0000 - val_mse: 4302312930875031945216.0000 - val_mae: 65592012800.0000\n",
      "Epoch 64/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3994458877164298174464.0000 - mse: 3994458682297006686208.0000 - mae: 62883704832.0000 - val_loss: 4280299617371421671424.0000 - val_mse: 4280299335896444960768.0000 - val_mae: 65424019456.0000\n",
      "Epoch 65/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4018517768922503053312.0000 - mse: 4018517192981396586496.0000 - mae: 63094386688.0000 - val_loss: 3591879096856590942208.0000 - val_mse: 3591878815381614231552.0000 - val_mae: 59932282880.0000\n",
      "Epoch 66/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3984397852918289530880.0000 - mse: 3984397922204437708800.0000 - mae: 62843944960.0000 - val_loss: 5009272800730910031872.0000 - val_mse: 5009272237780956610560.0000 - val_mae: 70776217600.0000\n",
      "Epoch 67/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4031319774839798497280.0000 - mse: 4031319800822104064000.0000 - mae: 63083659264.0000 - val_loss: 3540058427744252329984.0000 - val_mse: 3540057864794298908672.0000 - val_mae: 59498389504.0000\n",
      "Epoch 68/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4010721067961451282432.0000 - mse: 4010720054651534704640.0000 - mae: 63076573184.0000 - val_loss: 3564304963713084948480.0000 - val_mse: 3564304963713084948480.0000 - val_mae: 59701805056.0000\n",
      "Epoch 69/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4008180535447040032768.0000 - mse: 4008181150361604587520.0000 - mae: 63033995264.0000 - val_loss: 4128687343290877476864.0000 - val_mse: 4128687343290877476864.0000 - val_mae: 64254898176.0000\n",
      "Epoch 70/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4029879129613998227456.0000 - mse: 4029878085991392083968.0000 - mae: 63186743296.0000 - val_loss: 3532638184408206016512.0000 - val_mse: 3532638184408206016512.0000 - val_mae: 59435999232.0000\n",
      "Epoch 71/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3990466799846911639552.0000 - mse: 3990466522702319452160.0000 - mae: 62921998336.0000 - val_loss: 4518308946703441461248.0000 - val_mse: 4518308665228464750592.0000 - val_mae: 67218354176.0000\n",
      "Epoch 72/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4009011826321720344576.0000 - mse: 4009012064492854444032.0000 - mae: 62992556032.0000 - val_loss: 3341802372323031908352.0000 - val_mse: 3341802653798008619008.0000 - val_mae: 57808338944.0000\n",
      "Epoch 73/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4033656974161417863168.0000 - mse: 4033655761653825798144.0000 - mae: 63261155328.0000 - val_loss: 4181171730398299815936.0000 - val_mse: 4181171730398299815936.0000 - val_mae: 64662007808.0000\n",
      "Epoch 74/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3980800524769011171328.0000 - mse: 3980800672002075525120.0000 - mae: 62830075904.0000 - val_loss: 3872006371378756321280.0000 - val_mse: 3872006089903779610624.0000 - val_mae: 62225432576.0000\n",
      "Epoch 75/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 4020836767639089446912.0000 - mse: 4020837109739445813248.0000 - mae: 63121055744.0000 - val_loss: 3944014707295759892480.0000 - val_mse: 3944014988770736603136.0000 - val_mae: 62801399808.0000\n",
      "Epoch 76/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4009137606662851919872.0000 - mse: 4009137039382513975296.0000 - mae: 63056252928.0000 - val_loss: 4039338178483660521472.0000 - val_mse: 4039338178483660521472.0000 - val_mae: 63555768320.0000\n",
      "Epoch 77/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3956674473081362710528.0000 - mse: 3956674607323275067392.0000 - mae: 62663749632.0000 - val_loss: 3986695039489373372416.0000 - val_mse: 3986694758014396661760.0000 - val_mae: 63140270080.0000\n",
      "Epoch 78/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4051714944997294342144.0000 - mse: 4051713788784697933824.0000 - mae: 63408943104.0000 - val_loss: 4310518489396100988928.0000 - val_mse: 4310518207921124278272.0000 - val_mae: 65654530048.0000\n",
      "Epoch 79/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 4015932213402971144192.0000 - mse: 4015932689745239343104.0000 - mae: 63100981248.0000 - val_loss: 3474503468618293968896.0000 - val_mse: 3474503468618293968896.0000 - val_mae: 58944917504.0000\n",
      "Epoch 80/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4017742664783558082560.0000 - mse: 4017743418270418993152.0000 - mae: 63149469696.0000 - val_loss: 3637963868893519806464.0000 - val_mse: 3637963587418543095808.0000 - val_mae: 60315537408.0000\n",
      "Epoch 81/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4016026221714807980032.0000 - mse: 4016026139437507280896.0000 - mae: 63099338752.0000 - val_loss: 3676217444128404799488.0000 - val_mse: 3676217162653428088832.0000 - val_mae: 60631814144.0000\n",
      "Epoch 82/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3988534071406057488384.0000 - mse: 3988533915512224088064.0000 - mae: 62931165184.0000 - val_loss: 3835013804039535067136.0000 - val_mse: 3835013522564558356480.0000 - val_mae: 61927497728.0000\n",
      "Epoch 83/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4009853999452038627328.0000 - mse: 4009853956148196016128.0000 - mae: 63035867136.0000 - val_loss: 3992890866676728332288.0000 - val_mse: 3992891148151705042944.0000 - val_mae: 63189291008.0000\n",
      "Epoch 84/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4033821061081687654400.0000 - mse: 4033819861565248110592.0000 - mae: 63244017664.0000 - val_loss: 4351476757682198675456.0000 - val_mse: 4351476757682198675456.0000 - val_mae: 65965699072.0000\n",
      "Epoch 85/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4013369864412673343488.0000 - mse: 4013370704507218952192.0000 - mae: 63086473216.0000 - val_loss: 3338986496656018505728.0000 - val_mse: 3338986496656018505728.0000 - val_mae: 57783959552.0000\n",
      "Epoch 86/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4014050081171776208896.0000 - mse: 4014050185100998475776.0000 - mae: 63131930624.0000 - val_loss: 4260111106141826580480.0000 - val_mse: 4260111106141826580480.0000 - val_mae: 65269534720.0000\n",
      "Epoch 87/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4025190973339974762496.0000 - mse: 4025190401729252818944.0000 - mae: 63190204416.0000 - val_loss: 3998116449619361660928.0000 - val_mse: 3998115886669408239616.0000 - val_mae: 63230664704.0000\n",
      "Epoch 88/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3995840559878054084608.0000 - mse: 3995840442957679296512.0000 - mae: 62864834560.0000 - val_loss: 3883486609778877136896.0000 - val_mse: 3883486609778877136896.0000 - val_mae: 62317654016.0000\n",
      "Epoch 89/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4012636457543740162048.0000 - mse: 4012636617767957561344.0000 - mae: 63107153920.0000 - val_loss: 3557379271911119257600.0000 - val_mse: 3557378708961165836288.0000 - val_mae: 59643785216.0000\n",
      "Epoch 90/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4044842941299348209664.0000 - mse: 4044842984603190820864.0000 - mae: 63054352384.0000 - val_loss: 3519823191668523270144.0000 - val_mse: 3519823473143499980800.0000 - val_mae: 59328086016.0000\n",
      "Epoch 91/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3978682711374624194560.0000 - mse: 3978681728377397706752.0000 - mae: 62824738816.0000 - val_loss: 4391505595595197775872.0000 - val_mse: 4391505314120221065216.0000 - val_mae: 66268434432.0000\n",
      "Epoch 92/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4013380460862950080512.0000 - mse: 4013379711706473693184.0000 - mae: 63081181184.0000 - val_loss: 4373859647830230040576.0000 - val_mse: 4373859647830230040576.0000 - val_mae: 66135134208.0000\n",
      "Epoch 93/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4048486084914067275776.0000 - mse: 4048486115226756841472.0000 - mae: 63348277248.0000 - val_loss: 3285660217893254594560.0000 - val_mse: 3285660217893254594560.0000 - val_mae: 57320673280.0000\n",
      "Epoch 94/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3952059010967863296000.0000 - mse: 3952057573280290177024.0000 - mae: 62547709952.0000 - val_loss: 4541870935528913764352.0000 - val_mse: 4541870935528913764352.0000 - val_mae: 67393417216.0000\n",
      "Epoch 95/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4035342615206947192832.0000 - mse: 4035343204139206180864.0000 - mae: 63229972480.0000 - val_loss: 4048120197757032988672.0000 - val_mse: 4048120197757032988672.0000 - val_mae: 63624835072.0000\n",
      "Epoch 96/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3992499854630234619904.0000 - mse: 3992499053509147099136.0000 - mae: 62829998080.0000 - val_loss: 4088223345063883702272.0000 - val_mse: 4088223345063883702272.0000 - val_mae: 63939231744.0000\n",
      "Epoch 97/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4032743319378168119296.0000 - mse: 4032744345679236694016.0000 - mae: 63101546496.0000 - val_loss: 4163398274468882153472.0000 - val_mse: 4163397992993905442816.0000 - val_mae: 64524390400.0000\n",
      "Epoch 98/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3984635954766281179136.0000 - mse: 3984636894459665055744.0000 - mae: 62811160576.0000 - val_loss: 3482810921080932270080.0000 - val_mse: 3482810639605955559424.0000 - val_mae: 59015348224.0000\n",
      "Epoch 99/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4039544317765450989568.0000 - mse: 4039544781116566142976.0000 - mae: 63269715968.0000 - val_loss: 4086557013201756618752.0000 - val_mse: 4086557013201756618752.0000 - val_mae: 63926177792.0000\n",
      "Epoch 100/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3970073578362331725824.0000 - mse: 3970073097689679003648.0000 - mae: 62654980096.0000 - val_loss: 3899492402854551879680.0000 - val_mse: 3899492402854551879680.0000 - val_mae: 62445891584.0000\n",
      "Epoch 101/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4025798045578591469568.0000 - mse: 4025798387678947835904.0000 - mae: 63123705856.0000 - val_loss: 4262207531768367546368.0000 - val_mse: 4262206968818414125056.0000 - val_mae: 65285607424.0000\n",
      "Epoch 102/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4027481846210811658240.0000 - mse: 4027481608039677558784.0000 - mae: 63190388736.0000 - val_loss: 5110928614469870288896.0000 - val_mse: 5110928614469870288896.0000 - val_mae: 71490740224.0000\n",
      "Epoch 103/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4023489439784221802496.0000 - mse: 4023489166970013614080.0000 - mae: 63124676608.0000 - val_loss: 3995413445418009231360.0000 - val_mse: 3995413445418009231360.0000 - val_mae: 63209295872.0000\n",
      "Epoch 104/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4042548058492689186816.0000 - mse: 4042548400593045553152.0000 - mae: 63197868032.0000 - val_loss: 4530699475178244538368.0000 - val_mse: 4530699193703267827712.0000 - val_mae: 67310456832.0000\n",
      "Epoch 105/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3956005346436342349824.0000 - mse: 3956005259828657127424.0000 - mae: 62643638272.0000 - val_loss: 5097940233144533778432.0000 - val_mse: 5097940233144533778432.0000 - val_mae: 71399890944.0000\n",
      "Epoch 106/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4026084439871818563584.0000 - mse: 4026083240355379019776.0000 - mae: 63147532288.0000 - val_loss: 4087299262715342618624.0000 - val_mse: 4087299262715342618624.0000 - val_mae: 63931985920.0000\n",
      "Epoch 107/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4026763959439055650816.0000 - mse: 4026764128324042096640.0000 - mae: 63216619520.0000 - val_loss: 3533311191077521195008.0000 - val_mse: 3533311191077521195008.0000 - val_mae: 59441672192.0000\n",
      "Epoch 108/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3999506728145868292096.0000 - mse: 3999506091579382169600.0000 - mae: 62963761152.0000 - val_loss: 3851884569718641655808.0000 - val_mse: 3851884006768688234496.0000 - val_mae: 62063554560.0000\n",
      "Epoch 109/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3961860086577303126016.0000 - mse: 3961859939344238772224.0000 - mae: 62591234048.0000 - val_loss: 3431293119043534323712.0000 - val_mse: 3431292837568557613056.0000 - val_mae: 58577252352.0000\n",
      "Epoch 110/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4038365911949490782208.0000 - mse: 4038365963914101915648.0000 - mae: 63012753408.0000 - val_loss: 3602402320335895527424.0000 - val_mse: 3602402320335895527424.0000 - val_mae: 60020015104.0000\n",
      "Epoch 111/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4034829178867121389568.0000 - mse: 4034829512306709233664.0000 - mae: 63242387456.0000 - val_loss: 4015349192118518153216.0000 - val_mse: 4015349192118518153216.0000 - val_mae: 63366762496.0000\n",
      "Epoch 112/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3990534986077423796224.0000 - mse: 3990534639646683430912.0000 - mae: 62892900352.0000 - val_loss: 4560299665204113833984.0000 - val_mse: 4560299665204113833984.0000 - val_mae: 67529994240.0000\n",
      "Epoch 113/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4046708003155801538560.0000 - mse: 4046707474848922206208.0000 - mae: 63266893824.0000 - val_loss: 4552474942326534307840.0000 - val_mse: 4552475505276487729152.0000 - val_mae: 67472060416.0000\n",
      "Epoch 114/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3996257684141618233344.0000 - mse: 3996257870348141199360.0000 - mae: 62981320704.0000 - val_loss: 4585782157795682942976.0000 - val_mse: 4585781594845729521664.0000 - val_mae: 67718393856.0000\n",
      "Epoch 115/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4028399376687971696640.0000 - mse: 4028399497938731008000.0000 - mae: 63215976448.0000 - val_loss: 4156201522264344100864.0000 - val_mse: 4156201240789367390208.0000 - val_mae: 64468594688.0000\n",
      "Epoch 116/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3993407840940492062720.0000 - mse: 3993407373258992386048.0000 - mae: 62964232192.0000 - val_loss: 4038506701402457243648.0000 - val_mse: 4038506701402457243648.0000 - val_mae: 63549214720.0000\n",
      "Epoch 117/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4035773289242851540992.0000 - mse: 4035773297903620063232.0000 - mae: 63223746560.0000 - val_loss: 3730798819812321525760.0000 - val_mse: 3730799101287298236416.0000 - val_mae: 61080256512.0000\n",
      "Epoch 118/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3999572216546988130304.0000 - mse: 3999571675248955752448.0000 - mae: 62926467072.0000 - val_loss: 3426270761034086088704.0000 - val_mse: 3426270479559109378048.0000 - val_mae: 58534354944.0000\n",
      "Epoch 119/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3995193791006952652800.0000 - mse: 3995193050511244787712.0000 - mae: 62808961024.0000 - val_loss: 3521514011853624180736.0000 - val_mse: 3521514293328600891392.0000 - val_mae: 59342327808.0000\n",
      "Epoch 120/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 4021456783396250320896.0000 - mse: 4021456636163185967104.0000 - mae: 63181033472.0000 - val_loss: 4430200523593565077504.0000 - val_mse: 4430200242118588366848.0000 - val_mae: 66559758336.0000\n",
      "Epoch 121/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4034071400595589169152.0000 - mse: 4034072063144380858368.0000 - mae: 63134199808.0000 - val_loss: 3614246505880903221248.0000 - val_mse: 3614246505880903221248.0000 - val_mae: 60118614016.0000\n",
      "Epoch 122/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3984797737922126151680.0000 - mse: 3984798742571273682944.0000 - mae: 62872809472.0000 - val_loss: 4704567975667300302848.0000 - val_mse: 4704567975667300302848.0000 - val_mae: 68589846528.0000\n",
      "Epoch 123/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4016355530116022403072.0000 - mse: 4016354620735328616448.0000 - mae: 63127932928.0000 - val_loss: 4295801007288830918656.0000 - val_mse: 4295800725813854208000.0000 - val_mae: 65542336512.0000\n",
      "Epoch 124/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4006475212805062000640.0000 - mse: 4006476256427668144128.0000 - mae: 62980624384.0000 - val_loss: 4090153981929142091776.0000 - val_mse: 4090153981929142091776.0000 - val_mae: 63954313216.0000\n",
      "Epoch 125/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4045824574454666100736.0000 - mse: 4045825050796934299648.0000 - mae: 63363661824.0000 - val_loss: 4063294513751504453632.0000 - val_mse: 4063294513751504453632.0000 - val_mae: 63743979520.0000\n",
      "Epoch 126/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3972072908113060036608.0000 - mse: 3972072414449254793216.0000 - mae: 62727892992.0000 - val_loss: 3643849792131516334080.0000 - val_mse: 3643850073606493044736.0000 - val_mae: 60364296192.0000\n",
      "Epoch 127/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4014909450257979932672.0000 - mse: 4014908402304989265920.0000 - mae: 63130673152.0000 - val_loss: 4162199191068094758912.0000 - val_mse: 4162199472543071469568.0000 - val_mae: 64515084288.0000\n",
      "Epoch 128/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 4036746582078090379264.0000 - mse: 4036746919848062222336.0000 - mae: 63250702336.0000 - val_loss: 3747984555990367338496.0000 - val_mse: 3747984837465344049152.0000 - val_mae: 61220761600.0000\n",
      "Epoch 129/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4018327561124394303488.0000 - mse: 4018327478847093604352.0000 - mae: 63053586432.0000 - val_loss: 3875726344670964350976.0000 - val_mse: 3875726063195987640320.0000 - val_mae: 62255312896.0000\n",
      "Epoch 130/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3985995582833015914496.0000 - mse: 3985995855647224102912.0000 - mae: 62785658880.0000 - val_loss: 3141445669150619860992.0000 - val_mse: 3141445669150619860992.0000 - val_mae: 56048611328.0000\n",
      "Epoch 131/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 4023438917191094763520.0000 - mse: 4023438219999228985344.0000 - mae: 63130378240.0000 - val_loss: 4407439612551811301376.0000 - val_mse: 4407439049601857880064.0000 - val_mae: 66388578304.0000\n",
      "Epoch 132/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3995714225247738134528.0000 - mse: 3995713779218159501312.0000 - mae: 62921035776.0000 - val_loss: 3848610734264520015872.0000 - val_mse: 3848610734264520015872.0000 - val_mae: 62037168128.0000\n",
      "Epoch 133/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4043324085003480465408.0000 - mse: 4043323301203929989120.0000 - mae: 63316041728.0000 - val_loss: 3289288430343054950400.0000 - val_mse: 3289288430343054950400.0000 - val_mae: 57352306688.0000\n",
      "Epoch 134/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4008358310711945854976.0000 - mse: 4008358198121955590144.0000 - mae: 63088865280.0000 - val_loss: 3341359893659642757120.0000 - val_mse: 3341359612184666046464.0000 - val_mae: 57804500992.0000\n",
      "Epoch 135/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4052286863176512307200.0000 - mse: 4052286308887327408128.0000 - mae: 63328129024.0000 - val_loss: 3702426705109840822272.0000 - val_mse: 3702426986584817532928.0000 - val_mae: 60847558656.0000\n",
      "Epoch 136/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3974897959864314626048.0000 - mse: 3974897578790499647488.0000 - mae: 62721024000.0000 - val_loss: 3390490506419557629952.0000 - val_mse: 3390490224944580919296.0000 - val_mae: 58227945472.0000\n",
      "Epoch 137/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4013881516634185138176.0000 - mse: 4013881863064925503488.0000 - mae: 63092428800.0000 - val_loss: 3395800531855204155392.0000 - val_mse: 3395800813330180866048.0000 - val_mae: 58273513472.0000\n",
      "Epoch 138/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4008218092869701861376.0000 - mse: 4008218023583553683456.0000 - mae: 63025569792.0000 - val_loss: 4651948199096080138240.0000 - val_mse: 4651948199096080138240.0000 - val_mae: 68205191168.0000\n",
      "Epoch 139/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3994014432506456571904.0000 - mse: 3994014514783757271040.0000 - mae: 62801145856.0000 - val_loss: 4308034754201606160384.0000 - val_mse: 4308035035676582871040.0000 - val_mae: 65635610624.0000\n",
      "Epoch 140/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4038177250098562465792.0000 - mse: 4038176531254775644160.0000 - mae: 63289196544.0000 - val_loss: 3694302774332017868800.0000 - val_mse: 3694302774332017868800.0000 - val_mae: 60780789760.0000\n",
      "Epoch 143/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4040637354386166644736.0000 - mse: 4040637466976156909568.0000 - mae: 63332638720.0000 - val_loss: 3464271853214861623296.0000 - val_mse: 3464271290264908201984.0000 - val_mae: 58858061824.0000\n",
      "Epoch 144/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4005000080399038152704.0000 - mse: 4005000201649797464064.0000 - mae: 62917033984.0000 - val_loss: 5676713020906658594816.0000 - val_mse: 5676713020906658594816.0000 - val_mae: 75343978496.0000\n",
      "Epoch 145/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3977626911728135045120.0000 - mse: 3977626197214732746752.0000 - mae: 62737063936.0000 - val_loss: 3634270635724099289088.0000 - val_mse: 3634270072774145867776.0000 - val_mae: 60284911616.0000\n",
      "Epoch 146/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4033226009989768282112.0000 - mse: 4033226512314342047744.0000 - mae: 63246594048.0000 - val_loss: 4000361212558629142528.0000 - val_mse: 4000361212558629142528.0000 - val_mae: 63248404480.0000\n",
      "Epoch 147/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 4038858306952211464192.0000 - mse: 4038859671023252406272.0000 - mae: 63211679744.0000 - val_loss: 3756551528381532864512.0000 - val_mse: 3756551528381532864512.0000 - val_mae: 61290713088.0000\n",
      "Epoch 148/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3978872356222778998784.0000 - mse: 3978872849886584242176.0000 - mae: 62602584064.0000 - val_loss: 4105189812235071913984.0000 - val_mse: 4105189812235071913984.0000 - val_mae: 64071753728.0000\n",
      "Epoch 149/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3998800096042796711936.0000 - mse: 3998800433812768555008.0000 - mae: 62877929472.0000 - val_loss: 3247068028261386682368.0000 - val_mse: 3247068028261386682368.0000 - val_mae: 56983031808.0000\n",
      "Epoch 150/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3998831989322849714176.0000 - mse: 3998832521960113569792.0000 - mae: 62970052608.0000 - val_loss: 4976142070072158978048.0000 - val_mse: 4976141507122205556736.0000 - val_mae: 70541762560.0000\n",
      "Epoch 151/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4022144760203942821888.0000 - mse: 4022144279531290099712.0000 - mae: 63134945280.0000 - val_loss: 3986382602265224544256.0000 - val_mse: 3986382602265224544256.0000 - val_mae: 63137779712.0000\n",
      "Epoch 152/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4009335778037609267200.0000 - mse: 4009335479241094987776.0000 - mae: 63046213632.0000 - val_loss: 4323410043329449033728.0000 - val_mse: 4323410324804425744384.0000 - val_mae: 65752657920.0000\n",
      "Epoch 153/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 4023954960422243139584.0000 - mse: 4023953882156562907136.0000 - mae: 63160324096.0000 - val_loss: 3584752713396230553600.0000 - val_mse: 3584752431921253842944.0000 - val_mae: 59872784384.0000\n",
      "Epoch 154/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4022579067432238841856.0000 - mse: 4022578876895331352576.0000 - mae: 63176675328.0000 - val_loss: 4035438905631287803904.0000 - val_mse: 4035438624156311093248.0000 - val_mae: 63525097472.0000\n",
      "Epoch 155/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4017385871433463758848.0000 - mse: 4017385663575019749376.0000 - mae: 63165374464.0000 - val_loss: 4221196909136601677824.0000 - val_mse: 4221196909136601677824.0000 - val_mae: 64970731520.0000\n",
      "Epoch 156/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4021204880613631852544.0000 - mse: 4021204716059029929984.0000 - mae: 63180464128.0000 - val_loss: 3759482808788997636096.0000 - val_mse: 3759482245839044214784.0000 - val_mae: 61314609152.0000\n",
      "Epoch 157/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3993720806471520223232.0000 - mse: 3993720091958117924864.0000 - mae: 62909247488.0000 - val_loss: 4043107409896792915968.0000 - val_mse: 4043106846946839494656.0000 - val_mae: 63585431552.0000\n",
      "Epoch 158/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 4026110080077004800000.0000 - mse: 4026110261953143242752.0000 - mae: 63209754624.0000 - val_loss: 3294391853145795854336.0000 - val_mse: 3294391853145795854336.0000 - val_mae: 57396809728.0000\n",
      "Epoch 159/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4039924057821723230208.0000 - mse: 4039923927910195396608.0000 - mae: 63253008384.0000 - val_loss: 3741675575862374694912.0000 - val_mse: 3741675012912421273600.0000 - val_mae: 61169209344.0000\n",
      "Epoch 160/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3992662192075264491520.0000 - mse: 3992661464570709147648.0000 - mae: 62944260096.0000 - val_loss: 4019352047762320392192.0000 - val_mse: 4019351766287343681536.0000 - val_mae: 63398350848.0000\n",
      "Epoch 161/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4007312670146924380160.0000 - mse: 4007311955633522081792.0000 - mae: 63056187392.0000 - val_loss: 3813585677012506247168.0000 - val_mse: 3813585395537529536512.0000 - val_mae: 61754224640.0000\n",
      "Epoch 162/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4023649044756785266688.0000 - mse: 4023649607706738688000.0000 - mae: 63188180992.0000 - val_loss: 4043947612702274224128.0000 - val_mse: 4043947612702274224128.0000 - val_mae: 63592067072.0000\n",
      "Epoch 163/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 4005775063287223287808.0000 - mse: 4005774820785705189376.0000 - mae: 63070007296.0000 - val_loss: 4610992182609796136960.0000 - val_mse: 4610992182609796136960.0000 - val_mae: 67904294912.0000\n",
      "Epoch 164/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4030972785479662436352.0000 - mse: 4030972460700843114496.0000 - mae: 63198134272.0000 - val_loss: 4280419244236523700224.0000 - val_mse: 4280419244236523700224.0000 - val_mae: 65424896000.0000\n",
      "Epoch 165/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3966569167268056924160.0000 - mse: 3966568452754654625792.0000 - mae: 62623739904.0000 - val_loss: 3657409286184598765568.0000 - val_mse: 3657409286184598765568.0000 - val_mae: 60476493824.0000\n",
      "Epoch 166/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4056327930130999541760.0000 - mse: 4056327726602939006976.0000 - mae: 63445176320.0000 - val_loss: 4116914370914977579008.0000 - val_mse: 4116914370914977579008.0000 - val_mae: 64163192832.0000\n",
      "Epoch 167/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3966217197966025293824.0000 - mse: 3966216890508743016448.0000 - mae: 62719336448.0000 - val_loss: 4444744335640204673024.0000 - val_mse: 4444743772690251251712.0000 - val_mae: 66668900352.0000\n",
      "Epoch 168/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4058698508054472163328.0000 - mse: 4058698590331772862464.0000 - mae: 63493627904.0000 - val_loss: 3950173661261165756416.0000 - val_mse: 3950173661261165756416.0000 - val_mae: 62850420736.0000\n",
      "Epoch 169/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3986381922394896334848.0000 - mse: 3986382039315271122944.0000 - mae: 62935412736.0000 - val_loss: 4490302749470684610560.0000 - val_mse: 4490302186520731189248.0000 - val_mae: 67009728512.0000\n",
      "Epoch 170/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3996292448466433998848.0000 - mse: 3996291647345346478080.0000 - mae: 62919155712.0000 - val_loss: 3984054804207827419136.0000 - val_mse: 3984055085682804129792.0000 - val_mae: 63119343616.0000\n",
      "Epoch 171/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4061317147709727571968.0000 - mse: 4061316870565135384576.0000 - mae: 63492210688.0000 - val_loss: 3920960499328273612800.0000 - val_mse: 3920960217853296902144.0000 - val_mae: 62617554944.0000\n",
      "Epoch 172/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4002126740532933165056.0000 - mse: 4002126342137581666304.0000 - mae: 62946856960.0000 - val_loss: 3361465088296131493888.0000 - val_mse: 3361465369771108204544.0000 - val_mae: 57978130432.0000\n",
      "Epoch 173/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3990722153945783861248.0000 - mse: 3990722101981172727808.0000 - mae: 62914838528.0000 - val_loss: 4179554093707143675904.0000 - val_mse: 4179553812232166965248.0000 - val_mae: 64649469952.0000\n",
      "Epoch 174/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3990688324983967449088.0000 - mse: 3990688887933920870400.0000 - mae: 62904520704.0000 - val_loss: 4158138070104113414144.0000 - val_mse: 4158137788629136703488.0000 - val_mae: 64483627008.0000\n",
      "Epoch 175/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4028136894776613011456.0000 - mse: 4028136600310483255296.0000 - mae: 63180120064.0000 - val_loss: 3551812822771689324544.0000 - val_mse: 3551812259821735903232.0000 - val_mae: 59597086720.0000\n",
      "Epoch 176/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4024020279938377056256.0000 - mse: 4024020028776089911296.0000 - mae: 63198883840.0000 - val_loss: 4420579708889594855424.0000 - val_mse: 4420579990364571566080.0000 - val_mae: 66487447552.0000\n",
      "Epoch 177/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4012491441635738517504.0000 - mse: 4012491658154951573504.0000 - mae: 63087116288.0000 - val_loss: 4006536773547660935168.0000 - val_mse: 4006537055022637645824.0000 - val_mae: 63297200128.0000\n",
      "Epoch 178/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 4013539987888596844544.0000 - mse: 4013539589493245345792.0000 - mae: 62939308032.0000 - val_loss: 3629488094394808532992.0000 - val_mse: 3629487812919831822336.0000 - val_mae: 60245237760.0000\n",
      "Epoch 179/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4044394006033031757824.0000 - mse: 4044392906115430481920.0000 - mae: 63300472832.0000 - val_loss: 4187402460482766897152.0000 - val_mse: 4187402179007790186496.0000 - val_mae: 64710119424.0000\n",
      "Epoch 180/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3959986775338840489984.0000 - mse: 3959986723374229356544.0000 - mae: 62669414400.0000 - val_loss: 4204900352409984827392.0000 - val_mse: 4204900070935008116736.0000 - val_mae: 64845205504.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6240 samples, validate on 960 samples\n",
      "Epoch 1/180\n",
      "6240/6240 [==============================] - 64s 10ms/step - loss: 3633313354848062668800.0000 - mse: 3633313057853329637376.0000 - mae: 26616604672.0000 - val_loss: 2297875859997898833920.0000 - val_mse: 2297876000735387189248.0000 - val_mae: 47936163840.0000\n",
      "Epoch 2/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3158067927068365029376.0000 - mse: 3158068173900267651072.0000 - mae: 55527157760.0000 - val_loss: 2800460126963890126848.0000 - val_mse: 2800460126963890126848.0000 - val_mae: 52919390208.0000\n",
      "Epoch 3/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3047439535496799191040.0000 - mse: 3047439219378748391424.0000 - mae: 54914867200.0000 - val_loss: 2805268563991038263296.0000 - val_mse: 2805268563991038263296.0000 - val_mae: 52964765696.0000\n",
      "Epoch 4/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3061084714863176450048.0000 - mse: 3061084563299727572992.0000 - mae: 55071576064.0000 - val_loss: 2776578382564874518528.0000 - val_mse: 2776578382564874518528.0000 - val_mae: 52693254144.0000\n",
      "Epoch 5/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3068559456085098364928.0000 - mse: 3068558849831302332416.0000 - mae: 55144214528.0000 - val_loss: 3002467680874710171648.0000 - val_mse: 3002467680874710171648.0000 - val_mae: 54794776576.0000\n",
      "Epoch 6/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3104695482079036047360.0000 - mse: 3104695170291369771008.0000 - mae: 55489630208.0000 - val_loss: 2988932112194648145920.0000 - val_mse: 2988932112194648145920.0000 - val_mae: 54671134720.0000\n",
      "Epoch 7/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3076568579715496935424.0000 - mse: 3076568501768580759552.0000 - mae: 55237046272.0000 - val_loss: 3412725059254862479360.0000 - val_mse: 3412725340729839190016.0000 - val_mae: 58418532352.0000\n",
      "Epoch 8/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 2998426767648864337920.0000 - mse: 2998426544634075283456.0000 - mae: 54382780416.0000 - val_loss: 3177524287240461615104.0000 - val_mse: 3177524005765484904448.0000 - val_mae: 56369528832.0000\n",
      "Epoch 9/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3079706501729325940736.0000 - mse: 3079706666283927863296.0000 - mae: 55201464320.0000 - val_loss: 3326091846497926643712.0000 - val_mse: 3326091846497926643712.0000 - val_mae: 57672286208.0000\n",
      "Epoch 10/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3087761276270563819520.0000 - mse: 3087760791267526574080.0000 - mae: 55324004352.0000 - val_loss: 2763098264455224492032.0000 - val_mse: 2763098264455224492032.0000 - val_mae: 52565196800.0000\n",
      "Epoch 11/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3049914295969921368064.0000 - mse: 3049914228848965189632.0000 - mae: 54846623744.0000 - val_loss: 2084432822208254967808.0000 - val_mse: 2084432681470766612480.0000 - val_mae: 45655592960.0000\n",
      "Epoch 12/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3100667046855427227648.0000 - mse: 3100667544849616994304.0000 - mae: 55407439872.0000 - val_loss: 3067550324989748051968.0000 - val_mse: 3067550043514771341312.0000 - val_mae: 55385473024.0000\n",
      "Epoch 13/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3074570003451629535232.0000 - mse: 3074569747958958391296.0000 - mae: 55135465472.0000 - val_loss: 3723345362429023354880.0000 - val_mse: 3723345362429023354880.0000 - val_mae: 61019217920.0000\n",
      "Epoch 14/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3051112013134475821056.0000 - mse: 3051111904874869030912.0000 - mae: 54945570816.0000 - val_loss: 2927033231541184495616.0000 - val_mse: 2927033231541184495616.0000 - val_mae: 54102061056.0000\n",
      "Epoch 15/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3066610038343317061632.0000 - mse: 3066610198567534460928.0000 - mae: 55171846144.0000 - val_loss: 3277405119776284475392.0000 - val_mse: 3277404838301307764736.0000 - val_mae: 57248628736.0000\n",
      "Epoch 16/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3071636254725138546688.0000 - mse: 3071636215751679934464.0000 - mae: 55098695680.0000 - val_loss: 2849873903550375919616.0000 - val_mse: 2849873622075399208960.0000 - val_mae: 53384196096.0000\n",
      "Epoch 17/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3043442380803871342592.0000 - mse: 3043441993234480365568.0000 - mae: 54817923072.0000 - val_loss: 3730043903924783546368.0000 - val_mse: 3730043622449806835712.0000 - val_mae: 61074087936.0000\n",
      "Epoch 18/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3071352744467826737152.0000 - mse: 3071352488975155593216.0000 - mae: 55118245888.0000 - val_loss: 3408374863489799290880.0000 - val_mse: 3408374863489799290880.0000 - val_mae: 58381299712.0000\n",
      "Epoch 19/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3083034681340681781248.0000 - mse: 3083034544933577949184.0000 - mae: 55194206208.0000 - val_loss: 3889872714050488500224.0000 - val_mse: 3889872714050488500224.0000 - val_mae: 62368825344.0000\n",
      "Epoch 20/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3092033319425746665472.0000 - mse: 3092033018464040910848.0000 - mae: 55309664256.0000 - val_loss: 2839052316595758039040.0000 - val_mse: 2839052316595758039040.0000 - val_mae: 53282746368.0000\n",
      "Epoch 21/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3040340379327455035392.0000 - mse: 3040339857516152225792.0000 - mae: 54866874368.0000 - val_loss: 2779062117759369347072.0000 - val_mse: 2779062117759369347072.0000 - val_mae: 52716810240.0000\n",
      "Epoch 22/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3069949437980282716160.0000 - mse: 3069949899166206394368.0000 - mae: 55124262912.0000 - val_loss: 3046792952832220725248.0000 - val_mse: 3046792952832220725248.0000 - val_mae: 55197745152.0000\n",
      "Epoch 23/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3081998682376208646144.0000 - mse: 3081998435544306024448.0000 - mae: 55216652288.0000 - val_loss: 2925169304245406531584.0000 - val_mse: 2925169304245406531584.0000 - val_mae: 54084845568.0000\n",
      "Epoch 24/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3075159676206301315072.0000 - mse: 3075159719510143926272.0000 - mae: 55184957440.0000 - val_loss: 3010189102435836887040.0000 - val_mse: 3010189102435836887040.0000 - val_mae: 54865178624.0000\n",
      "Epoch 25/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3052526710697807577088.0000 - mse: 3052526879582793498624.0000 - mae: 54952361984.0000 - val_loss: 3367035759560212086784.0000 - val_mse: 3367035478085235376128.0000 - val_mae: 58026147840.0000\n",
      "Epoch 26/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3066592214481714544640.0000 - mse: 3066592465644001689600.0000 - mae: 55123693568.0000 - val_loss: 2882768758178643443712.0000 - val_mse: 2882768758178643443712.0000 - val_mae: 53691420672.0000\n",
      "Epoch 27/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3030395069444372037632.0000 - mse: 3030394783639011328000.0000 - mae: 54741889024.0000 - val_loss: 3158387929473810956288.0000 - val_mse: 3158387647998834245632.0000 - val_mae: 56199528448.0000\n",
      "Epoch 28/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3085668314951428079616.0000 - mse: 3085668306290659557376.0000 - mae: 55313920000.0000 - val_loss: 3232917999707165294592.0000 - val_mse: 3232917999707165294592.0000 - val_mae: 56858767360.0000\n",
      "Epoch 29/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3063382854118796689408.0000 - mse: 3063382806484570079232.0000 - mae: 55016669184.0000 - val_loss: 3211774725356567658496.0000 - val_mse: 3211774725356567658496.0000 - val_mae: 56672522240.0000\n",
      "Epoch 30/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3082127953007051538432.0000 - mse: 3082127069608662794240.0000 - mae: 55193296896.0000 - val_loss: 2525272175333043339264.0000 - val_mse: 2525272175333043339264.0000 - val_mae: 50252087296.0000\n",
      "Epoch 31/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3087579187942937067520.0000 - mse: 3087579239907548200960.0000 - mae: 55266680832.0000 - val_loss: 2978563699952534421504.0000 - val_mse: 2978563699952534421504.0000 - val_mae: 54576234496.0000\n",
      "Epoch 32/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3004976350421757460480.0000 - mse: 3004976185867155537920.0000 - mae: 54463332352.0000 - val_loss: 3697373947802907836416.0000 - val_mse: 3697373947802907836416.0000 - val_mae: 60806008832.0000\n",
      "Epoch 33/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3083783435091422216192.0000 - mse: 3083783549846605004800.0000 - mae: 55181934592.0000 - val_loss: 3369616322146695380992.0000 - val_mse: 3369616322146695380992.0000 - val_mae: 58048385024.0000\n",
      "Epoch 34/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3044998978563731881984.0000 - mse: 3044998831330667003904.0000 - mae: 54858518528.0000 - val_loss: 3433394611219656081408.0000 - val_mse: 3433394611219656081408.0000 - val_mae: 58595172352.0000\n",
      "Epoch 35/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3081715436272337027072.0000 - mse: 3081716116142665236480.0000 - mae: 55269871616.0000 - val_loss: 3140760840532282834944.0000 - val_mse: 3140760559057306124288.0000 - val_mae: 56042467328.0000\n",
      "Epoch 36/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3034747787658265100288.0000 - mse: 3034747231203888201728.0000 - mae: 54783655936.0000 - val_loss: 3626572013636086136832.0000 - val_mse: 3626572013636086136832.0000 - val_mae: 60221026304.0000\n",
      "Epoch 37/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3075730191341020250112.0000 - mse: 3075730550762913136640.0000 - mae: 55192965120.0000 - val_loss: 2675138178658074624000.0000 - val_mse: 2675137897183097913344.0000 - val_mae: 51721728000.0000\n",
      "Epoch 38/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3074310509505407877120.0000 - mse: 3074310228030431166464.0000 - mae: 55187722240.0000 - val_loss: 3082412485235047399424.0000 - val_mse: 3082412485235047399424.0000 - val_mae: 55519477760.0000\n",
      "Epoch 39/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3043925943987899727872.0000 - mse: 3043925567244469272576.0000 - mae: 54796439552.0000 - val_loss: 2602314691208516993024.0000 - val_mse: 2602314691208516993024.0000 - val_mae: 51012878336.0000\n",
      "Epoch 40/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3072859235351448256512.0000 - mse: 3072858943050511024128.0000 - mae: 55085240320.0000 - val_loss: 3107339346222589673472.0000 - val_mse: 3107339346222589673472.0000 - val_mae: 55743483904.0000\n",
      "Epoch 41/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3077044341712286121984.0000 - mse: 3077044194479221768192.0000 - mae: 55243620352.0000 - val_loss: 2694382622815782174720.0000 - val_mse: 2694382341340805464064.0000 - val_mae: 51907444736.0000\n",
      "Epoch 42/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3053789675927155769344.0000 - mse: 3053789857803294212096.0000 - mae: 54971420672.0000 - val_loss: 3044062927033104072704.0000 - val_mse: 3044062927033104072704.0000 - val_mae: 55173042176.0000\n",
      "Epoch 43/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3102139641656276156416.0000 - mse: 3102139658977813725184.0000 - mae: 55446941696.0000 - val_loss: 2567113712096059064320.0000 - val_mse: 2567113712096059064320.0000 - val_mae: 50666672128.0000\n",
      "Epoch 44/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3061307569428198588416.0000 - mse: 3061306928531328991232.0000 - mae: 55062130688.0000 - val_loss: 3873123264086344204288.0000 - val_mse: 3873122982611367493632.0000 - val_mae: 62234423296.0000\n",
      "Epoch 45/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3068435364593827053568.0000 - mse: 3068435282316526354432.0000 - mae: 55085834240.0000 - val_loss: 2606363990223476490240.0000 - val_mse: 2606363990223476490240.0000 - val_mae: 51052548096.0000\n",
      "Epoch 46/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3074262914252038209536.0000 - mse: 3074263221709320486912.0000 - mae: 55207198720.0000 - val_loss: 3341075322458188283904.0000 - val_mse: 3341075603933164994560.0000 - val_mae: 57802014720.0000\n",
      "Epoch 47/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3069798552236421283840.0000 - mse: 3069798465628736061440.0000 - mae: 55154098176.0000 - val_loss: 3184567917057669070848.0000 - val_mse: 3184567635582692360192.0000 - val_mae: 56431996928.0000\n",
      "Epoch 48/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3042049878625049116672.0000 - mse: 3042050380949622882304.0000 - mae: 54896869376.0000 - val_loss: 3546030200850145607680.0000 - val_mse: 3546029637900192186368.0000 - val_mae: 59548549120.0000\n",
      "Epoch 49/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3070868146321961254912.0000 - mse: 3070868070540236554240.0000 - mae: 54987431936.0000 - val_loss: 2366479474596610310144.0000 - val_mse: 2366479193121633599488.0000 - val_mae: 48646459392.0000\n",
      "Epoch 50/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3077299572395207032832.0000 - mse: 3077299773758075043840.0000 - mae: 55179149312.0000 - val_loss: 3540955769970005901312.0000 - val_mse: 3540955488495029190656.0000 - val_mae: 59505913856.0000\n",
      "Epoch 51/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3064432327073885847552.0000 - mse: 3064432426672724115456.0000 - mae: 55058440192.0000 - val_loss: 2681950998994379341824.0000 - val_mse: 2681950998994379341824.0000 - val_mae: 51787546624.0000\n",
      "Epoch 52/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3067794584644153245696.0000 - mse: 3067794363794556190720.0000 - mae: 55139696640.0000 - val_loss: 2906119922246559465472.0000 - val_mse: 2906119640771582754816.0000 - val_mae: 53908459520.0000\n",
      "Epoch 53/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3043177242201770426368.0000 - mse: 3043176843806418927616.0000 - mae: 54911627264.0000 - val_loss: 3249081137294821294080.0000 - val_mse: 3249080855819844583424.0000 - val_mae: 57000718336.0000\n",
      "Epoch 54/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3062638383118086569984.0000 - mse: 3062638305171170394112.0000 - mae: 55052066816.0000 - val_loss: 2914748819132601335808.0000 - val_mse: 2914748819132601335808.0000 - val_mae: 53988421632.0000\n",
      "Epoch 55/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3079062274793783427072.0000 - mse: 3079062651537213882368.0000 - mae: 55211634688.0000 - val_loss: 3327967595742726455296.0000 - val_mse: 3327967595742726455296.0000 - val_mae: 57688543232.0000\n",
      "Epoch 56/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3063882528497453760512.0000 - mse: 3063882143093254782976.0000 - mae: 55147520000.0000 - val_loss: 2717283989870914568192.0000 - val_mse: 2717283989870914568192.0000 - val_mae: 52127592448.0000\n",
      "Epoch 57/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3058229714174400266240.0000 - mse: 3058228999660997967872.0000 - mae: 55020417024.0000 - val_loss: 2795207522423492575232.0000 - val_mse: 2795207522423492575232.0000 - val_mae: 52869722112.0000\n",
      "Epoch 58/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3042159931010558459904.0000 - mse: 3042159874715563327488.0000 - mae: 54892507136.0000 - val_loss: 2849720499688068612096.0000 - val_mse: 2849720499688068612096.0000 - val_mae: 53382754304.0000\n",
      "Epoch 59/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3081999518140370255872.0000 - mse: 3081998998494259445760.0000 - mae: 55286702080.0000 - val_loss: 2604624193392427925504.0000 - val_mse: 2604624193392427925504.0000 - val_mae: 51035508736.0000\n",
      "Epoch 60/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3055476321621832105984.0000 - mse: 3055476174388767752192.0000 - mae: 55073808384.0000 - val_loss: 3170879788940229869568.0000 - val_mse: 3170879788940229869568.0000 - val_mae: 56310579200.0000\n",
      "Epoch 61/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3080395937876512604160.0000 - mse: 3080396279976868970496.0000 - mae: 55303372800.0000 - val_loss: 2933717980763085864960.0000 - val_mse: 2933717980763085864960.0000 - val_mae: 54163804160.0000\n",
      "Epoch 62/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3065159545823715917824.0000 - mse: 3065159195062591029248.0000 - mae: 55126908928.0000 - val_loss: 3244022750488354095104.0000 - val_mse: 3244023031963330805760.0000 - val_mae: 56956301312.0000\n",
      "Epoch 63/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3069713919206500597760.0000 - mse: 3069713460185769443328.0000 - mae: 55170224128.0000 - val_loss: 3127703779312628924416.0000 - val_mse: 3127703779312628924416.0000 - val_mae: 55925850112.0000\n",
      "Epoch 64/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3037174182069619654656.0000 - mse: 3037174108453087477760.0000 - mae: 54864121856.0000 - val_loss: 2763421960678441746432.0000 - val_mse: 2763421960678441746432.0000 - val_mae: 52568256512.0000\n",
      "Epoch 65/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3072153376221966630912.0000 - mse: 3072153003808920698880.0000 - mae: 55193001984.0000 - val_loss: 3121930446065316659200.0000 - val_mse: 3121930446065316659200.0000 - val_mae: 55874240512.0000\n",
      "Epoch 66/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3076600468665166462976.0000 - mse: 3076600308440949063680.0000 - mae: 55214968832.0000 - val_loss: 3515648073338974109696.0000 - val_mse: 3515648073338974109696.0000 - val_mae: 59292905472.0000\n",
      "Epoch 67/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3102759131271750746112.0000 - mse: 3102759466876530589696.0000 - mae: 55352954880.0000 - val_loss: 2785701830984997011456.0000 - val_mse: 2785701830984997011456.0000 - val_mae: 52779761664.0000\n",
      "Epoch 68/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3050123728508938616832.0000 - mse: 3050124209181591339008.0000 - mae: 54943444992.0000 - val_loss: 3015502505571203940352.0000 - val_mse: 3015502505571203940352.0000 - val_mae: 54913581056.0000\n",
      "Epoch 69/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3053746991329533493248.0000 - mse: 3053747355081810903040.0000 - mae: 54988922880.0000 - val_loss: 2565508178828901482496.0000 - val_mse: 2565508178828901482496.0000 - val_mae: 50650853376.0000\n",
      "Epoch 70/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3055689095052304318464.0000 - mse: 3055688969471161008128.0000 - mae: 55061819392.0000 - val_loss: 3002207879471206236160.0000 - val_mse: 3002207879471206236160.0000 - val_mae: 54792417280.0000\n",
      "Epoch 71/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3064977773614140489728.0000 - mse: 3064977643702612656128.0000 - mae: 55103102976.0000 - val_loss: 2733892139396750114816.0000 - val_mse: 2733891857921773404160.0000 - val_mae: 52286636032.0000\n",
      "Epoch 72/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3073914422248564850688.0000 - mse: 3073914192738199273472.0000 - mae: 55167369216.0000 - val_loss: 2722106219171921526784.0000 - val_mse: 2722105937696944816128.0000 - val_mae: 52173815808.0000\n",
      "Epoch 73/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3056115668214316793856.0000 - mse: 3056115404060877651968.0000 - mae: 55069921280.0000 - val_loss: 2937918713315515695104.0000 - val_mse: 2937918431840538984448.0000 - val_mae: 54202585088.0000\n",
      "Epoch 74/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3067611067289722421248.0000 - mse: 3067610842109740843008.0000 - mae: 55131791360.0000 - val_loss: 3128772821274175995904.0000 - val_mse: 3128772539799199285248.0000 - val_mae: 55935451136.0000\n",
      "Epoch 75/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3095056462439649378304.0000 - mse: 3095055778238936645632.0000 - mae: 55306268672.0000 - val_loss: 3067263220513503182848.0000 - val_mse: 3067263220513503182848.0000 - val_mae: 55382896640.0000\n",
      "Epoch 76/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3075178985389703561216.0000 - mse: 3075179141283536961536.0000 - mae: 55235411968.0000 - val_loss: 2730856994722879111168.0000 - val_mse: 2730856994722879111168.0000 - val_mae: 52257615872.0000\n",
      "Epoch 77/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3030265452382789304320.0000 - mse: 3030265586624701136896.0000 - mae: 54778310656.0000 - val_loss: 2572815269224310112256.0000 - val_mse: 2572815269224310112256.0000 - val_mae: 50722910208.0000\n",
      "Epoch 78/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3097575875670038282240.0000 - mse: 3097575542230450438144.0000 - mae: 55413317632.0000 - val_loss: 3018046757885691559936.0000 - val_mse: 3018047039360668270592.0000 - val_mae: 54936735744.0000\n",
      "Epoch 79/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3062421768636779200512.0000 - mse: 3062421287964126478336.0000 - mae: 55108681728.0000 - val_loss: 3274157180020020215808.0000 - val_mse: 3274156898545043505152.0000 - val_mae: 57220263936.0000\n",
      "Epoch 80/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3064569028644114006016.0000 - mse: 3064568379086475362304.0000 - mae: 55106228224.0000 - val_loss: 3204689155767830315008.0000 - val_mse: 3204689155767830315008.0000 - val_mae: 56609992704.0000\n",
      "Epoch 81/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3051929425127611564032.0000 - mse: 3051929589682213486592.0000 - mae: 54996992000.0000 - val_loss: 3321408102885461327872.0000 - val_mse: 3321408102885461327872.0000 - val_mae: 57631653888.0000\n",
      "Epoch 82/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3093766410656773308416.0000 - mse: 3093766059895648419840.0000 - mae: 55367360512.0000 - val_loss: 3016930428128057098240.0000 - val_mse: 3016930146653080387584.0000 - val_mae: 54926594048.0000\n",
      "Epoch 83/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3037224384214311895040.0000 - mse: 3037224210998941974528.0000 - mae: 54877253632.0000 - val_loss: 2922457855794752782336.0000 - val_mse: 2922457855794752782336.0000 - val_mae: 54059741184.0000\n",
      "Epoch 84/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3086537093962622042112.0000 - mse: 3086537219543765352448.0000 - mae: 55286321152.0000 - val_loss: 3364129811900651274240.0000 - val_mse: 3364129248950697852928.0000 - val_mae: 58001113088.0000\n",
      "Epoch 85/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3036818804755177930752.0000 - mse: 3036819168507455340544.0000 - mae: 54866853888.0000 - val_loss: 3046859662401701150720.0000 - val_mse: 3046859662401701150720.0000 - val_mae: 55198355456.0000\n",
      "Epoch 86/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3071773521410511601664.0000 - mse: 3071773575540314734592.0000 - mae: 55064080384.0000 - val_loss: 2011909387083870896128.0000 - val_mse: 2011909387083870896128.0000 - val_mae: 44854296576.0000\n",
      "Epoch 87/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3040363092192883900416.0000 - mse: 3040363219939219210240.0000 - mae: 54798548992.0000 - val_loss: 3239257942082596110336.0000 - val_mse: 3239258223557572820992.0000 - val_mae: 56914472960.0000\n",
      "Epoch 88/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3094991298817348861952.0000 - mse: 3094991320469269905408.0000 - mae: 55362871296.0000 - val_loss: 2794533671329247264768.0000 - val_mse: 2794533389854270554112.0000 - val_mae: 52863352832.0000\n",
      "Epoch 89/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3049439757306684768256.0000 - mse: 3049439662038231023616.0000 - mae: 54961053696.0000 - val_loss: 2724340004587097292800.0000 - val_mse: 2724339723112120582144.0000 - val_mae: 52195217408.0000\n",
      "Epoch 90/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3092188607005206249472.0000 - mse: 3092188392651185192960.0000 - mae: 55317626880.0000 - val_loss: 2798074063586313895936.0000 - val_mse: 2798074063586313895936.0000 - val_mae: 52896845824.0000\n",
      "Epoch 91/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3057990088031150276608.0000 - mse: 3057989745930793910272.0000 - mae: 55069343744.0000 - val_loss: 3258188260166294568960.0000 - val_mse: 3258188260166294568960.0000 - val_mae: 57080545280.0000\n",
      "Epoch 92/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3061043511256970428416.0000 - mse: 3061043749428104527872.0000 - mae: 55109718016.0000 - val_loss: 3060817725021805871104.0000 - val_mse: 3060817725021805871104.0000 - val_mae: 55324659712.0000\n",
      "Epoch 93/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3085470152237439778816.0000 - mse: 3085469584957101834240.0000 - mae: 55258533888.0000 - val_loss: 3560829029225685057536.0000 - val_mse: 3560828466275731636224.0000 - val_mae: 59672674304.0000\n",
      "Epoch 94/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3046508398952303296512.0000 - mse: 3046507818680812830720.0000 - mae: 54893826048.0000 - val_loss: 3050940768089028952064.0000 - val_mse: 3050940768089028952064.0000 - val_mae: 55235317760.0000\n",
      "Epoch 95/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3052213024157800595456.0000 - mse: 3052213034983761117184.0000 - mae: 54924312576.0000 - val_loss: 2304829558560047235072.0000 - val_mse: 2304829558560047235072.0000 - val_mae: 48008646656.0000\n",
      "Epoch 96/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3055795289065441460224.0000 - mse: 3055795648487334346752.0000 - mae: 55020695552.0000 - val_loss: 3477238279492014702592.0000 - val_mse: 3477238279492014702592.0000 - val_mae: 58968113152.0000\n",
      "Epoch 97/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3070159273245036511232.0000 - mse: 3070159316548879122432.0000 - mae: 55178813440.0000 - val_loss: 3270447058351997059072.0000 - val_mse: 3270447058351997059072.0000 - val_mae: 57187811328.0000\n",
      "Epoch 98/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3067011157506666790912.0000 - mse: 3067011300409347145728.0000 - mae: 55065481216.0000 - val_loss: 3300665367426722955264.0000 - val_mse: 3300665085951746244608.0000 - val_mae: 57451429888.0000\n",
      "Epoch 99/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3067712168770972155904.0000 - mse: 3067711891626379968512.0000 - mae: 55174422528.0000 - val_loss: 3754334631464959737856.0000 - val_mse: 3754334912939936448512.0000 - val_mae: 61272616960.0000\n",
      "Epoch 100/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3076751005813096054784.0000 - mse: 3076750616078512553984.0000 - mae: 55250952192.0000 - val_loss: 2979873965969122525184.0000 - val_mse: 2979873965969122525184.0000 - val_mae: 54588215296.0000\n",
      "Epoch 101/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3067901116427262230528.0000 - mse: 3067901324285706240000.0000 - mae: 55105929216.0000 - val_loss: 3455981007775849250816.0000 - val_mse: 3455981007775849250816.0000 - val_mae: 58787586048.0000\n",
      "Epoch 102/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3085996679328874102784.0000 - mse: 3085996224638527471616.0000 - mae: 55221067776.0000 - val_loss: 3134850991921265901568.0000 - val_mse: 3134850991921265901568.0000 - val_mae: 55989751808.0000\n",
      "Epoch 103/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3041842769337185206272.0000 - mse: 3041842370941833707520.0000 - mae: 54898917376.0000 - val_loss: 2664928518302825709568.0000 - val_mse: 2664928518302825709568.0000 - val_mae: 51622957056.0000\n",
      "Epoch 104/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3076605933610098753536.0000 - mse: 3076605093515553144832.0000 - mae: 55224057856.0000 - val_loss: 3279116487634685263872.0000 - val_mse: 3279116206159708553216.0000 - val_mae: 57263570944.0000\n",
      "Epoch 105/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3043000029886818025472.0000 - mse: 3043000358996021346304.0000 - mae: 54920667136.0000 - val_loss: 3456179166159453552640.0000 - val_mse: 3456179166159453552640.0000 - val_mae: 58789281792.0000\n",
      "Epoch 106/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3097071561284650008576.0000 - mse: 3097071139072184942592.0000 - mae: 55269826560.0000 - val_loss: 2031703129658629292032.0000 - val_mse: 2031702988921140936704.0000 - val_mae: 45074419712.0000\n",
      "Epoch 107/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3063649986862848147456.0000 - mse: 3063649926237468491776.0000 - mae: 54963101696.0000 - val_loss: 2741431165172968325120.0000 - val_mse: 2741430883697991614464.0000 - val_mae: 52358676480.0000\n",
      "Epoch 108/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3068260313140619116544.0000 - mse: 3068260486355989037056.0000 - mae: 55166144512.0000 - val_loss: 2478502574677777448960.0000 - val_mse: 2478502293202800738304.0000 - val_mae: 49784557568.0000\n",
      "Epoch 109/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3065364970592104087552.0000 - mse: 3065364108845636386816.0000 - mae: 55154020352.0000 - val_loss: 2869955172813844250624.0000 - val_mse: 2869955172813844250624.0000 - val_mae: 53571964928.0000\n",
      "Epoch 110/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3067221707284822884352.0000 - mse: 3067221562216950005760.0000 - mae: 55081476096.0000 - val_loss: 2788100842211501932544.0000 - val_mse: 2788100842211501932544.0000 - val_mae: 52802473984.0000\n",
      "Epoch 111/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3080431624573175201792.0000 - mse: 3080431745823934513152.0000 - mae: 55240617984.0000 - val_loss: 2520786871579159035904.0000 - val_mse: 2520786590104182325248.0000 - val_mae: 50207440896.0000\n",
      "Epoch 112/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3048401266206071324672.0000 - mse: 3048400456424215281664.0000 - mae: 54994481152.0000 - val_loss: 2958806971337213476864.0000 - val_mse: 2958806689862236766208.0000 - val_mae: 54394925056.0000\n",
      "Epoch 113/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3070446130889378758656.0000 - mse: 3070446421025123991552.0000 - mae: 54959439872.0000 - val_loss: 3120529545106227724288.0000 - val_mse: 3120529545106227724288.0000 - val_mae: 55861702656.0000\n",
      "Epoch 114/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3069013735045587795968.0000 - mse: 3069013994868643463168.0000 - mae: 55150989312.0000 - val_loss: 2650107171929149407232.0000 - val_mse: 2650106890454172696576.0000 - val_mae: 51479187456.0000\n",
      "Epoch 115/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3056628296937479077888.0000 - mse: 3056628532943421177856.0000 - mae: 55035514880.0000 - val_loss: 3381072635173795790848.0000 - val_mse: 3381072353698819080192.0000 - val_mae: 58146988032.0000\n",
      "Epoch 116/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3070391152330850435072.0000 - mse: 3070391533404665413632.0000 - mae: 55174590464.0000 - val_loss: 2542624544697301860352.0000 - val_mse: 2542624544697301860352.0000 - val_mae: 50424446976.0000\n",
      "Epoch 117/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3069536048673141030912.0000 - mse: 3069535849475465019392.0000 - mae: 55145451520.0000 - val_loss: 2781099152165824364544.0000 - val_mse: 2781099152165824364544.0000 - val_mae: 52736151552.0000\n",
      "Epoch 118/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3055115078801914068992.0000 - mse: 3055114760518671269888.0000 - mae: 54934986752.0000 - val_loss: 3453413111563317936128.0000 - val_mse: 3453413393038294646784.0000 - val_mae: 58765742080.0000\n",
      "Epoch 119/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3071063496451374448640.0000 - mse: 3071064258599003881472.0000 - mae: 55181344768.0000 - val_loss: 3057767380699192492032.0000 - val_mse: 3057767380699192492032.0000 - val_mae: 55297052672.0000\n",
      "Epoch 120/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3044878715132143730688.0000 - mse: 3044878360040634843136.0000 - mae: 54920892416.0000 - val_loss: 3194899174602856988672.0000 - val_mse: 3194898893127880278016.0000 - val_mae: 56523440128.0000\n",
      "Epoch 121/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3091810449704378957824.0000 - mse: 3091810371757462781952.0000 - mae: 55353479168.0000 - val_loss: 2981094722943116640256.0000 - val_mse: 2981094722943116640256.0000 - val_mae: 54599401472.0000\n",
      "Epoch 122/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3061191307271664566272.0000 - mse: 3061191523790877622272.0000 - mae: 54980591616.0000 - val_loss: 3343020877497212338176.0000 - val_mse: 3343021158972189048832.0000 - val_mae: 57818869760.0000\n",
      "Epoch 123/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3063355083364556079104.0000 - mse: 3063354377511922302976.0000 - mae: 55047372800.0000 - val_loss: 3112622913010425397248.0000 - val_mse: 3112622631535448686592.0000 - val_mae: 55790882816.0000\n",
      "Epoch 124/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3076484206508631916544.0000 - mse: 3076483777800590852096.0000 - mae: 55182192640.0000 - val_loss: 3336159080514959966208.0000 - val_mse: 3336158799039983255552.0000 - val_mae: 57759498240.0000\n",
      "Epoch 125/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3058408611008828932096.0000 - mse: 3058408862171116077056.0000 - mae: 55040872448.0000 - val_loss: 3050584139293536550912.0000 - val_mse: 3050584139293536550912.0000 - val_mae: 55232086016.0000\n",
      "Epoch 126/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3038153420522443505664.0000 - mse: 3038153359897063849984.0000 - mae: 54858063872.0000 - val_loss: 3418053099089018486784.0000 - val_mse: 3418052817614041776128.0000 - val_mae: 58464137216.0000\n",
      "Epoch 127/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3085392391527334936576.0000 - mse: 3085391897863529693184.0000 - mae: 55310774272.0000 - val_loss: 3922665393262210056192.0000 - val_mse: 3922665393262210056192.0000 - val_mae: 62631182336.0000\n",
      "Epoch 128/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3072844224074421370880.0000 - mse: 3072844587826698780672.0000 - mae: 55178371072.0000 - val_loss: 2885653032264997535744.0000 - val_mse: 2885653313739974246400.0000 - val_mae: 53718282240.0000\n",
      "Epoch 129/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3049267741452840468480.0000 - mse: 3049267399352484102144.0000 - mae: 54925996032.0000 - val_loss: 2831007480286390779904.0000 - val_mse: 2831007480286390779904.0000 - val_mae: 53207224320.0000\n",
      "Epoch 130/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3068362109648542105600.0000 - mse: 3068362098822581583872.0000 - mae: 55156080640.0000 - val_loss: 2253929734834017533952.0000 - val_mse: 2253929875571505889280.0000 - val_mae: 47475576832.0000\n",
      "Epoch 131/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3052255778041570525184.0000 - mse: 3052255256230267715584.0000 - mae: 54926192640.0000 - val_loss: 2989450026151795752960.0000 - val_mse: 2989450026151795752960.0000 - val_mae: 54675886080.0000\n",
      "Epoch 132/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3094309921515264540672.0000 - mse: 3094309869550653407232.0000 - mae: 55345782784.0000 - val_loss: 3215596874065321656320.0000 - val_mse: 3215596874065321656320.0000 - val_mae: 56706207744.0000\n",
      "Epoch 133/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3048803970290059051008.0000 - mse: 3048804373015795073024.0000 - mae: 54962675712.0000 - val_loss: 3067474889695989596160.0000 - val_mse: 3067474608221012885504.0000 - val_mae: 55384784896.0000\n",
      "Epoch 134/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3043069870324115832832.0000 - mse: 3043069883315268878336.0000 - mae: 54933794816.0000 - val_loss: 3822034429913453297664.0000 - val_mse: 3822034148438476587008.0000 - val_mae: 61822603264.0000\n",
      "Epoch 135/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3104145531939154558976.0000 - mse: 3104145449661853859840.0000 - mae: 55420399616.0000 - val_loss: 3400024908305677680640.0000 - val_mse: 3400025189780654391296.0000 - val_mae: 58309734400.0000\n",
      "Epoch 136/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3039126568289809465344.0000 - mse: 3039126137416575877120.0000 - mae: 54891270144.0000 - val_loss: 2611943668686811824128.0000 - val_mse: 2611943387211835113472.0000 - val_mae: 51107188736.0000\n",
      "Epoch 137/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3067722808525092028416.0000 - mse: 3067722869150471684096.0000 - mae: 55153684480.0000 - val_loss: 2481915740245370863616.0000 - val_mse: 2481915740245370863616.0000 - val_mae: 49818824704.0000\n",
      "Epoch 138/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3076490321011203375104.0000 - mse: 3076490251725055197184.0000 - mae: 55140372480.0000 - val_loss: 3192850036772403412992.0000 - val_mse: 3192850036772403412992.0000 - val_mae: 56505315328.0000\n",
      "Epoch 139/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3047596823713785315328.0000 - mse: 3047596282415752937472.0000 - mae: 54954848256.0000 - val_loss: 2819574248207380643840.0000 - val_mse: 2819574248207380643840.0000 - val_mae: 53099671552.0000\n",
      "Epoch 140/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3061682156327205339136.0000 - mse: 3061681853200307585024.0000 - mae: 54972375040.0000 - val_loss: 2885211679501515227136.0000 - val_mse: 2885211398026538516480.0000 - val_mae: 53714141184.0000\n",
      "Epoch 141/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3078388289457626284032.0000 - mse: 3078388518967991861248.0000 - mae: 55191433216.0000 - val_loss: 2937261469244896313344.0000 - val_mse: 2937261469244896313344.0000 - val_mae: 54196518912.0000\n",
      "Epoch 142/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3062418828305868783616.0000 - mse: 3062419036164312793088.0000 - mae: 55111757824.0000 - val_loss: 2591217821726676090880.0000 - val_mse: 2591217821726676090880.0000 - val_mae: 50903994368.0000\n",
      "Epoch 143/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3073719767145824911360.0000 - mse: 3073720537954222342144.0000 - mae: 55154376704.0000 - val_loss: 4242148217553082646528.0000 - val_mse: 4242147936078105935872.0000 - val_mae: 65131773952.0000\n",
      "Epoch 144/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3040384858869351841792.0000 - mse: 3040384330562472509440.0000 - mae: 54825725952.0000 - val_loss: 2525246279635185958912.0000 - val_mse: 2525245998160209248256.0000 - val_mae: 50251829248.0000\n",
      "Epoch 145/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3054812501862718636032.0000 - mse: 3054812174918707314688.0000 - mae: 54996213760.0000 - val_loss: 2385371793558452830208.0000 - val_mse: 2385371793558452830208.0000 - val_mae: 48840261632.0000\n",
      "Epoch 146/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3086159687983463661568.0000 - mse: 3086159480125019652096.0000 - mae: 55168405504.0000 - val_loss: 2620185818954853253120.0000 - val_mse: 2620185537479876542464.0000 - val_mae: 51187761152.0000\n",
      "Epoch 147/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3049929363541943910400.0000 - mse: 3049928865547754143744.0000 - mae: 55003332608.0000 - val_loss: 3336409593244232450048.0000 - val_mse: 3336409874719209160704.0000 - val_mae: 57761644544.0000\n",
      "Epoch 148/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3095239425504895303680.0000 - mse: 3095239581398728704000.0000 - mae: 55357886464.0000 - val_loss: 2832247377558801219584.0000 - val_mse: 2832247377558801219584.0000 - val_mae: 53218889728.0000\n",
      "Epoch 149/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3057095439304404500480.0000 - mse: 3057095218454807445504.0000 - mae: 55042023424.0000 - val_loss: 2413727019762330894336.0000 - val_mse: 2413727019762330894336.0000 - val_mae: 49129705472.0000\n",
      "Epoch 150/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3054026045621444018176.0000 - mse: 3054026015308754452480.0000 - mae: 55066169344.0000 - val_loss: 2763637289035625398272.0000 - val_mse: 2763637289035625398272.0000 - val_mae: 52570308608.0000\n",
      "Epoch 151/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3062084886393498632192.0000 - mse: 3062084925366957244416.0000 - mae: 55073742848.0000 - val_loss: 3245916514131663388672.0000 - val_mse: 3245916232656686678016.0000 - val_mae: 56972939264.0000\n",
      "Epoch 152/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3052911199020417810432.0000 - mse: 3052911374400980254720.0000 - mae: 54957932544.0000 - val_loss: 2645485352811560435712.0000 - val_mse: 2645485352811560435712.0000 - val_mae: 51434274816.0000\n",
      "Epoch 153/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3058640879829226356736.0000 - mse: 3058641360501879078912.0000 - mae: 55046406144.0000 - val_loss: 3098270503947949047808.0000 - val_mse: 3098270222472972337152.0000 - val_mae: 55662088192.0000\n",
      "Epoch 154/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3068889868734298587136.0000 - mse: 3068890145878890774528.0000 - mae: 55131336704.0000 - val_loss: 3113315059978156900352.0000 - val_mse: 3113314778503180189696.0000 - val_mae: 55797084160.0000\n",
      "Epoch 155/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3073406245160419328000.0000 - mse: 3073405567455283118080.0000 - mae: 55181410304.0000 - val_loss: 3189647133012412858368.0000 - val_mse: 3189647133012412858368.0000 - val_mae: 56476958720.0000\n",
      "Epoch 156/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3071821253070985625600.0000 - mse: 3071820863336402124800.0000 - mae: 55171416064.0000 - val_loss: 2637579283665711529984.0000 - val_mse: 2637579283665711529984.0000 - val_mae: 51357368320.0000\n",
      "Epoch 157/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3046137393281077608448.0000 - mse: 3046137679086438318080.0000 - mae: 54844907520.0000 - val_loss: 3278876108004574363648.0000 - val_mse: 3278876108004574363648.0000 - val_mae: 57261469696.0000\n",
      "Epoch 158/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3098595923664100655104.0000 - mse: 3098596170496003276800.0000 - mae: 55365636096.0000 - val_loss: 2899219281717521022976.0000 - val_mse: 2899219000242544312320.0000 - val_mae: 53844381696.0000\n",
      "Epoch 159/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3027200482187347492864.0000 - mse: 3027200042653345382400.0000 - mae: 54603948032.0000 - val_loss: 2640491705249736687616.0000 - val_mse: 2640491705249736687616.0000 - val_mae: 51385724928.0000\n",
      "Epoch 160/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3073320856478446256128.0000 - mse: 3073321124962269921280.0000 - mae: 55147065344.0000 - val_loss: 3686826236000629424128.0000 - val_mse: 3686826517475606134784.0000 - val_mae: 60719235072.0000\n",
      "Epoch 161/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3036110115719584153600.0000 - mse: 3036109851566144487424.0000 - mae: 54490882048.0000 - val_loss: 2396865261282479046656.0000 - val_mse: 2396864979807502336000.0000 - val_mae: 48957763584.0000\n",
      "Epoch 162/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3079045711074000109568.0000 - mse: 3079045481563634532352.0000 - mae: 55217553408.0000 - val_loss: 3848818462797332480000.0000 - val_mse: 3848818181322355769344.0000 - val_mae: 62038863872.0000\n",
      "Epoch 163/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3112972284081902977024.0000 - mse: 3112972786406476742656.0000 - mae: 55484661760.0000 - val_loss: 3347476626378542022656.0000 - val_mse: 3347476626378542022656.0000 - val_mae: 57857404928.0000\n",
      "Epoch 164/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3050735016376629985280.0000 - mse: 3050735009881053462528.0000 - mae: 54945161216.0000 - val_loss: 3134270027569335107584.0000 - val_mse: 3134270027569335107584.0000 - val_mae: 55984525312.0000\n",
      "Epoch 165/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3047296208438658334720.0000 - mse: 3047296230090579378176.0000 - mae: 54950850560.0000 - val_loss: 2865603006723944087552.0000 - val_mse: 2865603288198920798208.0000 - val_mae: 53531328512.0000\n",
      "Epoch 166/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3033620153432527601664.0000 - mse: 3033620205397138735104.0000 - mae: 54761111552.0000 - val_loss: 3595278188675348824064.0000 - val_mse: 3595277907200372113408.0000 - val_mae: 59960631296.0000\n",
      "Epoch 167/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3071303674718617337856.0000 - mse: 3071303512329207939072.0000 - mae: 55148380160.0000 - val_loss: 3194150732639783354368.0000 - val_mse: 3194150732639783354368.0000 - val_mae: 56516808704.0000\n",
      "Epoch 168/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3088554574518964191232.0000 - mse: 3088554550701850624000.0000 - mae: 55267774464.0000 - val_loss: 2628418117598709809152.0000 - val_mse: 2628418117598709809152.0000 - val_mae: 51268108288.0000\n",
      "Epoch 169/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3073597862498603106304.0000 - mse: 3073597533389399785472.0000 - mae: 55143161856.0000 - val_loss: 2815218422942783242240.0000 - val_mse: 2815218422942783242240.0000 - val_mae: 53058613248.0000\n",
      "Epoch 170/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3057421127504379772928.0000 - mse: 3057420603527884963840.0000 - mae: 55056482304.0000 - val_loss: 3603946492058130186240.0000 - val_mse: 3603945929108176764928.0000 - val_mae: 60032860160.0000\n",
      "Epoch 171/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3058308107120606707712.0000 - mse: 3058308375604430372864.0000 - mae: 55052406784.0000 - val_loss: 3399376108484359618560.0000 - val_mse: 3399376108484359618560.0000 - val_mae: 58304184320.0000\n",
      "Epoch 172/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3051271912573168517120.0000 - mse: 3051272064136617394176.0000 - mae: 54998896640.0000 - val_loss: 3416484439043810000896.0000 - val_mse: 3416484439043810000896.0000 - val_mae: 58450690048.0000\n",
      "Epoch 173/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3089947830184647327744.0000 - mse: 3089947570361591660544.0000 - mae: 55309684736.0000 - val_loss: 2851230049988167860224.0000 - val_mse: 2851230049988167860224.0000 - val_mae: 53396901888.0000\n",
      "Epoch 174/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3055261261748473167872.0000 - mse: 3055261127506560811008.0000 - mae: 55045890048.0000 - val_loss: 3670037098064768925696.0000 - val_mse: 3670037098064768925696.0000 - val_mae: 60580827136.0000\n",
      "Epoch 175/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3050429022764255412224.0000 - mse: 3050429328056345690112.0000 - mae: 54911897600.0000 - val_loss: 3026851576632177590272.0000 - val_mse: 3026851576632177590272.0000 - val_mae: 55016820736.0000\n",
      "Epoch 176/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3061430625957632802816.0000 - mse: 3061430777521081679872.0000 - mae: 55085481984.0000 - val_loss: 3361109585400545935360.0000 - val_mse: 3361109303925569224704.0000 - val_mae: 57975099392.0000\n",
      "Epoch 177/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3064475680715875811328.0000 - mse: 3064476055294114267136.0000 - mae: 55081562112.0000 - val_loss: 3809745232430266056704.0000 - val_mse: 3809744950955289346048.0000 - val_mae: 61723119616.0000\n",
      "Epoch 178/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3085683518930554978304.0000 - mse: 3085682942989448511488.0000 - mae: 55314022400.0000 - val_loss: 2608886850439734099968.0000 - val_mse: 2608886850439734099968.0000 - val_mae: 51077267456.0000\n",
      "Epoch 179/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3044604147118323531776.0000 - mse: 3044604203413318664192.0000 - mae: 54900256768.0000 - val_loss: 2856124336883212746752.0000 - val_mse: 2856124055408236036096.0000 - val_mae: 53442723840.0000\n",
      "Epoch 180/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3110754460463099019264.0000 - mse: 3110754763589996773376.0000 - mae: 55552729088.0000 - val_loss: 2536422806535435976704.0000 - val_mse: 2536422525060459266048.0000 - val_mae: 50362892288.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6240 samples, validate on 960 samples\n",
      "Epoch 1/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 4031929734828353650688.0000 - mse: 4031930320046589476864.0000 - mae: 37880180736.0000 - val_loss: 1933748289650949095424.0000 - val_mse: 1933748289650949095424.0000 - val_mae: 43974397952.0000\n",
      "Epoch 2/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3044289806690293383168.0000 - mse: 3044289795864332861440.0000 - mae: 54618411008.0000 - val_loss: 3069617758693687820288.0000 - val_mse: 3069617758693687820288.0000 - val_mae: 55404134400.0000\n",
      "Epoch 3/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3050429947301294243840.0000 - mse: 3050429609531322400768.0000 - mae: 55020621824.0000 - val_loss: 3240001880446042374144.0000 - val_mse: 3240001598971065663488.0000 - val_mae: 56921026560.0000\n",
      "Epoch 4/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3029235041779199770624.0000 - mse: 3029235106734963425280.0000 - mae: 54822248448.0000 - val_loss: 3127646921367333371904.0000 - val_mse: 3127646639892356661248.0000 - val_mae: 55925395456.0000\n",
      "Epoch 5/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3051442148639082807296.0000 - mse: 3051442075022550630400.0000 - mae: 54970376192.0000 - val_loss: 2842254375930818461696.0000 - val_mse: 2842254375930818461696.0000 - val_mae: 53312798720.0000\n",
      "Epoch 6/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3040404653055790809088.0000 - mse: 3040404596760795676672.0000 - mae: 54926028800.0000 - val_loss: 3093585634435576889344.0000 - val_mse: 3093585634435576889344.0000 - val_mae: 55620009984.0000\n",
      "Epoch 7/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3054260059586697363456.0000 - mse: 3054259921014401007616.0000 - mae: 54957334528.0000 - val_loss: 2684954899945835462656.0000 - val_mse: 2684954899945835462656.0000 - val_mae: 51816558592.0000\n",
      "Epoch 8/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3038199720990920409088.0000 - mse: 3038199803268221108224.0000 - mae: 54881665024.0000 - val_loss: 3099200778745977765888.0000 - val_mse: 3099200497271001055232.0000 - val_mae: 55670468608.0000\n",
      "Epoch 9/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3050719585052329508864.0000 - mse: 3050719247282357665792.0000 - mae: 55002746880.0000 - val_loss: 2983787875520284196864.0000 - val_mse: 2983787594045307486208.0000 - val_mae: 54624043008.0000\n",
      "Epoch 10/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3013295388827475509248.0000 - mse: 3013294897328862265344.0000 - mae: 54497259520.0000 - val_loss: 3619909500937344909312.0000 - val_mse: 3619909219462368198656.0000 - val_mae: 60165677056.0000\n",
      "Epoch 11/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3086961205301184823296.0000 - mse: 3086961120858691600384.0000 - mae: 55245074432.0000 - val_loss: 3010210776009043607552.0000 - val_mse: 3010210776009043607552.0000 - val_mae: 54865383424.0000\n",
      "Epoch 12/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3066062329177288212480.0000 - mse: 3066061322362948681728.0000 - mae: 55020048384.0000 - val_loss: 3961849243295123767296.0000 - val_mse: 3961849243295123767296.0000 - val_mae: 62943219712.0000\n",
      "Epoch 13/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3024912162078030036992.0000 - mse: 3024911932567664459776.0000 - mae: 54680436736.0000 - val_loss: 2701503376776608350208.0000 - val_mse: 2701503376776608350208.0000 - val_mae: 51975995392.0000\n",
      "Epoch 14/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3044768283838011932672.0000 - mse: 3044768584799717687296.0000 - mae: 54835830784.0000 - val_loss: 2923918147973927665664.0000 - val_mse: 2923918147973927665664.0000 - val_mae: 54073274368.0000\n",
      "Epoch 15/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3013671212051187367936.0000 - mse: 3013670666422770991104.0000 - mae: 54598127616.0000 - val_loss: 2738922941655499669504.0000 - val_mse: 2738922941655499669504.0000 - val_mae: 52334702592.0000\n",
      "Epoch 16/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3073552034042010927104.0000 - mse: 3073551371493219237888.0000 - mae: 55094059008.0000 - val_loss: 2850160445076667367424.0000 - val_mse: 2850160445076667367424.0000 - val_mae: 53386887168.0000\n",
      "Epoch 17/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3044213884228306206720.0000 - mse: 3044213516145644273664.0000 - mae: 54970990592.0000 - val_loss: 2680979347374774157312.0000 - val_mse: 2680979347374774157312.0000 - val_mae: 51778162688.0000\n",
      "Epoch 18/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3031053928748319506432.0000 - mse: 3031053998034467684352.0000 - mae: 54808518656.0000 - val_loss: 2575838310474182557696.0000 - val_mse: 2575838310474182557696.0000 - val_mae: 50752720896.0000\n",
      "Epoch 19/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3052578874506568269824.0000 - mse: 3052578389503531548672.0000 - mae: 55017078784.0000 - val_loss: 2327533470919017103360.0000 - val_mse: 2327533752393993814016.0000 - val_mae: 48244514816.0000\n",
      "Epoch 20/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3028911804576713670656.0000 - mse: 3028911691986722881536.0000 - mae: 54675865600.0000 - val_loss: 3550753913909303836672.0000 - val_mse: 3550754195384280547328.0000 - val_mae: 59588210688.0000\n",
      "Epoch 21/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3054947525409046528000.0000 - mse: 3054947564382505140224.0000 - mae: 55037792256.0000 - val_loss: 3447175626079409799168.0000 - val_mse: 3447175626079409799168.0000 - val_mae: 58712649728.0000\n",
      "Epoch 22/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3029869930241091698688.0000 - mse: 3029869551332469243904.0000 - mae: 54712303616.0000 - val_loss: 2717236420599850467328.0000 - val_mse: 2717236420599850467328.0000 - val_mae: 52127117312.0000\n",
      "Epoch 23/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3058821924534246965248.0000 - mse: 3058822067436927320064.0000 - mae: 55040798720.0000 - val_loss: 2659037528515248390144.0000 - val_mse: 2659037528515248390144.0000 - val_mae: 51565834240.0000\n",
      "Epoch 24/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3052521423298629206016.0000 - mse: 3052521250083259285504.0000 - mae: 54953164800.0000 - val_loss: 2642533524730795786240.0000 - val_mse: 2642533243255819075584.0000 - val_mae: 51405570048.0000\n",
      "Epoch 25/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3035478451228771352576.0000 - mse: 3035478221718405775360.0000 - mae: 54835146752.0000 - val_loss: 2838331177705425338368.0000 - val_mse: 2838330896230448627712.0000 - val_mae: 53276000256.0000\n",
      "Epoch 26/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3047956158999438032896.0000 - mse: 3047955444486035734528.0000 - mae: 54954868736.0000 - val_loss: 3240042412842688708608.0000 - val_mse: 3240042412842688708608.0000 - val_mae: 56921374720.0000\n",
      "Epoch 27/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3019622405005898678272.0000 - mse: 3019622454805317812224.0000 - mae: 54646190080.0000 - val_loss: 3540622785072557195264.0000 - val_mse: 3540622222122603773952.0000 - val_mae: 59503144960.0000\n",
      "Epoch 28/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3076983852739791421440.0000 - mse: 3076983677359228977152.0000 - mae: 55136321536.0000 - val_loss: 3003702511597539819520.0000 - val_mse: 3003702511597539819520.0000 - val_mae: 54806048768.0000\n",
      "Epoch 29/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 2997650496471362961408.0000 - mse: 2997650236648307294208.0000 - mae: 54486585344.0000 - val_loss: 3318967151887426519040.0000 - val_mse: 3318967151887426519040.0000 - val_mae: 57610469376.0000\n",
      "Epoch 30/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3079547732520924610560.0000 - mse: 3079547069972132921344.0000 - mae: 55123955712.0000 - val_loss: 2297921036731660894208.0000 - val_mse: 2297921036731660894208.0000 - val_mae: 47936630784.0000\n",
      "Epoch 31/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3028240530061100843008.0000 - mse: 3028240937117221388288.0000 - mae: 54733479936.0000 - val_loss: 3228652809385068724224.0000 - val_mse: 3228652527910092013568.0000 - val_mae: 56821235712.0000\n",
      "Epoch 32/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3017542662184708341760.0000 - mse: 3017542636202402775040.0000 - mae: 54655635456.0000 - val_loss: 3315459973677611745280.0000 - val_mse: 3315459973677611745280.0000 - val_mae: 57580056576.0000\n",
      "Epoch 33/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3063077406134839017472.0000 - mse: 3063077124659862306816.0000 - mae: 55087218688.0000 - val_loss: 3244000232490217242624.0000 - val_mse: 3244000232490217242624.0000 - val_mae: 56956112896.0000\n",
      "Epoch 34/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3032408100530890473472.0000 - mse: 3032407892672445939712.0000 - mae: 54820139008.0000 - val_loss: 3341040982511029583872.0000 - val_mse: 3341040701036052873216.0000 - val_mae: 57801715712.0000\n",
      "Epoch 35/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3079817153873055318016.0000 - mse: 3079817285949775151104.0000 - mae: 55206924288.0000 - val_loss: 2528599772507716714496.0000 - val_mse: 2528599772507716714496.0000 - val_mae: 50285166592.0000\n",
      "Epoch 36/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3020859701882563067904.0000 - mse: 3020859537327961145344.0000 - mae: 54704693248.0000 - val_loss: 2974853859759487975424.0000 - val_mse: 2974853859759487975424.0000 - val_mae: 54542209024.0000\n",
      "Epoch 37/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3027639840808879259648.0000 - mse: 3027639425091990716416.0000 - mae: 54694277120.0000 - val_loss: 2502371089752887656448.0000 - val_mse: 2502371089752887656448.0000 - val_mae: 50023710720.0000\n",
      "Epoch 38/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3073885421665195130880.0000 - mse: 3073885763765551497216.0000 - mae: 55152386048.0000 - val_loss: 3263840840648597962752.0000 - val_mse: 3263840559173621252096.0000 - val_mae: 57130045440.0000\n",
      "Epoch 39/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3002362158071133765632.0000 - mse: 3002362409233420386304.0000 - mae: 54507466752.0000 - val_loss: 2951478770318551547904.0000 - val_mse: 2951478770318551547904.0000 - val_mae: 54327508992.0000\n",
      "Epoch 40/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3054845280706352381952.0000 - mse: 3054845107490982461440.0000 - mae: 55031652352.0000 - val_loss: 3302238531071558811648.0000 - val_mse: 3302238249596582100992.0000 - val_mae: 57465110528.0000\n",
      "Epoch 41/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3063506802707387121664.0000 - mse: 3063506373999346057216.0000 - mae: 55136509952.0000 - val_loss: 3087759102417666310144.0000 - val_mse: 3087758820942689599488.0000 - val_mae: 55567585280.0000\n",
      "Epoch 42/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3025994857741139574784.0000 - mse: 3025995048278047064064.0000 - mae: 54775955456.0000 - val_loss: 3173178313600049086464.0000 - val_mse: 3173178313600049086464.0000 - val_mae: 56330948608.0000\n",
      "Epoch 43/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3011425578704684187648.0000 - mse: 3011425622008526798848.0000 - mae: 54550114304.0000 - val_loss: 3238895402312592785408.0000 - val_mse: 3238895402312592785408.0000 - val_mae: 56911327232.0000\n",
      "Epoch 44/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3067798867394183561216.0000 - mse: 3067798585919206850560.0000 - mae: 55122468864.0000 - val_loss: 3485960626070324510720.0000 - val_mse: 3485960626070324510720.0000 - val_mae: 59042033664.0000\n",
      "Epoch 45/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3042660926156413861888.0000 - mse: 3042660055749178163200.0000 - mae: 54853394432.0000 - val_loss: 2767997054949896749056.0000 - val_mse: 2767996773474920038400.0000 - val_mae: 52611764224.0000\n",
      "Epoch 46/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3056263174093266223104.0000 - mse: 3056262896948674035712.0000 - mae: 54988152832.0000 - val_loss: 2678624527719612809216.0000 - val_mse: 2678624246244636098560.0000 - val_mae: 51755433984.0000\n",
      "Epoch 47/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3040231091254767124480.0000 - mse: 3040230926700165201920.0000 - mae: 54801022976.0000 - val_loss: 2745822456284631269376.0000 - val_mse: 2745822456284631269376.0000 - val_mae: 52400607232.0000\n",
      "Epoch 48/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3021968219626997809152.0000 - mse: 3021968267261224419328.0000 - mae: 54715506688.0000 - val_loss: 3521337245568249888768.0000 - val_mse: 3521337245568249888768.0000 - val_mae: 59340877824.0000\n",
      "Epoch 49/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3062953405581637451776.0000 - mse: 3062953557145086328832.0000 - mae: 55033245696.0000 - val_loss: 2696400235448844156928.0000 - val_mse: 2696399953973867446272.0000 - val_mae: 51926876160.0000\n",
      "Epoch 50/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3031731267967084134400.0000 - mse: 3031731508303410233344.0000 - mae: 54807207936.0000 - val_loss: 3015426225852515352576.0000 - val_mse: 3015425944377538641920.0000 - val_mae: 54912888832.0000\n",
      "Epoch 51/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3044648520565806071808.0000 - mse: 3044648394984662237184.0000 - mae: 54935449600.0000 - val_loss: 2942121697667759210496.0000 - val_mse: 2942121697667759210496.0000 - val_mae: 54241345536.0000\n",
      "Epoch 52/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3058611996166231949312.0000 - mse: 3058611805629324460032.0000 - mae: 55042560000.0000 - val_loss: 3144626054912473563136.0000 - val_mse: 3144626054912473563136.0000 - val_mae: 56076972032.0000\n",
      "Epoch 53/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3001917020551810383872.0000 - mse: 3001916552870310707200.0000 - mae: 54516613120.0000 - val_loss: 3222208158318301544448.0000 - val_mse: 3222208158318301544448.0000 - val_mae: 56764485632.0000\n",
      "Epoch 54/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3057305508409908396032.0000 - mse: 3057305480262410305536.0000 - mae: 54995021824.0000 - val_loss: 3346665134020685201408.0000 - val_mse: 3346664852545708490752.0000 - val_mae: 57850343424.0000\n",
      "Epoch 55/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3062636469088245776384.0000 - mse: 3062636334846333419520.0000 - mae: 55070695424.0000 - val_loss: 2848607829105131388928.0000 - val_mse: 2848607829105131388928.0000 - val_mae: 53372342272.0000\n",
      "Epoch 56/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3024381438844250095616.0000 - mse: 3024381070761588162560.0000 - mae: 54734004224.0000 - val_loss: 3065113877591340613632.0000 - val_mse: 3065113596116363902976.0000 - val_mae: 55363457024.0000\n",
      "Epoch 57/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3061651964888165122048.0000 - mse: 3061651735377799544832.0000 - mae: 55103168512.0000 - val_loss: 3161027601805403488256.0000 - val_mse: 3161027601805403488256.0000 - val_mae: 56223023104.0000\n",
      "Epoch 58/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3070661000225831256064.0000 - mse: 3070661186432354222080.0000 - mae: 55174561792.0000 - val_loss: 2658703417717892841472.0000 - val_mse: 2658703417717892841472.0000 - val_mae: 51562606592.0000\n",
      "Epoch 59/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3017349851825661542400.0000 - mse: 3017349544368379265024.0000 - mae: 54583160832.0000 - val_loss: 3905202967182057668608.0000 - val_mse: 3905202967182057668608.0000 - val_mae: 62491594752.0000\n",
      "Epoch 60/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3035336646300696838144.0000 - mse: 3035336921280097026048.0000 - mae: 54731776000.0000 - val_loss: 4585676323204439736320.0000 - val_mse: 4585676323204439736320.0000 - val_mae: 67717607424.0000\n",
      "Epoch 61/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3027430650771380109312.0000 - mse: 3027430570659271409664.0000 - mae: 54659502080.0000 - val_loss: 3190502816941613252608.0000 - val_mse: 3190502816941613252608.0000 - val_mae: 56484548608.0000\n",
      "Epoch 62/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3076712560661661483008.0000 - mse: 3076713179906610036736.0000 - mae: 55137705984.0000 - val_loss: 2444446354295601758208.0000 - val_mse: 2444446072820625047552.0000 - val_mae: 49441349632.0000\n",
      "Epoch 63/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3038326635892726824960.0000 - mse: 3038326467007740903424.0000 - mae: 54804307968.0000 - val_loss: 2606282643955207110656.0000 - val_mse: 2606282362480230400000.0000 - val_mae: 51051757568.0000\n",
      "Epoch 64/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3055174389909891317760.0000 - mse: 3055174996163687350272.0000 - mae: 55019241472.0000 - val_loss: 2875008493070730657792.0000 - val_mse: 2875008211595753947136.0000 - val_mae: 53619109888.0000\n",
      "Epoch 65/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3024016833480956051456.0000 - mse: 3024016279191771152384.0000 - mae: 54710386688.0000 - val_loss: 3249626917274663256064.0000 - val_mse: 3249626635799686545408.0000 - val_mae: 57005490176.0000\n",
      "Epoch 66/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3058789386026939318272.0000 - mse: 3058789697814605594624.0000 - mae: 55071969280.0000 - val_loss: 3579100414388903870464.0000 - val_mse: 3579100414388903870464.0000 - val_mae: 59825582080.0000\n",
      "Epoch 67/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3052263063913083437056.0000 - mse: 3052263137529615613952.0000 - mae: 55022403584.0000 - val_loss: 3439967051925849899008.0000 - val_mse: 3439966770450873188352.0000 - val_mae: 58651230208.0000\n",
      "Epoch 68/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3031774712547143319552.0000 - mse: 3031774573974846963712.0000 - mae: 54802948096.0000 - val_loss: 2956184187504223584256.0000 - val_mse: 2956184187504223584256.0000 - val_mae: 54370775040.0000\n",
      "Epoch 69/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3027798605686897115136.0000 - mse: 3027797895503878815744.0000 - mae: 54742855680.0000 - val_loss: 2935333928604381741056.0000 - val_mse: 2935333928604381741056.0000 - val_mae: 54178738176.0000\n",
      "Epoch 70/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3058865713379855040512.0000 - mse: 3058865414583340761088.0000 - mae: 55088095232.0000 - val_loss: 2735234493560683233280.0000 - val_mse: 2735234493560683233280.0000 - val_mae: 52299476992.0000\n",
      "Epoch 71/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3019652174232474353664.0000 - mse: 3019651728202895720448.0000 - mae: 54694379520.0000 - val_loss: 3488471382862583562240.0000 - val_mse: 3488471101387606851584.0000 - val_mae: 59063275520.0000\n",
      "Epoch 72/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3041077419388779888640.0000 - mse: 3041077603430110855168.0000 - mae: 54802460672.0000 - val_loss: 3542014397357414678528.0000 - val_mse: 3542014678832391389184.0000 - val_mae: 59514818560.0000\n",
      "Epoch 73/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3066755049920933658624.0000 - mse: 3066755439655517159424.0000 - mae: 55126732800.0000 - val_loss: 3085029921043479789568.0000 - val_mse: 3085029921043479789568.0000 - val_mae: 55543066624.0000\n",
      "Epoch 74/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3027407563327713312768.0000 - mse: 3027407489711181135872.0000 - mae: 54801813504.0000 - val_loss: 3053464191255239983104.0000 - val_mse: 3053464191255239983104.0000 - val_mae: 55258165248.0000\n",
      "Epoch 75/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3042260114450346409984.0000 - mse: 3042259798332295610368.0000 - mae: 54885564416.0000 - val_loss: 3440402775189797994496.0000 - val_mse: 3440402212239844573184.0000 - val_mae: 58654969856.0000\n",
      "Epoch 76/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3056450162250679844864.0000 - mse: 3056450359283163332608.0000 - mae: 54928670720.0000 - val_loss: 3071149826991923920896.0000 - val_mse: 3071149826991923920896.0000 - val_mae: 55417978880.0000\n",
      "Epoch 77/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3034390847075106422784.0000 - mse: 3034390320933419089920.0000 - mae: 54801313792.0000 - val_loss: 3203058852702722195456.0000 - val_mse: 3203058852702722195456.0000 - val_mae: 56595574784.0000\n",
      "Epoch 78/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3043654996175318155264.0000 - mse: 3043655351266827042816.0000 - mae: 54829424640.0000 - val_loss: 2693900174705700110336.0000 - val_mse: 2693900174705700110336.0000 - val_mae: 51902803968.0000\n",
      "Epoch 79/180\n",
      "6240/6240 [==============================] - 63s 10ms/step - loss: 3042807483681210892288.0000 - mse: 3042807548636974546944.0000 - mae: 54872449024.0000 - val_loss: 3107407181691976941568.0000 - val_mse: 3107406900217000230912.0000 - val_mae: 55744131072.0000\n",
      "Epoch 80/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3059055275950708097024.0000 - mse: 3059055410192620453888.0000 - mae: 55072145408.0000 - val_loss: 2623239822452163870720.0000 - val_mse: 2623239822452163870720.0000 - val_mae: 51217567744.0000\n",
      "Epoch 81/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3044016947013062492160.0000 - mse: 3044017891036830367744.0000 - mae: 54945980416.0000 - val_loss: 2996330400482511028224.0000 - val_mse: 2996330119007534317568.0000 - val_mae: 54738731008.0000\n",
      "Epoch 82/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3040064724387186081792.0000 - mse: 3040064293513952493568.0000 - mae: 54896758784.0000 - val_loss: 2570727850797023887360.0000 - val_mse: 2570727850797023887360.0000 - val_mae: 50702331904.0000\n",
      "Epoch 83/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3054988995333884739584.0000 - mse: 3054988941204081606656.0000 - mae: 54940397568.0000 - val_loss: 3521112065586881363968.0000 - val_mse: 3521112065586881363968.0000 - val_mae: 59338956800.0000\n",
      "Epoch 84/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3031755990130807537664.0000 - mse: 3031756278101360771072.0000 - mae: 54757089280.0000 - val_loss: 2731645406132645658624.0000 - val_mse: 2731645124657668947968.0000 - val_mae: 52265140224.0000\n",
      "Epoch 85/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3002094358447906619392.0000 - mse: 3002094445055591841792.0000 - mae: 54442844160.0000 - val_loss: 3211683245989136695296.0000 - val_mse: 3211682964514159984640.0000 - val_mae: 56671727616.0000\n",
      "Epoch 86/180\n",
      "6240/6240 [==============================] - 63s 10ms/step - loss: 3068359323046273089536.0000 - mse: 3068359284072814477312.0000 - mae: 55124049920.0000 - val_loss: 3462033845675035197440.0000 - val_mse: 3462033845675035197440.0000 - val_mae: 58839048192.0000\n",
      "Epoch 87/180\n",
      "6240/6240 [==============================] - 61s 10ms/step - loss: 3057956226591451774976.0000 - mse: 3057956250408565342208.0000 - mae: 55052292096.0000 - val_loss: 2822422774971692482560.0000 - val_mse: 2822422774971692482560.0000 - val_mae: 53126483968.0000\n",
      "Epoch 88/180\n",
      "6240/6240 [==============================] - 62s 10ms/step - loss: 3030162571113609035776.0000 - mse: 3030162285308248326144.0000 - mae: 54756794368.0000 - val_loss: 2640420210605652180992.0000 - val_mse: 2640420210605652180992.0000 - val_mae: 51385024512.0000\n",
      "Epoch 89/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3023148621750100033536.0000 - mse: 3023148773313548910592.0000 - mae: 54730559488.0000 - val_loss: 2912019074808461393920.0000 - val_mse: 2912018793333484683264.0000 - val_mae: 53963128832.0000\n",
      "Epoch 90/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3052090316224299270144.0000 - mse: 3052091437793822113792.0000 - mae: 54943059968.0000 - val_loss: 3264817558817783939072.0000 - val_mse: 3264817558817783939072.0000 - val_mae: 57138606080.0000\n",
      "Epoch 91/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3034003734539636047872.0000 - mse: 3034003574315418648576.0000 - mae: 54783782912.0000 - val_loss: 4018120594739211272192.0000 - val_mse: 4018120594739211272192.0000 - val_mae: 63388647424.0000\n",
      "Epoch 92/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3039683148247988699136.0000 - mse: 3039683739345439686656.0000 - mae: 54783074304.0000 - val_loss: 2787801352836281794560.0000 - val_mse: 2787801352836281794560.0000 - val_mae: 52799614976.0000\n",
      "Epoch 93/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3049460616767651446784.0000 - mse: 3049460491186507612160.0000 - mae: 54970372096.0000 - val_loss: 3193404542476523405312.0000 - val_mse: 3193404261001546694656.0000 - val_mae: 56510238720.0000\n",
      "Epoch 94/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3038886294754112831488.0000 - mse: 3038886320736418398208.0000 - mae: 54857199616.0000 - val_loss: 2463018636208924262400.0000 - val_mse: 2463018354733947551744.0000 - val_mae: 49628790784.0000\n",
      "Epoch 95/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3034019687675239333888.0000 - mse: 3034019618389091155968.0000 - mae: 54826946560.0000 - val_loss: 3628447481405909237760.0000 - val_mse: 3628447762880885948416.0000 - val_mae: 60236615680.0000\n",
      "Epoch 96/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3042528942539834130432.0000 - mse: 3042528888410030997504.0000 - mae: 54808006656.0000 - val_loss: 3494051061325918896128.0000 - val_mse: 3494051342800895606784.0000 - val_mae: 59110498304.0000\n",
      "Epoch 97/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3032882416179337494528.0000 - mse: 3032881896533226684416.0000 - mae: 54777122816.0000 - val_loss: 2764661013525922054144.0000 - val_mse: 2764661013525922054144.0000 - val_mae: 52580052992.0000\n",
      "Epoch 98/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3048901226390089105408.0000 - mse: 3048900918932806828032.0000 - mae: 54984237056.0000 - val_loss: 3523899512281246990336.0000 - val_mse: 3523898949331293569024.0000 - val_mae: 59362426880.0000\n",
      "Epoch 99/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3027166075119232614400.0000 - mse: 3027165702706186682368.0000 - mae: 54732197888.0000 - val_loss: 3484907909657426657280.0000 - val_mse: 3484908191132403367936.0000 - val_mae: 59033120768.0000\n",
      "Epoch 100/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3059692431368759279616.0000 - mse: 3059692106589939957760.0000 - mae: 54964977664.0000 - val_loss: 2838857817386850975744.0000 - val_mse: 2838857817386850975744.0000 - val_mae: 53280911360.0000\n",
      "Epoch 101/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3042897629290290479104.0000 - mse: 3042897339154545246208.0000 - mae: 54813319168.0000 - val_loss: 3613998526426421133312.0000 - val_mse: 3613997963476467712000.0000 - val_mae: 60116561920.0000\n",
      "Epoch 102/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3047624923577229246464.0000 - mse: 3047624711388400713728.0000 - mae: 54954954752.0000 - val_loss: 3064489003143042957312.0000 - val_mse: 3064489003143042957312.0000 - val_mae: 55357820928.0000\n",
      "Epoch 103/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3047748720602370801664.0000 - mse: 3047748841853130113024.0000 - mae: 54992744448.0000 - val_loss: 2499254880285723983872.0000 - val_mse: 2499254880285723983872.0000 - val_mae: 49992560640.0000\n",
      "Epoch 104/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3052366599070285824000.0000 - mse: 3052366438846068424704.0000 - mae: 54981857280.0000 - val_loss: 3102949743960786993152.0000 - val_mse: 3102949743960786993152.0000 - val_mae: 55704154112.0000\n",
      "Epoch 105/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3008240587579173371904.0000 - mse: 3008239606747138883584.0000 - mae: 54513389568.0000 - val_loss: 3560965544589389725696.0000 - val_mse: 3560965826064366436352.0000 - val_mae: 59673808896.0000\n",
      "Epoch 106/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3070630650727765639168.0000 - mse: 3070630224184916049920.0000 - mae: 55151087616.0000 - val_loss: 3196240402866883264512.0000 - val_mse: 3196240402866883264512.0000 - val_mae: 56535289856.0000\n",
      "Epoch 107/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3053646617352838053888.0000 - mse: 3053646868515125198848.0000 - mae: 55071502336.0000 - val_loss: 3098336650567476051968.0000 - val_mse: 3098336650567476051968.0000 - val_mae: 55662694400.0000\n",
      "Epoch 108/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3018944745338699251712.0000 - mse: 3018944663061398552576.0000 - mae: 54670376960.0000 - val_loss: 3041402425553234952192.0000 - val_mse: 3041402144078258241536.0000 - val_mae: 55148933120.0000\n",
      "Epoch 109/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3051356402700408324096.0000 - mse: 3051357069579584012288.0000 - mae: 55018627072.0000 - val_loss: 2897603052401248436224.0000 - val_mse: 2897602770926271725568.0000 - val_mae: 53829390336.0000\n",
      "Epoch 110/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3049184749638553436160.0000 - mse: 3049184927184307879936.0000 - mae: 54948892672.0000 - val_loss: 2888619497044551139328.0000 - val_mse: 2888619497044551139328.0000 - val_mae: 53745893376.0000\n",
      "Epoch 111/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3032064882935442440192.0000 - mse: 3032065619100765782016.0000 - mae: 54800089088.0000 - val_loss: 3046611964422195773440.0000 - val_mse: 3046611964422195773440.0000 - val_mae: 55196119040.0000\n",
      "Epoch 112/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3033163336866863251456.0000 - mse: 3033163090034960629760.0000 - mae: 54844002304.0000 - val_loss: 2878839367503762685952.0000 - val_mse: 2878839367503762685952.0000 - val_mae: 53654802432.0000\n",
      "Epoch 113/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3067552957863376715776.0000 - mse: 3067552858264538447872.0000 - mae: 55124205568.0000 - val_loss: 3057566126090844372992.0000 - val_mse: 3057566126090844372992.0000 - val_mae: 55295262720.0000\n",
      "Epoch 114/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3040371638206214963200.0000 - mse: 3040371945663497240576.0000 - mae: 54864371712.0000 - val_loss: 2828310668534525984768.0000 - val_mse: 2828310387059549274112.0000 - val_mae: 53181857792.0000\n",
      "Epoch 115/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3035305824790747021312.0000 - mse: 3035305959032658853888.0000 - mae: 54843699200.0000 - val_loss: 3160839013571007348736.0000 - val_mse: 3160839013571007348736.0000 - val_mae: 56221364224.0000\n",
      "Epoch 116/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3054442087288943935488.0000 - mse: 3054441472374379380736.0000 - mae: 55025844224.0000 - val_loss: 2490894792002440790016.0000 - val_mse: 2490894510527464079360.0000 - val_mae: 49908858880.0000\n",
      "Epoch 117/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3035266530883998318592.0000 - mse: 3035266834010896072704.0000 - mae: 54851854336.0000 - val_loss: 3174313783656099872768.0000 - val_mse: 3174313783656099872768.0000 - val_mae: 56341041152.0000\n",
      "Epoch 118/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3043618854788308402176.0000 - mse: 3043619040994831368192.0000 - mae: 54928486400.0000 - val_loss: 3121890476618623746048.0000 - val_mse: 3121890476618623746048.0000 - val_mae: 55873892352.0000\n",
      "Epoch 119/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3062304963017020276736.0000 - mse: 3062305601748698398720.0000 - mae: 55045046272.0000 - val_loss: 3328318876513661353984.0000 - val_mse: 3328318876513661353984.0000 - val_mae: 57691598848.0000\n",
      "Epoch 120/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3047606281273002688512.0000 - mse: 3047605571089984389120.0000 - mae: 54928162816.0000 - val_loss: 3306182276970251812864.0000 - val_mse: 3306182276970251812864.0000 - val_mae: 57499406336.0000\n",
      "Epoch 121/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3016217930519509204992.0000 - mse: 3016218014962002427904.0000 - mae: 54628438016.0000 - val_loss: 3612673623711044075520.0000 - val_mse: 3612673623711044075520.0000 - val_mae: 60105515008.0000\n",
      "Epoch 122/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3034982135063106289664.0000 - mse: 3034982544284418310144.0000 - mae: 54763171840.0000 - val_loss: 2767744571895787290624.0000 - val_mse: 2767744571895787290624.0000 - val_mae: 52609368064.0000\n",
      "Epoch 123/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3040805135652654940160.0000 - mse: 3040804854177678229504.0000 - mae: 54845919232.0000 - val_loss: 3660631048768028934144.0000 - val_mse: 3660630767293052223488.0000 - val_mae: 60503146496.0000\n",
      "Epoch 124/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3027217420485369266176.0000 - mse: 3027216931151948021760.0000 - mae: 54690463744.0000 - val_loss: 3190151536170678353920.0000 - val_mse: 3190151536170678353920.0000 - val_mae: 56481419264.0000\n",
      "Epoch 125/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3079204939303133118464.0000 - mse: 3079205077875429474304.0000 - mae: 55139147776.0000 - val_loss: 3151773267521110540288.0000 - val_mse: 3151772986046133829632.0000 - val_mae: 56140673024.0000\n",
      "Epoch 126/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3028759236478567972864.0000 - mse: 3028758851074368995328.0000 - mae: 54691438592.0000 - val_loss: 3230373184442724253696.0000 - val_mse: 3230372902967747543040.0000 - val_mae: 56836362240.0000\n",
      "Epoch 127/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3038950873774538883072.0000 - mse: 3038951059981061849088.0000 - mae: 54854459392.0000 - val_loss: 2584513650731381686272.0000 - val_mse: 2584513369256404975616.0000 - val_mae: 50838114304.0000\n",
      "Epoch 128/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3043366575262258954240.0000 - mse: 3043366839415698620416.0000 - mae: 54892560384.0000 - val_loss: 3328160687576749965312.0000 - val_mse: 3328160406101773254656.0000 - val_mae: 57690210304.0000\n",
      "Epoch 129/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3040157808161984086016.0000 - mse: 3040157743206220431360.0000 - mae: 54822318080.0000 - val_loss: 3263348822389307736064.0000 - val_mse: 3263349103864284446720.0000 - val_mae: 57125720064.0000\n",
      "Epoch 130/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3039105438179827122176.0000 - mse: 3039105026793322577920.0000 - mae: 54912733184.0000 - val_loss: 2740655701612130467840.0000 - val_mse: 2740655420137153757184.0000 - val_mae: 52351254528.0000\n",
      "Epoch 131/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3054955434855892516864.0000 - mse: 3054955727156829749248.0000 - mae: 54909947904.0000 - val_loss: 2687139990190040285184.0000 - val_mse: 2687139990190040285184.0000 - val_mae: 51837620224.0000\n",
      "Epoch 132/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3049203266361636683776.0000 - mse: 3049203504532770783232.0000 - mae: 54886219776.0000 - val_loss: 2619766984189507796992.0000 - val_mse: 2619766702714531086336.0000 - val_mae: 51183665152.0000\n",
      "Epoch 133/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3058712785859815407616.0000 - mse: 3058712292196010164224.0000 - mae: 55007363072.0000 - val_loss: 2978050852544967606272.0000 - val_mse: 2978050571069990895616.0000 - val_mae: 54571520000.0000\n",
      "Epoch 134/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3037277851468734398464.0000 - mse: 3037277691244516999168.0000 - mae: 54785290240.0000 - val_loss: 3181166010489144082432.0000 - val_mse: 3181166010489144082432.0000 - val_mae: 56401809408.0000\n",
      "Epoch 135/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3019895812476738469888.0000 - mse: 3019895767007703859200.0000 - mae: 54685491200.0000 - val_loss: 3060835176470361931776.0000 - val_mse: 3060835176470361931776.0000 - val_mae: 55324831744.0000\n",
      "Epoch 136/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3089481187977103147008.0000 - mse: 3089480884850205392896.0000 - mae: 55309668352.0000 - val_loss: 2761368319248360800256.0000 - val_mse: 2761368319248360800256.0000 - val_mae: 52548726784.0000\n",
      "Epoch 137/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3034650358342865125376.0000 - mse: 3034649840861946314752.0000 - mae: 54794199040.0000 - val_loss: 2567365913675191812096.0000 - val_mse: 2567365913675191812096.0000 - val_mae: 50669162496.0000\n",
      "Epoch 138/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3035891877344179650560.0000 - mse: 3035891989934170439680.0000 - mae: 54831898624.0000 - val_loss: 2383304641329489772544.0000 - val_mse: 2383304641329489772544.0000 - val_mae: 48819081216.0000\n",
      "Epoch 139/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3027951243071190990848.0000 - mse: 3027951017891209412608.0000 - mae: 54610165760.0000 - val_loss: 3332424751998939693056.0000 - val_mse: 3332424470523962982400.0000 - val_mae: 57727135744.0000\n",
      "Epoch 140/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3067065464855634903040.0000 - mse: 3067064780654922170368.0000 - mae: 55052288000.0000 - val_loss: 2565548148275594395648.0000 - val_mse: 2565547866800617684992.0000 - val_mae: 50651246592.0000\n",
      "Epoch 141/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3009923238494373150720.0000 - mse: 3009923390057822027776.0000 - mae: 54618587136.0000 - val_loss: 2761670060423394623488.0000 - val_mse: 2761670060423394623488.0000 - val_mae: 52551602176.0000\n",
      "Epoch 142/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3040218844928087687168.0000 - mse: 3040218541801189933056.0000 - mae: 54881558528.0000 - val_loss: 2403932534997730197504.0000 - val_mse: 2403932816472706908160.0000 - val_mae: 49029914624.0000\n",
      "Epoch 143/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3048784422935522705408.0000 - mse: 3048784388292448616448.0000 - mae: 54987382784.0000 - val_loss: 3383623079937771044864.0000 - val_mse: 3383623079937771044864.0000 - val_mae: 58168926208.0000\n",
      "Epoch 144/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3065770684293149884416.0000 - mse: 3065770840186983284736.0000 - mae: 55115350016.0000 - val_loss: 2361002815974751076352.0000 - val_mse: 2361002956712239431680.0000 - val_mae: 48590143488.0000\n",
      "Epoch 145/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 2994540518427145011200.0000 - mse: 2994540501105607966720.0000 - mae: 54493585408.0000 - val_loss: 3147934793263707324416.0000 - val_mse: 3147934793263707324416.0000 - val_mae: 56106463232.0000\n",
      "Epoch 146/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3060791171105541324800.0000 - mse: 3060790703424041648128.0000 - mae: 55115513856.0000 - val_loss: 3081998154069329313792.0000 - val_mse: 3081998154069329313792.0000 - val_mae: 55515758592.0000\n",
      "Epoch 147/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3042637150181650333696.0000 - mse: 3042637256276064600064.0000 - mae: 54877585408.0000 - val_loss: 2752639498745586647040.0000 - val_mse: 2752639498745586647040.0000 - val_mae: 52465586176.0000\n",
      "Epoch 148/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3042230433996648480768.0000 - mse: 3042229961984764280832.0000 - mae: 54921457664.0000 - val_loss: 2384737067485970300928.0000 - val_mse: 2384737067485970300928.0000 - val_mae: 48833761280.0000\n",
      "Epoch 149/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3064647776681828810752.0000 - mse: 3064648036504884477952.0000 - mae: 54996361216.0000 - val_loss: 2989287052140280283136.0000 - val_mse: 2989287052140280283136.0000 - val_mae: 54674374656.0000\n",
      "Epoch 150/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 2992151451928086183936.0000 - mse: 2992151622978264629248.0000 - mae: 54364246016.0000 - val_loss: 3609814963847570653184.0000 - val_mse: 3609814963847570653184.0000 - val_mae: 60081713152.0000\n",
      "Epoch 151/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3039221440513306001408.0000 - mse: 3039220713008750657536.0000 - mae: 54873423872.0000 - val_loss: 3392313338368735838208.0000 - val_mse: 3392312775418782416896.0000 - val_mae: 58243551232.0000\n",
      "Epoch 152/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3064716296351928811520.0000 - mse: 3064716153449248456704.0000 - mae: 55115784192.0000 - val_loss: 2674831652408436719616.0000 - val_mse: 2674831652408436719616.0000 - val_mae: 51718778880.0000\n",
      "Epoch 153/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3063281417032766586880.0000 - mse: 3063281194017977532416.0000 - mae: 55001333760.0000 - val_loss: 2949746291836897460224.0000 - val_mse: 2949746291836897460224.0000 - val_mae: 54311567360.0000\n",
      "Epoch 154/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3009336939109037375488.0000 - mse: 3009336796206357020672.0000 - mae: 54600589312.0000 - val_loss: 3062446620712030437376.0000 - val_mse: 3062446620712030437376.0000 - val_mae: 55339380736.0000\n",
      "Epoch 155/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3058671105911341187072.0000 - mse: 3058671196849410408448.0000 - mae: 55073513472.0000 - val_loss: 3646523522935290855424.0000 - val_mse: 3646522959985337434112.0000 - val_mae: 60386447360.0000\n",
      "Epoch 156/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3063236132039398260736.0000 - mse: 3063235876546727116800.0000 - mae: 55068467200.0000 - val_loss: 3088780575108149280768.0000 - val_mse: 3088780575108149280768.0000 - val_mae: 55576801280.0000\n",
      "Epoch 157/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3040032867915398643712.0000 - mse: 3040033331266514321408.0000 - mae: 54814789632.0000 - val_loss: 3889987274366009737216.0000 - val_mse: 3889987274366009737216.0000 - val_mae: 62369779712.0000\n",
      "Epoch 158/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3010025052323833708544.0000 - mse: 3010025565474367995904.0000 - mae: 54506934272.0000 - val_loss: 3323223053535291637760.0000 - val_mse: 3323223335010268348416.0000 - val_mae: 57647386624.0000\n",
      "Epoch 159/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3054111847855113633792.0000 - mse: 3054111302226697781248.0000 - mae: 54976339968.0000 - val_loss: 3338900928263098466304.0000 - val_mse: 3338900646788121755648.0000 - val_mae: 57783218176.0000\n",
      "Epoch 160/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3030171461392488988672.0000 - mse: 3030171573982479777792.0000 - mae: 54692171776.0000 - val_loss: 2959481666856388919296.0000 - val_mse: 2959481666856388919296.0000 - val_mae: 54401122304.0000\n",
      "Epoch 161/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3083923100809673834496.0000 - mse: 3083923442910030200832.0000 - mae: 55209656320.0000 - val_loss: 2764859171909526355968.0000 - val_mse: 2764859171909526355968.0000 - val_mae: 52581937152.0000\n",
      "Epoch 162/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3037813639086903132160.0000 - mse: 3037813619600174088192.0000 - mae: 54721294336.0000 - val_loss: 3411684446265963184128.0000 - val_mse: 3411684164790986473472.0000 - val_mae: 58409619456.0000\n",
      "Epoch 163/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3018171483778255945728.0000 - mse: 3018171732775351091200.0000 - mae: 54648647680.0000 - val_loss: 3214985791890882822144.0000 - val_mse: 3214985510415906111488.0000 - val_mae: 56700833792.0000\n",
      "Epoch 164/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3064917529308355887104.0000 - mse: 3064917689532573286400.0000 - mae: 55083925504.0000 - val_loss: 3457243423046396542976.0000 - val_mse: 3457243141571419832320.0000 - val_mae: 58798333952.0000\n",
      "Epoch 165/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3018932373430877028352.0000 - mse: 3018931715212469862400.0000 - mae: 54680526848.0000 - val_loss: 2981418982116287315968.0000 - val_mse: 2981418982116287315968.0000 - val_mae: 54602371072.0000\n",
      "Epoch 166/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3052793345447669268480.0000 - mse: 3052792873435785068544.0000 - mae: 54976565248.0000 - val_loss: 3462566677805948469248.0000 - val_mse: 3462566396330971758592.0000 - val_mae: 58843590656.0000\n",
      "Epoch 167/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3030760471598369603584.0000 - mse: 3030760419633758470144.0000 - mae: 54768631808.0000 - val_loss: 2763359191758635270144.0000 - val_mse: 2763359191758635270144.0000 - val_mae: 52567654400.0000\n",
      "Epoch 168/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3032799086595078619136.0000 - mse: 3032799142890073751552.0000 - mae: 54777458688.0000 - val_loss: 3211031068468098105344.0000 - val_mse: 3211030786993121394688.0000 - val_mae: 56665964544.0000\n",
      "Epoch 169/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3046280949849584041984.0000 - mse: 3046280668374607331328.0000 - mae: 54950940672.0000 - val_loss: 3101777682157763821568.0000 - val_mse: 3101777400682787110912.0000 - val_mae: 55693590528.0000\n",
      "Epoch 170/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3049862021736361885696.0000 - mse: 3049861593028320296960.0000 - mae: 54963830784.0000 - val_loss: 2508730172426734796800.0000 - val_mse: 2508729890951758086144.0000 - val_mae: 50087211008.0000\n",
      "Epoch 171/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3028549609072258711552.0000 - mse: 3028549433691696267264.0000 - mae: 54755344384.0000 - val_loss: 3406636474033634279424.0000 - val_mse: 3406636474033634279424.0000 - val_mae: 58366402560.0000\n",
      "Epoch 172/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3066867362767025733632.0000 - mse: 3066866622271317868544.0000 - mae: 55140900864.0000 - val_loss: 3020808590357176516608.0000 - val_mse: 3020808590357176516608.0000 - val_mae: 54961893376.0000\n",
      "Epoch 173/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3032025047730661359616.0000 - mse: 3032024805229142736896.0000 - mae: 54816804864.0000 - val_loss: 3181046665099018764288.0000 - val_mse: 3181046383624042053632.0000 - val_mae: 56400793600.0000\n",
      "Epoch 174/180\n",
      "6240/6240 [==============================] - 59s 9ms/step - loss: 3059408089677670383616.0000 - mse: 3059408098338438905856.0000 - mae: 55056687104.0000 - val_loss: 2795843374395881947136.0000 - val_mse: 2795843374395881947136.0000 - val_mae: 52875743232.0000\n",
      "Epoch 175/180\n",
      "6240/6240 [==============================] - 60s 10ms/step - loss: 3042936044129034960896.0000 - mse: 3042935901226354606080.0000 - mae: 54907940864.0000 - val_loss: 3648054746808596824064.0000 - val_mse: 3648054183858643402752.0000 - val_mae: 60399153152.0000\n",
      "Epoch 176/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 3047518331168741130240.0000 - mse: 3047518595322180796416.0000 - mae: 54983942144.0000 - val_loss: 3062691503941768708096.0000 - val_mse: 3062691222466791997440.0000 - val_mae: 55341608960.0000\n",
      "Epoch 177/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3068151520896927924224.0000 - mse: 3068151274065025302528.0000 - mae: 55123861504.0000 - val_loss: 2639164832209522655232.0000 - val_mse: 2639164550734545944576.0000 - val_mae: 51372802048.0000\n",
      "Epoch 178/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3016492665253123850240.0000 - mse: 3016492453064295317504.0000 - mae: 54668677120.0000 - val_loss: 3130790996857191399424.0000 - val_mse: 3130790996857191399424.0000 - val_mae: 55953461248.0000\n",
      "Epoch 179/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3041778389514435166208.0000 - mse: 3041778194647143677952.0000 - mae: 54852493312.0000 - val_loss: 3537090555589815173120.0000 - val_mse: 3537090837064791883776.0000 - val_mae: 59473436672.0000\n",
      "Epoch 180/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3044054145013830909952.0000 - mse: 3044053919833849331712.0000 - mae: 54915612672.0000 - val_loss: 2659533768899189276672.0000 - val_mse: 2659533487424212566016.0000 - val_mae: 51570671616.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6240 samples, validate on 960 samples\n",
      "Epoch 1/180\n",
      "6240/6240 [==============================] - 59s 10ms/step - loss: 4453251201481706045440.0000 - mse: 4453250509436400697344.0000 - mae: 39713521664.0000 - val_loss: 2681336257645243269120.0000 - val_mse: 2681336257645243269120.0000 - val_mae: 51778990080.0000\n",
      "Epoch 2/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3484328865830913703936.0000 - mse: 3484328071205402705920.0000 - mae: 58537373696.0000 - val_loss: 2851376416976057401344.0000 - val_mse: 2851376416976057401344.0000 - val_mae: 53398151168.0000\n",
      "Epoch 3/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3493312406031266480128.0000 - mse: 3493312189512053424128.0000 - mae: 58708877312.0000 - val_loss: 3158745684169210200064.0000 - val_mse: 3158745684169210200064.0000 - val_mae: 56202276864.0000\n",
      "Epoch 4/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3481424435971035234304.0000 - mse: 3481423812395702157312.0000 - mae: 58704211968.0000 - val_loss: 4014391051297795080192.0000 - val_mse: 4014391332772771790848.0000 - val_mae: 63356203008.0000\n",
      "Epoch 5/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3450664711943798915072.0000 - mse: 3450664789890715090944.0000 - mae: 58400833536.0000 - val_loss: 3288682133243220197376.0000 - val_mse: 3288681851768243486720.0000 - val_mae: 57346400256.0000\n",
      "Epoch 6/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3485315533057930297344.0000 - mse: 3485314922473750265856.0000 - mae: 58790506496.0000 - val_loss: 3147761967628006981632.0000 - val_mse: 3147761967628006981632.0000 - val_mae: 56104361984.0000\n",
      "Epoch 7/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3475003580365760626688.0000 - mse: 3475002523752001961984.0000 - mae: 58603757568.0000 - val_loss: 3759457476041093677056.0000 - val_mse: 3759457476041093677056.0000 - val_mae: 61314392064.0000\n",
      "Epoch 8/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3463794324421295996928.0000 - mse: 3463794471654360350720.0000 - mae: 58523918336.0000 - val_loss: 2649677078164735524864.0000 - val_mse: 2649676515214782103552.0000 - val_mae: 51474112512.0000\n",
      "Epoch 9/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3450910738394980548608.0000 - mse: 3450910517545383493632.0000 - mae: 58481676288.0000 - val_loss: 3430529758906695024640.0000 - val_mse: 3430529758906695024640.0000 - val_mae: 58570629120.0000\n",
      "Epoch 10/180\n",
      "6240/6240 [==============================] - 56s 9ms/step - loss: 3499302449028340383744.0000 - mse: 3499302258491432894464.0000 - mae: 58887495680.0000 - val_loss: 4136643514982580879360.0000 - val_mse: 4136643233507604168704.0000 - val_mae: 64314368000.0000\n",
      "Epoch 11/180\n",
      "6240/6240 [==============================] - 57s 9ms/step - loss: 3458026832862345953280.0000 - mse: 3458026486431605587968.0000 - mae: 58520850432.0000 - val_loss: 3380728391277278658560.0000 - val_mse: 3380728954227232079872.0000 - val_mae: 58143535104.0000\n",
      "Epoch 12/180\n",
      "6240/6240 [==============================] - 58s 9ms/step - loss: 3471129090302491492352.0000 - mse: 3471128865122509914112.0000 - mae: 58665955328.0000 - val_loss: 3814347066824508571648.0000 - val_mse: 3814347348299485282304.0000 - val_mae: 61758541824.0000\n",
      "Epoch 13/180\n",
      "1248/6240 [=====>........................] - ETA: 45s - loss: 3537343320118901342208.0000 - mse: 3537343320118901342208.0000 - mae: 59152490496.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6f98ee4f0ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                         \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                         validation_data = (X_val, y_val))\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python-cpu/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# design the LSTM\n",
    "def regressor_tunning(bias_initializer = initializers.Ones() , kernel_initializer = 'he_normal'):\n",
    "    model = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       return_sequences = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units = units, \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = optimizers.RMSprop(lr = 0.0001)\n",
    "    model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "    return model\n",
    "\n",
    "  \n",
    "# LOOP STARTS\n",
    "for i in dates:\n",
    "    start_time = time.time()\n",
    "    # data\n",
    "    data = data_full.loc[data_full.index > 2018070000, :]\n",
    "\n",
    "    # reset index\n",
    "    data.reset_index(inplace = True)\n",
    "    data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "    # fill nan values\n",
    "    data.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    data_train, data_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle=False)    \n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # data scaling  (including offer (y))\n",
    "    sc_X = MinMaxScaler()\n",
    "    data_train = sc_X.fit_transform(data_train)\n",
    "    data_test = sc_X.transform(data_test)\n",
    "    \n",
    "    # divide features and labels\n",
    "    X_train = data_train[:, 0:15] \n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, 0:15] \n",
    "    y_test = data_test[:, -1] \n",
    "\n",
    "    # divide data into train and test \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "    # put data into correct shape\n",
    "    X_train, y_train = split_data(X_train, y_train, steps)\n",
    "    X_test, y_test = split_data(X_test, y_test, steps)\n",
    "    X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "    X_train = cut_data(X_train, batch_size)\n",
    "    y_train = cut_data(y_train, batch_size)\n",
    "    X_test = cut_data(X_test, batch_size)\n",
    "    y_test = cut_data(y_test, batch_size)\n",
    "    X_val = cut_data(X_val, batch_size)\n",
    "    y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "    model = regressor_tunning()\n",
    "    \n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs,\n",
    "                        shuffle = False, \n",
    "                        validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[15]) + (sc_X.data_min_[15])\n",
    "    y_test = (y_test * sc_X.data_range_[15]) + (sc_X.data_min_[15])\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "    \n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "    \n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,\n",
    "    \n",
    "                        'time': time_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col6,#T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col0,#T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col1,#T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col2,#T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col3,#T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col4,#T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col5{\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rmse_general</th>        <th class=\"col_heading level0 col1\" >mae_general</th>        <th class=\"col_heading level0 col2\" >rmse_spike</th>        <th class=\"col_heading level0 col3\" >mae_spike</th>        <th class=\"col_heading level0 col4\" >rmse_normal</th>        <th class=\"col_heading level0 col5\" >mae_normal</th>        <th class=\"col_heading level0 col6\" >time</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col0\" class=\"data row0 col0\" >86426637041772.875000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col1\" class=\"data row0 col1\" >86426637041772.875000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col2\" class=\"data row0 col2\" >86426637041769.578125</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col3\" class=\"data row0 col3\" >86426637041769.578125</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col4\" class=\"data row0 col4\" >86426637041773.375000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col5\" class=\"data row0 col5\" >86426637041773.359375</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row0_col6\" class=\"data row0 col6\" >10330.295277</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row1_col0\" class=\"data row1 col0\" >97267805585516.875000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row1_col1\" class=\"data row1 col1\" >97267805585516.875000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row1_col2\" class=\"data row1 col2\" >97267805585513.562500</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row1_col3\" class=\"data row1 col3\" >97267805585513.578125</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row1_col4\" class=\"data row1 col4\" >97267805585517.359375</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row1_col5\" class=\"data row1 col5\" >97267805585517.359375</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row1_col6\" class=\"data row1 col6\" >10549.204700</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col0\" class=\"data row2 col0\" >75544355930003.125000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col1\" class=\"data row2 col1\" >75544355930003.125000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col2\" class=\"data row2 col2\" >75544355930006.421875</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col3\" class=\"data row2 col3\" >75544355930006.421875</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col4\" class=\"data row2 col4\" >75544355930002.640625</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col5\" class=\"data row2 col5\" >75544355930002.640625</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row2_col6\" class=\"data row2 col6\" >10684.768069</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row3_col0\" class=\"data row3 col0\" >77356010045548.875000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row3_col1\" class=\"data row3 col1\" >77356010045548.875000</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row3_col2\" class=\"data row3 col2\" >77356010045545.578125</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row3_col3\" class=\"data row3 col3\" >77356010045545.578125</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row3_col4\" class=\"data row3 col4\" >77356010045549.359375</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row3_col5\" class=\"data row3 col5\" >77356010045549.359375</td>\n",
       "                        <td id=\"T_33483540_d2b9_11ea_85b8_4d5646126de4row3_col6\" class=\"data row3 col6\" >10588.151513</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73c1f83d10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_labels = ['12 ',\n",
    "                '10 ',\n",
    "                '8 ',\n",
    "                '6 ',\n",
    "                '4 ',\n",
    "                '2 ']\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged RMSE for different\\n predictive windows')\n",
    "plt.plot(rmse_gen, label = 'Overall error')\n",
    "plt.plot(rmse_spi, label = 'Spike regions')\n",
    "plt.plot(rmse_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE (£/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('RMSE_predictive_window.png')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged MAE for different\\n predictive windows')\n",
    "plt.plot(mae_gen, label = 'Overall error')\n",
    "plt.plot(mae_spi, label = 'Spike regions')\n",
    "plt.plot(mae_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('MAE (£/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('MAE_predictive_window.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
