{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parameter Tuning\n",
    "    \n",
    "    Look for the best kernel initializer and bias initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data; set X and y; fill nan values and split in test and training  data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "\n",
    "# for later use\n",
    "features_num = 15\n",
    "\n",
    "# data\n",
    "data = data.loc[data.index > 2018070000, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# fill nan values\n",
    "data.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "data_train, data_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data scaling  (including offer (y))\n",
    "sc_X = MinMaxScaler()\n",
    "data_train = sc_X.fit_transform(data_train)\n",
    "data_test = sc_X.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Keras libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "mae_gen = []\n",
    "rmse_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "hist_list = []\n",
    "y_pred_list = []\n",
    "prediction_list = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 28s 424ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 29s 440ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0637 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 28s 430ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 28s 437ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 29s 454ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0647 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 29s 452ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 28s 425ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 28s 430ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0647 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 28s 424ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 20/180\n",
      "65/65 [==============================] - 36s 554ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 28s 436ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 28s 424ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 27s 423ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 28s 437ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 29s 440ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 27s 416ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 27s 417ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 28s 431ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 27s 413ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 42s 648ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 37s 562ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 45s 688ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0649 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 36s 553ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 24s 363ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 27s 409ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 49/180\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 50/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 25s 387ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 27s 422ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 38s 580ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 25s 390ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 54/180\n",
      "65/65 [==============================] - 29s 454ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 28s 437ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 38s 585ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 36s 556ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 59/180\n",
      "65/65 [==============================] - 27s 413ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 30s 454ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 38s 581ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 62/180\n",
      "65/65 [==============================] - 32s 494ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 64/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 65/180\n",
      "65/65 [==============================] - 23s 360ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 23s 358ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 25s 387ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 26s 394ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 25s 386ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 25s 390ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0648 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 36s 555ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 29s 440ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 28s 430ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0647 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 31s 474ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 42s 643ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 39s 596ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 29s 440ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 29s 447ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 44s 683ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0635 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 96/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 98/180\n",
      "65/65 [==============================] - 31s 474ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 99/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 29s 447ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0647 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 103/180\n",
      "65/65 [==============================] - 33s 509ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0648 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 108/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 31s 476ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 110/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 31s 484ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0647 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0637 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 31s 476ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 32s 486ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0647 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 141/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 142/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 27s 409ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 145/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0637 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 146/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 147/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 149/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 150/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 151/180\n",
      "65/65 [==============================] - 26s 407ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 152/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 153/180\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 154/180\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 155/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 156/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 157/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 158/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 159/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 160/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0639 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 161/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 162/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 163/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 164/180\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0643 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 165/180\n",
      "65/65 [==============================] - 31s 484ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 166/180\n",
      "65/65 [==============================] - 31s 476ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0645 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 167/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 168/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 169/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 170/180\n",
      "65/65 [==============================] - 31s 476ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 171/180\n",
      "65/65 [==============================] - 38s 585ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 172/180\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 173/180\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 174/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 175/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 176/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 177/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 178/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0646 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 179/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0644 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 180/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0638 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0635\n",
      "Epoch 1/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 686.7604 - mse: 686.7604 - mae: 17.4429 - val_loss: 510.6619 - val_mse: 510.6619 - val_mae: 22.5978\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 543.5723 - mse: 543.5723 - mae: 23.1215 - val_loss: 578.5154 - val_mse: 578.5154 - val_mae: 24.0523\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 534.5405 - mse: 534.5405 - mae: 22.9079 - val_loss: 562.9570 - val_mse: 562.9570 - val_mae: 23.7267\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 31s 484ms/step - loss: 527.3775 - mse: 527.3775 - mae: 22.8536 - val_loss: 589.1157 - val_mse: 589.1157 - val_mae: 24.2717\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 537.9973 - mse: 537.9973 - mae: 23.0754 - val_loss: 480.1084 - val_mse: 480.1084 - val_mae: 21.9114\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 532.3139 - mse: 532.3139 - mae: 22.9133 - val_loss: 668.0671 - val_mse: 668.0671 - val_mae: 25.8470\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 538.6936 - mse: 538.6936 - mae: 23.0652 - val_loss: 467.7436 - val_mse: 467.7436 - val_mae: 21.6274\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 33s 509ms/step - loss: 531.1899 - mse: 531.1899 - mae: 22.9278 - val_loss: 469.9404 - val_mse: 469.9404 - val_mae: 21.6781\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 532.6819 - mse: 532.6819 - mae: 22.9503 - val_loss: 545.0991 - val_mse: 545.0991 - val_mae: 23.3473\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 529.0295 - mse: 529.0295 - mae: 22.8449 - val_loss: 620.5982 - val_mse: 620.5982 - val_mae: 24.9118\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 539.2713 - mse: 539.2713 - mae: 23.1133 - val_loss: 556.0887 - val_mse: 556.0887 - val_mae: 23.5815\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 529.3843 - mse: 529.3843 - mae: 22.8942 - val_loss: 440.7173 - val_mse: 440.7173 - val_mae: 20.9932\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 516.7997 - mse: 516.7997 - mae: 22.6170 - val_loss: 574.7397 - val_mse: 574.7396 - val_mae: 23.9737\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 34s 524ms/step - loss: 514.3242 - mse: 514.3242 - mae: 22.5679 - val_loss: 539.3766 - val_mse: 539.3766 - val_mae: 23.2245\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 514.7658 - mse: 514.7658 - mae: 22.5188 - val_loss: 475.0368 - val_mse: 475.0368 - val_mae: 21.7953\n",
      "Epoch 16/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 33s 502ms/step - loss: 514.7709 - mse: 514.7709 - mae: 22.5918 - val_loss: 496.3664 - val_mse: 496.3664 - val_mae: 22.2793\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 521.5306 - mse: 521.5306 - mae: 22.6871 - val_loss: 400.5416 - val_mse: 400.5416 - val_mae: 20.0135\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 511.6643 - mse: 511.6643 - mae: 22.4612 - val_loss: 606.9275 - val_mse: 606.9275 - val_mae: 24.6359\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 38s 590ms/step - loss: 519.8469 - mse: 519.8468 - mae: 22.6929 - val_loss: 580.4811 - val_mse: 580.4811 - val_mae: 24.0932\n",
      "Epoch 20/180\n",
      "65/65 [==============================] - 264s 4s/step - loss: 514.7852 - mse: 514.7852 - mae: 22.5984 - val_loss: 505.4376 - val_mse: 505.4376 - val_mae: 22.4819\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 515.7129 - mse: 515.7129 - mae: 22.5893 - val_loss: 665.6041 - val_mse: 665.6041 - val_mae: 25.7993\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 42s 648ms/step - loss: 511.2415 - mse: 511.2415 - mae: 22.3619 - val_loss: 545.4432 - val_mse: 545.4432 - val_mae: 23.3547\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 39s 599ms/step - loss: 522.9524 - mse: 522.9524 - mae: 22.7408 - val_loss: 517.0634 - val_mse: 517.0634 - val_mae: 22.7390\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 510.1265 - mse: 510.1265 - mae: 22.4825 - val_loss: 547.4992 - val_mse: 547.4992 - val_mae: 23.3987\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 517.7106 - mse: 517.7106 - mae: 22.6211 - val_loss: 466.7761 - val_mse: 466.7761 - val_mae: 21.6050\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 37s 567ms/step - loss: 519.7393 - mse: 519.7393 - mae: 22.6635 - val_loss: 479.6725 - val_mse: 479.6725 - val_mae: 21.9014\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 516.6733 - mse: 516.6733 - mae: 22.5997 - val_loss: 555.1926 - val_mse: 555.1926 - val_mae: 23.5625\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 36s 560ms/step - loss: 511.0521 - mse: 511.0521 - mae: 22.4852 - val_loss: 475.8398 - val_mse: 475.8398 - val_mae: 21.8137\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 517.5732 - mse: 517.5732 - mae: 22.6316 - val_loss: 567.6826 - val_mse: 567.6826 - val_mae: 23.8261\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 510.7935 - mse: 510.7935 - mae: 22.4869 - val_loss: 531.6227 - val_mse: 531.6227 - val_mae: 23.0569\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 519.6036 - mse: 519.6036 - mae: 22.6831 - val_loss: 527.9509 - val_mse: 527.9509 - val_mae: 22.9772\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 510.9864 - mse: 510.9864 - mae: 22.4705 - val_loss: 603.4762 - val_mse: 603.4762 - val_mae: 24.5657\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 515.2741 - mse: 515.2741 - mae: 22.5809 - val_loss: 508.4605 - val_mse: 508.4605 - val_mae: 22.5490\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 515.7103 - mse: 515.7103 - mae: 22.5747 - val_loss: 621.8953 - val_mse: 621.8953 - val_mae: 24.9378\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 520.3513 - mse: 520.3513 - mae: 22.6767 - val_loss: 517.9388 - val_mse: 517.9388 - val_mae: 22.7582\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 515.1558 - mse: 515.1558 - mae: 22.5586 - val_loss: 544.4822 - val_mse: 544.4822 - val_mae: 23.3341\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 513.1970 - mse: 513.1970 - mae: 22.5259 - val_loss: 589.3577 - val_mse: 589.3577 - val_mae: 24.2767\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 515.5375 - mse: 515.5375 - mae: 22.5966 - val_loss: 500.0794 - val_mse: 500.0794 - val_mae: 22.3624\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 514.9398 - mse: 514.9398 - mae: 22.5772 - val_loss: 507.1964 - val_mse: 507.1964 - val_mae: 22.5210\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 520.2265 - mse: 520.2265 - mae: 22.7187 - val_loss: 527.4734 - val_mse: 527.4734 - val_mae: 22.9668\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 34s 517ms/step - loss: 509.7286 - mse: 509.7286 - mae: 22.4186 - val_loss: 545.9851 - val_mse: 545.9851 - val_mae: 23.3663\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 520.8186 - mse: 520.8186 - mae: 22.6780 - val_loss: 690.6755 - val_mse: 690.6755 - val_mae: 26.2807\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 514.4650 - mse: 514.4650 - mae: 22.5096 - val_loss: 465.2416 - val_mse: 465.2416 - val_mae: 21.5694\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 516.1000 - mse: 516.1000 - mae: 22.6294 - val_loss: 460.2798 - val_mse: 460.2798 - val_mae: 21.4541\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 516.2125 - mse: 516.2125 - mae: 22.6241 - val_loss: 527.3469 - val_mse: 527.3469 - val_mae: 22.9640\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 515.4236 - mse: 515.4236 - mae: 22.6077 - val_loss: 505.9896 - val_mse: 505.9896 - val_mae: 22.4942\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 514.8743 - mse: 514.8743 - mae: 22.5740 - val_loss: 569.5868 - val_mse: 569.5868 - val_mae: 23.8660\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 28s 435ms/step - loss: 518.8024 - mse: 518.8024 - mae: 22.6658 - val_loss: 432.2637 - val_mse: 432.2637 - val_mae: 20.7909\n",
      "Epoch 49/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 515.7352 - mse: 515.7352 - mae: 22.5700 - val_loss: 378.0835 - val_mse: 378.0835 - val_mae: 19.4443\n",
      "Epoch 50/180\n",
      "65/65 [==============================] - 28s 430ms/step - loss: 509.6560 - mse: 509.6560 - mae: 22.4357 - val_loss: 519.7007 - val_mse: 519.7007 - val_mae: 22.7969\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 516.7698 - mse: 516.7698 - mae: 22.6085 - val_loss: 524.5736 - val_mse: 524.5736 - val_mae: 22.9035\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 519.2192 - mse: 519.2192 - mae: 22.6743 - val_loss: 545.8740 - val_mse: 545.8740 - val_mae: 23.3639\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 516.1822 - mse: 516.1821 - mae: 22.6126 - val_loss: 441.1483 - val_mse: 441.1483 - val_mae: 21.0035\n",
      "Epoch 54/180\n",
      "65/65 [==============================] - 27s 416ms/step - loss: 511.2197 - mse: 511.2197 - mae: 22.4850 - val_loss: 621.0322 - val_mse: 621.0322 - val_mae: 24.9205\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 523.5262 - mse: 523.5262 - mae: 22.7275 - val_loss: 395.6530 - val_mse: 395.6530 - val_mae: 19.8910\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 28s 428ms/step - loss: 517.0161 - mse: 517.0161 - mae: 22.6108 - val_loss: 435.6344 - val_mse: 435.6344 - val_mae: 20.8718\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 27s 417ms/step - loss: 509.6421 - mse: 509.6421 - mae: 22.4585 - val_loss: 487.3363 - val_mse: 487.3363 - val_mae: 22.0757\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 29s 452ms/step - loss: 514.8652 - mse: 514.8652 - mae: 22.5834 - val_loss: 701.2195 - val_mse: 701.2195 - val_mae: 26.4805\n",
      "Epoch 59/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 519.2975 - mse: 519.2975 - mae: 22.6807 - val_loss: 493.4439 - val_mse: 493.4439 - val_mae: 22.2136\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 518.0715 - mse: 518.0715 - mae: 22.6680 - val_loss: 497.1821 - val_mse: 497.1821 - val_mae: 22.2976\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 516.5632 - mse: 516.5632 - mae: 22.6251 - val_loss: 413.4935 - val_mse: 413.4935 - val_mae: 20.3345\n",
      "Epoch 62/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 29s 444ms/step - loss: 513.4707 - mse: 513.4707 - mae: 22.5313 - val_loss: 551.5530 - val_mse: 551.5530 - val_mae: 23.4851\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 512.4210 - mse: 512.4210 - mae: 22.5070 - val_loss: 538.0227 - val_mse: 538.0227 - val_mae: 23.1953\n",
      "Epoch 64/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 515.7849 - mse: 515.7849 - mae: 22.5929 - val_loss: 459.3216 - val_mse: 459.3216 - val_mae: 21.4318\n",
      "Epoch 65/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 516.5708 - mse: 516.5708 - mae: 22.5731 - val_loss: 425.7456 - val_mse: 425.7456 - val_mae: 20.6336\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 27s 414ms/step - loss: 513.1524 - mse: 513.1524 - mae: 22.5301 - val_loss: 471.7858 - val_mse: 471.7858 - val_mae: 21.7206\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 28s 431ms/step - loss: 520.3586 - mse: 520.3586 - mae: 22.6757 - val_loss: 465.4967 - val_mse: 465.4967 - val_mae: 21.5753\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 514.3501 - mse: 514.3501 - mae: 22.4401 - val_loss: 420.8011 - val_mse: 420.8011 - val_mae: 20.5134\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 512.7608 - mse: 512.7608 - mae: 22.5129 - val_loss: 639.1876 - val_mse: 639.1876 - val_mae: 25.2821\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 519.0199 - mse: 519.0199 - mae: 22.6093 - val_loss: 624.9321 - val_mse: 624.9321 - val_mae: 24.9986\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 29s 454ms/step - loss: 512.9763 - mse: 512.9763 - mae: 22.4483 - val_loss: 594.7321 - val_mse: 594.7321 - val_mae: 24.3871\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 513.0029 - mse: 513.0029 - mae: 22.5169 - val_loss: 621.0413 - val_mse: 621.0413 - val_mae: 24.9207\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 519.4162 - mse: 519.4162 - mae: 22.6373 - val_loss: 500.8178 - val_mse: 500.8178 - val_mae: 22.3789\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 514.7991 - mse: 514.7991 - mae: 22.5839 - val_loss: 530.3558 - val_mse: 530.3558 - val_mae: 23.0294\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 515.6608 - mse: 515.6608 - mae: 22.5865 - val_loss: 521.1553 - val_mse: 521.1553 - val_mae: 22.8288\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 29s 452ms/step - loss: 512.5200 - mse: 512.5200 - mae: 22.5324 - val_loss: 546.8177 - val_mse: 546.8177 - val_mae: 23.3841\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 517.9712 - mse: 517.9712 - mae: 22.5557 - val_loss: 614.6936 - val_mse: 614.6936 - val_mae: 24.7930\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 513.4376 - mse: 513.4376 - mae: 22.4967 - val_loss: 451.2755 - val_mse: 451.2755 - val_mae: 21.2432\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 28s 423ms/step - loss: 519.6732 - mse: 519.6732 - mae: 22.6887 - val_loss: 493.0958 - val_mse: 493.0958 - val_mae: 22.2057\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 514.6931 - mse: 514.6931 - mae: 22.5091 - val_loss: 577.3684 - val_mse: 577.3684 - val_mae: 24.0285\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 27s 416ms/step - loss: 516.0407 - mse: 516.0407 - mae: 22.6090 - val_loss: 476.5714 - val_mse: 476.5714 - val_mae: 21.8305\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 27s 414ms/step - loss: 514.7428 - mse: 514.7428 - mae: 22.5638 - val_loss: 505.2757 - val_mse: 505.2757 - val_mae: 22.4783\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 27s 418ms/step - loss: 509.7295 - mse: 509.7295 - mae: 22.4665 - val_loss: 539.7806 - val_mse: 539.7805 - val_mae: 23.2332\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 517.7022 - mse: 517.7022 - mae: 22.6525 - val_loss: 453.6500 - val_mse: 453.6500 - val_mae: 21.2990\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 513.1526 - mse: 513.1526 - mae: 22.5408 - val_loss: 636.5297 - val_mse: 636.5297 - val_mae: 25.2295\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 27s 418ms/step - loss: 519.8426 - mse: 519.8426 - mae: 22.6507 - val_loss: 506.3227 - val_mse: 506.3227 - val_mae: 22.5016\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 27s 415ms/step - loss: 511.6188 - mse: 511.6188 - mae: 22.4771 - val_loss: 438.2546 - val_mse: 438.2546 - val_mae: 20.9345\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 27s 419ms/step - loss: 518.0942 - mse: 518.0942 - mae: 22.6251 - val_loss: 542.9890 - val_mse: 542.9890 - val_mae: 23.3021\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 29s 440ms/step - loss: 512.8048 - mse: 512.8048 - mae: 22.5353 - val_loss: 503.2850 - val_mse: 503.2850 - val_mae: 22.4340\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 27s 419ms/step - loss: 516.5280 - mse: 516.5280 - mae: 22.6142 - val_loss: 507.9008 - val_mse: 507.9008 - val_mae: 22.5366\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 27s 419ms/step - loss: 516.5257 - mse: 516.5257 - mae: 22.6309 - val_loss: 487.6591 - val_mse: 487.6591 - val_mae: 22.0830\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 28s 431ms/step - loss: 516.6141 - mse: 516.6141 - mae: 22.6327 - val_loss: 520.7839 - val_mse: 520.7839 - val_mae: 22.8207\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 29s 452ms/step - loss: 512.1194 - mse: 512.1194 - mae: 22.5173 - val_loss: 523.4521 - val_mse: 523.4521 - val_mae: 22.8791\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 518.9349 - mse: 518.9349 - mae: 22.6688 - val_loss: 496.2706 - val_mse: 496.2706 - val_mae: 22.2771\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 513.1957 - mse: 513.1957 - mae: 22.5063 - val_loss: 457.5031 - val_mse: 457.5031 - val_mae: 21.3893\n",
      "Epoch 96/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 517.5711 - mse: 517.5711 - mae: 22.6511 - val_loss: 558.4049 - val_mse: 558.4049 - val_mae: 23.6306\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 511.7267 - mse: 511.7267 - mae: 22.5222 - val_loss: 512.3715 - val_mse: 512.3715 - val_mae: 22.6356\n",
      "Epoch 98/180\n",
      "65/65 [==============================] - 27s 422ms/step - loss: 519.1042 - mse: 519.1042 - mae: 22.6436 - val_loss: 525.5149 - val_mse: 525.5149 - val_mae: 22.9241\n",
      "Epoch 99/180\n",
      "65/65 [==============================] - 27s 418ms/step - loss: 517.1049 - mse: 517.1049 - mae: 22.6272 - val_loss: 544.4056 - val_mse: 544.4056 - val_mae: 23.3325\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 509.7241 - mse: 509.7241 - mae: 22.4841 - val_loss: 480.0144 - val_mse: 480.0144 - val_mae: 21.9092\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 523.9927 - mse: 523.9926 - mae: 22.7168 - val_loss: 344.2211 - val_mse: 344.2211 - val_mae: 18.5532\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 28s 428ms/step - loss: 508.5011 - mse: 508.5011 - mae: 22.4070 - val_loss: 477.7359 - val_mse: 477.7359 - val_mae: 21.8571\n",
      "Epoch 103/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 518.7904 - mse: 518.7904 - mae: 22.6278 - val_loss: 553.4309 - val_mse: 553.4309 - val_mae: 23.5251\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 27s 415ms/step - loss: 511.3720 - mse: 511.3719 - mae: 22.4909 - val_loss: 662.5673 - val_mse: 662.5673 - val_mae: 25.7404\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 27s 418ms/step - loss: 521.9805 - mse: 521.9805 - mae: 22.7308 - val_loss: 439.0421 - val_mse: 439.0421 - val_mae: 20.9533\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 28s 435ms/step - loss: 510.7856 - mse: 510.7856 - mae: 22.4834 - val_loss: 474.7948 - val_mse: 474.7948 - val_mae: 21.7898\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 27s 417ms/step - loss: 517.1294 - mse: 517.1294 - mae: 22.6256 - val_loss: 564.7211 - val_mse: 564.7211 - val_mae: 23.7638\n",
      "Epoch 108/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 27s 418ms/step - loss: 515.2806 - mse: 515.2806 - mae: 22.5760 - val_loss: 523.2178 - val_mse: 523.2178 - val_mae: 22.8739\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 514.8397 - mse: 514.8397 - mae: 22.5847 - val_loss: 443.5629 - val_mse: 443.5629 - val_mae: 21.0609\n",
      "Epoch 110/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 518.4902 - mse: 518.4902 - mae: 22.6333 - val_loss: 494.5765 - val_mse: 494.5765 - val_mae: 22.2390\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 514.4496 - mse: 514.4496 - mae: 22.5292 - val_loss: 574.2791 - val_mse: 574.2791 - val_mae: 23.9641\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 515.6041 - mse: 515.6041 - mae: 22.5788 - val_loss: 593.6430 - val_mse: 593.6431 - val_mae: 24.3648\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 28s 423ms/step - loss: 512.1238 - mse: 512.1238 - mae: 22.5323 - val_loss: 477.5891 - val_mse: 477.5891 - val_mae: 21.8538\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 522.1265 - mse: 522.1265 - mae: 22.7417 - val_loss: 431.0604 - val_mse: 431.0604 - val_mae: 20.7620\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 513.2809 - mse: 513.2809 - mae: 22.5369 - val_loss: 467.1344 - val_mse: 467.1344 - val_mae: 21.6133\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 27s 418ms/step - loss: 512.3848 - mse: 512.3848 - mae: 22.4641 - val_loss: 460.8042 - val_mse: 460.8042 - val_mae: 21.4663\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 517.8997 - mse: 517.8997 - mae: 22.6491 - val_loss: 525.2256 - val_mse: 525.2256 - val_mae: 22.9178\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 28s 428ms/step - loss: 515.7242 - mse: 515.7242 - mae: 22.5500 - val_loss: 550.2454 - val_mse: 550.2454 - val_mae: 23.4573\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 28s 437ms/step - loss: 516.0099 - mse: 516.0099 - mae: 22.5948 - val_loss: 407.5199 - val_mse: 407.5199 - val_mae: 20.1871\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 28s 431ms/step - loss: 516.2217 - mse: 516.2217 - mae: 22.5868 - val_loss: 460.2400 - val_mse: 460.2400 - val_mae: 21.4532\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 515.1050 - mse: 515.1050 - mae: 22.5481 - val_loss: 416.4415 - val_mse: 416.4415 - val_mae: 20.4069\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 28s 425ms/step - loss: 512.8297 - mse: 512.8297 - mae: 22.5355 - val_loss: 460.3228 - val_mse: 460.3228 - val_mae: 21.4551\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 516.1491 - mse: 516.1491 - mae: 22.6214 - val_loss: 438.4855 - val_mse: 438.4855 - val_mae: 20.9400\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 517.5800 - mse: 517.5800 - mae: 22.6000 - val_loss: 643.1111 - val_mse: 643.1111 - val_mae: 25.3596\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 27s 417ms/step - loss: 512.3815 - mse: 512.3815 - mae: 22.5153 - val_loss: 511.9144 - val_mse: 511.9144 - val_mae: 22.6255\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 28s 435ms/step - loss: 516.9556 - mse: 516.9556 - mae: 22.6436 - val_loss: 521.4916 - val_mse: 521.4916 - val_mae: 22.8362\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 516.6227 - mse: 516.6227 - mae: 22.6241 - val_loss: 414.7461 - val_mse: 414.7461 - val_mae: 20.3653\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 27s 418ms/step - loss: 514.0642 - mse: 514.0642 - mae: 22.5185 - val_loss: 517.3400 - val_mse: 517.3400 - val_mae: 22.7451\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 27s 422ms/step - loss: 517.0525 - mse: 517.0525 - mae: 22.6273 - val_loss: 426.8342 - val_mse: 426.8342 - val_mae: 20.6599\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 27s 419ms/step - loss: 519.7552 - mse: 519.7552 - mae: 22.6490 - val_loss: 536.8534 - val_mse: 536.8534 - val_mae: 23.1701\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 515.7767 - mse: 515.7767 - mae: 22.5848 - val_loss: 456.1614 - val_mse: 456.1614 - val_mae: 21.3579\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 514.1558 - mse: 514.1558 - mae: 22.5598 - val_loss: 513.5895 - val_mse: 513.5895 - val_mae: 22.6625\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 515.4906 - mse: 515.4906 - mae: 22.5839 - val_loss: 659.7786 - val_mse: 659.7786 - val_mae: 25.6861\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 27s 415ms/step - loss: 512.7131 - mse: 512.7131 - mae: 22.4795 - val_loss: 652.3859 - val_mse: 652.3859 - val_mae: 25.5418\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 519.9164 - mse: 519.9164 - mae: 22.6386 - val_loss: 520.7572 - val_mse: 520.7572 - val_mae: 22.8201\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 515.2187 - mse: 515.2187 - mae: 22.5960 - val_loss: 500.6056 - val_mse: 500.6056 - val_mae: 22.3742\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 512.5067 - mse: 512.5067 - mae: 22.5185 - val_loss: 504.2103 - val_mse: 504.2103 - val_mae: 22.4546\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 28s 424ms/step - loss: 519.2936 - mse: 519.2936 - mae: 22.6592 - val_loss: 485.3693 - val_mse: 485.3693 - val_mae: 22.0311\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 27s 422ms/step - loss: 514.8261 - mse: 514.8261 - mae: 22.5686 - val_loss: 606.8369 - val_mse: 606.8369 - val_mae: 24.6340\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 516.2935 - mse: 516.2935 - mae: 22.6150 - val_loss: 502.9243 - val_mse: 502.9243 - val_mae: 22.4259\n",
      "Epoch 141/180\n",
      "65/65 [==============================] - 27s 417ms/step - loss: 513.7325 - mse: 513.7325 - mae: 22.5614 - val_loss: 552.9440 - val_mse: 552.9440 - val_mae: 23.5147\n",
      "Epoch 142/180\n",
      "65/65 [==============================] - 27s 419ms/step - loss: 517.3489 - mse: 517.3489 - mae: 22.6372 - val_loss: 585.6913 - val_mse: 585.6913 - val_mae: 24.2010\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 27s 419ms/step - loss: 516.4521 - mse: 516.4521 - mae: 22.6097 - val_loss: 522.4858 - val_mse: 522.4858 - val_mae: 22.8579\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 517.5236 - mse: 517.5236 - mae: 22.6480 - val_loss: 473.4863 - val_mse: 473.4863 - val_mae: 21.7597\n",
      "Epoch 145/180\n",
      "65/65 [==============================] - 28s 425ms/step - loss: 517.4025 - mse: 517.4025 - mae: 22.6529 - val_loss: 496.7099 - val_mse: 496.7099 - val_mae: 22.2870\n",
      "Epoch 146/180\n",
      "65/65 [==============================] - 27s 417ms/step - loss: 514.6099 - mse: 514.6099 - mae: 22.5520 - val_loss: 503.3316 - val_mse: 503.3316 - val_mae: 22.4350\n",
      "Epoch 147/180\n",
      "65/65 [==============================] - 27s 422ms/step - loss: 515.8596 - mse: 515.8596 - mae: 22.6094 - val_loss: 502.6515 - val_mse: 502.6515 - val_mae: 22.4199\n",
      "Epoch 148/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 513.7647 - mse: 513.7647 - mae: 22.5640 - val_loss: 510.7515 - val_mse: 510.7515 - val_mae: 22.5998\n",
      "Epoch 149/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 515.2794 - mse: 515.2794 - mae: 22.5667 - val_loss: 494.0829 - val_mse: 494.0829 - val_mae: 22.2279\n",
      "Epoch 150/180\n",
      "65/65 [==============================] - 28s 424ms/step - loss: 515.2401 - mse: 515.2401 - mae: 22.5892 - val_loss: 430.3552 - val_mse: 430.3552 - val_mae: 20.7450\n",
      "Epoch 151/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 518.9530 - mse: 518.9530 - mae: 22.6265 - val_loss: 651.7154 - val_mse: 651.7154 - val_mae: 25.5287\n",
      "Epoch 152/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 516.0706 - mse: 516.0706 - mae: 22.5995 - val_loss: 512.6170 - val_mse: 512.6169 - val_mae: 22.6410\n",
      "Epoch 153/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 513.8267 - mse: 513.8267 - mae: 22.5570 - val_loss: 450.4888 - val_mse: 450.4888 - val_mae: 21.2247\n",
      "Epoch 154/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 33s 506ms/step - loss: 512.0977 - mse: 512.0977 - mae: 22.4910 - val_loss: 660.0250 - val_mse: 660.0250 - val_mae: 25.6909\n",
      "Epoch 155/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 518.3047 - mse: 518.3047 - mae: 22.6378 - val_loss: 579.0410 - val_mse: 579.0410 - val_mae: 24.0632\n",
      "Epoch 156/180\n",
      "65/65 [==============================] - 34s 515ms/step - loss: 516.0322 - mse: 516.0322 - mae: 22.6042 - val_loss: 570.9159 - val_mse: 570.9159 - val_mae: 23.8938\n",
      "Epoch 157/180\n",
      "65/65 [==============================] - 33s 506ms/step - loss: 516.6349 - mse: 516.6349 - mae: 22.6266 - val_loss: 529.8398 - val_mse: 529.8398 - val_mae: 23.0182\n",
      "Epoch 158/180\n",
      "65/65 [==============================] - 34s 522ms/step - loss: 511.8941 - mse: 511.8941 - mae: 22.5182 - val_loss: 479.8324 - val_mse: 479.8324 - val_mae: 21.9051\n",
      "Epoch 159/180\n",
      "65/65 [==============================] - 33s 510ms/step - loss: 513.9542 - mse: 513.9542 - mae: 22.4771 - val_loss: 491.8260 - val_mse: 491.8260 - val_mae: 22.1771\n",
      "Epoch 160/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 520.2495 - mse: 520.2495 - mae: 22.6756 - val_loss: 578.0798 - val_mse: 578.0798 - val_mae: 24.0433\n",
      "Epoch 161/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 514.5789 - mse: 514.5789 - mae: 22.5944 - val_loss: 365.4942 - val_mse: 365.4942 - val_mae: 19.1179\n",
      "Epoch 162/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 516.2375 - mse: 516.2375 - mae: 22.5767 - val_loss: 475.0511 - val_mse: 475.0511 - val_mae: 21.7956\n",
      "Epoch 163/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 513.4515 - mse: 513.4515 - mae: 22.5588 - val_loss: 450.8933 - val_mse: 450.8933 - val_mae: 21.2342\n",
      "Epoch 164/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 510.9332 - mse: 510.9332 - mae: 22.4287 - val_loss: 416.9052 - val_mse: 416.9052 - val_mae: 20.4182\n",
      "Epoch 165/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 517.2665 - mse: 517.2665 - mae: 22.5905 - val_loss: 468.3885 - val_mse: 468.3885 - val_mae: 21.6423\n",
      "Epoch 166/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 511.9365 - mse: 511.9365 - mae: 22.4871 - val_loss: 564.1081 - val_mse: 564.1081 - val_mae: 23.7509\n",
      "Epoch 167/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 520.7500 - mse: 520.7500 - mae: 22.7122 - val_loss: 535.2615 - val_mse: 535.2615 - val_mae: 23.1357\n",
      "Epoch 168/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 517.6474 - mse: 517.6474 - mae: 22.5954 - val_loss: 383.4531 - val_mse: 383.4531 - val_mae: 19.5819\n",
      "Epoch 169/180\n",
      "65/65 [==============================] - 29s 444ms/step - loss: 514.6306 - mse: 514.6306 - mae: 22.5645 - val_loss: 548.7795 - val_mse: 548.7795 - val_mae: 23.4260\n",
      "Epoch 170/180\n",
      "65/65 [==============================] - 29s 445ms/step - loss: 513.4813 - mse: 513.4813 - mae: 22.5598 - val_loss: 492.6555 - val_mse: 492.6555 - val_mae: 22.1958\n",
      "Epoch 171/180\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 515.2309 - mse: 515.2309 - mae: 22.4879 - val_loss: 543.0324 - val_mse: 543.0325 - val_mae: 23.3030\n",
      "Epoch 172/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 518.9265 - mse: 518.9265 - mae: 22.6705 - val_loss: 532.9714 - val_mse: 532.9714 - val_mae: 23.0861\n",
      "Epoch 173/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 511.6772 - mse: 511.6772 - mae: 22.5024 - val_loss: 471.8749 - val_mse: 471.8749 - val_mae: 21.7227\n",
      "Epoch 174/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 519.4546 - mse: 519.4546 - mae: 22.6499 - val_loss: 617.0927 - val_mse: 617.0927 - val_mae: 24.8413\n",
      "Epoch 175/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 510.4739 - mse: 510.4739 - mae: 22.4252 - val_loss: 498.4510 - val_mse: 498.4510 - val_mae: 22.3260\n",
      "Epoch 176/180\n",
      "65/65 [==============================] - 31s 469ms/step - loss: 525.7415 - mse: 525.7415 - mae: 22.8209 - val_loss: 482.2535 - val_mse: 482.2535 - val_mae: 21.9602\n",
      "Epoch 177/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 510.9681 - mse: 510.9681 - mae: 22.5041 - val_loss: 518.0175 - val_mse: 518.0175 - val_mae: 22.7600\n",
      "Epoch 178/180\n",
      "65/65 [==============================] - 29s 447ms/step - loss: 521.0356 - mse: 521.0356 - mae: 22.7157 - val_loss: 497.8980 - val_mse: 497.8980 - val_mae: 22.3136\n",
      "Epoch 179/180\n",
      "65/65 [==============================] - 30s 454ms/step - loss: 509.3244 - mse: 509.3244 - mae: 22.4536 - val_loss: 591.3923 - val_mse: 591.3923 - val_mae: 24.3185\n",
      "Epoch 180/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 517.6964 - mse: 517.6964 - mae: 22.6545 - val_loss: 507.9060 - val_mse: 507.9060 - val_mae: 22.5367\n",
      "Epoch 1/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 2360.2688 - mse: 2360.2688 - mae: 35.4575 - val_loss: 2635.0498 - val_mse: 2635.0496 - val_mae: 51.3327\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 28s 438ms/step - loss: 1855.6880 - mse: 1855.6880 - mae: 42.5556 - val_loss: 1386.0852 - val_mse: 1386.0852 - val_mae: 37.2301\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 1833.5466 - mse: 1833.5466 - mae: 42.5053 - val_loss: 1977.4841 - val_mse: 1977.4841 - val_mae: 44.4689\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 1845.4130 - mse: 1845.4130 - mae: 42.7432 - val_loss: 1744.9801 - val_mse: 1744.9801 - val_mae: 41.7729\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 1827.9805 - mse: 1827.9805 - mae: 42.5501 - val_loss: 1631.5404 - val_mse: 1631.5404 - val_mae: 40.3923\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 1829.5231 - mse: 1829.5231 - mae: 42.5462 - val_loss: 1876.1216 - val_mse: 1876.1216 - val_mae: 43.3142\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 1831.0145 - mse: 1831.0145 - mae: 42.6112 - val_loss: 1709.6232 - val_mse: 1709.6232 - val_mae: 41.3476\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 1840.1920 - mse: 1840.1920 - mae: 42.6793 - val_loss: 2010.1096 - val_mse: 2010.1096 - val_mae: 44.8342\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 1806.3157 - mse: 1806.3157 - mae: 42.2645 - val_loss: 2019.0144 - val_mse: 2019.0144 - val_mae: 44.9334\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 29s 450ms/step - loss: 1839.1403 - mse: 1839.1403 - mae: 42.5879 - val_loss: 1645.2329 - val_mse: 1645.2329 - val_mae: 40.5615\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 1836.1173 - mse: 1836.1173 - mae: 42.6444 - val_loss: 1808.9789 - val_mse: 1808.9789 - val_mae: 42.5321\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 1823.2725 - mse: 1823.2725 - mae: 42.4632 - val_loss: 2471.9326 - val_mse: 2471.9326 - val_mae: 49.7185\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 1834.2317 - mse: 1834.2317 - mae: 42.6352 - val_loss: 1772.3997 - val_mse: 1772.3997 - val_mae: 42.0999\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 31s 476ms/step - loss: 1821.1733 - mse: 1821.1733 - mae: 42.4399 - val_loss: 1475.3414 - val_mse: 1475.3414 - val_mae: 38.4102\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 32s 494ms/step - loss: 1826.4559 - mse: 1826.4559 - mae: 42.5018 - val_loss: 1981.5956 - val_mse: 1981.5956 - val_mae: 44.5151\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 1830.4099 - mse: 1830.4099 - mae: 42.5675 - val_loss: 1956.0503 - val_mse: 1956.0503 - val_mae: 44.2272\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 30s 458ms/step - loss: 1834.2594 - mse: 1834.2594 - mae: 42.6188 - val_loss: 1679.1039 - val_mse: 1679.1039 - val_mae: 40.9769\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 1838.7898 - mse: 1838.7898 - mae: 42.6598 - val_loss: 1549.4066 - val_mse: 1549.4066 - val_mae: 39.3625\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 1821.7509 - mse: 1821.7509 - mae: 42.4612 - val_loss: 1675.8352 - val_mse: 1675.8352 - val_mae: 40.9370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 1821.6472 - mse: 1821.6472 - mae: 42.4211 - val_loss: 1903.4608 - val_mse: 1903.4608 - val_mae: 43.6287\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 1841.6190 - mse: 1841.6190 - mae: 42.6705 - val_loss: 1656.0271 - val_mse: 1656.0271 - val_mae: 40.6943\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 1826.3540 - mse: 1826.3541 - mae: 42.4937 - val_loss: 1828.7811 - val_mse: 1828.7811 - val_mae: 42.7642\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 1833.1901 - mse: 1833.1901 - mae: 42.6206 - val_loss: 1828.9042 - val_mse: 1828.9042 - val_mae: 42.7657\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 1839.8308 - mse: 1839.8308 - mae: 42.5851 - val_loss: 1568.0328 - val_mse: 1568.0328 - val_mae: 39.5984\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 1825.3108 - mse: 1825.3108 - mae: 42.5146 - val_loss: 2182.9758 - val_mse: 2182.9758 - val_mae: 46.7223\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 1811.2214 - mse: 1811.2214 - mae: 42.3040 - val_loss: 2043.5566 - val_mse: 2043.5566 - val_mae: 45.2057\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 1856.4884 - mse: 1856.4884 - mae: 42.8346 - val_loss: 1349.9417 - val_mse: 1349.9417 - val_mae: 36.7415\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 1818.9912 - mse: 1818.9912 - mae: 42.3632 - val_loss: 1898.6538 - val_mse: 1898.6538 - val_mae: 43.5735\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 1825.6282 - mse: 1825.6282 - mae: 42.4661 - val_loss: 2004.1827 - val_mse: 2004.1827 - val_mae: 44.7681\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 1836.0160 - mse: 1836.0160 - mae: 42.6144 - val_loss: 1955.9236 - val_mse: 1955.9236 - val_mae: 44.2258\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 1805.2930 - mse: 1805.2930 - mae: 42.2350 - val_loss: 1610.2562 - val_mse: 1610.2562 - val_mae: 40.1280\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 1836.2438 - mse: 1836.2438 - mae: 42.6719 - val_loss: 1746.6375 - val_mse: 1746.6375 - val_mae: 41.7928\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 1836.1587 - mse: 1836.1587 - mae: 42.6649 - val_loss: 2069.9370 - val_mse: 2069.9373 - val_mae: 45.4965\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 1826.9458 - mse: 1826.9457 - mae: 42.4726 - val_loss: 1450.0731 - val_mse: 1450.0731 - val_mae: 38.0798\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 31s 484ms/step - loss: 1831.5039 - mse: 1831.5039 - mae: 42.5935 - val_loss: 1798.5532 - val_mse: 1798.5532 - val_mae: 42.4093\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 1840.3146 - mse: 1840.3146 - mae: 42.6870 - val_loss: 1797.1652 - val_mse: 1797.1652 - val_mae: 42.3930\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 30s 458ms/step - loss: 1824.2135 - mse: 1824.2136 - mae: 42.5025 - val_loss: 1995.1605 - val_mse: 1995.1605 - val_mae: 44.6672\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 1830.6713 - mse: 1830.6713 - mae: 42.5554 - val_loss: 1776.1069 - val_mse: 1776.1069 - val_mae: 42.1439\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 1838.9584 - mse: 1838.9584 - mae: 42.5723 - val_loss: 2469.4761 - val_mse: 2469.4761 - val_mae: 49.6938\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 1819.9122 - mse: 1819.9122 - mae: 42.2329 - val_loss: 1835.2372 - val_mse: 1835.2372 - val_mae: 42.8397\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 1825.4473 - mse: 1825.4473 - mae: 42.5395 - val_loss: 1889.0653 - val_mse: 1889.0653 - val_mae: 43.4634\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 1829.6901 - mse: 1829.6901 - mae: 42.5876 - val_loss: 2093.6677 - val_mse: 2093.6675 - val_mae: 45.7566\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 1821.9072 - mse: 1821.9072 - mae: 42.4334 - val_loss: 1538.9520 - val_mse: 1538.9520 - val_mae: 39.2295\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 1838.6470 - mse: 1838.6470 - mae: 42.6280 - val_loss: 1590.9976 - val_mse: 1590.9976 - val_mae: 39.8873\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 1821.1733 - mse: 1821.1732 - mae: 42.4164 - val_loss: 1986.0031 - val_mse: 1986.0031 - val_mae: 44.5646\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 1842.3715 - mse: 1842.3715 - mae: 42.7125 - val_loss: 1971.8938 - val_mse: 1971.8938 - val_mae: 44.4060\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 1819.2145 - mse: 1819.2145 - mae: 42.3810 - val_loss: 1455.3638 - val_mse: 1455.3638 - val_mae: 38.1492\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 1829.9161 - mse: 1829.9161 - mae: 42.5441 - val_loss: 2016.3132 - val_mse: 2016.3132 - val_mae: 44.9034\n",
      "Epoch 49/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 1831.2249 - mse: 1831.2249 - mae: 42.5920 - val_loss: 2013.7439 - val_mse: 2013.7439 - val_mae: 44.8747\n",
      "Epoch 50/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 1834.0458 - mse: 1834.0458 - mae: 42.6079 - val_loss: 1995.9885 - val_mse: 1995.9885 - val_mae: 44.6765\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 1809.5195 - mse: 1809.5195 - mae: 42.3307 - val_loss: 2006.2268 - val_mse: 2006.2268 - val_mae: 44.7909\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 29s 450ms/step - loss: 1840.6638 - mse: 1840.6638 - mae: 42.6373 - val_loss: 1620.8226 - val_mse: 1620.8226 - val_mae: 40.2594\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 1822.2561 - mse: 1822.2561 - mae: 42.4743 - val_loss: 1851.5653 - val_mse: 1851.5653 - val_mae: 43.0298\n",
      "Epoch 54/180\n",
      "65/65 [==============================] - 31s 484ms/step - loss: 1835.4246 - mse: 1835.4246 - mae: 42.6464 - val_loss: 1588.2153 - val_mse: 1588.2153 - val_mae: 39.8524\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 1820.5642 - mse: 1820.5642 - mae: 42.3745 - val_loss: 2070.8435 - val_mse: 2070.8440 - val_mae: 45.5065\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 1832.1340 - mse: 1832.1340 - mae: 42.6055 - val_loss: 2306.3191 - val_mse: 2306.3191 - val_mae: 48.0241\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 30s 454ms/step - loss: 1844.3508 - mse: 1844.3508 - mae: 42.6851 - val_loss: 1446.3757 - val_mse: 1446.3757 - val_mae: 38.0312\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 1820.4329 - mse: 1820.4329 - mae: 42.3039 - val_loss: 1561.4325 - val_mse: 1561.4325 - val_mae: 39.5150\n",
      "Epoch 59/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 1819.9957 - mse: 1819.9957 - mae: 42.4350 - val_loss: 1806.0464 - val_mse: 1806.0464 - val_mae: 42.4976\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 1842.1271 - mse: 1842.1271 - mae: 42.7064 - val_loss: 1290.8867 - val_mse: 1290.8867 - val_mae: 35.9289\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 1813.7534 - mse: 1813.7534 - mae: 42.3529 - val_loss: 1694.4977 - val_mse: 1694.4977 - val_mae: 41.1643\n",
      "Epoch 62/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 1839.0829 - mse: 1839.0829 - mae: 42.5044 - val_loss: 2109.9055 - val_mse: 2109.9055 - val_mae: 45.9337\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 1817.6144 - mse: 1817.6144 - mae: 42.4482 - val_loss: 1760.7797 - val_mse: 1760.7797 - val_mae: 41.9616\n",
      "Epoch 64/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 1837.9904 - mse: 1837.9904 - mae: 42.6372 - val_loss: 1743.3164 - val_mse: 1743.3164 - val_mae: 41.7530\n",
      "Epoch 65/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 30s 459ms/step - loss: 1823.6067 - mse: 1823.6067 - mae: 42.4772 - val_loss: 1593.3156 - val_mse: 1593.3156 - val_mae: 39.9163\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 1833.5400 - mse: 1833.5400 - mae: 42.6394 - val_loss: 1961.7566 - val_mse: 1961.7566 - val_mae: 44.2917\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 1840.4523 - mse: 1840.4523 - mae: 42.7079 - val_loss: 2231.1047 - val_mse: 2231.1047 - val_mae: 47.2346\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 1825.6669 - mse: 1825.6669 - mae: 42.5131 - val_loss: 2412.4255 - val_mse: 2412.4255 - val_mae: 49.1164\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 1831.7256 - mse: 1831.7256 - mae: 42.5601 - val_loss: 1797.9242 - val_mse: 1797.9242 - val_mae: 42.4019\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 1824.7460 - mse: 1824.7460 - mae: 42.5161 - val_loss: 1530.8781 - val_mse: 1530.8781 - val_mae: 39.1264\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 1820.0710 - mse: 1820.0710 - mae: 42.4310 - val_loss: 2282.3123 - val_mse: 2282.3125 - val_mae: 47.7735\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 30s 458ms/step - loss: 1839.3136 - mse: 1839.3136 - mae: 42.6661 - val_loss: 1773.3280 - val_mse: 1773.3280 - val_mae: 42.1109\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 31s 485ms/step - loss: 1836.7274 - mse: 1836.7274 - mae: 42.6156 - val_loss: 1673.6453 - val_mse: 1673.6453 - val_mae: 40.9102\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 1824.0394 - mse: 1824.0394 - mae: 42.5067 - val_loss: 2065.6423 - val_mse: 2065.6423 - val_mae: 45.4493\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 1834.3625 - mse: 1834.3625 - mae: 42.5899 - val_loss: 1932.8561 - val_mse: 1932.8561 - val_mae: 43.9642\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 1814.3827 - mse: 1814.3827 - mae: 42.3857 - val_loss: 1750.5190 - val_mse: 1750.5190 - val_mae: 41.8392\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 1830.0814 - mse: 1830.0814 - mae: 42.5488 - val_loss: 1944.8359 - val_mse: 1944.8359 - val_mae: 44.1003\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 27737s 427s/step - loss: 1829.3455 - mse: 1829.3455 - mae: 42.5943 - val_loss: 1635.6061 - val_mse: 1635.6061 - val_mae: 40.4426\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 36s 557ms/step - loss: 1815.9821 - mse: 1815.9821 - mae: 42.4146 - val_loss: 2087.8828 - val_mse: 2087.8828 - val_mae: 45.6933\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 25s 383ms/step - loss: 1828.7816 - mse: 1828.7816 - mae: 42.5167 - val_loss: 1976.4938 - val_mse: 1976.4938 - val_mae: 44.4578\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 23s 347ms/step - loss: 1831.6725 - mse: 1831.6725 - mae: 42.5087 - val_loss: 2415.9346 - val_mse: 2415.9346 - val_mae: 49.1521\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 1826.8372 - mse: 1826.8372 - mae: 42.5021 - val_loss: 1729.5210 - val_mse: 1729.5210 - val_mae: 41.5875\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 41s 634ms/step - loss: 1840.6226 - mse: 1840.6226 - mae: 42.6797 - val_loss: 1436.6539 - val_mse: 1436.6539 - val_mae: 37.9032\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 29s 452ms/step - loss: 1814.0493 - mse: 1814.0493 - mae: 42.3496 - val_loss: 1670.4436 - val_mse: 1670.4436 - val_mae: 40.8710\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 1839.2552 - mse: 1839.2552 - mae: 42.6801 - val_loss: 1793.4050 - val_mse: 1793.4050 - val_mae: 42.3486\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 1820.6036 - mse: 1820.6036 - mae: 42.4132 - val_loss: 1880.0417 - val_mse: 1880.0417 - val_mae: 43.3594\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 1812.5155 - mse: 1812.5155 - mae: 42.2896 - val_loss: 2484.6467 - val_mse: 2484.6467 - val_mae: 49.8462\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 1838.6501 - mse: 1838.6501 - mae: 42.5887 - val_loss: 2256.1162 - val_mse: 2256.1162 - val_mae: 47.4986\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 1835.5358 - mse: 1835.5358 - mae: 42.5910 - val_loss: 1882.3470 - val_mse: 1882.3470 - val_mae: 43.3860\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 36s 557ms/step - loss: 1829.2893 - mse: 1829.2893 - mae: 42.5486 - val_loss: 1609.3505 - val_mse: 1609.3505 - val_mae: 40.1167\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 1823.9529 - mse: 1823.9529 - mae: 42.5025 - val_loss: 1937.0161 - val_mse: 1937.0161 - val_mae: 44.0115\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 1827.1337 - mse: 1827.1337 - mae: 42.5266 - val_loss: 1892.5338 - val_mse: 1892.5338 - val_mae: 43.5033\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 26s 403ms/step - loss: 1815.7115 - mse: 1815.7115 - mae: 42.4026 - val_loss: 1793.2528 - val_mse: 1793.2528 - val_mae: 42.3468\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 27s 411ms/step - loss: 1849.0225 - mse: 1849.0225 - mae: 42.7590 - val_loss: 1893.9194 - val_mse: 1893.9194 - val_mae: 43.5192\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 26s 395ms/step - loss: 1812.6794 - mse: 1812.6794 - mae: 42.3860 - val_loss: 1951.2806 - val_mse: 1951.2806 - val_mae: 44.1733\n",
      "Epoch 96/180\n",
      "65/65 [==============================] - 26s 404ms/step - loss: 1831.8182 - mse: 1831.8182 - mae: 42.5760 - val_loss: 1643.1368 - val_mse: 1643.1368 - val_mae: 40.5356\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 27s 415ms/step - loss: 1832.5261 - mse: 1832.5261 - mae: 42.5877 - val_loss: 1671.5411 - val_mse: 1671.5411 - val_mae: 40.8845\n",
      "Epoch 98/180\n",
      "65/65 [==============================] - 26s 402ms/step - loss: 1830.1276 - mse: 1830.1276 - mae: 42.4283 - val_loss: 1633.8959 - val_mse: 1633.8959 - val_mae: 40.4215\n",
      "Epoch 99/180\n",
      "65/65 [==============================] - 25s 387ms/step - loss: 1812.5472 - mse: 1812.5472 - mae: 42.3638 - val_loss: 1708.7455 - val_mse: 1708.7455 - val_mae: 41.3370\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 27s 415ms/step - loss: 1825.8134 - mse: 1825.8134 - mae: 42.4932 - val_loss: 1820.1487 - val_mse: 1820.1487 - val_mae: 42.6632\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 27s 420ms/step - loss: 1836.6147 - mse: 1836.6147 - mae: 42.5912 - val_loss: 1694.9208 - val_mse: 1694.9208 - val_mae: 41.1694\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 1824.5583 - mse: 1824.5583 - mae: 42.4959 - val_loss: 1912.6698 - val_mse: 1912.6698 - val_mae: 43.7341\n",
      "Epoch 103/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 1844.7075 - mse: 1844.7075 - mae: 42.6401 - val_loss: 1875.2596 - val_mse: 1875.2596 - val_mae: 43.3043\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 28s 437ms/step - loss: 1817.4553 - mse: 1817.4553 - mae: 42.3727 - val_loss: 1689.9862 - val_mse: 1689.9862 - val_mae: 41.1094\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 1816.5662 - mse: 1816.5662 - mae: 42.3806 - val_loss: 2068.5691 - val_mse: 2068.5691 - val_mae: 45.4815\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 38s 580ms/step - loss: 1844.2899 - mse: 1844.2899 - mae: 42.7539 - val_loss: 1467.0160 - val_mse: 1467.0160 - val_mae: 38.3016\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 1817.4952 - mse: 1817.4952 - mae: 42.3918 - val_loss: 2075.0007 - val_mse: 2075.0007 - val_mae: 45.5522\n",
      "Epoch 108/180\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 1838.3361 - mse: 1838.3361 - mae: 42.6435 - val_loss: 2141.7837 - val_mse: 2141.7834 - val_mae: 46.2794\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 28s 437ms/step - loss: 1822.8597 - mse: 1822.8597 - mae: 42.4813 - val_loss: 1927.5553 - val_mse: 1927.5553 - val_mae: 43.9039\n",
      "Epoch 110/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 30s 460ms/step - loss: 1816.1182 - mse: 1816.1182 - mae: 42.3563 - val_loss: 2195.5200 - val_mse: 2195.5200 - val_mae: 46.8564\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 29s 439ms/step - loss: 1826.9579 - mse: 1826.9579 - mae: 42.5300 - val_loss: 2025.4801 - val_mse: 2025.4801 - val_mae: 45.0053\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 31s 484ms/step - loss: 1854.9482 - mse: 1854.9482 - mae: 42.8070 - val_loss: 1775.7854 - val_mse: 1775.7854 - val_mae: 42.1401\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 1809.4999 - mse: 1809.4999 - mae: 42.3063 - val_loss: 1564.1672 - val_mse: 1564.1672 - val_mae: 39.5495\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 1833.8779 - mse: 1833.8779 - mae: 42.6008 - val_loss: 2310.4504 - val_mse: 2310.4504 - val_mae: 48.0671\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 44s 678ms/step - loss: 1825.1373 - mse: 1825.1373 - mae: 42.2945 - val_loss: 1683.5898 - val_mse: 1683.5898 - val_mae: 41.0316\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 43s 656ms/step - loss: 1838.6995 - mse: 1838.6995 - mae: 42.6417 - val_loss: 1771.5095 - val_mse: 1771.5095 - val_mae: 42.0893\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 46s 702ms/step - loss: 1826.0261 - mse: 1826.0261 - mae: 42.5227 - val_loss: 1531.9911 - val_mse: 1531.9911 - val_mae: 39.1406\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 1831.1772 - mse: 1831.1772 - mae: 42.5625 - val_loss: 1849.6204 - val_mse: 1849.6204 - val_mae: 43.0072\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 1811.6459 - mse: 1811.6459 - mae: 42.2902 - val_loss: 1856.1935 - val_mse: 1856.1935 - val_mae: 43.0836\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 33s 506ms/step - loss: 1846.9187 - mse: 1846.9187 - mae: 42.6604 - val_loss: 1719.5367 - val_mse: 1719.5367 - val_mae: 41.4673\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 1828.8019 - mse: 1828.8019 - mae: 42.4396 - val_loss: 1926.7587 - val_mse: 1926.7587 - val_mae: 43.8948\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 1829.7765 - mse: 1829.7765 - mae: 42.5857 - val_loss: 1918.1556 - val_mse: 1918.1556 - val_mae: 43.7967\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 39s 597ms/step - loss: 1824.3607 - mse: 1824.3607 - mae: 42.5363 - val_loss: 1887.3151 - val_mse: 1887.3151 - val_mae: 43.4432\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 29s 454ms/step - loss: 1819.2086 - mse: 1819.2086 - mae: 42.4396 - val_loss: 1610.2765 - val_mse: 1610.2765 - val_mae: 40.1282\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 1835.1118 - mse: 1835.1116 - mae: 42.5950 - val_loss: 1457.9176 - val_mse: 1457.9176 - val_mae: 38.1827\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 1837.6631 - mse: 1837.6631 - mae: 42.6559 - val_loss: 1404.3556 - val_mse: 1404.3556 - val_mae: 37.4747\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 1830.9207 - mse: 1830.9207 - mae: 42.5165 - val_loss: 2083.7905 - val_mse: 2083.7905 - val_mae: 45.6485\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 40s 619ms/step - loss: 1809.9645 - mse: 1809.9645 - mae: 42.3090 - val_loss: 1931.4315 - val_mse: 1931.4315 - val_mae: 43.9480\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 1842.7930 - mse: 1842.7930 - mae: 42.7094 - val_loss: 1931.3743 - val_mse: 1931.3743 - val_mae: 43.9474\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 1811.4961 - mse: 1811.4961 - mae: 42.3053 - val_loss: 1656.0288 - val_mse: 1656.0288 - val_mae: 40.6943\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 43s 664ms/step - loss: 1849.7960 - mse: 1849.7960 - mae: 42.7981 - val_loss: 1855.1693 - val_mse: 1855.1693 - val_mae: 43.0717\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 36s 551ms/step - loss: 1807.8478 - mse: 1807.8478 - mae: 42.2362 - val_loss: 1825.3774 - val_mse: 1825.3774 - val_mae: 42.7244\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 1851.0875 - mse: 1851.0875 - mae: 42.7898 - val_loss: 1644.5410 - val_mse: 1644.5410 - val_mae: 40.5529\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 1821.4097 - mse: 1821.4097 - mae: 42.4330 - val_loss: 2119.8792 - val_mse: 2119.8792 - val_mae: 46.0421\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 41s 634ms/step - loss: 1820.9696 - mse: 1820.9696 - mae: 42.4556 - val_loss: 1886.7125 - val_mse: 1886.7125 - val_mae: 43.4363\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 1850.1906 - mse: 1850.1906 - mae: 42.8035 - val_loss: 1429.9127 - val_mse: 1429.9127 - val_mae: 37.8142\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 1806.4888 - mse: 1806.4888 - mae: 42.1413 - val_loss: 1897.6888 - val_mse: 1897.6888 - val_mae: 43.5625\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 1842.7634 - mse: 1842.7634 - mae: 42.6800 - val_loss: 1863.4850 - val_mse: 1863.4850 - val_mae: 43.1681\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 1824.4093 - mse: 1824.4092 - mae: 42.5430 - val_loss: 1977.3340 - val_mse: 1977.3340 - val_mae: 44.4672\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 1830.8503 - mse: 1830.8503 - mae: 42.5744 - val_loss: 1816.6528 - val_mse: 1816.6528 - val_mae: 42.6222\n",
      "Epoch 141/180\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 1821.0782 - mse: 1821.0782 - mae: 42.4690 - val_loss: 1774.4288 - val_mse: 1774.4288 - val_mae: 42.1240\n",
      "Epoch 142/180\n",
      "65/65 [==============================] - 37s 567ms/step - loss: 1848.7261 - mse: 1848.7261 - mae: 42.7829 - val_loss: 1580.9891 - val_mse: 1580.9891 - val_mae: 39.7616\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 39s 601ms/step - loss: 1811.4211 - mse: 1811.4211 - mae: 42.3619 - val_loss: 1911.8503 - val_mse: 1911.8503 - val_mae: 43.7247\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 1838.5292 - mse: 1838.5292 - mae: 42.6951 - val_loss: 1749.1461 - val_mse: 1749.1461 - val_mae: 41.8228\n",
      "Epoch 145/180\n",
      "65/65 [==============================] - 47s 727ms/step - loss: 1824.7981 - mse: 1824.7981 - mae: 42.4955 - val_loss: 1963.7429 - val_mse: 1963.7429 - val_mae: 44.3141\n",
      "Epoch 146/180\n",
      "65/65 [==============================] - 36s 553ms/step - loss: 1825.2480 - mse: 1825.2480 - mae: 42.5010 - val_loss: 1746.6959 - val_mse: 1746.6959 - val_mae: 41.7935\n",
      "Epoch 147/180\n",
      "65/65 [==============================] - 43s 660ms/step - loss: 1843.4733 - mse: 1843.4733 - mae: 42.7129 - val_loss: 1661.4099 - val_mse: 1661.4099 - val_mae: 40.7604\n",
      "Epoch 148/180\n",
      "65/65 [==============================] - 41s 627ms/step - loss: 1817.7532 - mse: 1817.7531 - mae: 42.4309 - val_loss: 1995.3817 - val_mse: 1995.3817 - val_mae: 44.6697\n",
      "Epoch 149/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 1848.1799 - mse: 1848.1799 - mae: 42.7757 - val_loss: 1832.0538 - val_mse: 1832.0538 - val_mae: 42.8025\n",
      "Epoch 150/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 1809.6737 - mse: 1809.6737 - mae: 42.2967 - val_loss: 1626.7561 - val_mse: 1626.7561 - val_mae: 40.3331\n",
      "Epoch 151/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 1835.4088 - mse: 1835.4088 - mae: 42.6067 - val_loss: 1619.5447 - val_mse: 1619.5447 - val_mae: 40.2436\n",
      "Epoch 152/180\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 1816.6337 - mse: 1816.6337 - mae: 42.2710 - val_loss: 1971.9347 - val_mse: 1971.9347 - val_mae: 44.4065\n",
      "Epoch 153/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 1821.2256 - mse: 1821.2256 - mae: 42.3523 - val_loss: 1701.7487 - val_mse: 1701.7487 - val_mae: 41.2522\n",
      "Epoch 154/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 1842.1189 - mse: 1842.1189 - mae: 42.6660 - val_loss: 1784.9059 - val_mse: 1784.9059 - val_mae: 42.2481\n",
      "Epoch 155/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 37s 567ms/step - loss: 1845.7029 - mse: 1845.7029 - mae: 42.6871 - val_loss: 1567.8922 - val_mse: 1567.8922 - val_mae: 39.5966\n",
      "Epoch 156/180\n",
      "65/65 [==============================] - 40s 613ms/step - loss: 1813.3049 - mse: 1813.3049 - mae: 42.3834 - val_loss: 1649.4772 - val_mse: 1649.4772 - val_mae: 40.6137\n",
      "Epoch 157/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 1825.1605 - mse: 1825.1605 - mae: 42.4415 - val_loss: 1831.6116 - val_mse: 1831.6116 - val_mae: 42.7973\n",
      "Epoch 158/180\n",
      "65/65 [==============================] - 40s 620ms/step - loss: 1834.0957 - mse: 1834.0957 - mae: 42.6307 - val_loss: 1996.3978 - val_mse: 1996.3978 - val_mae: 44.6811\n",
      "Epoch 159/180\n",
      "65/65 [==============================] - 39s 606ms/step - loss: 1826.2075 - mse: 1826.2075 - mae: 42.5496 - val_loss: 1786.2924 - val_mse: 1786.2924 - val_mae: 42.2645\n",
      "Epoch 160/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 1822.3833 - mse: 1822.3833 - mae: 42.3608 - val_loss: 1124.7260 - val_mse: 1124.7260 - val_mae: 33.5369\n",
      "Epoch 161/180\n",
      "65/65 [==============================] - 25s 388ms/step - loss: 1833.0920 - mse: 1833.0920 - mae: 42.5514 - val_loss: 1801.3175 - val_mse: 1801.3175 - val_mae: 42.4419\n",
      "Epoch 162/180\n",
      "65/65 [==============================] - 39s 603ms/step - loss: 1837.2814 - mse: 1837.2814 - mae: 42.6733 - val_loss: 1882.2018 - val_mse: 1882.2018 - val_mae: 43.3843\n",
      "Epoch 163/180\n",
      "65/65 [==============================] - 27s 414ms/step - loss: 1829.0078 - mse: 1829.0078 - mae: 42.5866 - val_loss: 1946.3402 - val_mse: 1946.3402 - val_mae: 44.1173\n",
      "Epoch 164/180\n",
      "65/65 [==============================] - 28s 434ms/step - loss: 1815.2126 - mse: 1815.2126 - mae: 42.3761 - val_loss: 1947.9364 - val_mse: 1947.9364 - val_mae: 44.1354\n",
      "Epoch 165/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 1815.0758 - mse: 1815.0758 - mae: 42.3054 - val_loss: 2078.4556 - val_mse: 2078.4556 - val_mae: 45.5901\n",
      "Epoch 166/180\n",
      "65/65 [==============================] - 32s 486ms/step - loss: 1834.3098 - mse: 1834.3098 - mae: 42.6056 - val_loss: 2158.8284 - val_mse: 2158.8284 - val_mae: 46.4632\n",
      "Epoch 167/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 1838.2333 - mse: 1838.2333 - mae: 42.6303 - val_loss: 1679.1312 - val_mse: 1679.1312 - val_mae: 40.9772\n",
      "Epoch 168/180\n",
      "65/65 [==============================] - 40s 608ms/step - loss: 1815.4912 - mse: 1815.4912 - mae: 42.4103 - val_loss: 2268.6694 - val_mse: 2268.6694 - val_mae: 47.6305\n",
      "Epoch 169/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 1836.3938 - mse: 1836.3938 - mae: 42.5742 - val_loss: 1737.7059 - val_mse: 1737.7059 - val_mae: 41.6858\n",
      "Epoch 170/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 1833.7705 - mse: 1833.7705 - mae: 42.4866 - val_loss: 2034.4810 - val_mse: 2034.4810 - val_mae: 45.1052\n",
      "Epoch 171/180\n",
      "65/65 [==============================] - 38s 590ms/step - loss: 1808.9087 - mse: 1808.9087 - mae: 42.3297 - val_loss: 1956.5746 - val_mse: 1956.5746 - val_mae: 44.2332\n",
      "Epoch 172/180\n",
      "65/65 [==============================] - 27s 422ms/step - loss: 1862.3472 - mse: 1862.3472 - mae: 42.9224 - val_loss: 1885.1339 - val_mse: 1885.1339 - val_mae: 43.4181\n",
      "Epoch 173/180\n",
      "65/65 [==============================] - 42s 646ms/step - loss: 1824.0199 - mse: 1824.0199 - mae: 42.4035 - val_loss: 1469.8263 - val_mse: 1469.8263 - val_mae: 38.3383\n",
      "Epoch 174/180\n",
      "65/65 [==============================] - 36s 554ms/step - loss: 1815.2756 - mse: 1815.2756 - mae: 42.3556 - val_loss: 1522.3619 - val_mse: 1522.3619 - val_mae: 39.0174\n",
      "Epoch 175/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 1808.7742 - mse: 1808.7742 - mae: 42.2296 - val_loss: 1870.6823 - val_mse: 1870.6823 - val_mae: 43.2514\n",
      "Epoch 176/180\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 1837.9580 - mse: 1837.9580 - mae: 42.5915 - val_loss: 1549.6801 - val_mse: 1549.6801 - val_mae: 39.3660\n",
      "Epoch 177/180\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 1840.7233 - mse: 1840.7233 - mae: 42.6396 - val_loss: 1459.5280 - val_mse: 1459.5280 - val_mae: 38.2038\n",
      "Epoch 178/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 1826.7566 - mse: 1826.7566 - mae: 42.4769 - val_loss: 1763.1566 - val_mse: 1763.1566 - val_mae: 41.9899\n",
      "Epoch 179/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 1830.8586 - mse: 1830.8586 - mae: 42.5574 - val_loss: 1770.2540 - val_mse: 1770.2540 - val_mae: 42.0744\n",
      "Epoch 180/180\n",
      "65/65 [==============================] - 33s 509ms/step - loss: 1830.0316 - mse: 1830.0316 - mae: 42.5871 - val_loss: 1695.5679 - val_mse: 1695.5679 - val_mae: 41.1773\n",
      "Epoch 1/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 16406.0801 - mse: 16406.0801 - mae: 76.4980 - val_loss: 8618.6865 - val_mse: 8618.6865 - val_mae: 92.8369\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 12451.9355 - mse: 12451.9355 - mae: 111.0304 - val_loss: 11209.7705 - val_mse: 11209.7705 - val_mae: 105.8762\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 12320.9023 - mse: 12320.9023 - mae: 110.5148 - val_loss: 11177.1836 - val_mse: 11177.1836 - val_mae: 105.7222\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 12451.5947 - mse: 12451.5947 - mae: 110.7973 - val_loss: 10695.9229 - val_mse: 10695.9248 - val_mae: 103.4211\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 12404.7207 - mse: 12404.7207 - mae: 110.7926 - val_loss: 11171.8066 - val_mse: 11171.8066 - val_mae: 105.6968\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 12367.5898 - mse: 12367.5898 - mae: 110.6840 - val_loss: 12991.7148 - val_mse: 12991.7148 - val_mae: 113.9812\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 29s 444ms/step - loss: 12419.3848 - mse: 12419.3848 - mae: 110.9589 - val_loss: 14365.0898 - val_mse: 14365.0898 - val_mae: 119.8544\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 12397.9756 - mse: 12397.9756 - mae: 110.8402 - val_loss: 11238.4580 - val_mse: 11238.4580 - val_mae: 106.0116\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 12362.7012 - mse: 12362.7012 - mae: 110.7910 - val_loss: 14137.3369 - val_mse: 14137.3369 - val_mae: 118.9005\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 12424.6934 - mse: 12424.6934 - mae: 111.0298 - val_loss: 12361.5293 - val_mse: 12361.5293 - val_mae: 111.1824\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 12391.7207 - mse: 12391.7207 - mae: 110.8400 - val_loss: 13960.8184 - val_mse: 13960.8184 - val_mae: 118.1559\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 12398.4707 - mse: 12398.4707 - mae: 110.8309 - val_loss: 9842.9131 - val_mse: 9842.9131 - val_mae: 99.2114\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 28s 438ms/step - loss: 12420.1113 - mse: 12420.1113 - mae: 110.8403 - val_loss: 11265.8242 - val_mse: 11265.8242 - val_mae: 106.1406\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 29s 439ms/step - loss: 12417.6680 - mse: 12417.6680 - mae: 110.9711 - val_loss: 11878.5586 - val_mse: 11878.5586 - val_mae: 108.9888\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 12432.4043 - mse: 12432.4043 - mae: 111.0974 - val_loss: 13128.2793 - val_mse: 13128.2793 - val_mae: 114.5787\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 12368.8047 - mse: 12368.8047 - mae: 110.8009 - val_loss: 11982.4873 - val_mse: 11982.4873 - val_mae: 109.4645\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 28s 438ms/step - loss: 12425.2178 - mse: 12425.2178 - mae: 110.8788 - val_loss: 12676.8389 - val_mse: 12676.8389 - val_mae: 112.5915\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 26s 399ms/step - loss: 12510.9971 - mse: 12510.9971 - mae: 111.4120 - val_loss: 10877.1777 - val_mse: 10877.1777 - val_mae: 104.2937\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 27s 417ms/step - loss: 12384.9707 - mse: 12384.9707 - mae: 110.5381 - val_loss: 11182.3594 - val_mse: 11182.3594 - val_mae: 105.7467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/180\n",
      "65/65 [==============================] - 28s 428ms/step - loss: 12309.2979 - mse: 12309.2979 - mae: 110.4119 - val_loss: 9169.0225 - val_mse: 9169.0225 - val_mae: 95.7550\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 27s 423ms/step - loss: 12424.9521 - mse: 12424.9521 - mae: 110.9342 - val_loss: 10869.9492 - val_mse: 10869.9492 - val_mae: 104.2590\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 28s 430ms/step - loss: 12402.2832 - mse: 12402.2832 - mae: 110.9608 - val_loss: 12103.6523 - val_mse: 12103.6523 - val_mae: 110.0166\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 12384.0049 - mse: 12384.0049 - mae: 110.8147 - val_loss: 11560.3740 - val_mse: 11560.3740 - val_mae: 107.5192\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 27s 410ms/step - loss: 12417.0947 - mse: 12417.0947 - mae: 110.9171 - val_loss: 15478.9180 - val_mse: 15478.9180 - val_mae: 124.4143\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 25s 392ms/step - loss: 12537.8145 - mse: 12537.8145 - mae: 111.5626 - val_loss: 14176.8564 - val_mse: 14176.8564 - val_mae: 119.0666\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 25s 388ms/step - loss: 12340.2363 - mse: 12340.2363 - mae: 110.6870 - val_loss: 11473.4775 - val_mse: 11473.4775 - val_mae: 107.1143\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 25s 391ms/step - loss: 12388.6885 - mse: 12388.6885 - mae: 110.7347 - val_loss: 9847.5479 - val_mse: 9847.5479 - val_mae: 99.2348\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 25s 383ms/step - loss: 12392.4873 - mse: 12392.4873 - mae: 110.6185 - val_loss: 8897.0791 - val_mse: 8897.0791 - val_mae: 94.3243\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 27s 421ms/step - loss: 12498.3799 - mse: 12498.3799 - mae: 111.3210 - val_loss: 11849.8584 - val_mse: 11849.8584 - val_mae: 108.8570\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 26s 402ms/step - loss: 12413.9453 - mse: 12413.9453 - mae: 111.0713 - val_loss: 13186.1924 - val_mse: 13186.1924 - val_mae: 114.8312\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 27s 410ms/step - loss: 12398.2773 - mse: 12398.2773 - mae: 110.8442 - val_loss: 13482.9590 - val_mse: 13482.9590 - val_mae: 116.1162\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 27s 408ms/step - loss: 12440.7617 - mse: 12440.7617 - mae: 111.0768 - val_loss: 11435.4854 - val_mse: 11435.4854 - val_mae: 106.9368\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 27s 411ms/step - loss: 12428.8486 - mse: 12428.8486 - mae: 111.1504 - val_loss: 13874.7793 - val_mse: 13874.7793 - val_mae: 117.7912\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 26s 405ms/step - loss: 12383.4277 - mse: 12383.4277 - mae: 110.9329 - val_loss: 11909.2939 - val_mse: 11909.2939 - val_mae: 109.1297\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 26s 399ms/step - loss: 12354.6055 - mse: 12354.6055 - mae: 110.6606 - val_loss: 12070.2637 - val_mse: 12070.2637 - val_mae: 109.8647\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 26s 407ms/step - loss: 12472.1309 - mse: 12472.1309 - mae: 111.2291 - val_loss: 13614.5352 - val_mse: 13614.5352 - val_mae: 116.6813\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 25s 387ms/step - loss: 12418.8066 - mse: 12418.8066 - mae: 111.0304 - val_loss: 11007.9258 - val_mse: 11007.9258 - val_mae: 104.9187\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 27s 413ms/step - loss: 12331.4756 - mse: 12331.4756 - mae: 110.6057 - val_loss: 14194.3242 - val_mse: 14194.3242 - val_mae: 119.1399\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 26s 399ms/step - loss: 12386.3535 - mse: 12386.3535 - mae: 110.8117 - val_loss: 12093.7500 - val_mse: 12093.7500 - val_mae: 109.9716\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 26s 395ms/step - loss: 12274.8975 - mse: 12274.8975 - mae: 110.2640 - val_loss: 13414.8330 - val_mse: 13414.8330 - val_mae: 115.8224\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 27s 408ms/step - loss: 12484.0889 - mse: 12484.0889 - mae: 111.0861 - val_loss: 15811.1055 - val_mse: 15811.1055 - val_mae: 125.7422\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 29s 444ms/step - loss: 12538.7295 - mse: 12538.7295 - mae: 111.4770 - val_loss: 12265.0615 - val_mse: 12265.0615 - val_mae: 110.7477\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 27s 416ms/step - loss: 12344.9561 - mse: 12344.9561 - mae: 110.7170 - val_loss: 12296.6162 - val_mse: 12296.6162 - val_mae: 110.8901\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 25s 390ms/step - loss: 12530.7471 - mse: 12530.7471 - mae: 111.5411 - val_loss: 13524.2627 - val_mse: 13524.2627 - val_mae: 116.2938\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 25s 387ms/step - loss: 12415.4707 - mse: 12415.4707 - mae: 111.0766 - val_loss: 9829.8135 - val_mse: 9829.8135 - val_mae: 99.1454\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 25s 388ms/step - loss: 12453.9678 - mse: 12453.9678 - mae: 111.1557 - val_loss: 11771.1230 - val_mse: 11771.1230 - val_mae: 108.4948\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 26s 407ms/step - loss: 12285.5537 - mse: 12285.5537 - mae: 110.4332 - val_loss: 12326.5146 - val_mse: 12326.5146 - val_mae: 111.0248\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 12525.4717 - mse: 12525.4717 - mae: 111.3366 - val_loss: 15211.3896 - val_mse: 15211.3896 - val_mae: 123.3344\n",
      "Epoch 49/180\n",
      "65/65 [==============================] - 25s 391ms/step - loss: 12345.8848 - mse: 12345.8848 - mae: 110.6342 - val_loss: 11680.2588 - val_mse: 11680.2588 - val_mae: 108.0752\n",
      "Epoch 50/180\n",
      "65/65 [==============================] - 25s 385ms/step - loss: 12303.0957 - mse: 12303.0957 - mae: 110.3958 - val_loss: 12409.6240 - val_mse: 12409.6240 - val_mae: 111.3985\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 25s 389ms/step - loss: 12437.4170 - mse: 12437.4170 - mae: 110.9718 - val_loss: 12021.9805 - val_mse: 12021.9805 - val_mae: 109.6448\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 26s 397ms/step - loss: 12491.0283 - mse: 12491.0283 - mae: 111.3131 - val_loss: 11905.4219 - val_mse: 11905.4219 - val_mae: 109.1120\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 25s 381ms/step - loss: 12440.6553 - mse: 12440.6553 - mae: 110.9103 - val_loss: 13924.4639 - val_mse: 13924.4639 - val_mae: 118.0020\n",
      "Epoch 54/180\n",
      "65/65 [==============================] - 25s 383ms/step - loss: 12407.0527 - mse: 12407.0518 - mae: 110.7720 - val_loss: 9783.5332 - val_mse: 9783.5342 - val_mae: 98.9117\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 12473.0088 - mse: 12473.0088 - mae: 111.1372 - val_loss: 11150.4570 - val_mse: 11150.4570 - val_mae: 105.5957\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 26s 406ms/step - loss: 12355.4473 - mse: 12355.4473 - mae: 110.5518 - val_loss: 15171.7275 - val_mse: 15171.7275 - val_mae: 123.1736\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 12330.3652 - mse: 12330.3652 - mae: 110.5904 - val_loss: 11847.8135 - val_mse: 11847.8135 - val_mae: 108.8476\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 25s 388ms/step - loss: 12460.0205 - mse: 12460.0205 - mae: 111.1669 - val_loss: 13069.2207 - val_mse: 13069.2207 - val_mae: 114.3207\n",
      "Epoch 59/180\n",
      "65/65 [==============================] - 25s 385ms/step - loss: 12348.9678 - mse: 12348.9678 - mae: 110.7492 - val_loss: 11813.8232 - val_mse: 11813.8232 - val_mae: 108.6914\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 25s 388ms/step - loss: 12402.7344 - mse: 12402.7344 - mae: 110.9508 - val_loss: 11024.9277 - val_mse: 11024.9277 - val_mae: 104.9996\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 26s 401ms/step - loss: 12452.2793 - mse: 12452.2793 - mae: 111.1027 - val_loss: 9589.6416 - val_mse: 9589.6416 - val_mae: 97.9267\n",
      "Epoch 62/180\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 12394.4922 - mse: 12394.4922 - mae: 110.7050 - val_loss: 14001.2812 - val_mse: 14001.2812 - val_mae: 118.3270\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 25s 387ms/step - loss: 12414.4541 - mse: 12414.4541 - mae: 110.9500 - val_loss: 13165.5293 - val_mse: 13165.5293 - val_mae: 114.7411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/180\n",
      "65/65 [==============================] - 25s 390ms/step - loss: 12420.9951 - mse: 12420.9951 - mae: 111.0647 - val_loss: 10410.9561 - val_mse: 10410.9561 - val_mae: 102.0341\n",
      "Epoch 65/180\n",
      "65/65 [==============================] - 27s 418ms/step - loss: 12402.7734 - mse: 12402.7734 - mae: 110.8391 - val_loss: 9932.3389 - val_mse: 9932.3398 - val_mae: 99.6611\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 26s 407ms/step - loss: 12336.3281 - mse: 12336.3281 - mae: 110.5772 - val_loss: 11655.9395 - val_mse: 11655.9395 - val_mae: 107.9627\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 25s 391ms/step - loss: 12421.6973 - mse: 12421.6973 - mae: 110.9962 - val_loss: 11592.2168 - val_mse: 11592.2168 - val_mae: 107.6671\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 26s 401ms/step - loss: 12396.4951 - mse: 12396.4951 - mae: 110.8640 - val_loss: 15313.9590 - val_mse: 15313.9590 - val_mae: 123.7496\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 25s 385ms/step - loss: 12453.8291 - mse: 12453.8291 - mae: 110.9671 - val_loss: 11662.7852 - val_mse: 11662.7852 - val_mae: 107.9944\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 26s 399ms/step - loss: 12380.2715 - mse: 12380.2715 - mae: 110.8507 - val_loss: 12322.4248 - val_mse: 12322.4248 - val_mae: 111.0064\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 25s 384ms/step - loss: 12439.0537 - mse: 12439.0537 - mae: 111.0102 - val_loss: 13263.8838 - val_mse: 13263.8838 - val_mae: 115.1689\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 25s 387ms/step - loss: 12281.7041 - mse: 12281.7041 - mae: 110.4079 - val_loss: 12580.7949 - val_mse: 12580.7949 - val_mae: 112.1641\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 12418.7607 - mse: 12418.7607 - mae: 110.8618 - val_loss: 11852.6055 - val_mse: 11852.6055 - val_mae: 108.8697\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 29s 450ms/step - loss: 12396.0615 - mse: 12396.0615 - mae: 110.9347 - val_loss: 12980.0742 - val_mse: 12980.0742 - val_mae: 113.9301\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 12538.2100 - mse: 12538.2100 - mae: 111.4255 - val_loss: 13031.3311 - val_mse: 13031.3311 - val_mae: 114.1549\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 28s 437ms/step - loss: 12364.7051 - mse: 12364.7051 - mae: 110.7711 - val_loss: 11978.9434 - val_mse: 11978.9434 - val_mae: 109.4484\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 12445.4141 - mse: 12445.4141 - mae: 111.1082 - val_loss: 12387.4336 - val_mse: 12387.4336 - val_mae: 111.2988\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 12433.5195 - mse: 12433.5195 - mae: 111.1055 - val_loss: 12763.1455 - val_mse: 12763.1455 - val_mae: 112.9741\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 34s 517ms/step - loss: 12408.7266 - mse: 12408.7266 - mae: 110.9346 - val_loss: 13234.5498 - val_mse: 13234.5498 - val_mae: 115.0415\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 12420.4756 - mse: 12420.4756 - mae: 110.9421 - val_loss: 15551.4395 - val_mse: 15551.4395 - val_mae: 124.7054\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 12439.7432 - mse: 12439.7432 - mae: 110.9939 - val_loss: 11435.2314 - val_mse: 11435.2314 - val_mae: 106.9356\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 12319.4385 - mse: 12319.4385 - mae: 110.6143 - val_loss: 13605.4316 - val_mse: 13605.4316 - val_mae: 116.6423\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 12439.1963 - mse: 12439.1963 - mae: 111.1514 - val_loss: 12422.1006 - val_mse: 12422.1006 - val_mae: 111.4545\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 12455.3496 - mse: 12455.3496 - mae: 110.9728 - val_loss: 11191.7852 - val_mse: 11191.7852 - val_mae: 105.7912\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 12387.6436 - mse: 12387.6436 - mae: 110.8642 - val_loss: 13641.5830 - val_mse: 13641.5830 - val_mae: 116.7972\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 12375.3818 - mse: 12375.3818 - mae: 110.7725 - val_loss: 12207.5898 - val_mse: 12207.5898 - val_mae: 110.4879\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 12551.7910 - mse: 12551.7910 - mae: 111.5376 - val_loss: 14725.0312 - val_mse: 14725.0312 - val_mae: 121.3467\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 29s 447ms/step - loss: 12396.2627 - mse: 12396.2627 - mae: 110.9124 - val_loss: 14499.6816 - val_mse: 14499.6816 - val_mae: 120.4146\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 12356.5957 - mse: 12356.5957 - mae: 110.5854 - val_loss: 12722.0684 - val_mse: 12722.0684 - val_mae: 112.7921\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 12428.6270 - mse: 12428.6270 - mae: 110.8974 - val_loss: 12659.4736 - val_mse: 12659.4736 - val_mae: 112.5143\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 12366.9180 - mse: 12366.9180 - mae: 110.5967 - val_loss: 12559.9521 - val_mse: 12559.9521 - val_mae: 112.0712\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 32s 497ms/step - loss: 12421.1191 - mse: 12421.1191 - mae: 111.0498 - val_loss: 13092.3535 - val_mse: 13092.3535 - val_mae: 114.4218\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 12416.6592 - mse: 12416.6592 - mae: 111.0296 - val_loss: 13017.9150 - val_mse: 13017.9150 - val_mae: 114.0961\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 12612.3242 - mse: 12612.3242 - mae: 111.9028 - val_loss: 13426.7119 - val_mse: 13426.7119 - val_mae: 115.8737\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 12268.3359 - mse: 12268.3359 - mae: 110.3733 - val_loss: 11327.6406 - val_mse: 11327.6406 - val_mae: 106.4314\n",
      "Epoch 96/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 12430.3037 - mse: 12430.3037 - mae: 111.0207 - val_loss: 13448.1211 - val_mse: 13448.1211 - val_mae: 115.9660\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 12366.4873 - mse: 12366.4873 - mae: 110.7616 - val_loss: 13792.1387 - val_mse: 13792.1387 - val_mae: 117.4399\n",
      "Epoch 98/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 12420.5068 - mse: 12420.5068 - mae: 111.0681 - val_loss: 11093.7607 - val_mse: 11093.7607 - val_mae: 105.3269\n",
      "Epoch 99/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 12410.9277 - mse: 12410.9277 - mae: 110.9976 - val_loss: 13535.6553 - val_mse: 13535.6553 - val_mae: 116.3428\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 12471.6260 - mse: 12471.6260 - mae: 111.2963 - val_loss: 12225.8086 - val_mse: 12225.8086 - val_mae: 110.5704\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 12411.9473 - mse: 12411.9473 - mae: 111.0343 - val_loss: 11643.3398 - val_mse: 11643.3398 - val_mae: 107.9043\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 12368.5986 - mse: 12368.5986 - mae: 110.8025 - val_loss: 11793.8574 - val_mse: 11793.8574 - val_mae: 108.5995\n",
      "Epoch 103/180\n",
      "65/65 [==============================] - 33s 509ms/step - loss: 12451.6465 - mse: 12451.6465 - mae: 111.1551 - val_loss: 14096.4219 - val_mse: 14096.4219 - val_mae: 118.7283\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 12299.5859 - mse: 12299.5859 - mae: 110.5495 - val_loss: 12825.6533 - val_mse: 12825.6533 - val_mae: 113.2504\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 12575.6709 - mse: 12575.6709 - mae: 111.5819 - val_loss: 9809.8105 - val_mse: 9809.8105 - val_mae: 99.0445\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 12277.4307 - mse: 12277.4307 - mae: 110.3188 - val_loss: 12486.9326 - val_mse: 12486.9326 - val_mae: 111.7449\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 12488.0420 - mse: 12488.0420 - mae: 111.3498 - val_loss: 10290.5156 - val_mse: 10290.5156 - val_mae: 101.4422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 12414.1641 - mse: 12414.1641 - mae: 110.7813 - val_loss: 11297.2637 - val_mse: 11297.2637 - val_mae: 106.2886\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 32s 497ms/step - loss: 12309.2217 - mse: 12309.2217 - mae: 110.4865 - val_loss: 13445.4873 - val_mse: 13445.4873 - val_mae: 115.9547\n",
      "Epoch 110/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 12420.3145 - mse: 12420.3145 - mae: 111.0458 - val_loss: 11809.6094 - val_mse: 11809.6094 - val_mae: 108.6720\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 12424.4111 - mse: 12424.4111 - mae: 111.0929 - val_loss: 12428.7500 - val_mse: 12428.7500 - val_mae: 111.4843\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 12380.8350 - mse: 12380.8350 - mae: 110.9167 - val_loss: 12698.4355 - val_mse: 12698.4355 - val_mae: 112.6873\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 12523.6221 - mse: 12523.6221 - mae: 111.3793 - val_loss: 12290.4805 - val_mse: 12290.4805 - val_mae: 110.8624\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 12298.1611 - mse: 12298.1611 - mae: 110.4863 - val_loss: 15118.4248 - val_mse: 15118.4248 - val_mae: 122.9570\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 12464.2988 - mse: 12464.2988 - mae: 111.1809 - val_loss: 11995.4883 - val_mse: 11995.4883 - val_mae: 109.5239\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 29s 439ms/step - loss: 12426.8848 - mse: 12426.8848 - mae: 111.1038 - val_loss: 9878.4824 - val_mse: 9878.4824 - val_mae: 99.3906\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 33s 510ms/step - loss: 12321.6113 - mse: 12321.6113 - mae: 110.5333 - val_loss: 13000.3096 - val_mse: 13000.3096 - val_mae: 114.0189\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 12475.6934 - mse: 12475.6934 - mae: 111.1250 - val_loss: 11650.0381 - val_mse: 11650.0381 - val_mae: 107.9353\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 29s 452ms/step - loss: 12352.0254 - mse: 12352.0254 - mae: 110.7662 - val_loss: 12768.6436 - val_mse: 12768.6436 - val_mae: 112.9984\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 12423.2393 - mse: 12423.2393 - mae: 111.0919 - val_loss: 13112.3340 - val_mse: 13112.3340 - val_mae: 114.5091\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 12371.3037 - mse: 12371.3037 - mae: 110.8249 - val_loss: 14134.8857 - val_mse: 14134.8857 - val_mae: 118.8902\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 12465.0420 - mse: 12465.0420 - mae: 111.2259 - val_loss: 12583.1943 - val_mse: 12583.1943 - val_mae: 112.1748\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 12290.0908 - mse: 12290.0908 - mae: 110.4252 - val_loss: 11090.2910 - val_mse: 11090.2910 - val_mae: 105.3104\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 12423.9131 - mse: 12423.9131 - mae: 110.8771 - val_loss: 12933.0254 - val_mse: 12933.0254 - val_mae: 113.7235\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 12427.6631 - mse: 12427.6631 - mae: 111.1098 - val_loss: 12151.1709 - val_mse: 12151.1709 - val_mae: 110.2323\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 29s 439ms/step - loss: 12412.8975 - mse: 12412.8975 - mae: 110.9203 - val_loss: 15028.9883 - val_mse: 15028.9883 - val_mae: 122.5928\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 12428.6055 - mse: 12428.6055 - mae: 110.9550 - val_loss: 12134.7324 - val_mse: 12134.7324 - val_mae: 110.1578\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 12424.6055 - mse: 12424.6055 - mae: 111.0155 - val_loss: 11174.8789 - val_mse: 11174.8789 - val_mae: 105.7113\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 12371.8145 - mse: 12371.8145 - mae: 110.8371 - val_loss: 12170.8906 - val_mse: 12170.8906 - val_mae: 110.3218\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 12342.5000 - mse: 12342.5000 - mae: 110.7222 - val_loss: 12995.9170 - val_mse: 12995.9170 - val_mae: 113.9996\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 12456.6475 - mse: 12456.6475 - mae: 111.0767 - val_loss: 11494.7031 - val_mse: 11494.7031 - val_mae: 107.2133\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 12501.2979 - mse: 12501.2979 - mae: 111.2997 - val_loss: 11625.3818 - val_mse: 11625.3818 - val_mae: 107.8211\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 29s 445ms/step - loss: 12372.7695 - mse: 12372.7695 - mae: 110.7146 - val_loss: 12968.9775 - val_mse: 12968.9775 - val_mae: 113.8814\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 12498.2607 - mse: 12498.2607 - mae: 111.2949 - val_loss: 13409.8926 - val_mse: 13409.8926 - val_mae: 115.8011\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 12339.4463 - mse: 12339.4463 - mae: 110.6833 - val_loss: 13712.6846 - val_mse: 13712.6846 - val_mae: 117.1012\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 39s 598ms/step - loss: 12476.0732 - mse: 12476.0732 - mae: 111.2489 - val_loss: 12506.5771 - val_mse: 12506.5771 - val_mae: 111.8328\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 12306.4932 - mse: 12306.4932 - mae: 110.3850 - val_loss: 12148.0791 - val_mse: 12148.0791 - val_mae: 110.2183\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 12407.0283 - mse: 12407.0283 - mae: 111.0074 - val_loss: 11788.9736 - val_mse: 11788.9736 - val_mae: 108.5770\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 38s 578ms/step - loss: 12387.2607 - mse: 12387.2607 - mae: 110.6623 - val_loss: 16862.1445 - val_mse: 16862.1445 - val_mae: 129.8543\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 40s 611ms/step - loss: 12424.7021 - mse: 12424.7021 - mae: 111.0196 - val_loss: 13183.9355 - val_mse: 13183.9355 - val_mae: 114.8213\n",
      "Epoch 141/180\n",
      "65/65 [==============================] - 43s 657ms/step - loss: 12420.6768 - mse: 12420.6768 - mae: 111.0724 - val_loss: 12730.6621 - val_mse: 12730.6621 - val_mae: 112.8302\n",
      "Epoch 142/180\n",
      "65/65 [==============================] - 40s 617ms/step - loss: 12469.4229 - mse: 12469.4229 - mae: 111.2475 - val_loss: 14628.8584 - val_mse: 14628.8584 - val_mae: 120.9498\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 40s 619ms/step - loss: 12355.8223 - mse: 12355.8223 - mae: 110.7871 - val_loss: 12118.8857 - val_mse: 12118.8857 - val_mae: 110.0858\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 12491.3164 - mse: 12491.3164 - mae: 111.4243 - val_loss: 11532.5498 - val_mse: 11532.5498 - val_mae: 107.3897\n",
      "Epoch 145/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 12348.3779 - mse: 12348.3779 - mae: 110.6359 - val_loss: 13336.5449 - val_mse: 13336.5449 - val_mae: 115.4840\n",
      "Epoch 146/180\n",
      "65/65 [==============================] - 37s 562ms/step - loss: 12572.6387 - mse: 12572.6387 - mae: 111.7173 - val_loss: 11791.0176 - val_mse: 11791.0176 - val_mae: 108.5864\n",
      "Epoch 147/180\n",
      "65/65 [==============================] - 38s 589ms/step - loss: 12278.1553 - mse: 12278.1553 - mae: 110.3209 - val_loss: 11342.5596 - val_mse: 11342.5596 - val_mae: 106.5014\n",
      "Epoch 148/180\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 12504.2793 - mse: 12504.2793 - mae: 111.1569 - val_loss: 12161.2227 - val_mse: 12161.2227 - val_mae: 110.2779\n",
      "Epoch 149/180\n",
      "65/65 [==============================] - 34s 522ms/step - loss: 12345.0488 - mse: 12345.0488 - mae: 110.5752 - val_loss: 13447.7373 - val_mse: 13447.7373 - val_mae: 115.9644\n",
      "Epoch 150/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 12416.6035 - mse: 12416.6035 - mae: 110.9809 - val_loss: 13597.2480 - val_mse: 13597.2480 - val_mae: 116.6072\n",
      "Epoch 151/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 34s 518ms/step - loss: 12492.3652 - mse: 12492.3652 - mae: 111.3667 - val_loss: 12265.9238 - val_mse: 12265.9238 - val_mae: 110.7516\n",
      "Epoch 152/180\n",
      "65/65 [==============================] - 33s 506ms/step - loss: 12487.1855 - mse: 12487.1855 - mae: 111.3162 - val_loss: 10707.1338 - val_mse: 10707.1338 - val_mae: 103.4753\n",
      "Epoch 153/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 12334.5928 - mse: 12334.5928 - mae: 110.4682 - val_loss: 13251.6113 - val_mse: 13251.6113 - val_mae: 115.1156\n",
      "Epoch 154/180\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 12321.4307 - mse: 12321.4307 - mae: 110.4011 - val_loss: 14688.7012 - val_mse: 14688.7012 - val_mae: 121.1969\n",
      "Epoch 155/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 12359.7979 - mse: 12359.7979 - mae: 110.7633 - val_loss: 15978.5840 - val_mse: 15978.5840 - val_mae: 126.4064\n",
      "Epoch 156/180\n",
      "65/65 [==============================] - 37s 573ms/step - loss: 12480.0742 - mse: 12480.0742 - mae: 111.1939 - val_loss: 11702.5869 - val_mse: 11702.5869 - val_mae: 108.1785\n",
      "Epoch 157/180\n",
      "65/65 [==============================] - 38s 586ms/step - loss: 12374.9424 - mse: 12374.9424 - mae: 110.8036 - val_loss: 14572.9414 - val_mse: 14572.9414 - val_mae: 120.7185\n",
      "Epoch 158/180\n",
      "65/65 [==============================] - 38s 585ms/step - loss: 12435.9434 - mse: 12435.9434 - mae: 110.8699 - val_loss: 11269.2607 - val_mse: 11269.2607 - val_mae: 106.1568\n",
      "Epoch 159/180\n",
      "65/65 [==============================] - 34s 524ms/step - loss: 12566.9844 - mse: 12566.9844 - mae: 111.5592 - val_loss: 10614.0586 - val_mse: 10614.0586 - val_mae: 103.0245\n",
      "Epoch 160/180\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 12286.9395 - mse: 12286.9395 - mae: 110.3712 - val_loss: 13169.7275 - val_mse: 13169.7275 - val_mae: 114.7594\n",
      "Epoch 161/180\n",
      "65/65 [==============================] - 38s 585ms/step - loss: 12413.0625 - mse: 12413.0625 - mae: 111.0091 - val_loss: 11055.9795 - val_mse: 11055.9795 - val_mae: 105.1474\n",
      "Epoch 162/180\n",
      "65/65 [==============================] - 39s 598ms/step - loss: 12484.9375 - mse: 12484.9375 - mae: 111.3210 - val_loss: 10482.9238 - val_mse: 10482.9238 - val_mae: 102.3861\n",
      "Epoch 163/180\n",
      "65/65 [==============================] - 39s 593ms/step - loss: 12369.4590 - mse: 12369.4590 - mae: 110.8612 - val_loss: 12848.7959 - val_mse: 12848.7959 - val_mae: 113.3525\n",
      "Epoch 164/180\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 12398.1680 - mse: 12398.1680 - mae: 110.9637 - val_loss: 13508.2070 - val_mse: 13508.2070 - val_mae: 116.2248\n",
      "Epoch 165/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 12350.5068 - mse: 12350.5068 - mae: 110.5970 - val_loss: 10956.4717 - val_mse: 10956.4717 - val_mae: 104.6732\n",
      "Epoch 166/180\n",
      "65/65 [==============================] - 33s 510ms/step - loss: 12536.8740 - mse: 12536.8740 - mae: 111.6009 - val_loss: 12340.9502 - val_mse: 12340.9502 - val_mae: 111.0898\n",
      "Epoch 167/180\n",
      "65/65 [==============================] - 34s 521ms/step - loss: 12405.1602 - mse: 12405.1602 - mae: 111.0123 - val_loss: 12379.4258 - val_mse: 12379.4258 - val_mae: 111.2629\n",
      "Epoch 168/180\n",
      "65/65 [==============================] - 41s 630ms/step - loss: 12343.4541 - mse: 12343.4541 - mae: 110.6372 - val_loss: 11671.2324 - val_mse: 11671.2324 - val_mae: 108.0335\n",
      "Epoch 169/180\n",
      "65/65 [==============================] - 50s 767ms/step - loss: 12421.6885 - mse: 12421.6885 - mae: 110.9970 - val_loss: 12188.8574 - val_mse: 12188.8574 - val_mae: 110.4032\n",
      "Epoch 170/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 12498.4990 - mse: 12498.4990 - mae: 111.2455 - val_loss: 10568.7051 - val_mse: 10568.7051 - val_mae: 102.8042\n",
      "Epoch 171/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 12312.9229 - mse: 12312.9229 - mae: 110.5005 - val_loss: 12113.4893 - val_mse: 12113.4893 - val_mae: 110.0613\n",
      "Epoch 172/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 12422.3398 - mse: 12422.3398 - mae: 110.8840 - val_loss: 12435.0322 - val_mse: 12435.0322 - val_mae: 111.5125\n",
      "Epoch 173/180\n",
      "65/65 [==============================] - 34s 517ms/step - loss: 12385.7549 - mse: 12385.7549 - mae: 110.8569 - val_loss: 11296.1475 - val_mse: 11296.1475 - val_mae: 106.2833\n",
      "Epoch 174/180\n",
      "65/65 [==============================] - 34s 523ms/step - loss: 12480.3271 - mse: 12480.3271 - mae: 111.2330 - val_loss: 12498.4561 - val_mse: 12498.4561 - val_mae: 111.7965\n",
      "Epoch 175/180\n",
      "65/65 [==============================] - 31s 484ms/step - loss: 12330.1445 - mse: 12330.1445 - mae: 110.4500 - val_loss: 11058.0449 - val_mse: 11058.0449 - val_mae: 105.1572\n",
      "Epoch 176/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 12449.9717 - mse: 12449.9717 - mae: 111.1296 - val_loss: 12868.9375 - val_mse: 12868.9375 - val_mae: 113.4413\n",
      "Epoch 177/180\n",
      "65/65 [==============================] - 34s 523ms/step - loss: 12408.6611 - mse: 12408.6611 - mae: 110.9706 - val_loss: 9384.0713 - val_mse: 9384.0713 - val_mae: 96.8714\n",
      "Epoch 178/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 12398.2471 - mse: 12398.2471 - mae: 110.9470 - val_loss: 12407.9053 - val_mse: 12407.9053 - val_mae: 111.3908\n",
      "Epoch 179/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 12424.3066 - mse: 12424.3066 - mae: 111.0506 - val_loss: 12019.7178 - val_mse: 12019.7178 - val_mae: 109.6345\n",
      "Epoch 180/180\n",
      "65/65 [==============================] - 33s 513ms/step - loss: 12538.0342 - mse: 12538.0361 - mae: 111.4133 - val_loss: 12452.5068 - val_mse: 12452.5068 - val_mae: 111.5908\n",
      "Epoch 1/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 25398.9004 - mse: 25398.9004 - mae: 95.3181 - val_loss: 12448.3057 - val_mse: 12448.3057 - val_mae: 111.5717\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 19110.5762 - mse: 19110.5762 - mae: 137.6984 - val_loss: 17669.1543 - val_mse: 17669.1543 - val_mae: 132.9254\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 19353.9492 - mse: 19353.9492 - mae: 138.4622 - val_loss: 15134.7168 - val_mse: 15134.7168 - val_mae: 123.0224\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 19158.3965 - mse: 19158.3965 - mae: 137.8930 - val_loss: 17987.5195 - val_mse: 17987.5195 - val_mae: 134.1175\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 19096.3008 - mse: 19096.3008 - mae: 137.4574 - val_loss: 20681.3047 - val_mse: 20681.3047 - val_mae: 143.8095\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 29s 450ms/step - loss: 19154.5840 - mse: 19154.5859 - mae: 137.9115 - val_loss: 20926.8535 - val_mse: 20926.8535 - val_mae: 144.6610\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 19208.7637 - mse: 19208.7637 - mae: 138.0873 - val_loss: 18947.5664 - val_mse: 18947.5664 - val_mae: 137.6495\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 19314.2793 - mse: 19314.2793 - mae: 138.4619 - val_loss: 19541.9512 - val_mse: 19541.9512 - val_mae: 139.7920\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 19052.8516 - mse: 19052.8516 - mae: 137.4166 - val_loss: 19294.7285 - val_mse: 19294.7285 - val_mae: 138.9053\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 30s 457ms/step - loss: 19225.8125 - mse: 19225.8125 - mae: 138.1825 - val_loss: 16486.3301 - val_mse: 16486.3301 - val_mae: 128.3985\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 31s 485ms/step - loss: 19170.1211 - mse: 19170.1211 - mae: 137.8788 - val_loss: 20211.5000 - val_mse: 20211.5000 - val_mae: 142.1661\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 31s 476ms/step - loss: 19211.8672 - mse: 19211.8672 - mae: 137.9707 - val_loss: 18735.3770 - val_mse: 18735.3770 - val_mae: 136.8772\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 19054.5039 - mse: 19054.5039 - mae: 137.5261 - val_loss: 19130.1816 - val_mse: 19130.1816 - val_mae: 138.3103\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 19242.0195 - mse: 19242.0195 - mae: 138.2455 - val_loss: 16905.4160 - val_mse: 16905.4160 - val_mae: 130.0208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 19033.3672 - mse: 19033.3672 - mae: 137.3178 - val_loss: 19353.2480 - val_mse: 19353.2480 - val_mae: 139.1154\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 30s 458ms/step - loss: 19311.1211 - mse: 19311.1191 - mae: 138.4944 - val_loss: 16262.9971 - val_mse: 16262.9971 - val_mae: 127.5255\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 19060.2832 - mse: 19060.2832 - mae: 137.3334 - val_loss: 16468.8105 - val_mse: 16468.8105 - val_mae: 128.3304\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 30s 456ms/step - loss: 19070.9043 - mse: 19070.9043 - mae: 137.4617 - val_loss: 19585.7559 - val_mse: 19585.7559 - val_mae: 139.9482\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 30s 458ms/step - loss: 19239.2109 - mse: 19239.2109 - mae: 138.1183 - val_loss: 19508.1895 - val_mse: 19508.1895 - val_mae: 139.6717\n",
      "Epoch 20/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 19091.2090 - mse: 19091.2090 - mae: 137.5894 - val_loss: 19126.6699 - val_mse: 19126.6699 - val_mae: 138.2984\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 19222.9023 - mse: 19222.9004 - mae: 138.0596 - val_loss: 17706.9395 - val_mse: 17706.9395 - val_mae: 133.0672\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 19148.4180 - mse: 19148.4180 - mae: 137.8327 - val_loss: 16666.9824 - val_mse: 16666.9824 - val_mae: 129.1003\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 19093.6387 - mse: 19093.6387 - mae: 137.6846 - val_loss: 18954.2930 - val_mse: 18954.2930 - val_mae: 137.6737\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 19258.0273 - mse: 19258.0273 - mae: 138.1651 - val_loss: 17531.1172 - val_mse: 17531.1172 - val_mae: 132.4049\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 30s 464ms/step - loss: 19206.2402 - mse: 19206.2402 - mae: 138.1299 - val_loss: 20894.3926 - val_mse: 20894.3926 - val_mae: 144.5448\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 19018.7930 - mse: 19018.7930 - mae: 136.8236 - val_loss: 21795.3750 - val_mse: 21795.3730 - val_mae: 147.6323\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 30s 462ms/step - loss: 19226.4219 - mse: 19226.4219 - mae: 138.1785 - val_loss: 19689.3750 - val_mse: 19689.3750 - val_mae: 140.3164\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 19180.1113 - mse: 19180.1133 - mae: 138.0125 - val_loss: 20276.2637 - val_mse: 20276.2637 - val_mae: 142.3947\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 19047.5176 - mse: 19047.5156 - mae: 137.4259 - val_loss: 20578.0293 - val_mse: 20578.0293 - val_mae: 143.4504\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 19248.2734 - mse: 19248.2734 - mae: 137.9738 - val_loss: 20062.5371 - val_mse: 20062.5352 - val_mae: 141.6406\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 19129.6152 - mse: 19129.6152 - mae: 137.6186 - val_loss: 21678.0234 - val_mse: 21678.0195 - val_mae: 147.2346\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 34s 523ms/step - loss: 19295.2012 - mse: 19295.2012 - mae: 138.4720 - val_loss: 18450.6152 - val_mse: 18450.6152 - val_mae: 135.8316\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 19225.2090 - mse: 19225.2090 - mae: 138.1138 - val_loss: 14009.4580 - val_mse: 14009.4580 - val_mae: 118.3612\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 19124.8789 - mse: 19124.8789 - mae: 137.4249 - val_loss: 17217.1895 - val_mse: 17217.1895 - val_mae: 131.2130\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 19148.5098 - mse: 19148.5098 - mae: 137.6864 - val_loss: 19450.3125 - val_mse: 19450.3125 - val_mae: 139.4640\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 31s 474ms/step - loss: 19128.1914 - mse: 19128.1914 - mae: 137.7334 - val_loss: 20185.9746 - val_mse: 20185.9746 - val_mae: 142.0759\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 19236.6445 - mse: 19236.6445 - mae: 137.9767 - val_loss: 16017.7812 - val_mse: 16017.7812 - val_mae: 126.5610\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 19089.1445 - mse: 19089.1445 - mae: 137.4317 - val_loss: 15933.9277 - val_mse: 15933.9277 - val_mae: 126.2267\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 19048.2715 - mse: 19048.2715 - mae: 137.4778 - val_loss: 20630.3418 - val_mse: 20630.3418 - val_mae: 143.6319\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 19241.3223 - mse: 19241.3184 - mae: 138.1352 - val_loss: 16882.7305 - val_mse: 16882.7285 - val_mae: 129.9296\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 18816.5586 - mse: 18816.5586 - mae: 136.3495 - val_loss: 21905.3730 - val_mse: 21905.3730 - val_mae: 148.0039\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 19361.7969 - mse: 19361.7988 - mae: 138.2171 - val_loss: 17319.3262 - val_mse: 17319.3262 - val_mae: 131.5992\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 19102.6992 - mse: 19102.6992 - mae: 137.6773 - val_loss: 17801.1289 - val_mse: 17801.1289 - val_mae: 133.4200\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19204.4062 - mse: 19204.4062 - mae: 137.8952 - val_loss: 23178.1953 - val_mse: 23178.1953 - val_mae: 152.2426\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 19162.2812 - mse: 19162.2812 - mae: 137.6194 - val_loss: 20051.0605 - val_mse: 20051.0605 - val_mae: 141.6018\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 19213.7402 - mse: 19213.7402 - mae: 137.9602 - val_loss: 18498.7500 - val_mse: 18498.7500 - val_mae: 136.0099\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19213.0371 - mse: 19213.0391 - mae: 137.9201 - val_loss: 20031.6543 - val_mse: 20031.6543 - val_mae: 141.5330\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 19101.4727 - mse: 19101.4727 - mae: 137.5218 - val_loss: 16879.1270 - val_mse: 16879.1270 - val_mae: 129.9190\n",
      "Epoch 49/180\n",
      "65/65 [==============================] - 34s 523ms/step - loss: 19241.1016 - mse: 19241.1016 - mae: 138.1965 - val_loss: 19657.0820 - val_mse: 19657.0820 - val_mae: 140.2033\n",
      "Epoch 50/180\n",
      "65/65 [==============================] - 38s 589ms/step - loss: 19111.1094 - mse: 19111.1094 - mae: 137.7889 - val_loss: 18872.4785 - val_mse: 18872.4805 - val_mae: 137.3770\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 41s 628ms/step - loss: 19140.6309 - mse: 19140.6309 - mae: 137.8298 - val_loss: 18362.4453 - val_mse: 18362.4453 - val_mae: 135.5073\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 36s 547ms/step - loss: 19158.1797 - mse: 19158.1797 - mae: 137.7660 - val_loss: 20903.6797 - val_mse: 20903.6797 - val_mae: 144.5810\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 26s 398ms/step - loss: 19182.6055 - mse: 19182.6055 - mae: 137.9047 - val_loss: 17733.2988 - val_mse: 17733.2988 - val_mae: 133.1661\n",
      "Epoch 54/180\n",
      "65/65 [==============================] - 29s 442ms/step - loss: 19116.2520 - mse: 19116.2520 - mae: 137.7296 - val_loss: 24024.2383 - val_mse: 24024.2383 - val_mae: 154.9973\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 30s 455ms/step - loss: 19245.7344 - mse: 19245.7344 - mae: 138.0514 - val_loss: 20881.9160 - val_mse: 20881.9160 - val_mae: 144.5055\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 19147.2480 - mse: 19147.2480 - mae: 137.8929 - val_loss: 20217.4570 - val_mse: 20217.4570 - val_mae: 142.1877\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 19179.6777 - mse: 19179.6777 - mae: 137.8618 - val_loss: 17778.0176 - val_mse: 17778.0137 - val_mae: 133.3339\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 19249.8203 - mse: 19249.8203 - mae: 138.2346 - val_loss: 19054.3242 - val_mse: 19054.3242 - val_mae: 138.0357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19266.8711 - mse: 19266.8711 - mae: 138.2413 - val_loss: 17852.9395 - val_mse: 17852.9375 - val_mae: 133.6149\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 18911.5117 - mse: 18911.5117 - mae: 137.0525 - val_loss: 21834.7871 - val_mse: 21834.7871 - val_mae: 147.7646\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 19218.2109 - mse: 19218.2109 - mae: 137.8837 - val_loss: 21159.6777 - val_mse: 21159.6777 - val_mae: 145.4623\n",
      "Epoch 62/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 19213.9785 - mse: 19213.9785 - mae: 138.0744 - val_loss: 19403.2207 - val_mse: 19403.2207 - val_mae: 139.2954\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 31s 476ms/step - loss: 19197.0742 - mse: 19197.0742 - mae: 138.0130 - val_loss: 22140.5723 - val_mse: 22140.5723 - val_mae: 148.7962\n",
      "Epoch 64/180\n",
      "65/65 [==============================] - 31s 469ms/step - loss: 19081.1289 - mse: 19081.1289 - mae: 137.4814 - val_loss: 22740.5117 - val_mse: 22740.5117 - val_mae: 150.7994\n",
      "Epoch 65/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 19283.6445 - mse: 19283.6445 - mae: 138.2807 - val_loss: 20187.5762 - val_mse: 20187.5762 - val_mae: 142.0805\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 19268.2441 - mse: 19268.2441 - mae: 138.3265 - val_loss: 21108.2988 - val_mse: 21108.2988 - val_mae: 145.2869\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 19014.2715 - mse: 19014.2715 - mae: 137.2662 - val_loss: 20775.0977 - val_mse: 20775.0977 - val_mae: 144.1346\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 19238.8125 - mse: 19238.8125 - mae: 138.0732 - val_loss: 19557.0742 - val_mse: 19557.0742 - val_mae: 139.8465\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 19131.6367 - mse: 19131.6367 - mae: 137.8218 - val_loss: 23702.0898 - val_mse: 23702.0898 - val_mae: 153.9545\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 19156.9785 - mse: 19156.9785 - mae: 137.7325 - val_loss: 19668.9941 - val_mse: 19668.9941 - val_mae: 140.2462\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 19129.8711 - mse: 19129.8711 - mae: 137.5773 - val_loss: 22426.5273 - val_mse: 22426.5273 - val_mae: 149.7525\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 19257.0117 - mse: 19257.0117 - mae: 137.8734 - val_loss: 18699.2832 - val_mse: 18699.2832 - val_mae: 136.7452\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 19175.9727 - mse: 19175.9727 - mae: 137.9576 - val_loss: 21436.2754 - val_mse: 21436.2754 - val_mae: 146.4071\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 19143.2051 - mse: 19143.2051 - mae: 137.9325 - val_loss: 19797.2012 - val_mse: 19797.2012 - val_mae: 140.7025\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 19145.7930 - mse: 19145.7930 - mae: 137.7806 - val_loss: 14322.6670 - val_mse: 14322.6670 - val_mae: 119.6727\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 32s 486ms/step - loss: 19158.8418 - mse: 19158.8418 - mae: 137.4282 - val_loss: 19517.4102 - val_mse: 19517.4102 - val_mae: 139.7047\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 19138.9238 - mse: 19138.9238 - mae: 137.5883 - val_loss: 20675.2598 - val_mse: 20675.2598 - val_mae: 143.7883\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 32s 486ms/step - loss: 19154.7031 - mse: 19154.7031 - mae: 137.7898 - val_loss: 18364.0371 - val_mse: 18364.0371 - val_mae: 135.5135\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 19057.4121 - mse: 19057.4121 - mae: 137.4078 - val_loss: 19607.0801 - val_mse: 19607.0801 - val_mae: 140.0244\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 19190.3633 - mse: 19190.3633 - mae: 137.8878 - val_loss: 15806.2627 - val_mse: 15806.2627 - val_mae: 125.7217\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 19169.0703 - mse: 19169.0703 - mae: 137.6752 - val_loss: 17396.1406 - val_mse: 17396.1406 - val_mae: 131.8908\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19336.4805 - mse: 19336.4805 - mae: 138.5415 - val_loss: 20202.7266 - val_mse: 20202.7266 - val_mae: 142.1363\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 19100.8867 - mse: 19100.8867 - mae: 137.5497 - val_loss: 15300.7520 - val_mse: 15300.7520 - val_mae: 123.6955\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19082.7812 - mse: 19082.7812 - mae: 137.4835 - val_loss: 15675.6758 - val_mse: 15675.6758 - val_mae: 125.2013\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 33s 500ms/step - loss: 19179.8945 - mse: 19179.8945 - mae: 137.9285 - val_loss: 20460.3398 - val_mse: 20460.3398 - val_mae: 143.0391\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 19134.9277 - mse: 19134.9277 - mae: 137.8327 - val_loss: 16946.9805 - val_mse: 16946.9805 - val_mae: 130.1805\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 19202.8555 - mse: 19202.8535 - mae: 137.8251 - val_loss: 23513.4277 - val_mse: 23513.4277 - val_mae: 153.3406\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 19117.5137 - mse: 19117.5137 - mae: 137.7055 - val_loss: 16427.8359 - val_mse: 16427.8359 - val_mae: 128.1711\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 19328.2129 - mse: 19328.2129 - mae: 138.4040 - val_loss: 17509.8066 - val_mse: 17509.8066 - val_mae: 132.3238\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 19126.9277 - mse: 19126.9277 - mae: 137.8254 - val_loss: 17936.8418 - val_mse: 17936.8418 - val_mae: 133.9284\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19276.1465 - mse: 19276.1465 - mae: 138.3234 - val_loss: 20782.8652 - val_mse: 20782.8652 - val_mae: 144.1622\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 19172.5332 - mse: 19172.5332 - mae: 137.7778 - val_loss: 17978.7988 - val_mse: 17978.7988 - val_mae: 134.0839\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19190.2969 - mse: 19190.2969 - mae: 137.6441 - val_loss: 20882.9277 - val_mse: 20882.9277 - val_mae: 144.5091\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 19077.2832 - mse: 19077.2832 - mae: 137.4206 - val_loss: 18056.2422 - val_mse: 18056.2422 - val_mae: 134.3708\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 19233.6699 - mse: 19233.6699 - mae: 138.1490 - val_loss: 15414.8379 - val_mse: 15414.8379 - val_mae: 124.1556\n",
      "Epoch 96/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 18950.2148 - mse: 18950.2148 - mae: 137.0679 - val_loss: 19353.9609 - val_mse: 19353.9609 - val_mae: 139.1167\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19316.9492 - mse: 19316.9492 - mae: 138.3594 - val_loss: 19909.6230 - val_mse: 19909.6230 - val_mae: 141.1014\n",
      "Epoch 98/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19237.3320 - mse: 19237.3320 - mae: 138.1234 - val_loss: 20839.8750 - val_mse: 20839.8750 - val_mae: 144.3591\n",
      "Epoch 99/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 19141.0918 - mse: 19141.0918 - mae: 137.6656 - val_loss: 21930.1816 - val_mse: 21930.1816 - val_mae: 148.0875\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 19121.0996 - mse: 19121.0996 - mae: 137.6919 - val_loss: 19550.9121 - val_mse: 19550.9121 - val_mae: 139.8242\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19354.4453 - mse: 19354.4453 - mae: 138.5760 - val_loss: 18681.1660 - val_mse: 18681.1660 - val_mae: 136.6790\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 18981.1543 - mse: 18981.1543 - mae: 137.1079 - val_loss: 15799.5635 - val_mse: 15799.5635 - val_mae: 125.6962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 19116.0547 - mse: 19116.0547 - mae: 137.6129 - val_loss: 22220.0488 - val_mse: 22220.0488 - val_mae: 149.0630\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 19227.1367 - mse: 19227.1367 - mae: 138.1370 - val_loss: 20213.5078 - val_mse: 20213.5039 - val_mae: 142.1726\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 19152.1582 - mse: 19152.1582 - mae: 137.8043 - val_loss: 16923.8848 - val_mse: 16923.8848 - val_mae: 130.0917\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 19251.0859 - mse: 19251.0859 - mae: 138.2540 - val_loss: 22243.7949 - val_mse: 22243.7949 - val_mae: 149.1421\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 19145.0645 - mse: 19145.0645 - mae: 137.8599 - val_loss: 19987.1016 - val_mse: 19987.1016 - val_mae: 141.3752\n",
      "Epoch 108/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 19208.1660 - mse: 19208.1660 - mae: 138.0436 - val_loss: 19168.1895 - val_mse: 19168.1895 - val_mae: 138.4492\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 19055.2734 - mse: 19055.2734 - mae: 137.4285 - val_loss: 17847.4434 - val_mse: 17847.4453 - val_mae: 133.5928\n",
      "Epoch 110/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 19326.1367 - mse: 19326.1387 - mae: 138.4171 - val_loss: 18504.5020 - val_mse: 18504.5039 - val_mae: 136.0312\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 19087.4531 - mse: 19087.4531 - mae: 137.5251 - val_loss: 20306.2461 - val_mse: 20306.2461 - val_mae: 142.4997\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 19238.3301 - mse: 19238.3301 - mae: 138.1765 - val_loss: 19791.1738 - val_mse: 19791.1738 - val_mae: 140.6809\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 19120.2871 - mse: 19120.2871 - mae: 137.7993 - val_loss: 19202.5215 - val_mse: 19202.5215 - val_mae: 138.5724\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 19227.2676 - mse: 19227.2676 - mae: 138.0482 - val_loss: 18918.6992 - val_mse: 18918.6992 - val_mae: 137.5448\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 19074.0156 - mse: 19074.0156 - mae: 137.5489 - val_loss: 19948.8848 - val_mse: 19948.8848 - val_mae: 141.2405\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19352.1953 - mse: 19352.1953 - mae: 138.5333 - val_loss: 20527.5898 - val_mse: 20527.5898 - val_mae: 143.2744\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 19012.5684 - mse: 19012.5684 - mae: 137.3421 - val_loss: 19611.7090 - val_mse: 19611.7090 - val_mae: 140.0418\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 19320.8652 - mse: 19320.8652 - mae: 138.4987 - val_loss: 20892.0195 - val_mse: 20892.0195 - val_mae: 144.5401\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 19137.9316 - mse: 19137.9316 - mae: 137.8218 - val_loss: 20555.4609 - val_mse: 20555.4609 - val_mae: 143.3716\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 19057.3633 - mse: 19057.3613 - mae: 137.5786 - val_loss: 19923.2676 - val_mse: 19923.2695 - val_mae: 141.1481\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 19205.9004 - mse: 19205.9004 - mae: 137.8771 - val_loss: 14766.0137 - val_mse: 14766.0137 - val_mae: 121.5153\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 19128.5469 - mse: 19128.5469 - mae: 137.6966 - val_loss: 18969.7305 - val_mse: 18969.7305 - val_mae: 137.7292\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 19150.1445 - mse: 19150.1445 - mae: 137.7682 - val_loss: 22401.2988 - val_mse: 22401.2988 - val_mae: 149.6706\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 19152.0508 - mse: 19152.0508 - mae: 137.6561 - val_loss: 18222.5449 - val_mse: 18222.5449 - val_mae: 134.9882\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 19315.5098 - mse: 19315.5098 - mae: 138.4713 - val_loss: 20441.7422 - val_mse: 20441.7422 - val_mae: 142.9746\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 19131.1211 - mse: 19131.1211 - mae: 137.8990 - val_loss: 20464.4512 - val_mse: 20464.4512 - val_mae: 143.0533\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 19306.0430 - mse: 19306.0430 - mae: 138.3498 - val_loss: 15274.2520 - val_mse: 15274.2520 - val_mae: 123.5890\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 19067.5059 - mse: 19067.5059 - mae: 137.5686 - val_loss: 20518.9277 - val_mse: 20518.9277 - val_mae: 143.2439\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19078.0938 - mse: 19078.0938 - mae: 137.5862 - val_loss: 18474.7637 - val_mse: 18474.7637 - val_mae: 135.9218\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19319.4531 - mse: 19319.4531 - mae: 138.3447 - val_loss: 20275.3887 - val_mse: 20275.3887 - val_mae: 142.3876\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 19099.5234 - mse: 19099.5234 - mae: 137.6355 - val_loss: 18538.8613 - val_mse: 18538.8613 - val_mae: 136.1566\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 18908.5137 - mse: 18908.5137 - mae: 136.6790 - val_loss: 19503.0605 - val_mse: 19503.0605 - val_mae: 139.6491\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 19448.9883 - mse: 19448.9883 - mae: 138.8193 - val_loss: 17149.1289 - val_mse: 17149.1289 - val_mae: 130.9546\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 19067.4160 - mse: 19067.4160 - mae: 137.2125 - val_loss: 16548.0352 - val_mse: 16548.0352 - val_mae: 128.6381\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 19201.3086 - mse: 19201.3086 - mae: 138.0560 - val_loss: 21965.1699 - val_mse: 21965.1699 - val_mae: 148.2064\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 19155.3281 - mse: 19155.3281 - mae: 137.8459 - val_loss: 20125.8301 - val_mse: 20125.8301 - val_mae: 141.8654\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 19154.0918 - mse: 19154.0938 - mae: 137.7702 - val_loss: 19487.6621 - val_mse: 19487.6621 - val_mae: 139.5979\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19190.7402 - mse: 19190.7402 - mae: 138.0026 - val_loss: 17237.4805 - val_mse: 17237.4805 - val_mae: 131.2900\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 19168.9922 - mse: 19168.9922 - mae: 137.8115 - val_loss: 19031.6016 - val_mse: 19031.6016 - val_mae: 137.9550\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 19174.3164 - mse: 19174.3164 - mae: 137.9932 - val_loss: 19055.2949 - val_mse: 19055.2949 - val_mae: 138.0393\n",
      "Epoch 141/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 19102.3281 - mse: 19102.3281 - mae: 137.6411 - val_loss: 17001.1699 - val_mse: 17001.1699 - val_mae: 130.3874\n",
      "Epoch 142/180\n",
      "65/65 [==============================] - 34s 524ms/step - loss: 19209.2988 - mse: 19209.3008 - mae: 138.0459 - val_loss: 16968.3242 - val_mse: 16968.3242 - val_mae: 130.2620\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 46s 711ms/step - loss: 19185.0137 - mse: 19185.0137 - mae: 137.8792 - val_loss: 17823.7539 - val_mse: 17823.7520 - val_mae: 133.5054\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 42s 640ms/step - loss: 19313.9199 - mse: 19313.9199 - mae: 138.4489 - val_loss: 19929.0391 - val_mse: 19929.0371 - val_mae: 141.1702\n",
      "Epoch 145/180\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 19120.5293 - mse: 19120.5273 - mae: 137.5807 - val_loss: 15946.1699 - val_mse: 15946.1699 - val_mae: 126.2775\n",
      "Epoch 146/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 40s 612ms/step - loss: 19118.3496 - mse: 19118.3496 - mae: 137.5537 - val_loss: 15502.2695 - val_mse: 15502.2695 - val_mae: 124.5077\n",
      "Epoch 147/180\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 19190.7363 - mse: 19190.7363 - mae: 137.4263 - val_loss: 17370.9551 - val_mse: 17370.9551 - val_mae: 131.7989\n",
      "Epoch 148/180\n",
      "65/65 [==============================] - 34s 517ms/step - loss: 19107.3125 - mse: 19107.3125 - mae: 137.6145 - val_loss: 20756.5586 - val_mse: 20756.5586 - val_mae: 144.0702\n",
      "Epoch 149/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 19315.6250 - mse: 19315.6250 - mae: 138.3320 - val_loss: 17457.5898 - val_mse: 17457.5898 - val_mae: 132.1271\n",
      "Epoch 150/180\n",
      "65/65 [==============================] - 36s 551ms/step - loss: 19050.0625 - mse: 19050.0625 - mae: 137.4062 - val_loss: 16678.2109 - val_mse: 16678.2129 - val_mae: 129.1414\n",
      "Epoch 151/180\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 19161.4902 - mse: 19161.4902 - mae: 137.9010 - val_loss: 17141.7500 - val_mse: 17141.7500 - val_mae: 130.9264\n",
      "Epoch 152/180\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 19182.9219 - mse: 19182.9219 - mae: 137.9239 - val_loss: 20864.3965 - val_mse: 20864.3945 - val_mae: 144.4450\n",
      "Epoch 153/180\n",
      "65/65 [==============================] - 37s 574ms/step - loss: 19059.3047 - mse: 19059.3047 - mae: 137.5423 - val_loss: 18199.7617 - val_mse: 18199.7617 - val_mae: 134.9062\n",
      "Epoch 154/180\n",
      "65/65 [==============================] - 34s 523ms/step - loss: 19191.6582 - mse: 19191.6582 - mae: 138.0070 - val_loss: 18602.7676 - val_mse: 18602.7637 - val_mae: 136.3914\n",
      "Epoch 155/180\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 19194.7578 - mse: 19194.7578 - mae: 137.6825 - val_loss: 24020.5215 - val_mse: 24020.5215 - val_mae: 154.9846\n",
      "Epoch 156/180\n",
      "65/65 [==============================] - 36s 556ms/step - loss: 19241.7480 - mse: 19241.7480 - mae: 138.1027 - val_loss: 18209.7832 - val_mse: 18209.7832 - val_mae: 134.9436\n",
      "Epoch 157/180\n",
      "65/65 [==============================] - 37s 563ms/step - loss: 19154.1445 - mse: 19154.1445 - mae: 137.8072 - val_loss: 22794.6562 - val_mse: 22794.6562 - val_mae: 150.9779\n",
      "Epoch 158/180\n",
      "65/65 [==============================] - 36s 549ms/step - loss: 19112.2207 - mse: 19112.2207 - mae: 137.5984 - val_loss: 23249.3574 - val_mse: 23249.3574 - val_mae: 152.4773\n",
      "Epoch 159/180\n",
      "65/65 [==============================] - 43s 663ms/step - loss: 19141.5469 - mse: 19141.5469 - mae: 137.6739 - val_loss: 18131.7461 - val_mse: 18131.7461 - val_mae: 134.6528\n",
      "Epoch 160/180\n",
      "65/65 [==============================] - 36s 556ms/step - loss: 19375.6719 - mse: 19375.6719 - mae: 138.5930 - val_loss: 18435.5430 - val_mse: 18435.5430 - val_mae: 135.7775\n",
      "Epoch 161/180\n",
      "65/65 [==============================] - 36s 555ms/step - loss: 19011.8652 - mse: 19011.8652 - mae: 137.3951 - val_loss: 17853.5098 - val_mse: 17853.5098 - val_mae: 133.6158\n",
      "Epoch 162/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 19256.1172 - mse: 19256.1172 - mae: 138.2075 - val_loss: 20543.5176 - val_mse: 20543.5176 - val_mae: 143.3294\n",
      "Epoch 163/180\n",
      "65/65 [==============================] - 36s 550ms/step - loss: 19203.0586 - mse: 19203.0586 - mae: 137.8805 - val_loss: 19023.8535 - val_mse: 19023.8535 - val_mae: 137.9258\n",
      "Epoch 164/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 19169.8223 - mse: 19169.8223 - mae: 137.9003 - val_loss: 16515.1250 - val_mse: 16515.1250 - val_mae: 128.5112\n",
      "Epoch 165/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 19134.1387 - mse: 19134.1387 - mae: 137.7643 - val_loss: 19266.5352 - val_mse: 19266.5352 - val_mae: 138.8018\n",
      "Epoch 166/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 19046.5371 - mse: 19046.5371 - mae: 137.4195 - val_loss: 18291.1797 - val_mse: 18291.1797 - val_mae: 135.2448\n",
      "Epoch 167/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 19227.1211 - mse: 19227.1211 - mae: 137.8723 - val_loss: 20019.1152 - val_mse: 20019.1152 - val_mae: 141.4877\n",
      "Epoch 168/180\n",
      "65/65 [==============================] - 31s 474ms/step - loss: 19133.2656 - mse: 19133.2656 - mae: 137.6398 - val_loss: 21294.3711 - val_mse: 21294.3730 - val_mae: 145.9251\n",
      "Epoch 169/180\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 19083.0078 - mse: 19083.0078 - mae: 137.4881 - val_loss: 18687.1953 - val_mse: 18687.1953 - val_mae: 136.7009\n",
      "Epoch 170/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 19242.8066 - mse: 19242.8066 - mae: 138.0595 - val_loss: 16565.7227 - val_mse: 16565.7227 - val_mae: 128.7071\n",
      "Epoch 171/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 19117.5684 - mse: 19117.5684 - mae: 137.6593 - val_loss: 25118.1738 - val_mse: 25118.1738 - val_mae: 158.4871\n",
      "Epoch 172/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 19183.1152 - mse: 19183.1152 - mae: 137.7643 - val_loss: 21008.3574 - val_mse: 21008.3574 - val_mae: 144.9422\n",
      "Epoch 173/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 19161.3496 - mse: 19161.3496 - mae: 137.8418 - val_loss: 19367.9941 - val_mse: 19367.9941 - val_mae: 139.1689\n",
      "Epoch 174/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 19213.9531 - mse: 19213.9531 - mae: 138.1583 - val_loss: 18459.0703 - val_mse: 18459.0684 - val_mae: 135.8637\n",
      "Epoch 175/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 19305.3711 - mse: 19305.3730 - mae: 138.4053 - val_loss: 17642.4258 - val_mse: 17642.4258 - val_mae: 132.8244\n",
      "Epoch 176/180\n",
      "65/65 [==============================] - 37s 562ms/step - loss: 19078.2051 - mse: 19078.2051 - mae: 137.4662 - val_loss: 17702.4473 - val_mse: 17702.4473 - val_mae: 133.0501\n",
      "Epoch 177/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 19131.9160 - mse: 19131.9160 - mae: 137.8466 - val_loss: 19354.7324 - val_mse: 19354.7324 - val_mae: 139.1195\n",
      "Epoch 178/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 19187.0918 - mse: 19187.0918 - mae: 137.7926 - val_loss: 14960.0957 - val_mse: 14960.0957 - val_mae: 122.3114\n",
      "Epoch 179/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 19200.8887 - mse: 19200.8887 - mae: 137.8758 - val_loss: 17120.2148 - val_mse: 17120.2148 - val_mae: 130.8427\n",
      "Epoch 180/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 19193.8945 - mse: 19193.8945 - mae: 137.8913 - val_loss: 20353.3184 - val_mse: 20353.3184 - val_mae: 142.6650\n",
      "Epoch 1/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 39019.6367 - mse: 39019.6367 - mae: 110.1785 - val_loss: 18107.9629 - val_mse: 18107.9629 - val_mae: 134.5658\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 28974.7305 - mse: 28974.7305 - mae: 168.4863 - val_loss: 23462.3828 - val_mse: 23462.3828 - val_mae: 153.1743\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 36s 561ms/step - loss: 29843.8789 - mse: 29843.8789 - mae: 171.8996 - val_loss: 28000.2871 - val_mse: 28000.2871 - val_mae: 167.3329\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 38s 580ms/step - loss: 29708.4863 - mse: 29708.4863 - mae: 171.8020 - val_loss: 28959.4043 - val_mse: 28959.4043 - val_mae: 170.1746\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 39s 600ms/step - loss: 29482.7402 - mse: 29482.7402 - mae: 171.0802 - val_loss: 28973.3184 - val_mse: 28973.3184 - val_mae: 170.2155\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 29890.1953 - mse: 29890.1953 - mae: 172.2075 - val_loss: 29156.5898 - val_mse: 29156.5898 - val_mae: 170.7530\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 39s 593ms/step - loss: 29672.4082 - mse: 29672.4082 - mae: 171.6696 - val_loss: 35081.3320 - val_mse: 35081.3320 - val_mae: 187.3001\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 38s 582ms/step - loss: 29498.9688 - mse: 29498.9688 - mae: 170.8791 - val_loss: 34061.9492 - val_mse: 34061.9492 - val_mae: 184.5588\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 29674.5156 - mse: 29674.5156 - mae: 171.2475 - val_loss: 36580.3750 - val_mse: 36580.3750 - val_mae: 191.2600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/180\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 29855.1562 - mse: 29855.1562 - mae: 171.8941 - val_loss: 30769.8086 - val_mse: 30769.8086 - val_mae: 175.4132\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 37s 567ms/step - loss: 29532.8770 - mse: 29532.8770 - mae: 171.2268 - val_loss: 29469.1523 - val_mse: 29469.1523 - val_mae: 171.6658\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 37s 572ms/step - loss: 30024.6895 - mse: 30024.6895 - mae: 172.4089 - val_loss: 30754.9395 - val_mse: 30754.9395 - val_mae: 175.3709\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 39s 604ms/step - loss: 29333.6543 - mse: 29333.6543 - mae: 170.6131 - val_loss: 37584.6211 - val_mse: 37584.6211 - val_mae: 193.8675\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 36s 553ms/step - loss: 29717.8340 - mse: 29717.8340 - mae: 171.7136 - val_loss: 30064.6855 - val_mse: 30064.6855 - val_mae: 173.3917\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 29830.9512 - mse: 29830.9512 - mae: 172.0413 - val_loss: 29497.7441 - val_mse: 29497.7441 - val_mae: 171.7491\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 33s 510ms/step - loss: 29791.8926 - mse: 29791.8926 - mae: 171.9906 - val_loss: 29711.7930 - val_mse: 29711.7930 - val_mae: 172.3711\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 33s 500ms/step - loss: 29450.6641 - mse: 29450.6641 - mae: 170.9928 - val_loss: 29041.8809 - val_mse: 29041.8809 - val_mae: 170.4168\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 29829.6562 - mse: 29829.6562 - mae: 171.5469 - val_loss: 24084.3848 - val_mse: 24084.3848 - val_mae: 155.1915\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 29382.7031 - mse: 29382.7031 - mae: 170.8201 - val_loss: 29837.1562 - val_mse: 29837.1562 - val_mae: 172.7343\n",
      "Epoch 20/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 29798.5664 - mse: 29798.5664 - mae: 171.8904 - val_loss: 34720.1016 - val_mse: 34720.1016 - val_mae: 186.3333\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 29752.6230 - mse: 29752.6230 - mae: 171.9231 - val_loss: 30602.9883 - val_mse: 30602.9883 - val_mae: 174.9371\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 31s 473ms/step - loss: 29683.0254 - mse: 29683.0254 - mae: 171.7260 - val_loss: 29322.7266 - val_mse: 29322.7266 - val_mae: 171.2388\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 29735.9121 - mse: 29735.9102 - mae: 171.7407 - val_loss: 23454.8945 - val_mse: 23454.8945 - val_mae: 153.1499\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 29311.3750 - mse: 29311.3750 - mae: 170.5042 - val_loss: 26130.7617 - val_mse: 26130.7617 - val_mae: 161.6501\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 29937.0234 - mse: 29937.0234 - mae: 172.2414 - val_loss: 31869.5117 - val_mse: 31869.5117 - val_mae: 178.5204\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 29430.6738 - mse: 29430.6738 - mae: 170.8785 - val_loss: 29692.9590 - val_mse: 29692.9590 - val_mae: 172.3165\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 29931.2363 - mse: 29931.2363 - mae: 172.3529 - val_loss: 29804.4258 - val_mse: 29804.4258 - val_mae: 172.6396\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 29726.6562 - mse: 29726.6562 - mae: 171.6609 - val_loss: 25697.0215 - val_mse: 25697.0215 - val_mae: 160.3029\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 29623.8125 - mse: 29623.8125 - mae: 171.4779 - val_loss: 28110.4453 - val_mse: 28110.4453 - val_mae: 167.6617\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 29453.4785 - mse: 29453.4785 - mae: 170.8691 - val_loss: 29579.9336 - val_mse: 29579.9336 - val_mae: 171.9882\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 29962.6504 - mse: 29962.6504 - mae: 172.1947 - val_loss: 28668.0352 - val_mse: 28668.0352 - val_mae: 169.3164\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 29509.1230 - mse: 29509.1230 - mae: 170.9663 - val_loss: 34023.1211 - val_mse: 34023.1211 - val_mae: 184.4536\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 29625.7676 - mse: 29625.7676 - mae: 170.7817 - val_loss: 24297.3750 - val_mse: 24297.3750 - val_mae: 155.8762\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 36s 549ms/step - loss: 29881.1992 - mse: 29881.1992 - mae: 171.4842 - val_loss: 26073.8125 - val_mse: 26073.8125 - val_mae: 161.4739\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 29752.7207 - mse: 29752.7207 - mae: 171.6989 - val_loss: 34252.9219 - val_mse: 34252.9219 - val_mae: 185.0754\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 29653.2949 - mse: 29653.2949 - mae: 171.3408 - val_loss: 33170.2539 - val_mse: 33170.2539 - val_mae: 182.1270\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 29701.4824 - mse: 29701.4824 - mae: 171.5377 - val_loss: 27325.7285 - val_mse: 27325.7285 - val_mae: 165.3050\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 29578.8535 - mse: 29578.8535 - mae: 170.8440 - val_loss: 30740.5996 - val_mse: 30740.5996 - val_mae: 175.3300\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 29467.9902 - mse: 29467.9902 - mae: 170.8795 - val_loss: 31993.7227 - val_mse: 31993.7227 - val_mae: 178.8679\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 38s 582ms/step - loss: 29913.3203 - mse: 29913.3203 - mae: 172.2478 - val_loss: 32796.0859 - val_mse: 32796.0859 - val_mae: 181.0969\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 36s 550ms/step - loss: 29429.5059 - mse: 29429.5059 - mae: 170.8003 - val_loss: 28951.7617 - val_mse: 28951.7617 - val_mae: 170.1522\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 37s 575ms/step - loss: 29637.1484 - mse: 29637.1484 - mae: 171.5427 - val_loss: 30109.4980 - val_mse: 30109.4980 - val_mae: 173.5209\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 39s 594ms/step - loss: 29572.0430 - mse: 29572.0430 - mae: 171.0505 - val_loss: 28185.7871 - val_mse: 28185.7871 - val_mae: 167.8862\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 37s 571ms/step - loss: 30025.9727 - mse: 30025.9727 - mae: 172.6358 - val_loss: 26883.2598 - val_mse: 26883.2598 - val_mae: 163.9612\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 37s 570ms/step - loss: 29395.9824 - mse: 29395.9824 - mae: 170.7863 - val_loss: 25669.0898 - val_mse: 25669.0898 - val_mae: 160.2158\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 36s 550ms/step - loss: 29779.5156 - mse: 29779.5156 - mae: 171.8753 - val_loss: 26008.8574 - val_mse: 26008.8574 - val_mae: 161.2726\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 40s 615ms/step - loss: 29673.8711 - mse: 29673.8711 - mae: 171.6097 - val_loss: 32077.9062 - val_mse: 32077.9062 - val_mae: 179.1031\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 40s 612ms/step - loss: 29467.1855 - mse: 29467.1855 - mae: 170.9288 - val_loss: 35013.1523 - val_mse: 35013.1523 - val_mae: 187.1180\n",
      "Epoch 49/180\n",
      "65/65 [==============================] - 39s 594ms/step - loss: 29896.3535 - mse: 29896.3535 - mae: 172.0975 - val_loss: 27976.5039 - val_mse: 27976.5039 - val_mae: 167.2618\n",
      "Epoch 50/180\n",
      "65/65 [==============================] - 40s 619ms/step - loss: 29488.2305 - mse: 29488.2305 - mae: 171.0117 - val_loss: 34146.1172 - val_mse: 34146.1172 - val_mae: 184.7867\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 29738.6660 - mse: 29738.6660 - mae: 171.8710 - val_loss: 29426.1562 - val_mse: 29426.1562 - val_mae: 171.5405\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 36s 547ms/step - loss: 30036.0684 - mse: 30036.0684 - mae: 172.6081 - val_loss: 26413.1465 - val_mse: 26413.1465 - val_mae: 162.5212\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 39s 604ms/step - loss: 29288.6816 - mse: 29288.6816 - mae: 170.4187 - val_loss: 30848.0996 - val_mse: 30848.0996 - val_mae: 175.6363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/180\n",
      "65/65 [==============================] - 40s 608ms/step - loss: 29789.5176 - mse: 29789.5176 - mae: 171.9549 - val_loss: 31775.0371 - val_mse: 31775.0371 - val_mae: 178.2555\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 39s 594ms/step - loss: 29648.4023 - mse: 29648.4023 - mae: 171.4218 - val_loss: 30247.6738 - val_mse: 30247.6738 - val_mae: 173.9185\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 36s 552ms/step - loss: 29747.6484 - mse: 29747.6484 - mae: 171.3646 - val_loss: 29225.4707 - val_mse: 29225.4707 - val_mae: 170.9546\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 38s 591ms/step - loss: 29785.3438 - mse: 29785.3438 - mae: 171.6945 - val_loss: 23133.1621 - val_mse: 23133.1621 - val_mae: 152.0959\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 29739.7129 - mse: 29739.7129 - mae: 171.5205 - val_loss: 26011.7246 - val_mse: 26011.7246 - val_mae: 161.2815\n",
      "Epoch 59/180\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 29343.6504 - mse: 29343.6504 - mae: 170.6510 - val_loss: 31064.5957 - val_mse: 31064.5957 - val_mae: 176.2515\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 38s 592ms/step - loss: 29812.8848 - mse: 29812.8848 - mae: 171.8874 - val_loss: 29014.2617 - val_mse: 29014.2617 - val_mae: 170.3357\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 29784.7480 - mse: 29784.7480 - mae: 171.9689 - val_loss: 27472.9902 - val_mse: 27472.9902 - val_mae: 165.7498\n",
      "Epoch 62/180\n",
      "65/65 [==============================] - 36s 552ms/step - loss: 29349.9414 - mse: 29349.9414 - mae: 170.5423 - val_loss: 30084.8418 - val_mse: 30084.8418 - val_mae: 173.4498\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 40s 609ms/step - loss: 30010.3145 - mse: 30010.3145 - mae: 172.2914 - val_loss: 28691.2891 - val_mse: 28691.2891 - val_mae: 169.3850\n",
      "Epoch 64/180\n",
      "65/65 [==============================] - 38s 578ms/step - loss: 29681.1895 - mse: 29681.1895 - mae: 171.4605 - val_loss: 33094.3047 - val_mse: 33094.3047 - val_mae: 181.9184\n",
      "Epoch 65/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 29730.9668 - mse: 29730.9668 - mae: 171.1630 - val_loss: 29443.3184 - val_mse: 29443.3184 - val_mae: 171.5905\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 29672.6562 - mse: 29672.6562 - mae: 171.5390 - val_loss: 33615.3945 - val_mse: 33615.3945 - val_mae: 183.3450\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 29669.4531 - mse: 29669.4531 - mae: 171.5882 - val_loss: 29520.8965 - val_mse: 29520.8965 - val_mae: 171.8165\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 34s 527ms/step - loss: 29806.5957 - mse: 29806.5957 - mae: 171.9155 - val_loss: 30336.2461 - val_mse: 30336.2461 - val_mae: 174.1730\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 29514.8301 - mse: 29514.8301 - mae: 171.1534 - val_loss: 28042.2559 - val_mse: 28042.2559 - val_mae: 167.4582\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 29738.5312 - mse: 29738.5312 - mae: 171.6741 - val_loss: 27778.9961 - val_mse: 27778.9961 - val_mae: 166.6703\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 36s 555ms/step - loss: 29660.4121 - mse: 29660.4121 - mae: 171.5115 - val_loss: 26384.2461 - val_mse: 26384.2461 - val_mae: 162.4323\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 34s 521ms/step - loss: 29971.5215 - mse: 29971.5215 - mae: 171.7367 - val_loss: 28037.4453 - val_mse: 28037.4453 - val_mae: 167.4439\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 29233.8301 - mse: 29233.8301 - mae: 169.6525 - val_loss: 33802.1680 - val_mse: 33802.1680 - val_mae: 183.8537\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 29449.5234 - mse: 29449.5234 - mae: 170.7817 - val_loss: 36817.6250 - val_mse: 36817.6250 - val_mae: 191.8792\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 29766.6895 - mse: 29766.6895 - mae: 171.8189 - val_loss: 30790.6289 - val_mse: 30790.6289 - val_mae: 175.4726\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 29700.9492 - mse: 29700.9492 - mae: 171.4950 - val_loss: 31930.5918 - val_mse: 31930.5918 - val_mae: 178.6913\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 29998.5059 - mse: 29998.5059 - mae: 172.4598 - val_loss: 27766.7715 - val_mse: 27766.7715 - val_mae: 166.6336\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 29441.0352 - mse: 29441.0352 - mae: 170.7751 - val_loss: 33531.8438 - val_mse: 33531.8438 - val_mae: 183.1170\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 29584.1328 - mse: 29584.1328 - mae: 171.2602 - val_loss: 31760.8203 - val_mse: 31760.8203 - val_mae: 178.2157\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 29860.9766 - mse: 29860.9766 - mae: 171.9561 - val_loss: 24602.7676 - val_mse: 24602.7676 - val_mae: 156.8527\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 33s 509ms/step - loss: 29746.2363 - mse: 29746.2363 - mae: 171.8521 - val_loss: 33781.9102 - val_mse: 33781.9102 - val_mae: 183.7986\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 29531.5176 - mse: 29531.5176 - mae: 171.0020 - val_loss: 27790.7539 - val_mse: 27790.7539 - val_mae: 166.7056\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 29723.1738 - mse: 29723.1738 - mae: 171.8038 - val_loss: 31957.1895 - val_mse: 31957.1895 - val_mae: 178.7657\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 29723.7305 - mse: 29723.7305 - mae: 171.7791 - val_loss: 30733.3340 - val_mse: 30733.3340 - val_mae: 175.3092\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 37s 570ms/step - loss: 29747.1055 - mse: 29747.1055 - mae: 171.8195 - val_loss: 23698.4297 - val_mse: 23698.4297 - val_mae: 153.9429\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 29478.9160 - mse: 29478.9160 - mae: 170.9825 - val_loss: 26480.3926 - val_mse: 26480.3926 - val_mae: 162.7280\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 29725.5957 - mse: 29725.5957 - mae: 171.4830 - val_loss: 24658.9785 - val_mse: 24658.9785 - val_mae: 157.0318\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 29817.9121 - mse: 29817.9121 - mae: 171.5026 - val_loss: 26970.4824 - val_mse: 26970.4824 - val_mae: 164.2269\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 29582.3145 - mse: 29582.3125 - mae: 171.3110 - val_loss: 25653.2949 - val_mse: 25653.2949 - val_mae: 160.1665\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 29364.9531 - mse: 29364.9531 - mae: 170.2335 - val_loss: 31705.5215 - val_mse: 31705.5215 - val_mae: 178.0604\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 36s 553ms/step - loss: 29758.1895 - mse: 29758.1895 - mae: 171.7507 - val_loss: 30741.0762 - val_mse: 30741.0762 - val_mae: 175.3313\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 29510.5977 - mse: 29510.5977 - mae: 170.8821 - val_loss: 35037.5000 - val_mse: 35037.4961 - val_mae: 187.1831\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 34s 517ms/step - loss: 29996.9785 - mse: 29996.9785 - mae: 172.3211 - val_loss: 28994.2871 - val_mse: 28994.2871 - val_mae: 170.2771\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 34s 525ms/step - loss: 29670.7383 - mse: 29670.7383 - mae: 171.4556 - val_loss: 37416.3203 - val_mse: 37416.3203 - val_mae: 193.4330\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 29682.9277 - mse: 29682.9277 - mae: 171.4921 - val_loss: 27689.0742 - val_mse: 27689.0742 - val_mae: 166.4003\n",
      "Epoch 96/180\n",
      "65/65 [==============================] - 34s 525ms/step - loss: 29828.3145 - mse: 29828.3145 - mae: 172.0751 - val_loss: 28878.8340 - val_mse: 28878.8340 - val_mae: 169.9377\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 29809.4688 - mse: 29809.4688 - mae: 171.6993 - val_loss: 31318.0508 - val_mse: 31318.0508 - val_mae: 176.9690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/180\n",
      "65/65 [==============================] - 37s 568ms/step - loss: 29567.5059 - mse: 29567.5059 - mae: 171.1083 - val_loss: 26330.6484 - val_mse: 26330.6484 - val_mae: 162.2672\n",
      "Epoch 99/180\n",
      "65/65 [==============================] - 40s 620ms/step - loss: 29713.1309 - mse: 29713.1309 - mae: 171.7309 - val_loss: 24412.2441 - val_mse: 24412.2441 - val_mae: 156.2442\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 36s 557ms/step - loss: 29642.8008 - mse: 29642.8047 - mae: 171.3214 - val_loss: 23296.2090 - val_mse: 23296.2090 - val_mae: 152.6310\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 29612.9531 - mse: 29612.9531 - mae: 171.0905 - val_loss: 25137.5371 - val_mse: 25137.5371 - val_mae: 158.5482\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 29850.5684 - mse: 29850.5684 - mae: 172.0873 - val_loss: 31645.4785 - val_mse: 31645.4785 - val_mae: 177.8918\n",
      "Epoch 103/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 29642.6367 - mse: 29642.6367 - mae: 171.5362 - val_loss: 34097.6836 - val_mse: 34097.6836 - val_mae: 184.6556\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 36s 546ms/step - loss: 29678.1777 - mse: 29678.1777 - mae: 171.7198 - val_loss: 32814.7305 - val_mse: 32814.7305 - val_mae: 181.1484\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 29612.1094 - mse: 29612.1094 - mae: 171.4708 - val_loss: 29657.5605 - val_mse: 29657.5605 - val_mae: 172.2137\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 29948.1621 - mse: 29948.1621 - mae: 172.3908 - val_loss: 29271.2227 - val_mse: 29271.2227 - val_mae: 171.0883\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 29380.3418 - mse: 29380.3418 - mae: 170.5052 - val_loss: 30740.2441 - val_mse: 30740.2441 - val_mae: 175.3289\n",
      "Epoch 108/180\n",
      "65/65 [==============================] - 34s 522ms/step - loss: 29937.9121 - mse: 29937.9121 - mae: 172.2280 - val_loss: 24181.3789 - val_mse: 24181.3789 - val_mae: 155.5036\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 29437.5332 - mse: 29437.5332 - mae: 170.7984 - val_loss: 27046.6680 - val_mse: 27046.6680 - val_mae: 164.4587\n",
      "Epoch 110/180\n",
      "65/65 [==============================] - 39s 604ms/step - loss: 29936.6973 - mse: 29936.6973 - mae: 172.1460 - val_loss: 29209.5371 - val_mse: 29209.5371 - val_mae: 170.9080\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 29313.7363 - mse: 29313.7363 - mae: 170.5033 - val_loss: 29764.5078 - val_mse: 29764.5078 - val_mae: 172.5239\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 29837.8965 - mse: 29837.8965 - mae: 172.0114 - val_loss: 26009.6797 - val_mse: 26009.6797 - val_mae: 161.2752\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 33s 506ms/step - loss: 29865.4258 - mse: 29865.4258 - mae: 172.1965 - val_loss: 30834.5938 - val_mse: 30834.5938 - val_mae: 175.5978\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 29479.9160 - mse: 29479.9160 - mae: 171.1264 - val_loss: 26406.6582 - val_mse: 26406.6582 - val_mae: 162.5013\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 29684.9844 - mse: 29684.9844 - mae: 171.6507 - val_loss: 26754.4004 - val_mse: 26754.4004 - val_mae: 163.5677\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 29735.3418 - mse: 29735.3418 - mae: 171.7587 - val_loss: 30237.3047 - val_mse: 30237.3047 - val_mae: 173.8887\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 29809.0059 - mse: 29809.0059 - mae: 172.0414 - val_loss: 24625.1758 - val_mse: 24625.1758 - val_mae: 156.9241\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 29489.7871 - mse: 29489.7871 - mae: 171.0340 - val_loss: 27290.1914 - val_mse: 27290.1914 - val_mae: 165.1974\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 38s 589ms/step - loss: 29688.7129 - mse: 29688.7129 - mae: 171.6998 - val_loss: 27633.2910 - val_mse: 27633.2910 - val_mae: 166.2326\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 36s 554ms/step - loss: 29834.7812 - mse: 29834.7812 - mae: 172.0757 - val_loss: 27600.5879 - val_mse: 27600.5879 - val_mae: 166.1342\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 29283.7773 - mse: 29283.7773 - mae: 170.5069 - val_loss: 32947.5391 - val_mse: 32947.5391 - val_mae: 181.5146\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 34s 524ms/step - loss: 29897.3203 - mse: 29897.3203 - mae: 172.0859 - val_loss: 28227.7051 - val_mse: 28227.7051 - val_mae: 168.0110\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 29600.9512 - mse: 29600.9512 - mae: 171.3900 - val_loss: 28302.5449 - val_mse: 28302.5449 - val_mae: 168.2336\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 29700.1582 - mse: 29700.1582 - mae: 171.6153 - val_loss: 25959.7480 - val_mse: 25959.7480 - val_mae: 161.1203\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 29614.3965 - mse: 29614.3965 - mae: 171.4479 - val_loss: 30830.9453 - val_mse: 30830.9453 - val_mae: 175.5874\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 38s 579ms/step - loss: 29814.1641 - mse: 29814.1641 - mae: 171.9732 - val_loss: 21453.2520 - val_mse: 21453.2520 - val_mae: 146.4693\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 41s 638ms/step - loss: 29502.8301 - mse: 29502.8301 - mae: 170.9220 - val_loss: 32227.1680 - val_mse: 32227.1680 - val_mae: 179.5193\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 44s 679ms/step - loss: 29689.7617 - mse: 29689.7617 - mae: 171.5344 - val_loss: 35251.7266 - val_mse: 35251.7266 - val_mae: 187.7544\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 42s 647ms/step - loss: 29670.2188 - mse: 29670.2188 - mae: 171.5459 - val_loss: 34071.3242 - val_mse: 34071.3242 - val_mae: 184.5842\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 29690.5645 - mse: 29690.5645 - mae: 171.6429 - val_loss: 36129.4609 - val_mse: 36129.4609 - val_mae: 190.0775\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 37s 565ms/step - loss: 29803.1797 - mse: 29803.1797 - mae: 171.8868 - val_loss: 26872.9863 - val_mse: 26872.9863 - val_mae: 163.9298\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 29524.2109 - mse: 29524.2109 - mae: 171.0790 - val_loss: 33811.5273 - val_mse: 33811.5273 - val_mae: 183.8791\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 29658.7598 - mse: 29658.7598 - mae: 171.3914 - val_loss: 23603.2598 - val_mse: 23603.2598 - val_mae: 153.6335\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 37s 564ms/step - loss: 29627.5957 - mse: 29627.5957 - mae: 171.3182 - val_loss: 28547.4863 - val_mse: 28547.4863 - val_mae: 168.9600\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 36s 549ms/step - loss: 29638.2324 - mse: 29638.2324 - mae: 171.4462 - val_loss: 26013.9121 - val_mse: 26013.9121 - val_mae: 161.2883\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 29676.7383 - mse: 29676.7383 - mae: 171.6924 - val_loss: 33081.0195 - val_mse: 33081.0234 - val_mae: 181.8819\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 40s 620ms/step - loss: 29885.3438 - mse: 29885.3438 - mae: 172.1501 - val_loss: 26328.0176 - val_mse: 26328.0176 - val_mae: 162.2591\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 39s 598ms/step - loss: 29591.2852 - mse: 29591.2852 - mae: 171.3749 - val_loss: 29545.5996 - val_mse: 29545.5996 - val_mae: 171.8883\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 41s 638ms/step - loss: 29906.8496 - mse: 29906.8496 - mae: 172.3272 - val_loss: 27665.9258 - val_mse: 27665.9258 - val_mae: 166.3308\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 36s 561ms/step - loss: 29335.0137 - mse: 29335.0137 - mae: 170.3933 - val_loss: 29134.6660 - val_mse: 29134.6660 - val_mae: 170.6888\n",
      "Epoch 141/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 37s 565ms/step - loss: 29824.9082 - mse: 29824.9082 - mae: 171.9530 - val_loss: 33568.0625 - val_mse: 33568.0625 - val_mae: 183.2159\n",
      "Epoch 142/180\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 29657.4160 - mse: 29657.4160 - mae: 171.3284 - val_loss: 31788.8379 - val_mse: 31788.8379 - val_mae: 178.2942\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 37s 565ms/step - loss: 29941.1562 - mse: 29941.1562 - mae: 172.1342 - val_loss: 23384.4590 - val_mse: 23384.4590 - val_mae: 152.9198\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 43s 657ms/step - loss: 29365.9590 - mse: 29365.9590 - mae: 170.7015 - val_loss: 30376.7754 - val_mse: 30376.7754 - val_mae: 174.2893\n",
      "Epoch 145/180\n",
      "65/65 [==============================] - 40s 608ms/step - loss: 29845.9277 - mse: 29845.9277 - mae: 171.9855 - val_loss: 33766.7031 - val_mse: 33766.7031 - val_mae: 183.7572\n",
      "Epoch 146/180\n",
      "65/65 [==============================] - 39s 606ms/step - loss: 29437.9043 - mse: 29437.9043 - mae: 170.8038 - val_loss: 28994.3887 - val_mse: 28994.3887 - val_mae: 170.2774\n",
      "Epoch 147/180\n",
      "65/65 [==============================] - 44s 680ms/step - loss: 29791.0156 - mse: 29791.0156 - mae: 171.9091 - val_loss: 34721.4258 - val_mse: 34721.4258 - val_mae: 186.3369\n",
      "Epoch 148/180\n",
      "65/65 [==============================] - 41s 633ms/step - loss: 29346.5273 - mse: 29346.5273 - mae: 170.2319 - val_loss: 39496.0352 - val_mse: 39496.0391 - val_mae: 198.7361\n",
      "Epoch 149/180\n",
      "65/65 [==============================] - 39s 605ms/step - loss: 29968.7266 - mse: 29968.7266 - mae: 172.2720 - val_loss: 32247.1426 - val_mse: 32247.1426 - val_mae: 179.5749\n",
      "Epoch 150/180\n",
      "65/65 [==============================] - 40s 611ms/step - loss: 29810.6777 - mse: 29810.6777 - mae: 172.0346 - val_loss: 26652.7324 - val_mse: 26652.7324 - val_mae: 163.2566\n",
      "Epoch 151/180\n",
      "65/65 [==============================] - 40s 617ms/step - loss: 29848.6660 - mse: 29848.6660 - mae: 172.0696 - val_loss: 33531.6719 - val_mse: 33531.6719 - val_mae: 183.1165\n",
      "Epoch 152/180\n",
      "65/65 [==============================] - 42s 649ms/step - loss: 29526.0684 - mse: 29526.0684 - mae: 171.0778 - val_loss: 28052.3652 - val_mse: 28052.3652 - val_mae: 167.4884\n",
      "Epoch 153/180\n",
      "65/65 [==============================] - 43s 654ms/step - loss: 29741.8613 - mse: 29741.8613 - mae: 171.7439 - val_loss: 34172.0820 - val_mse: 34172.0820 - val_mae: 184.8569\n",
      "Epoch 154/180\n",
      "65/65 [==============================] - 42s 640ms/step - loss: 29442.5898 - mse: 29442.5898 - mae: 170.8415 - val_loss: 33460.7383 - val_mse: 33460.7383 - val_mae: 182.9228\n",
      "Epoch 155/180\n",
      "65/65 [==============================] - 47s 726ms/step - loss: 29765.5273 - mse: 29765.5332 - mae: 171.7300 - val_loss: 37227.1680 - val_mse: 37227.1680 - val_mae: 192.9434\n",
      "Epoch 156/180\n",
      "65/65 [==============================] - 41s 630ms/step - loss: 29661.1875 - mse: 29661.1875 - mae: 171.2372 - val_loss: 33451.2578 - val_mse: 33451.2617 - val_mae: 182.8969\n",
      "Epoch 157/180\n",
      "65/65 [==============================] - 38s 589ms/step - loss: 29609.3672 - mse: 29609.3672 - mae: 171.0762 - val_loss: 26673.9414 - val_mse: 26673.9414 - val_mae: 163.3216\n",
      "Epoch 158/180\n",
      "65/65 [==============================] - 41s 632ms/step - loss: 29394.4355 - mse: 29394.4355 - mae: 170.7109 - val_loss: 30051.6250 - val_mse: 30051.6250 - val_mae: 173.3540\n",
      "Epoch 159/180\n",
      "65/65 [==============================] - 41s 624ms/step - loss: 29868.5137 - mse: 29868.5137 - mae: 172.0304 - val_loss: 32265.9512 - val_mse: 32265.9512 - val_mae: 179.6272\n",
      "Epoch 160/180\n",
      "65/65 [==============================] - 40s 612ms/step - loss: 29540.7324 - mse: 29540.7324 - mae: 171.3093 - val_loss: 35771.8047 - val_mse: 35771.8086 - val_mae: 189.1344\n",
      "Epoch 161/180\n",
      "65/65 [==============================] - 42s 642ms/step - loss: 29702.7852 - mse: 29702.7852 - mae: 171.5490 - val_loss: 23923.6348 - val_mse: 23923.6348 - val_mae: 154.6727\n",
      "Epoch 162/180\n",
      "65/65 [==============================] - 43s 654ms/step - loss: 29754.0234 - mse: 29754.0234 - mae: 171.5320 - val_loss: 31685.4961 - val_mse: 31685.4961 - val_mae: 178.0042\n",
      "Epoch 163/180\n",
      "65/65 [==============================] - 46s 703ms/step - loss: 29638.6719 - mse: 29638.6719 - mae: 171.3270 - val_loss: 24186.3945 - val_mse: 24186.3945 - val_mae: 155.5197\n",
      "Epoch 164/180\n",
      "65/65 [==============================] - 41s 634ms/step - loss: 29441.1582 - mse: 29441.1582 - mae: 170.9066 - val_loss: 28506.3066 - val_mse: 28506.3066 - val_mae: 168.8381\n",
      "Epoch 165/180\n",
      "65/65 [==============================] - 40s 623ms/step - loss: 29930.1289 - mse: 29930.1289 - mae: 172.3865 - val_loss: 27685.3379 - val_mse: 27685.3379 - val_mae: 166.3891\n",
      "Epoch 166/180\n",
      "65/65 [==============================] - 41s 632ms/step - loss: 29605.2285 - mse: 29605.2285 - mae: 171.3220 - val_loss: 31444.0996 - val_mse: 31444.0996 - val_mae: 177.3248\n",
      "Epoch 167/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 29633.3555 - mse: 29633.3555 - mae: 171.3172 - val_loss: 28693.2207 - val_mse: 28693.2207 - val_mae: 169.3907\n",
      "Epoch 168/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 29469.3125 - mse: 29469.3125 - mae: 170.9043 - val_loss: 30479.7070 - val_mse: 30479.7070 - val_mae: 174.5844\n",
      "Epoch 169/180\n",
      "65/65 [==============================] - 40s 608ms/step - loss: 29564.7363 - mse: 29564.7363 - mae: 171.1125 - val_loss: 28768.5586 - val_mse: 28768.5586 - val_mae: 169.6130\n",
      "Epoch 170/180\n",
      "65/65 [==============================] - 40s 615ms/step - loss: 29751.4043 - mse: 29751.4043 - mae: 171.7541 - val_loss: 32982.7383 - val_mse: 32982.7383 - val_mae: 181.6115\n",
      "Epoch 171/180\n",
      "65/65 [==============================] - 42s 648ms/step - loss: 29784.2324 - mse: 29784.2324 - mae: 171.9103 - val_loss: 29479.7266 - val_mse: 29479.7266 - val_mae: 171.6966\n",
      "Epoch 172/180\n",
      "65/65 [==============================] - 40s 616ms/step - loss: 29904.8379 - mse: 29904.8379 - mae: 172.2355 - val_loss: 30194.9160 - val_mse: 30194.9160 - val_mae: 173.7668\n",
      "Epoch 173/180\n",
      "65/65 [==============================] - 40s 618ms/step - loss: 29508.5820 - mse: 29508.5820 - mae: 171.1066 - val_loss: 40916.5234 - val_mse: 40916.5273 - val_mae: 202.2783\n",
      "Epoch 174/180\n",
      "65/65 [==============================] - 40s 623ms/step - loss: 29618.2461 - mse: 29618.2461 - mae: 171.3434 - val_loss: 31082.2422 - val_mse: 31082.2422 - val_mae: 176.3016\n",
      "Epoch 175/180\n",
      "65/65 [==============================] - 41s 630ms/step - loss: 29680.5215 - mse: 29680.5215 - mae: 171.5653 - val_loss: 31551.1914 - val_mse: 31551.1914 - val_mae: 177.6265\n",
      "Epoch 176/180\n",
      "65/65 [==============================] - 39s 600ms/step - loss: 29713.0000 - mse: 29713.0000 - mae: 171.6591 - val_loss: 30514.8105 - val_mse: 30514.8105 - val_mae: 174.6849\n",
      "Epoch 177/180\n",
      "65/65 [==============================] - 41s 626ms/step - loss: 29407.0410 - mse: 29407.0410 - mae: 170.8007 - val_loss: 32439.3398 - val_mse: 32439.3398 - val_mae: 180.1092\n",
      "Epoch 178/180\n",
      "65/65 [==============================] - 42s 647ms/step - loss: 29970.8301 - mse: 29970.8301 - mae: 172.2055 - val_loss: 29839.2422 - val_mse: 29839.2422 - val_mae: 172.7404\n",
      "Epoch 179/180\n",
      "65/65 [==============================] - 41s 628ms/step - loss: 29436.3105 - mse: 29436.3105 - mae: 170.5863 - val_loss: 32164.8457 - val_mse: 32164.8457 - val_mae: 179.3456\n",
      "Epoch 180/180\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 29776.8613 - mse: 29776.8613 - mae: 171.6088 - val_loss: 33754.8359 - val_mse: 33754.8359 - val_mae: 183.7249\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "steps = 96\n",
    "n_hidden = 1\n",
    "units = 150\n",
    "batch_size = 96\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Dictionary to include the parameters\n",
    "parameters = {'bias_initializer':[initializers.Zeros(),\n",
    "                                 initializers.Ones()],\n",
    "              'kernel_initializer': ['glorot_uniform',\n",
    "                                     'he_normal',\n",
    "                                     'he_uniform']\n",
    "               }\n",
    "\n",
    "all_param = ParameterGrid(parameters)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# divide features and labels\n",
    "X_train = data_train[:, 0:15] \n",
    "y_train = data_train[:, -1]\n",
    "X_test = data_test[:, 0:15] \n",
    "y_test = data_test[:, -1] \n",
    "\n",
    "# divide data into train and test \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "         X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "# put data into correct shape\n",
    "X_train, y_train = split_data(X_train, y_train, steps)\n",
    "X_test, y_test = split_data(X_test, y_test, steps)\n",
    "X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "X_train = cut_data(X_train, batch_size)\n",
    "y_train = cut_data(y_train, batch_size)\n",
    "X_test = cut_data(X_test, batch_size)\n",
    "y_test = cut_data(y_test, batch_size)\n",
    "X_val = cut_data(X_val, batch_size)\n",
    "y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "# inverse of test set should not be inside the loop \n",
    "y_test = (y_test * sc_X.data_range_[15]) + (sc_X.data_min_[15])\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "for i in range(len(all_param)):\n",
    "    \n",
    "    bias_initializer = all_param[i]['bias_initializer']\n",
    "    kernel_initializer = all_param[i]['kernel_initializer']\n",
    "\n",
    "    # design the LSTM\n",
    "    def regressor_tunning(bias_initializer, kernel_initializer):\n",
    "        model = Sequential()\n",
    "        if n_hidden == 0:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           batch_input_shape = (batch_size, steps, features_num), \n",
    "                           stateful = True,\n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        else:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           batch_input_shape = (batch_size, steps, features_num), \n",
    "                           stateful = True,\n",
    "                           return_sequences = True,\n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(LSTM(units = units, \n",
    "                           batch_input_shape = (batch_size, steps, features_num), \n",
    "                           stateful = True,\n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        optimizer = optimizers.RMSprop(lr = i)\n",
    "        model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "        return model\n",
    "\n",
    "    model = regressor_tunning(bias_initializer, kernel_initializer)\n",
    "\n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = 180,\n",
    "                        shuffle = False, \n",
    "                        validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15\n",
    "    y_pred = (y_pred * sc_X.data_range_[15]) + (sc_X.data_min_[15])\n",
    "\n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "    \n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_param</th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>99.690302</td>\n",
       "      <td>93.814721</td>\n",
       "      <td>96.021883</td>\n",
       "      <td>91.317437</td>\n",
       "      <td>100.223807</td>\n",
       "      <td>94.185704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>33798.251775</td>\n",
       "      <td>33798.235473</td>\n",
       "      <td>33794.808429</td>\n",
       "      <td>33794.795412</td>\n",
       "      <td>33798.763269</td>\n",
       "      <td>33798.746509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>61772.781259</td>\n",
       "      <td>61772.772340</td>\n",
       "      <td>61776.219521</td>\n",
       "      <td>61776.212400</td>\n",
       "      <td>61772.270474</td>\n",
       "      <td>61772.261304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>167393.084225</td>\n",
       "      <td>167393.080933</td>\n",
       "      <td>167396.523622</td>\n",
       "      <td>167396.520994</td>\n",
       "      <td>167392.573282</td>\n",
       "      <td>167392.569897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>213763.156019</td>\n",
       "      <td>213763.153442</td>\n",
       "      <td>213759.715439</td>\n",
       "      <td>213759.713381</td>\n",
       "      <td>213763.667128</td>\n",
       "      <td>213763.664478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'bias_initializer': &lt;tensorflow.python.ops.in...</td>\n",
       "      <td>275594.192308</td>\n",
       "      <td>275594.190308</td>\n",
       "      <td>275597.631965</td>\n",
       "      <td>275597.630369</td>\n",
       "      <td>275593.681328</td>\n",
       "      <td>275593.679272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           all_param   rmse_general  \\\n",
       "0  {'bias_initializer': <tensorflow.python.ops.in...      99.690302   \n",
       "1  {'bias_initializer': <tensorflow.python.ops.in...   33798.251775   \n",
       "2  {'bias_initializer': <tensorflow.python.ops.in...   61772.781259   \n",
       "3  {'bias_initializer': <tensorflow.python.ops.in...  167393.084225   \n",
       "4  {'bias_initializer': <tensorflow.python.ops.in...  213763.156019   \n",
       "5  {'bias_initializer': <tensorflow.python.ops.in...  275594.192308   \n",
       "\n",
       "     mae_general     rmse_spike      mae_spike    rmse_normal     mae_normal  \n",
       "0      93.814721      96.021883      91.317437     100.223807      94.185704  \n",
       "1   33798.235473   33794.808429   33794.795412   33798.763269   33798.746509  \n",
       "2   61772.772340   61776.219521   61776.212400   61772.270474   61772.261304  \n",
       "3  167393.080933  167396.523622  167396.520994  167392.573282  167392.569897  \n",
       "4  213763.153442  213759.715439  213759.713381  213763.667128  213763.664478  \n",
       "5  275594.190308  275597.631965  275597.630369  275593.681328  275593.679272  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'all_param':all_param,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "results.to_csv('Results_LSTM_4_kernel_bias.csv')\n",
    "\n",
    "y_pred = pd.DataFrame({'all_param': all_param,\n",
    "                       'Predicitons': y_pred_list})\n",
    "\n",
    "y_pred.to_csv('Pedictions_LSTM_4_kernel_bias.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col0 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col2 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col3 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col4 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col5 {\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rmse_general</th>        <th class=\"col_heading level0 col1\" >mae_general</th>        <th class=\"col_heading level0 col2\" >rmse_spike</th>        <th class=\"col_heading level0 col3\" >mae_spike</th>        <th class=\"col_heading level0 col4\" >rmse_normal</th>        <th class=\"col_heading level0 col5\" >mae_normal</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col0\" class=\"data row0 col0\" >99.690302</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col1\" class=\"data row0 col1\" >93.814721</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col2\" class=\"data row0 col2\" >96.021883</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col3\" class=\"data row0 col3\" >91.317437</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col4\" class=\"data row0 col4\" >100.223807</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row0_col5\" class=\"data row0 col5\" >94.185704</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row1_col0\" class=\"data row1 col0\" >33798.251775</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row1_col1\" class=\"data row1 col1\" >33798.235473</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row1_col2\" class=\"data row1 col2\" >33794.808429</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row1_col3\" class=\"data row1 col3\" >33794.795412</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row1_col4\" class=\"data row1 col4\" >33798.763269</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row1_col5\" class=\"data row1 col5\" >33798.746509</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row2_col0\" class=\"data row2 col0\" >61772.781259</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row2_col1\" class=\"data row2 col1\" >61772.772340</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row2_col2\" class=\"data row2 col2\" >61776.219521</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row2_col3\" class=\"data row2 col3\" >61776.212400</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row2_col4\" class=\"data row2 col4\" >61772.270474</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row2_col5\" class=\"data row2 col5\" >61772.261304</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row3_col0\" class=\"data row3 col0\" >167393.084225</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row3_col1\" class=\"data row3 col1\" >167393.080933</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row3_col2\" class=\"data row3 col2\" >167396.523622</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row3_col3\" class=\"data row3 col3\" >167396.520994</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row3_col4\" class=\"data row3 col4\" >167392.573282</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row3_col5\" class=\"data row3 col5\" >167392.569897</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row4_col0\" class=\"data row4 col0\" >213763.156019</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row4_col1\" class=\"data row4 col1\" >213763.153442</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row4_col2\" class=\"data row4 col2\" >213759.715439</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row4_col3\" class=\"data row4 col3\" >213759.713381</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row4_col4\" class=\"data row4 col4\" >213763.667128</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row4_col5\" class=\"data row4 col5\" >213763.664478</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row5_col0\" class=\"data row5 col0\" >275594.192308</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row5_col1\" class=\"data row5 col1\" >275594.190308</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row5_col2\" class=\"data row5 col2\" >275597.631965</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row5_col3\" class=\"data row5 col3\" >275597.630369</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row5_col4\" class=\"data row5 col4\" >275593.681328</td>\n",
       "                        <td id=\"T_9fb1cd58_d275_11ea_b23b_7cb27da2bf47row5_col5\" class=\"data row5 col5\" >275593.679272</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27629a0a9c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
