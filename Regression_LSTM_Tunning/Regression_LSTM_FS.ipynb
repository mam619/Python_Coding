{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection of LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\maria\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "# empty list to append metric values\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import keras libraries, packages and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import data\n",
    "data_full = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "\n",
    "data_full = data_full.loc[data_full.index > 2018070000, :]\n",
    "\n",
    "# reset index\n",
    "data_full.reset_index(inplace = True)\n",
    "data_full.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# fill nan values\n",
    "data_full.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "# parameters\n",
    "features_num = 15\n",
    "steps = 96\n",
    "n_hidden = 1\n",
    "units = 150\n",
    "batch_size = 96\n",
    "epochs = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# design the LSTM\n",
    "def regressor_tunning(features_num = features_num, bias_initializer = initializers.Ones() , kernel_initializer = 'he_normal'):\n",
    "    model = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       return_sequences = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units = units, \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = optimizers.RMSprop(lr = 0.0001)\n",
    "    model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First prediction with one feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 0.1580 - mse: 0.1580 - mae: 0.3146 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0609\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 0.1074 - mse: 0.1074 - mae: 0.2614 - val_loss: 0.0031 - val_mse: 0.0031 - val_mae: 0.0452\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 0.0680 - mse: 0.0680 - mae: 0.2076 - val_loss: 0.0023 - val_mse: 0.0023 - val_mae: 0.0364\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 0.0496 - mse: 0.0496 - mae: 0.1773 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0290\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 0.0324 - mse: 0.0324 - mae: 0.1421 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0250\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 0.0224 - mse: 0.0224 - mae: 0.1188 - val_loss: 0.0021 - val_mse: 0.0021 - val_mae: 0.0316\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0991 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0197\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 63s 975ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0842 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0206\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 63s 971ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0716 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0165\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 63s 975ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0630 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0222\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0563 - val_loss: 0.0016 - val_mse: 0.0016 - val_mae: 0.0237\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0493 - val_loss: 0.0015 - val_mse: 0.0015 - val_mae: 0.0213\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0442 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0194\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0393 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0197\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 33s 500ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0350 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0197\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0317 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0175\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0289 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0166\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0264 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0167\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 26s 401ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0251 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0169\n",
      "Epoch 20/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0235 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0173\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0226 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0214 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0181\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 33s 513ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0207 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0184\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0203 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0200 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0194 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0198\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0192 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0192 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 9.9420e-04 - mse: 9.9420e-04 - mae: 0.0190 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 9.7545e-04 - mse: 9.7545e-04 - mae: 0.0187 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 9.8266e-04 - mse: 9.8266e-04 - mae: 0.0186 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 9.7730e-04 - mse: 9.7730e-04 - mae: 0.0186 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0202\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 64s 977ms/step - loss: 9.6119e-04 - mse: 9.6119e-04 - mae: 0.0185 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0198\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 56s 856ms/step - loss: 9.5803e-04 - mse: 9.5803e-04 - mae: 0.0184 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0200\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 9.6415e-04 - mse: 9.6415e-04 - mae: 0.0185 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0200\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 27s 408ms/step - loss: 9.6557e-04 - mse: 9.6557e-04 - mae: 0.0184 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0199\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 9.7079e-04 - mse: 9.7079e-04 - mae: 0.0186 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 9.6640e-04 - mse: 9.6640e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 9.5310e-04 - mse: 9.5310e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 34s 521ms/step - loss: 9.6168e-04 - mse: 9.6168e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 9.5414e-04 - mse: 9.5414e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 9.6548e-04 - mse: 9.6548e-04 - mae: 0.0185 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 9.5392e-04 - mse: 9.5392e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 9.5838e-04 - mse: 9.5838e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 9.4762e-04 - mse: 9.4762e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 9.4729e-04 - mse: 9.4729e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 9.6375e-04 - mse: 9.6375e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 9.4685e-04 - mse: 9.4685e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 49/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 26s 405ms/step - loss: 9.4180e-04 - mse: 9.4180e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 50/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 9.4066e-04 - mse: 9.4066e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 34s 522ms/step - loss: 9.3970e-04 - mse: 9.3970e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 36s 553ms/step - loss: 9.4099e-04 - mse: 9.4099e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 53/180\n",
      "53/65 [=======================>......] - ETA: 6s - loss: 8.5406e-04 - mse: 8.5406e-04 - mae: 0.0179"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# get data ready with one feature\n",
    "data = data_full.loc[:,['PrevDay', 'Offers']]\n",
    "\n",
    "# divide data into train and test \n",
    "data_train, data_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle=False) \n",
    "\n",
    "sc_X = MinMaxScaler()\n",
    "data_train = sc_X.fit_transform(data_train)\n",
    "data_test = sc_X.transform(data_test)\n",
    "\n",
    "# divide features and labels\n",
    "X_train = data_train[:, 0] \n",
    "y_train = data_train[:, -1]\n",
    "X_test = data_test[:, 0] \n",
    "y_test = data_test[:, -1]\n",
    "\n",
    "# divide data into train and test \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "# function to split data into correct shape for RNN for 1 feature only\n",
    "def split_data_1(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# put data into correct shape\n",
    "X_train, y_train = split_data_1(X_train, y_train, steps)\n",
    "X_test, y_test = split_data_1(X_test, y_test, steps)\n",
    "X_val, y_val = split_data_1(X_val, y_val, steps)\n",
    "\n",
    "X_train = cut_data(X_train, batch_size)\n",
    "y_train = cut_data(y_train, batch_size)\n",
    "X_test = cut_data(X_test, batch_size)\n",
    "y_test = cut_data(y_test, batch_size)\n",
    "X_val = cut_data(X_val, batch_size)\n",
    "y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "features_num = 1\n",
    "\n",
    "model = regressor_tunning(features_num = features_num)\n",
    "\n",
    "# fitting the LSTM to the training set\n",
    "history = model.fit(X_train,\n",
    "                    y_train, \n",
    "                    batch_size = batch_size, \n",
    "                    epochs = epochs,\n",
    "                    shuffle = False, \n",
    "                    validation_data = (X_val, y_val))\n",
    "\n",
    "# make new predicitons with test set\n",
    "y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "# prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "y_pred = (y_pred * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "y_test = (y_test * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    " \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mae_error = mae(y_test, y_pred)\n",
    "  \n",
    "rmse_gen.append(rmse_error)\n",
    "mae_gen.append(mae_error)\n",
    "    \n",
    "# =============================================================================\n",
    "# Metrics evaluation on spike regions\n",
    "# =============================================================================\n",
    "    \n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "    \n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "rmse_spi.append(rmse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "    \n",
    "# =============================================================================\n",
    "# Metric evaluation on normal regions\n",
    "# =============================================================================\n",
    "    \n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    " \n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "  \n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "rmse_nor.append(rmse_normal)\n",
    "mae_nor.append(mae_normal)\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "time_count.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to continue testing other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features list; order made according to Linear Regression FS\n",
    "features_list = ['APXP', \n",
    "                 'LOLP',  \n",
    "                 'In_gen',\n",
    "                 'Ren_R',\n",
    "                 'DA_imb_France', \n",
    "                 'Rene',\n",
    "                 'ratio_offers_vol',\n",
    "                 'DA_price_france',\n",
    "                 'TSDF',\n",
    "                 'dino_bin',\n",
    "                 'DA_margin',\n",
    "                 'Im_Pr']\n",
    "\n",
    "best_score = rmse_error\n",
    "\n",
    "# LOOP STARTS\n",
    "for i in features_list:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # data recovery for in case there is no improvement\n",
    "    data_recovery = data\n",
    "    \n",
    "    # update feature number\n",
    "    features_num = features_num + 1\n",
    "    \n",
    "    # add new feature\n",
    "    data = pd.concat([data, data_full.loc[:,i]], axis = 1)    \n",
    "\n",
    "    # divide data into train and test \n",
    "    data_train, data_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle=False)    \n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # data scaling  (including offer (y))\n",
    "    sc_X = MinMaxScaler()\n",
    "    data_train = sc_X.fit_transform(data_train)\n",
    "    data_test = sc_X.transform(data_test)\n",
    "    \n",
    "    # divide features and labels\n",
    "    X_train = data_train[:, 0: features_num] \n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, 0:features_num] \n",
    "    y_test = data_test[:, -1] \n",
    "\n",
    "    # divide data into train and test \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "    # put data into correct shape\n",
    "    X_train, y_train = split_data(X_train, y_train, steps)\n",
    "    X_test, y_test = split_data(X_test, y_test, steps)\n",
    "    X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "    X_train = cut_data(X_train, batch_size)\n",
    "    y_train = cut_data(y_train, batch_size)\n",
    "    X_test = cut_data(X_test, batch_size)\n",
    "    y_test = cut_data(y_test, batch_size)\n",
    "    X_val = cut_data(X_val, batch_size)\n",
    "    y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "    model = regressor_tunning(features_num = features_num)\n",
    "    \n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs,\n",
    "                        shuffle = False, \n",
    "                        validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "    y_test = (y_test * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "    \n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)\n",
    "    \n",
    "    # condition of improvement for FS\n",
    "    if best_score < rmse_gen[-1]:\n",
    "        data = data_recovery\n",
    "        features_num = features_num - 1\n",
    "    else:\n",
    "        data = data\n",
    "        best_score = rmse_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,\n",
    "    \n",
    "                        'time': time_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "dates_labels = ['12 ',\n",
    "                '10 ',\n",
    "                '8 ',\n",
    "                '6 ',\n",
    "                '4 ',\n",
    "                '2 ']\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged RMSE for different\\n predictive windows')\n",
    "plt.plot(rmse_gen, label = 'Overall error')\n",
    "plt.plot(rmse_spi, label = 'Spike regions')\n",
    "plt.plot(rmse_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE (£/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('RMSE_predictive_window.png')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged MAE for different\\n predictive windows')\n",
    "plt.plot(mae_gen, label = 'Overall error')\n",
    "plt.plot(mae_spi, label = 'Spike regions')\n",
    "plt.plot(mae_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('MAE (£/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('MAE_predictive_window.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
