{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply FS in ANN\n",
    "Condition to improvement only if RMSE on normal regions improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 23.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 69.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.0 pytz-2020.1\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (1.18.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 62.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=ca2eef3fb4f2a87fd5f7a944e1f117331cca61e93a3b9ae350c70187150e770e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.16.0 scikit-learn-0.23.2 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# empty list to append metric values\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data & treat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > 2018090000, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential # to initialise the NN\n",
    "from keras.layers import Dense # to create layers\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "\n",
    "# parameters for ANN\n",
    "splits = 7\n",
    "epochs = 80\n",
    "\n",
    "# divide into features and label\n",
    "X = data.iloc[:, 0:14]\n",
    "y = data.loc[:, 'Offers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do first prediciton with first feature in list from Linear Regression FS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/series.py:4523: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 13342.8296 - mse: 13342.8301 - mae: 109.9652 - val_loss: 34649.4106 - val_mse: 34649.4102 - val_mae: 132.8451\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 13250.4004 - mse: 13250.3994 - mae: 109.5460 - val_loss: 34471.7046 - val_mse: 34471.7031 - val_mae: 132.1760\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 13002.3266 - mse: 13002.3271 - mae: 108.4144 - val_loss: 33987.6898 - val_mse: 33987.6914 - val_mae: 130.3369\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 12358.2243 - mse: 12358.2227 - mae: 105.4548 - val_loss: 32738.7081 - val_mse: 32738.7070 - val_mae: 125.4684\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 10790.4475 - mse: 10790.4482 - mae: 97.6580 - val_loss: 29793.4759 - val_mse: 29793.4766 - val_mae: 113.1634\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 7319.2238 - mse: 7319.2241 - mae: 77.5278 - val_loss: 24024.0371 - val_mse: 24024.0352 - val_mae: 84.0011\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 3326.7058 - mse: 3326.7061 - mae: 43.6486 - val_loss: 18326.7498 - val_mse: 18326.7500 - val_mae: 40.3025\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2262.7726 - mse: 2262.7727 - mae: 33.9647 - val_loss: 17768.9311 - val_mse: 17768.9316 - val_mae: 37.2406\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 435us/step - loss: 2300.3300 - mse: 2300.3303 - mae: 33.2550 - val_loss: 17790.5104 - val_mse: 17790.5098 - val_mae: 37.3218\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 605us/step - loss: 2153.3535 - mse: 2153.3538 - mae: 32.6105 - val_loss: 17757.5827 - val_mse: 17757.5820 - val_mae: 37.1897\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2432.4281 - mse: 2432.4282 - mae: 35.0841 - val_loss: 17953.1519 - val_mse: 17953.1523 - val_mae: 37.9865\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2454.6658 - mse: 2454.6658 - mae: 35.9457 - val_loss: 17926.7467 - val_mse: 17926.7461 - val_mae: 37.8675\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2514.4222 - mse: 2514.4224 - mae: 35.7060 - val_loss: 17794.2151 - val_mse: 17794.2168 - val_mae: 37.3270\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2654.5329 - mse: 2654.5330 - mae: 36.0854 - val_loss: 17994.2054 - val_mse: 17994.2031 - val_mae: 38.1907\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 682us/step - loss: 2272.8698 - mse: 2272.8701 - mae: 33.2394 - val_loss: 17802.1351 - val_mse: 17802.1348 - val_mae: 37.3520\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2294.3612 - mse: 2294.3613 - mae: 34.9016 - val_loss: 17885.9222 - val_mse: 17885.9199 - val_mae: 37.6797\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 698us/step - loss: 2374.1464 - mse: 2374.1465 - mae: 34.3795 - val_loss: 18260.8500 - val_mse: 18260.8496 - val_mae: 39.8152\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 727us/step - loss: 2046.7331 - mse: 2046.7332 - mae: 32.1425 - val_loss: 17732.1574 - val_mse: 17732.1582 - val_mae: 37.1437\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 745us/step - loss: 2138.2864 - mse: 2138.2866 - mae: 33.7150 - val_loss: 17864.5450 - val_mse: 17864.5449 - val_mae: 37.5871\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2303.7472 - mse: 2303.7473 - mae: 33.5706 - val_loss: 18030.4989 - val_mse: 18030.5000 - val_mae: 38.3879\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2271.6194 - mse: 2271.6191 - mae: 33.0453 - val_loss: 17904.3233 - val_mse: 17904.3242 - val_mae: 37.7531\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2065.2074 - mse: 2065.2075 - mae: 31.7053 - val_loss: 17874.1342 - val_mse: 17874.1348 - val_mae: 37.6225\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 481us/step - loss: 2200.1255 - mse: 2200.1257 - mae: 33.2228 - val_loss: 17748.5918 - val_mse: 17748.5918 - val_mae: 37.1576\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 507us/step - loss: 2344.1100 - mse: 2344.1101 - mae: 34.6518 - val_loss: 17880.5792 - val_mse: 17880.5801 - val_mae: 37.6456\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 1984.9214 - mse: 1984.9214 - mae: 30.1068 - val_loss: 17759.2499 - val_mse: 17759.2480 - val_mae: 37.1742\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 2326.4664 - mse: 2326.4666 - mae: 34.4382 - val_loss: 18029.2649 - val_mse: 18029.2637 - val_mae: 38.3676\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2227.6752 - mse: 2227.6753 - mae: 33.9030 - val_loss: 17896.7274 - val_mse: 17896.7266 - val_mae: 37.7095\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 642us/step - loss: 1978.1040 - mse: 1978.1039 - mae: 31.4672 - val_loss: 17741.5275 - val_mse: 17741.5273 - val_mae: 37.1452\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 698us/step - loss: 2169.3223 - mse: 2169.3225 - mae: 32.3009 - val_loss: 17783.4279 - val_mse: 17783.4258 - val_mae: 37.2628\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2147.3182 - mse: 2147.3181 - mae: 32.3826 - val_loss: 17758.9471 - val_mse: 17758.9453 - val_mae: 37.1676\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 654us/step - loss: 1980.8922 - mse: 1980.8922 - mae: 31.8944 - val_loss: 17508.5460 - val_mse: 17508.5469 - val_mae: 37.4469\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 2212.1067 - mse: 2212.1064 - mae: 32.5483 - val_loss: 17755.9413 - val_mse: 17755.9414 - val_mae: 37.1583\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 658us/step - loss: 1990.5948 - mse: 1990.5948 - mae: 30.4199 - val_loss: 17811.2295 - val_mse: 17811.2305 - val_mae: 37.3663\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2214.0070 - mse: 2214.0073 - mae: 32.8815 - val_loss: 17773.1401 - val_mse: 17773.1406 - val_mae: 37.2170\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 667us/step - loss: 2074.8001 - mse: 2074.8000 - mae: 31.5801 - val_loss: 17873.5075 - val_mse: 17873.5078 - val_mae: 37.6017\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 2071.6819 - mse: 2071.6821 - mae: 31.9651 - val_loss: 17832.7835 - val_mse: 17832.7832 - val_mae: 37.4441\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2118.2118 - mse: 2118.2114 - mae: 31.9869 - val_loss: 17754.6997 - val_mse: 17754.6992 - val_mae: 37.1506\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2215.2543 - mse: 2215.2539 - mae: 32.3881 - val_loss: 17996.7438 - val_mse: 17996.7441 - val_mae: 38.1574\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2157.9538 - mse: 2157.9539 - mae: 32.6628 - val_loss: 17585.3265 - val_mse: 17585.3262 - val_mae: 37.2343\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 503us/step - loss: 2320.8174 - mse: 2320.8169 - mae: 32.4367 - val_loss: 17863.9505 - val_mse: 17863.9512 - val_mae: 37.5509\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2043.1641 - mse: 2043.1643 - mae: 31.6370 - val_loss: 17822.2877 - val_mse: 17822.2852 - val_mae: 37.3917\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 1965.6855 - mse: 1965.6854 - mae: 31.6435 - val_loss: 17902.8087 - val_mse: 17902.8086 - val_mae: 37.7057\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 511us/step - loss: 1891.8090 - mse: 1891.8088 - mae: 30.0794 - val_loss: 17789.2108 - val_mse: 17789.2109 - val_mae: 37.2623\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 545us/step - loss: 2249.7646 - mse: 2249.7644 - mae: 33.3966 - val_loss: 17915.5731 - val_mse: 17915.5723 - val_mae: 37.7592\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 474us/step - loss: 1996.3129 - mse: 1996.3130 - mae: 31.1170 - val_loss: 17790.3184 - val_mse: 17790.3184 - val_mae: 37.2654\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 2115.4173 - mse: 2115.4172 - mae: 31.6805 - val_loss: 17882.6735 - val_mse: 17882.6738 - val_mae: 37.6171\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2076.7211 - mse: 2076.7209 - mae: 31.7472 - val_loss: 17713.4501 - val_mse: 17713.4492 - val_mae: 37.0955\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 1944.1859 - mse: 1944.1860 - mae: 30.6477 - val_loss: 17706.0365 - val_mse: 17706.0371 - val_mae: 37.0902\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 491us/step - loss: 1938.8843 - mse: 1938.8844 - mae: 30.9235 - val_loss: 17786.8522 - val_mse: 17786.8516 - val_mae: 37.2465\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 445us/step - loss: 2017.6473 - mse: 2017.6475 - mae: 31.9411 - val_loss: 17644.6096 - val_mse: 17644.6074 - val_mae: 37.1034\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2022.8638 - mse: 2022.8639 - mae: 31.4088 - val_loss: 17772.8497 - val_mse: 17772.8496 - val_mae: 37.1913\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 490us/step - loss: 1900.9783 - mse: 1900.9783 - mae: 30.1844 - val_loss: 17684.9606 - val_mse: 17684.9609 - val_mae: 37.0864\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 1983.1221 - mse: 1983.1219 - mae: 30.8540 - val_loss: 17644.9707 - val_mse: 17644.9707 - val_mae: 37.1042\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 1958.7060 - mse: 1958.7061 - mae: 31.5271 - val_loss: 17755.7070 - val_mse: 17755.7070 - val_mae: 37.1376\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 486us/step - loss: 1998.1779 - mse: 1998.1779 - mae: 30.9584 - val_loss: 17861.8774 - val_mse: 17861.8789 - val_mae: 37.5315\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 2009.2768 - mse: 2009.2769 - mae: 31.3414 - val_loss: 17733.5743 - val_mse: 17733.5762 - val_mae: 37.1103\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 516us/step - loss: 2136.8641 - mse: 2136.8643 - mae: 31.3154 - val_loss: 17793.9181 - val_mse: 17793.9180 - val_mae: 37.2702\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 1948.3427 - mse: 1948.3429 - mae: 30.7502 - val_loss: 17724.4130 - val_mse: 17724.4121 - val_mae: 37.0999\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 2062.5947 - mse: 2062.5945 - mae: 31.2310 - val_loss: 17851.7394 - val_mse: 17851.7402 - val_mae: 37.4878\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 1927.1016 - mse: 1927.1018 - mae: 29.3352 - val_loss: 17771.8445 - val_mse: 17771.8457 - val_mae: 37.1797\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 660us/step - loss: 1976.5228 - mse: 1976.5228 - mae: 30.4965 - val_loss: 17630.4407 - val_mse: 17630.4395 - val_mae: 37.1253\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 688us/step - loss: 2057.4159 - mse: 2057.4155 - mae: 31.6414 - val_loss: 17858.2893 - val_mse: 17858.2891 - val_mae: 37.5026\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 1762.1013 - mse: 1762.1012 - mae: 28.6149 - val_loss: 17606.0742 - val_mse: 17606.0762 - val_mae: 37.1765\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 1917.8297 - mse: 1917.8298 - mae: 29.7280 - val_loss: 17661.1135 - val_mse: 17661.1113 - val_mae: 37.0831\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 1931.9856 - mse: 1931.9855 - mae: 29.7277 - val_loss: 17650.5149 - val_mse: 17650.5137 - val_mae: 37.0912\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 1570.4864 - mse: 1570.4866 - mae: 26.7793 - val_loss: 17573.1404 - val_mse: 17573.1426 - val_mae: 37.2642\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 2166.7010 - mse: 2166.7007 - mae: 32.5414 - val_loss: 17810.4426 - val_mse: 17810.4414 - val_mae: 37.3144\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2055.7322 - mse: 2055.7322 - mae: 31.4249 - val_loss: 17928.5727 - val_mse: 17928.5742 - val_mae: 37.7766\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 1999.5727 - mse: 1999.5725 - mae: 30.5510 - val_loss: 17769.1198 - val_mse: 17769.1191 - val_mae: 37.1511\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 513us/step - loss: 1925.5760 - mse: 1925.5760 - mae: 30.2072 - val_loss: 17701.3783 - val_mse: 17701.3789 - val_mae: 37.0728\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 643us/step - loss: 2015.8760 - mse: 2015.8759 - mae: 30.8128 - val_loss: 17781.7688 - val_mse: 17781.7676 - val_mae: 37.2001\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 534us/step - loss: 1924.4527 - mse: 1924.4530 - mae: 29.8684 - val_loss: 17643.2458 - val_mse: 17643.2461 - val_mae: 37.0970\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 1965.5202 - mse: 1965.5203 - mae: 30.7005 - val_loss: 17860.8225 - val_mse: 17860.8223 - val_mae: 37.4973\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 1883.7863 - mse: 1883.7861 - mae: 29.3274 - val_loss: 17750.2624 - val_mse: 17750.2617 - val_mae: 37.1049\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 529us/step - loss: 2005.5722 - mse: 2005.5721 - mae: 30.3239 - val_loss: 17883.5763 - val_mse: 17883.5762 - val_mae: 37.5785\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 1968.2801 - mse: 1968.2802 - mae: 29.9162 - val_loss: 17671.9938 - val_mse: 17671.9941 - val_mae: 37.0692\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 1994.6474 - mse: 1994.6472 - mae: 30.5938 - val_loss: 17635.2219 - val_mse: 17635.2227 - val_mae: 37.1111\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2078.2907 - mse: 2078.2903 - mae: 30.9299 - val_loss: 17808.7904 - val_mse: 17808.7891 - val_mae: 37.2900\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 1848.9315 - mse: 1848.9315 - mae: 29.1495 - val_loss: 17608.5505 - val_mse: 17608.5527 - val_mae: 37.1670\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 1885.8667 - mse: 1885.8665 - mae: 30.5917 - val_loss: 17717.9180 - val_mse: 17717.9160 - val_mae: 37.0675\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4228.1702 - mse: 4228.1709 - mae: 34.1913 - val_loss: 2195.3029 - val_mse: 2195.3030 - val_mae: 31.1805\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4176.5389 - mse: 4176.5391 - mae: 34.7120 - val_loss: 2311.0960 - val_mse: 2311.0959 - val_mae: 31.4952\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4078.2602 - mse: 4078.2598 - mae: 33.9932 - val_loss: 2334.8132 - val_mse: 2334.8130 - val_mae: 31.5602\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4248.7413 - mse: 4248.7407 - mae: 34.4793 - val_loss: 2319.3981 - val_mse: 2319.3982 - val_mae: 31.5220\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 686us/step - loss: 4272.4990 - mse: 4272.4990 - mae: 34.8169 - val_loss: 2330.4658 - val_mse: 2330.4658 - val_mae: 31.5550\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4129.4837 - mse: 4129.4839 - mae: 34.3816 - val_loss: 2328.1674 - val_mse: 2328.1672 - val_mae: 31.5529\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4174.7881 - mse: 4174.7881 - mae: 34.1911 - val_loss: 2361.7607 - val_mse: 2361.7607 - val_mae: 31.6426\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4101.3090 - mse: 4101.3091 - mae: 34.3592 - val_loss: 2368.1162 - val_mse: 2368.1162 - val_mae: 31.6618\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4043.1736 - mse: 4043.1741 - mae: 34.8431 - val_loss: 2297.4002 - val_mse: 2297.4001 - val_mae: 31.4739\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4376.9661 - mse: 4376.9658 - mae: 35.0453 - val_loss: 2332.4070 - val_mse: 2332.4070 - val_mae: 31.5704\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4054.3462 - mse: 4054.3462 - mae: 34.0998 - val_loss: 2229.6902 - val_mse: 2229.6902 - val_mae: 31.2975\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 516us/step - loss: 4266.1900 - mse: 4266.1909 - mae: 35.3417 - val_loss: 2379.5918 - val_mse: 2379.5920 - val_mae: 31.7018\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4134.5050 - mse: 4134.5054 - mae: 33.1883 - val_loss: 2302.8211 - val_mse: 2302.8210 - val_mae: 31.4929\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 554us/step - loss: 4057.1816 - mse: 4057.1816 - mae: 33.8151 - val_loss: 2311.7751 - val_mse: 2311.7751 - val_mae: 31.5180\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4328.7766 - mse: 4328.7764 - mae: 35.2598 - val_loss: 2348.4713 - val_mse: 2348.4712 - val_mae: 31.6162\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 3960.1318 - mse: 3960.1323 - mae: 32.5281 - val_loss: 2265.2554 - val_mse: 2265.2554 - val_mae: 31.3950\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4317.0342 - mse: 4317.0347 - mae: 34.9256 - val_loss: 2383.5387 - val_mse: 2383.5388 - val_mae: 31.7261\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 511us/step - loss: 4237.5564 - mse: 4237.5566 - mae: 34.7356 - val_loss: 2403.5303 - val_mse: 2403.5300 - val_mae: 31.7958\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 440us/step - loss: 4157.7028 - mse: 4157.7017 - mae: 34.2137 - val_loss: 2313.8769 - val_mse: 2313.8770 - val_mae: 31.5333\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4207.0922 - mse: 4207.0923 - mae: 34.6444 - val_loss: 2353.2359 - val_mse: 2353.2361 - val_mae: 31.6413\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 518us/step - loss: 3960.4403 - mse: 3960.4414 - mae: 32.8031 - val_loss: 2331.0393 - val_mse: 2331.0393 - val_mae: 31.5834\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4198.2267 - mse: 4198.2271 - mae: 33.6568 - val_loss: 2334.2677 - val_mse: 2334.2681 - val_mae: 31.5966\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 504us/step - loss: 4196.4346 - mse: 4196.4346 - mae: 34.5496 - val_loss: 2433.3876 - val_mse: 2433.3877 - val_mae: 31.9092\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 3990.0601 - mse: 3990.0598 - mae: 33.3558 - val_loss: 2308.3473 - val_mse: 2308.3472 - val_mae: 31.5292\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 3862.3065 - mse: 3862.3059 - mae: 32.6545 - val_loss: 2276.9360 - val_mse: 2276.9360 - val_mae: 31.4451\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 532us/step - loss: 4158.4739 - mse: 4158.4736 - mae: 33.6451 - val_loss: 2318.4124 - val_mse: 2318.4124 - val_mae: 31.5584\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4102.9458 - mse: 4102.9453 - mae: 33.6069 - val_loss: 2355.5453 - val_mse: 2355.5452 - val_mae: 31.6717\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 3993.7686 - mse: 3993.7695 - mae: 33.4891 - val_loss: 2335.9578 - val_mse: 2335.9578 - val_mae: 31.6183\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4023.2786 - mse: 4023.2786 - mae: 33.8456 - val_loss: 2364.5355 - val_mse: 2364.5354 - val_mae: 31.7016\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4085.8482 - mse: 4085.8486 - mae: 34.3938 - val_loss: 2319.2532 - val_mse: 2319.2529 - val_mae: 31.5752\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - ETA: 0s - loss: 4244.5374 - mse: 4244.5376 - mae: 33.26 - 1s 620us/step - loss: 4160.6231 - mse: 4160.6235 - mae: 33.2489 - val_loss: 2326.1556 - val_mse: 2326.1555 - val_mae: 31.5978\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4075.4702 - mse: 4075.4697 - mae: 33.7778 - val_loss: 2278.6502 - val_mse: 2278.6499 - val_mae: 31.4629\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 622us/step - loss: 3813.1331 - mse: 3813.1328 - mae: 32.8173 - val_loss: 2308.3390 - val_mse: 2308.3394 - val_mae: 31.5478\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 3943.7645 - mse: 3943.7644 - mae: 33.6640 - val_loss: 2285.4724 - val_mse: 2285.4722 - val_mae: 31.4826\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4099.6058 - mse: 4099.6060 - mae: 33.8883 - val_loss: 2362.4349 - val_mse: 2362.4348 - val_mae: 31.7103\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 554us/step - loss: 3887.1145 - mse: 3887.1143 - mae: 33.9468 - val_loss: 2300.3635 - val_mse: 2300.3633 - val_mae: 31.5293\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 4062.3761 - mse: 4062.3765 - mae: 34.0899 - val_loss: 2374.9892 - val_mse: 2374.9893 - val_mae: 31.7518\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4106.1324 - mse: 4106.1323 - mae: 34.5393 - val_loss: 2354.8913 - val_mse: 2354.8911 - val_mae: 31.7007\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 534us/step - loss: 4014.4224 - mse: 4014.4224 - mae: 33.5855 - val_loss: 2296.3413 - val_mse: 2296.3413 - val_mae: 31.5322\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 662us/step - loss: 4088.8072 - mse: 4088.8069 - mae: 33.4315 - val_loss: 2290.7480 - val_mse: 2290.7483 - val_mae: 31.5186\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4114.1451 - mse: 4114.1450 - mae: 33.9660 - val_loss: 2365.0174 - val_mse: 2365.0173 - val_mae: 31.7393\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 3977.6985 - mse: 3977.6985 - mae: 32.7831 - val_loss: 2301.0227 - val_mse: 2301.0225 - val_mae: 31.5513\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4234.6232 - mse: 4234.6230 - mae: 34.3844 - val_loss: 2273.7716 - val_mse: 2273.7715 - val_mae: 31.4779\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4049.0440 - mse: 4049.0442 - mae: 33.2714 - val_loss: 2291.1708 - val_mse: 2291.1709 - val_mae: 31.5262\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 3838.8051 - mse: 3838.8057 - mae: 33.0607 - val_loss: 2295.0668 - val_mse: 2295.0669 - val_mae: 31.5389\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4112.8695 - mse: 4112.8696 - mae: 35.1534 - val_loss: 2352.0409 - val_mse: 2352.0408 - val_mae: 31.7110\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 3988.8846 - mse: 3988.8840 - mae: 33.2273 - val_loss: 2325.5015 - val_mse: 2325.5017 - val_mae: 31.6346\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4149.4989 - mse: 4149.4985 - mae: 33.4644 - val_loss: 2379.4155 - val_mse: 2379.4155 - val_mae: 31.7947\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 4145.4770 - mse: 4145.4771 - mae: 34.5033 - val_loss: 2336.3632 - val_mse: 2336.3630 - val_mae: 31.6743\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4019.0107 - mse: 4019.0107 - mae: 33.5538 - val_loss: 2308.1442 - val_mse: 2308.1443 - val_mae: 31.5919\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4267.6444 - mse: 4267.6450 - mae: 34.6373 - val_loss: 2399.6399 - val_mse: 2399.6399 - val_mae: 31.8681\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 523us/step - loss: 4190.4286 - mse: 4190.4287 - mae: 33.5380 - val_loss: 2369.3211 - val_mse: 2369.3210 - val_mae: 31.7741\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4125.4572 - mse: 4125.4570 - mae: 33.6987 - val_loss: 2292.3506 - val_mse: 2292.3506 - val_mae: 31.5524\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 4082.9728 - mse: 4082.9722 - mae: 33.4309 - val_loss: 2341.7367 - val_mse: 2341.7371 - val_mae: 31.6988\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4070.1907 - mse: 4070.1904 - mae: 33.3197 - val_loss: 2345.9666 - val_mse: 2345.9666 - val_mae: 31.7125\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4163.0125 - mse: 4163.0127 - mae: 33.7024 - val_loss: 2347.5470 - val_mse: 2347.5471 - val_mae: 31.7197\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 510us/step - loss: 4166.5308 - mse: 4166.5312 - mae: 33.4798 - val_loss: 2302.4706 - val_mse: 2302.4705 - val_mae: 31.5875\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4174.5022 - mse: 4174.5024 - mae: 33.9416 - val_loss: 2337.5920 - val_mse: 2337.5920 - val_mae: 31.6938\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 3860.4625 - mse: 3860.4626 - mae: 32.0904 - val_loss: 2291.3440 - val_mse: 2291.3440 - val_mae: 31.5581\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4255.3234 - mse: 4255.3232 - mae: 34.2946 - val_loss: 2333.3456 - val_mse: 2333.3455 - val_mae: 31.6837\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4164.6582 - mse: 4164.6587 - mae: 33.3503 - val_loss: 2319.4499 - val_mse: 2319.4497 - val_mae: 31.6460\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 556us/step - loss: 3955.1351 - mse: 3955.1357 - mae: 32.8835 - val_loss: 2277.8451 - val_mse: 2277.8450 - val_mae: 31.5317\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 643us/step - loss: 3961.2333 - mse: 3961.2336 - mae: 32.7872 - val_loss: 2292.8518 - val_mse: 2292.8521 - val_mae: 31.5729\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4029.4580 - mse: 4029.4580 - mae: 31.9841 - val_loss: 2278.6869 - val_mse: 2278.6870 - val_mae: 31.5341\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4017.6807 - mse: 4017.6809 - mae: 33.4831 - val_loss: 2312.5172 - val_mse: 2312.5173 - val_mae: 31.6304\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4016.3661 - mse: 4016.3662 - mae: 32.5199 - val_loss: 2337.3584 - val_mse: 2337.3584 - val_mae: 31.7092\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 4027.4618 - mse: 4027.4619 - mae: 32.8700 - val_loss: 2305.0686 - val_mse: 2305.0688 - val_mae: 31.6140\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4189.3625 - mse: 4189.3628 - mae: 33.6541 - val_loss: 2279.5308 - val_mse: 2279.5308 - val_mae: 31.5451\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4143.3375 - mse: 4143.3379 - mae: 33.9202 - val_loss: 2315.6265 - val_mse: 2315.6265 - val_mae: 31.6475\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 529us/step - loss: 4140.3628 - mse: 4140.3633 - mae: 34.0346 - val_loss: 2274.0162 - val_mse: 2274.0161 - val_mae: 31.5354\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 530us/step - loss: 4188.5549 - mse: 4188.5542 - mae: 34.3783 - val_loss: 2285.0723 - val_mse: 2285.0723 - val_mae: 31.5700\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 657us/step - loss: 4170.5461 - mse: 4170.5464 - mae: 34.2316 - val_loss: 2370.2637 - val_mse: 2370.2634 - val_mae: 31.8132\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 643us/step - loss: 3982.8043 - mse: 3982.8042 - mae: 32.9065 - val_loss: 2315.4693 - val_mse: 2315.4692 - val_mae: 31.6535\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4034.5736 - mse: 4034.5735 - mae: 33.6650 - val_loss: 2338.2502 - val_mse: 2338.2502 - val_mae: 31.7237\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4114.6712 - mse: 4114.6714 - mae: 32.4658 - val_loss: 2323.8487 - val_mse: 2323.8486 - val_mae: 31.6825\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 519us/step - loss: 3955.0937 - mse: 3955.0933 - mae: 31.8180 - val_loss: 2299.7746 - val_mse: 2299.7747 - val_mae: 31.6135\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4089.5040 - mse: 4089.5039 - mae: 33.3599 - val_loss: 2298.4304 - val_mse: 2298.4302 - val_mae: 31.6135\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4096.3075 - mse: 4096.3076 - mae: 33.5044 - val_loss: 2321.0037 - val_mse: 2321.0037 - val_mae: 31.6797\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 3998.4843 - mse: 3998.4839 - mae: 32.6060 - val_loss: 2329.3195 - val_mse: 2329.3196 - val_mae: 31.7089\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 3909.3165 - mse: 3909.3164 - mae: 32.4819 - val_loss: 2313.8989 - val_mse: 2313.8987 - val_mae: 31.6646\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3442.7509 - mse: 3442.7498 - mae: 33.0670 - val_loss: 1464.8030 - val_mse: 1464.8030 - val_mae: 26.0514\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3322.6790 - mse: 3322.6787 - mae: 32.2539 - val_loss: 1463.7134 - val_mse: 1463.7133 - val_mae: 25.5169\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 516us/step - loss: 3335.1476 - mse: 3335.1472 - mae: 32.4517 - val_loss: 1463.7168 - val_mse: 1463.7168 - val_mae: 25.7140\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3449.7310 - mse: 3449.7300 - mae: 33.3423 - val_loss: 1463.9104 - val_mse: 1463.9104 - val_mae: 25.8060\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3433.0137 - mse: 3433.0129 - mae: 33.2554 - val_loss: 1463.9371 - val_mse: 1463.9370 - val_mae: 25.6689\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3395.7262 - mse: 3395.7261 - mae: 32.4104 - val_loss: 1465.5315 - val_mse: 1465.5314 - val_mae: 26.1317\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3335.7833 - mse: 3335.7832 - mae: 32.1647 - val_loss: 1464.2398 - val_mse: 1464.2397 - val_mae: 25.8295\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3463.3047 - mse: 3463.3059 - mae: 33.2795 - val_loss: 1464.4355 - val_mse: 1464.4355 - val_mae: 25.8757\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3330.9802 - mse: 3330.9810 - mae: 32.4542 - val_loss: 1464.6937 - val_mse: 1464.6937 - val_mae: 25.9437\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3196.6136 - mse: 3196.6140 - mae: 31.6937 - val_loss: 1467.1387 - val_mse: 1467.1387 - val_mae: 26.3518\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3416.0735 - mse: 3416.0732 - mae: 33.2406 - val_loss: 1467.0083 - val_mse: 1467.0082 - val_mae: 25.0013\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3356.4025 - mse: 3356.4016 - mae: 32.3762 - val_loss: 1464.3336 - val_mse: 1464.3334 - val_mae: 25.7531\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3363.7287 - mse: 3363.7288 - mae: 33.4392 - val_loss: 1464.3736 - val_mse: 1464.3737 - val_mae: 25.6624\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3352.1633 - mse: 3352.1628 - mae: 32.6550 - val_loss: 1465.5384 - val_mse: 1465.5385 - val_mae: 26.0776\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3361.4650 - mse: 3361.4658 - mae: 32.3950 - val_loss: 1464.2473 - val_mse: 1464.2472 - val_mae: 25.6155\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3285.3540 - mse: 3285.3547 - mae: 32.4774 - val_loss: 1464.4601 - val_mse: 1464.4598 - val_mae: 25.7426\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 514us/step - loss: 3425.7071 - mse: 3425.7070 - mae: 32.5808 - val_loss: 1464.7096 - val_mse: 1464.7096 - val_mae: 25.7702\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 524us/step - loss: 3271.6056 - mse: 3271.6055 - mae: 31.7293 - val_loss: 1468.6887 - val_mse: 1468.6885 - val_mae: 26.4859\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3346.6748 - mse: 3346.6760 - mae: 32.6319 - val_loss: 1466.0447 - val_mse: 1466.0448 - val_mae: 26.1365\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 523us/step - loss: 3348.6865 - mse: 3348.6870 - mae: 32.0368 - val_loss: 1466.5301 - val_mse: 1466.5303 - val_mae: 26.2119\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3280.5113 - mse: 3280.5117 - mae: 32.3883 - val_loss: 1464.6830 - val_mse: 1464.6829 - val_mae: 25.4568\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3225.1205 - mse: 3225.1204 - mae: 31.5795 - val_loss: 1468.1604 - val_mse: 1468.1603 - val_mae: 26.4202\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3314.5619 - mse: 3314.5618 - mae: 33.0153 - val_loss: 1464.8280 - val_mse: 1464.8280 - val_mae: 25.7557\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3298.1797 - mse: 3298.1799 - mae: 32.1944 - val_loss: 1465.5992 - val_mse: 1465.5992 - val_mae: 26.0249\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 509us/step - loss: 3357.7809 - mse: 3357.7805 - mae: 32.4978 - val_loss: 1464.9381 - val_mse: 1464.9380 - val_mae: 25.7542\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3426.6784 - mse: 3426.6790 - mae: 33.2098 - val_loss: 1465.1940 - val_mse: 1465.1940 - val_mae: 25.7837\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3364.3706 - mse: 3364.3708 - mae: 31.9424 - val_loss: 1465.6862 - val_mse: 1465.6862 - val_mae: 25.9620\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3308.0396 - mse: 3308.0391 - mae: 32.2538 - val_loss: 1465.3992 - val_mse: 1465.3993 - val_mae: 25.8938\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3309.3525 - mse: 3309.3525 - mae: 32.1439 - val_loss: 1467.9441 - val_mse: 1467.9443 - val_mae: 26.3571\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3378.1355 - mse: 3378.1357 - mae: 32.0875 - val_loss: 1465.1036 - val_mse: 1465.1036 - val_mae: 25.7617\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3377.3459 - mse: 3377.3464 - mae: 32.0623 - val_loss: 1465.1911 - val_mse: 1465.1912 - val_mae: 25.4139\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 537us/step - loss: 3327.2279 - mse: 3327.2278 - mae: 32.1348 - val_loss: 1465.1081 - val_mse: 1465.1082 - val_mae: 25.8104\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3320.8920 - mse: 3320.8921 - mae: 32.0221 - val_loss: 1464.9870 - val_mse: 1464.9869 - val_mae: 25.7792\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3284.3508 - mse: 3284.3511 - mae: 32.2375 - val_loss: 1465.0390 - val_mse: 1465.0391 - val_mae: 25.8319\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3310.1713 - mse: 3310.1711 - mae: 31.9016 - val_loss: 1464.8727 - val_mse: 1464.8727 - val_mae: 25.7573\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3378.7650 - mse: 3378.7659 - mae: 33.2192 - val_loss: 1465.2820 - val_mse: 1465.2821 - val_mae: 25.9038\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3359.3840 - mse: 3359.3840 - mae: 32.9327 - val_loss: 1464.9675 - val_mse: 1464.9674 - val_mae: 25.5694\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3277.1075 - mse: 3277.1079 - mae: 31.9145 - val_loss: 1464.9291 - val_mse: 1464.9292 - val_mae: 25.4852\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3381.0140 - mse: 3381.0142 - mae: 32.2163 - val_loss: 1464.8505 - val_mse: 1464.8505 - val_mae: 25.7135\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 510us/step - loss: 3355.7184 - mse: 3355.7188 - mae: 32.2245 - val_loss: 1465.4056 - val_mse: 1465.4056 - val_mae: 25.9380\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3238.0183 - mse: 3238.0188 - mae: 31.8317 - val_loss: 1465.2065 - val_mse: 1465.2065 - val_mae: 25.9014\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3319.6888 - mse: 3319.6887 - mae: 32.2819 - val_loss: 1465.2312 - val_mse: 1465.2311 - val_mae: 25.9238\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3383.9155 - mse: 3383.9150 - mae: 32.8883 - val_loss: 1465.2621 - val_mse: 1465.2622 - val_mae: 25.9737\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 535us/step - loss: 3308.3898 - mse: 3308.3896 - mae: 31.5346 - val_loss: 1464.8498 - val_mse: 1464.8497 - val_mae: 25.9091\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3414.0671 - mse: 3414.0671 - mae: 32.5685 - val_loss: 1465.2943 - val_mse: 1465.2943 - val_mae: 26.0304\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3374.8152 - mse: 3374.8149 - mae: 31.9774 - val_loss: 1464.2992 - val_mse: 1464.2991 - val_mae: 25.6230\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3354.6480 - mse: 3354.6482 - mae: 32.0495 - val_loss: 1465.9752 - val_mse: 1465.9751 - val_mae: 26.1396\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3349.3639 - mse: 3349.3645 - mae: 32.2606 - val_loss: 1464.3254 - val_mse: 1464.3256 - val_mae: 25.6623\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3411.0852 - mse: 3411.0852 - mae: 32.3513 - val_loss: 1464.3083 - val_mse: 1464.3081 - val_mae: 25.5896\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3434.2380 - mse: 3434.2388 - mae: 31.8663 - val_loss: 1466.0424 - val_mse: 1466.0425 - val_mae: 26.1455\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3319.4427 - mse: 3319.4434 - mae: 32.2166 - val_loss: 1467.8450 - val_mse: 1467.8451 - val_mae: 26.4096\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3326.8949 - mse: 3326.8943 - mae: 32.5021 - val_loss: 1465.2351 - val_mse: 1465.2350 - val_mae: 26.0211\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3240.9053 - mse: 3240.9058 - mae: 31.5246 - val_loss: 1464.6334 - val_mse: 1464.6333 - val_mae: 25.8902\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3296.8740 - mse: 3296.8743 - mae: 31.9182 - val_loss: 1464.3272 - val_mse: 1464.3271 - val_mae: 25.5967\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3373.9984 - mse: 3373.9993 - mae: 31.7994 - val_loss: 1464.5551 - val_mse: 1464.5552 - val_mae: 25.6733\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3310.9244 - mse: 3310.9248 - mae: 31.9925 - val_loss: 1465.7804 - val_mse: 1465.7804 - val_mae: 26.0723\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3217.6699 - mse: 3217.6702 - mae: 31.3863 - val_loss: 1465.6207 - val_mse: 1465.6208 - val_mae: 26.0542\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3289.1116 - mse: 3289.1121 - mae: 32.5864 - val_loss: 1466.5413 - val_mse: 1466.5413 - val_mae: 26.2448\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 523us/step - loss: 3182.9228 - mse: 3182.9229 - mae: 31.7145 - val_loss: 1464.4436 - val_mse: 1464.4437 - val_mae: 25.8242\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 540us/step - loss: 3310.5367 - mse: 3310.5361 - mae: 32.2355 - val_loss: 1464.3835 - val_mse: 1464.3835 - val_mae: 25.6649\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3289.1634 - mse: 3289.1643 - mae: 32.0489 - val_loss: 1464.9597 - val_mse: 1464.9597 - val_mae: 25.9343\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 476us/step - loss: 3327.9069 - mse: 3327.9070 - mae: 31.6438 - val_loss: 1465.5525 - val_mse: 1465.5525 - val_mae: 26.0849\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3474.3358 - mse: 3474.3362 - mae: 33.1855 - val_loss: 1465.1809 - val_mse: 1465.1810 - val_mae: 25.9964\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3313.8673 - mse: 3313.8672 - mae: 32.1921 - val_loss: 1464.6506 - val_mse: 1464.6505 - val_mae: 25.7341\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 496us/step - loss: 3320.5234 - mse: 3320.5234 - mae: 31.9493 - val_loss: 1465.5954 - val_mse: 1465.5955 - val_mae: 26.0347\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3271.7289 - mse: 3271.7280 - mae: 31.9360 - val_loss: 1469.2224 - val_mse: 1469.2224 - val_mae: 26.5432\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3403.4408 - mse: 3403.4419 - mae: 31.9998 - val_loss: 1464.8943 - val_mse: 1464.8942 - val_mae: 25.3482\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3323.6975 - mse: 3323.6973 - mae: 31.8836 - val_loss: 1464.7861 - val_mse: 1464.7859 - val_mae: 25.7919\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3237.9327 - mse: 3237.9338 - mae: 31.2243 - val_loss: 1467.6063 - val_mse: 1467.6063 - val_mae: 26.3342\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 537us/step - loss: 3368.7155 - mse: 3368.7151 - mae: 31.9607 - val_loss: 1464.7759 - val_mse: 1464.7761 - val_mae: 25.5981\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3363.6923 - mse: 3363.6919 - mae: 31.6802 - val_loss: 1464.6957 - val_mse: 1464.6958 - val_mae: 25.7640\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 513us/step - loss: 3304.8261 - mse: 3304.8264 - mae: 31.4708 - val_loss: 1466.3259 - val_mse: 1466.3259 - val_mae: 26.1623\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3270.1288 - mse: 3270.1292 - mae: 31.4685 - val_loss: 1464.8285 - val_mse: 1464.8285 - val_mae: 25.7433\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3368.9681 - mse: 3368.9683 - mae: 32.3624 - val_loss: 1465.3065 - val_mse: 1465.3066 - val_mae: 25.9715\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3293.4494 - mse: 3293.4485 - mae: 31.9962 - val_loss: 1469.6132 - val_mse: 1469.6132 - val_mae: 26.5814\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3241.8595 - mse: 3241.8599 - mae: 32.1611 - val_loss: 1467.6551 - val_mse: 1467.6550 - val_mae: 26.3238\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3273.5823 - mse: 3273.5820 - mae: 31.6786 - val_loss: 1464.8281 - val_mse: 1464.8280 - val_mae: 25.5600\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3305.5982 - mse: 3305.5984 - mae: 32.1414 - val_loss: 1465.4694 - val_mse: 1465.4692 - val_mae: 25.9823\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3283.5406 - mse: 3283.5415 - mae: 31.8082 - val_loss: 1466.4853 - val_mse: 1466.4854 - val_mae: 26.1546\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3245.8960 - mse: 3245.8960 - mae: 32.2626 - val_loss: 1464.8463 - val_mse: 1464.8464 - val_mae: 25.5111\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2916.1367 - mse: 2916.1362 - mae: 31.5558 - val_loss: 1069.3384 - val_mse: 1069.3383 - val_mae: 24.2992\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2919.3982 - mse: 2919.3989 - mae: 31.3856 - val_loss: 1071.4440 - val_mse: 1071.4438 - val_mae: 23.9135\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 505us/step - loss: 2924.0139 - mse: 2924.0137 - mae: 31.2309 - val_loss: 1070.5940 - val_mse: 1070.5940 - val_mae: 23.9679\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 553us/step - loss: 2846.3709 - mse: 2846.3721 - mae: 30.8644 - val_loss: 1068.9665 - val_mse: 1068.9666 - val_mae: 24.3486\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2981.0216 - mse: 2981.0205 - mae: 31.3866 - val_loss: 1068.6300 - val_mse: 1068.6299 - val_mae: 24.3190\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2950.1276 - mse: 2950.1270 - mae: 31.0557 - val_loss: 1068.4790 - val_mse: 1068.4790 - val_mae: 24.1841\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2865.4993 - mse: 2865.4998 - mae: 30.8056 - val_loss: 1067.8220 - val_mse: 1067.8219 - val_mae: 24.3667\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2959.0687 - mse: 2959.0686 - mae: 31.5322 - val_loss: 1069.0432 - val_mse: 1069.0432 - val_mae: 23.9605\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2873.7917 - mse: 2873.7920 - mae: 31.2942 - val_loss: 1071.3208 - val_mse: 1071.3210 - val_mae: 23.7681\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 537us/step - loss: 2903.3559 - mse: 2903.3560 - mae: 30.9377 - val_loss: 1067.7847 - val_mse: 1067.7848 - val_mae: 24.4417\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2880.2529 - mse: 2880.2527 - mae: 31.1069 - val_loss: 1068.0109 - val_mse: 1068.0107 - val_mae: 24.1269\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2917.5582 - mse: 2917.5586 - mae: 30.9386 - val_loss: 1067.3193 - val_mse: 1067.3191 - val_mae: 24.3704\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2864.4536 - mse: 2864.4531 - mae: 31.2203 - val_loss: 1067.4447 - val_mse: 1067.4446 - val_mae: 24.1421\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2921.3786 - mse: 2921.3792 - mae: 31.1343 - val_loss: 1068.5187 - val_mse: 1068.5187 - val_mae: 23.9639\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2828.8622 - mse: 2828.8618 - mae: 30.3392 - val_loss: 1067.5745 - val_mse: 1067.5747 - val_mae: 24.1189\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2859.7699 - mse: 2859.7698 - mae: 31.1744 - val_loss: 1067.4079 - val_mse: 1067.4080 - val_mae: 24.1563\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2885.7656 - mse: 2885.7659 - mae: 31.5174 - val_loss: 1071.1597 - val_mse: 1071.1597 - val_mae: 23.7335\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2952.1164 - mse: 2952.1169 - mae: 30.8318 - val_loss: 1067.7057 - val_mse: 1067.7058 - val_mae: 24.0209\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 532us/step - loss: 2883.5691 - mse: 2883.5684 - mae: 30.7588 - val_loss: 1067.3767 - val_mse: 1067.3767 - val_mae: 24.1303\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2899.9193 - mse: 2899.9182 - mae: 30.7079 - val_loss: 1066.7324 - val_mse: 1066.7323 - val_mae: 24.3095\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2849.3866 - mse: 2849.3875 - mae: 30.4345 - val_loss: 1069.2893 - val_mse: 1069.2894 - val_mae: 23.8526\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2938.7816 - mse: 2938.7817 - mae: 30.9136 - val_loss: 1066.2369 - val_mse: 1066.2369 - val_mae: 24.2357\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2884.3814 - mse: 2884.3818 - mae: 30.7988 - val_loss: 1067.3562 - val_mse: 1067.3563 - val_mae: 23.9409\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2868.7847 - mse: 2868.7854 - mae: 30.7062 - val_loss: 1065.8459 - val_mse: 1065.8459 - val_mae: 24.2782\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2796.6536 - mse: 2796.6531 - mae: 30.2454 - val_loss: 1065.6055 - val_mse: 1065.6053 - val_mae: 24.1432\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2844.8217 - mse: 2844.8223 - mae: 30.4012 - val_loss: 1065.5458 - val_mse: 1065.5458 - val_mae: 24.1293\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2838.9057 - mse: 2838.9072 - mae: 30.6509 - val_loss: 1065.0238 - val_mse: 1065.0239 - val_mae: 24.4356\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2838.8873 - mse: 2838.8870 - mae: 30.6752 - val_loss: 1064.8428 - val_mse: 1064.8428 - val_mae: 24.5006\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2865.6448 - mse: 2865.6448 - mae: 30.6646 - val_loss: 1066.4682 - val_mse: 1066.4684 - val_mae: 23.8562\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2884.4194 - mse: 2884.4194 - mae: 30.7861 - val_loss: 1066.4621 - val_mse: 1066.4619 - val_mae: 23.8147\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2939.9033 - mse: 2939.9033 - mae: 31.1233 - val_loss: 1063.3606 - val_mse: 1063.3606 - val_mae: 24.2113\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2921.2543 - mse: 2921.2549 - mae: 30.8464 - val_loss: 1063.7564 - val_mse: 1063.7562 - val_mae: 24.0196\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 532us/step - loss: 2949.1022 - mse: 2949.1023 - mae: 30.9753 - val_loss: 1062.5949 - val_mse: 1062.5950 - val_mae: 24.1487\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2904.9682 - mse: 2904.9683 - mae: 30.6290 - val_loss: 1064.2340 - val_mse: 1064.2340 - val_mae: 23.8036\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2948.2056 - mse: 2948.2061 - mae: 31.0904 - val_loss: 1060.9329 - val_mse: 1060.9329 - val_mae: 24.1621\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2960.2909 - mse: 2960.2915 - mae: 30.9797 - val_loss: 1061.1857 - val_mse: 1061.1858 - val_mae: 24.1714\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 532us/step - loss: 2863.1698 - mse: 2863.1692 - mae: 30.3914 - val_loss: 1060.3720 - val_mse: 1060.3719 - val_mae: 24.3828\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 482us/step - loss: 2853.3489 - mse: 2853.3494 - mae: 30.6432 - val_loss: 1060.4449 - val_mse: 1060.4451 - val_mae: 23.9708\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2856.2775 - mse: 2856.2778 - mae: 31.1513 - val_loss: 1058.7765 - val_mse: 1058.7766 - val_mae: 24.1778\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2806.0736 - mse: 2806.0735 - mae: 30.7143 - val_loss: 1059.3790 - val_mse: 1059.3790 - val_mae: 23.9857\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2891.8090 - mse: 2891.8096 - mae: 30.7134 - val_loss: 1059.0236 - val_mse: 1059.0236 - val_mae: 24.2267\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2885.1536 - mse: 2885.1548 - mae: 30.8717 - val_loss: 1058.9703 - val_mse: 1058.9703 - val_mae: 24.3357\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2807.6666 - mse: 2807.6663 - mae: 30.4976 - val_loss: 1059.2331 - val_mse: 1059.2330 - val_mae: 24.0454\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2843.4833 - mse: 2843.4824 - mae: 30.4970 - val_loss: 1058.5336 - val_mse: 1058.5336 - val_mae: 24.0001\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2883.5754 - mse: 2883.5747 - mae: 30.8623 - val_loss: 1057.0708 - val_mse: 1057.0709 - val_mae: 24.1697\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2926.2211 - mse: 2926.2219 - mae: 31.2836 - val_loss: 1057.3526 - val_mse: 1057.3525 - val_mae: 24.0138\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2896.1991 - mse: 2896.1985 - mae: 30.9038 - val_loss: 1057.6158 - val_mse: 1057.6160 - val_mae: 24.0586\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2920.9674 - mse: 2920.9670 - mae: 30.5796 - val_loss: 1058.9156 - val_mse: 1058.9155 - val_mae: 23.8785\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2909.9255 - mse: 2909.9258 - mae: 30.6786 - val_loss: 1056.8928 - val_mse: 1056.8929 - val_mae: 24.0909\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2908.6707 - mse: 2908.6702 - mae: 31.0640 - val_loss: 1057.3925 - val_mse: 1057.3926 - val_mae: 23.9251\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2875.6191 - mse: 2875.6187 - mae: 30.9978 - val_loss: 1055.6760 - val_mse: 1055.6761 - val_mae: 24.2039\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2872.1687 - mse: 2872.1697 - mae: 30.6355 - val_loss: 1058.1526 - val_mse: 1058.1527 - val_mae: 23.7624\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2888.4414 - mse: 2888.4407 - mae: 30.4830 - val_loss: 1056.0789 - val_mse: 1056.0787 - val_mae: 23.9548\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2865.0594 - mse: 2865.0608 - mae: 31.1055 - val_loss: 1055.8886 - val_mse: 1055.8885 - val_mae: 23.9929\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2857.9742 - mse: 2857.9739 - mae: 30.3519 - val_loss: 1054.9935 - val_mse: 1054.9935 - val_mae: 24.3636\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2923.3386 - mse: 2923.3386 - mae: 30.7744 - val_loss: 1055.6234 - val_mse: 1055.6234 - val_mae: 24.1151\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2912.6458 - mse: 2912.6462 - mae: 30.5135 - val_loss: 1055.6381 - val_mse: 1055.6382 - val_mae: 24.0102\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2921.4305 - mse: 2921.4297 - mae: 30.7994 - val_loss: 1055.6055 - val_mse: 1055.6056 - val_mae: 24.1498\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2840.9419 - mse: 2840.9426 - mae: 29.9768 - val_loss: 1056.0981 - val_mse: 1056.0981 - val_mae: 24.3165\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2831.6388 - mse: 2831.6377 - mae: 30.3810 - val_loss: 1055.4743 - val_mse: 1055.4744 - val_mae: 24.3204\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2853.5647 - mse: 2853.5642 - mae: 30.1511 - val_loss: 1055.3491 - val_mse: 1055.3492 - val_mae: 24.1976\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2888.0892 - mse: 2888.0891 - mae: 31.1183 - val_loss: 1055.9733 - val_mse: 1055.9733 - val_mae: 23.9494\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 523us/step - loss: 2826.0032 - mse: 2826.0029 - mae: 30.4175 - val_loss: 1054.5871 - val_mse: 1054.5870 - val_mae: 24.3102\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2832.7252 - mse: 2832.7253 - mae: 30.3655 - val_loss: 1054.1765 - val_mse: 1054.1765 - val_mae: 24.0723\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2876.2387 - mse: 2876.2385 - mae: 30.1862 - val_loss: 1054.6964 - val_mse: 1054.6964 - val_mae: 24.0269\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2917.3520 - mse: 2917.3516 - mae: 30.7237 - val_loss: 1054.7140 - val_mse: 1054.7139 - val_mae: 24.0316\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2812.0900 - mse: 2812.0898 - mae: 30.3464 - val_loss: 1054.2790 - val_mse: 1054.2791 - val_mae: 24.1341\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 484us/step - loss: 2845.8340 - mse: 2845.8335 - mae: 30.7658 - val_loss: 1054.2611 - val_mse: 1054.2611 - val_mae: 24.1392\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 472us/step - loss: 2900.3082 - mse: 2900.3086 - mae: 30.5416 - val_loss: 1054.4062 - val_mse: 1054.4061 - val_mae: 24.1768\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2864.3197 - mse: 2864.3193 - mae: 30.6813 - val_loss: 1055.0428 - val_mse: 1055.0428 - val_mae: 24.0484\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2851.1081 - mse: 2851.1082 - mae: 30.8806 - val_loss: 1055.3747 - val_mse: 1055.3746 - val_mae: 24.0197\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2875.4709 - mse: 2875.4719 - mae: 30.4311 - val_loss: 1054.2666 - val_mse: 1054.2665 - val_mae: 24.2672\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 536us/step - loss: 2895.3832 - mse: 2895.3833 - mae: 30.7941 - val_loss: 1053.3641 - val_mse: 1053.3643 - val_mae: 24.4166\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2836.8657 - mse: 2836.8662 - mae: 29.9967 - val_loss: 1053.3094 - val_mse: 1053.3093 - val_mae: 24.2342\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2803.8501 - mse: 2803.8499 - mae: 30.1292 - val_loss: 1052.3060 - val_mse: 1052.3060 - val_mae: 24.3285\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2821.0496 - mse: 2821.0505 - mae: 30.5651 - val_loss: 1052.0519 - val_mse: 1052.0519 - val_mae: 24.2112\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 440us/step - loss: 2854.5411 - mse: 2854.5413 - mae: 30.3524 - val_loss: 1051.8299 - val_mse: 1051.8301 - val_mae: 24.3375\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 510us/step - loss: 2722.2013 - mse: 2722.2012 - mae: 29.6039 - val_loss: 1052.3658 - val_mse: 1052.3657 - val_mae: 24.3740\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 451us/step - loss: 2929.1410 - mse: 2929.1414 - mae: 30.3254 - val_loss: 1053.1635 - val_mse: 1053.1633 - val_mae: 24.0682\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2913.2403 - mse: 2913.2405 - mae: 30.8995 - val_loss: 1053.0530 - val_mse: 1053.0529 - val_mae: 24.1731\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 512us/step - loss: 2586.5962 - mse: 2586.5969 - mae: 30.1474 - val_loss: 1539.0813 - val_mse: 1539.0814 - val_mae: 28.2212\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 547us/step - loss: 2522.0476 - mse: 2522.0481 - mae: 29.3775 - val_loss: 1537.1621 - val_mse: 1537.1620 - val_mae: 28.3202\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2526.9574 - mse: 2526.9570 - mae: 29.9543 - val_loss: 1548.5826 - val_mse: 1548.5825 - val_mae: 27.7073\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 561us/step - loss: 2538.5283 - mse: 2538.5281 - mae: 29.7583 - val_loss: 1545.5543 - val_mse: 1545.5543 - val_mae: 27.7997\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2554.2842 - mse: 2554.2839 - mae: 29.7202 - val_loss: 1540.0540 - val_mse: 1540.0542 - val_mae: 27.9895\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 543us/step - loss: 2515.8785 - mse: 2515.8782 - mae: 29.4403 - val_loss: 1545.2632 - val_mse: 1545.2633 - val_mae: 27.7591\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 549us/step - loss: 2531.9434 - mse: 2531.9431 - mae: 29.6901 - val_loss: 1547.7999 - val_mse: 1547.7998 - val_mae: 27.6198\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2530.4961 - mse: 2530.4956 - mae: 29.9857 - val_loss: 1550.4817 - val_mse: 1550.4819 - val_mae: 27.5325\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2563.8762 - mse: 2563.8762 - mae: 29.9062 - val_loss: 1546.1131 - val_mse: 1546.1132 - val_mae: 27.6754\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2553.0783 - mse: 2553.0776 - mae: 29.8135 - val_loss: 1546.9754 - val_mse: 1546.9752 - val_mae: 27.6155\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 529us/step - loss: 2603.8795 - mse: 2603.8796 - mae: 30.2166 - val_loss: 1543.7380 - val_mse: 1543.7379 - val_mae: 27.7430\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2563.9131 - mse: 2563.9126 - mae: 29.9629 - val_loss: 1539.7078 - val_mse: 1539.7075 - val_mae: 27.9023\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2571.0011 - mse: 2571.0022 - mae: 29.9675 - val_loss: 1547.8289 - val_mse: 1547.8290 - val_mae: 27.5605\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2555.5575 - mse: 2555.5569 - mae: 29.7725 - val_loss: 1537.8185 - val_mse: 1537.8187 - val_mae: 27.9753\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2554.9629 - mse: 2554.9634 - mae: 29.9519 - val_loss: 1541.2483 - val_mse: 1541.2487 - val_mae: 27.7996\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2539.4246 - mse: 2539.4233 - mae: 29.3920 - val_loss: 1544.7328 - val_mse: 1544.7328 - val_mae: 27.6104\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2492.1039 - mse: 2492.1042 - mae: 29.7714 - val_loss: 1540.2905 - val_mse: 1540.2904 - val_mae: 27.8078\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2607.5636 - mse: 2607.5640 - mae: 30.3889 - val_loss: 1544.8851 - val_mse: 1544.8850 - val_mae: 27.6194\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 531us/step - loss: 2580.1340 - mse: 2580.1335 - mae: 30.5474 - val_loss: 1541.8374 - val_mse: 1541.8374 - val_mae: 27.7579\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 544us/step - loss: 2551.5710 - mse: 2551.5715 - mae: 29.6771 - val_loss: 1548.5525 - val_mse: 1548.5524 - val_mae: 27.5043\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2542.4561 - mse: 2542.4568 - mae: 29.8788 - val_loss: 1535.2600 - val_mse: 1535.2599 - val_mae: 28.0356\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 545us/step - loss: 2552.3595 - mse: 2552.3594 - mae: 29.9486 - val_loss: 1544.8390 - val_mse: 1544.8389 - val_mae: 27.6497\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2585.8429 - mse: 2585.8425 - mae: 29.7162 - val_loss: 1535.5933 - val_mse: 1535.5933 - val_mae: 28.0501\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2573.5886 - mse: 2573.5889 - mae: 29.8461 - val_loss: 1537.2740 - val_mse: 1537.2739 - val_mae: 27.9371\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2509.3385 - mse: 2509.3376 - mae: 29.8186 - val_loss: 1540.8779 - val_mse: 1540.8778 - val_mae: 27.7941\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2588.3689 - mse: 2588.3689 - mae: 29.7331 - val_loss: 1538.5580 - val_mse: 1538.5581 - val_mae: 27.9180\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2552.6610 - mse: 2552.6616 - mae: 29.8591 - val_loss: 1540.1037 - val_mse: 1540.1036 - val_mae: 27.8179\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2522.9116 - mse: 2522.9116 - mae: 29.3805 - val_loss: 1537.6808 - val_mse: 1537.6808 - val_mae: 27.9016\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 602us/step - loss: 2548.8568 - mse: 2548.8562 - mae: 30.1292 - val_loss: 1538.0377 - val_mse: 1538.0376 - val_mae: 27.9118\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2530.0794 - mse: 2530.0793 - mae: 29.5231 - val_loss: 1535.1924 - val_mse: 1535.1924 - val_mae: 27.9774\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2588.4183 - mse: 2588.4180 - mae: 29.5527 - val_loss: 1548.7392 - val_mse: 1548.7390 - val_mae: 27.4908\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2536.1091 - mse: 2536.1089 - mae: 29.4292 - val_loss: 1541.5045 - val_mse: 1541.5044 - val_mae: 27.7467\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2541.8295 - mse: 2541.8303 - mae: 30.0695 - val_loss: 1541.0679 - val_mse: 1541.0676 - val_mae: 27.7591\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2555.9578 - mse: 2555.9575 - mae: 29.6813 - val_loss: 1539.6793 - val_mse: 1539.6793 - val_mae: 27.8010\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2532.0320 - mse: 2532.0315 - mae: 29.5867 - val_loss: 1538.6209 - val_mse: 1538.6207 - val_mae: 27.8780\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2494.3116 - mse: 2494.3123 - mae: 29.6239 - val_loss: 1537.4226 - val_mse: 1537.4227 - val_mae: 27.9270\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2554.1403 - mse: 2554.1406 - mae: 30.0088 - val_loss: 1538.4392 - val_mse: 1538.4392 - val_mae: 27.8543\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2540.5059 - mse: 2540.5054 - mae: 29.7525 - val_loss: 1537.2145 - val_mse: 1537.2146 - val_mae: 27.9066\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2554.2979 - mse: 2554.2981 - mae: 29.6766 - val_loss: 1541.3515 - val_mse: 1541.3514 - val_mae: 27.7838\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 513us/step - loss: 2547.6126 - mse: 2547.6133 - mae: 29.6467 - val_loss: 1538.7430 - val_mse: 1538.7429 - val_mae: 27.8207\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 533us/step - loss: 2528.2763 - mse: 2528.2766 - mae: 29.4903 - val_loss: 1542.8254 - val_mse: 1542.8257 - val_mae: 27.6601\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2515.4764 - mse: 2515.4756 - mae: 29.5691 - val_loss: 1540.8770 - val_mse: 1540.8771 - val_mae: 27.7342\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2532.6822 - mse: 2532.6819 - mae: 29.9142 - val_loss: 1543.5800 - val_mse: 1543.5801 - val_mae: 27.6391\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2568.4566 - mse: 2568.4561 - mae: 29.6703 - val_loss: 1546.5008 - val_mse: 1546.5009 - val_mae: 27.5592\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2539.6071 - mse: 2539.6072 - mae: 30.1253 - val_loss: 1539.9288 - val_mse: 1539.9288 - val_mae: 27.8294\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 525us/step - loss: 2554.9681 - mse: 2554.9675 - mae: 29.7824 - val_loss: 1541.2430 - val_mse: 1541.2429 - val_mae: 27.7297\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 481us/step - loss: 2529.6669 - mse: 2529.6663 - mae: 29.3587 - val_loss: 1537.0234 - val_mse: 1537.0233 - val_mae: 27.9352\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2532.3611 - mse: 2532.3613 - mae: 29.5083 - val_loss: 1532.6306 - val_mse: 1532.6306 - val_mae: 28.1587\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 549us/step - loss: 2516.7931 - mse: 2516.7922 - mae: 29.3630 - val_loss: 1547.6259 - val_mse: 1547.6259 - val_mae: 27.4892\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2557.5499 - mse: 2557.5503 - mae: 30.0258 - val_loss: 1547.6526 - val_mse: 1547.6527 - val_mae: 27.5205\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2541.8176 - mse: 2541.8176 - mae: 29.7248 - val_loss: 1542.5170 - val_mse: 1542.5171 - val_mae: 27.7040\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2469.5988 - mse: 2469.5984 - mae: 29.2836 - val_loss: 1533.1782 - val_mse: 1533.1781 - val_mae: 28.1283\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2538.2494 - mse: 2538.2488 - mae: 29.2537 - val_loss: 1536.5209 - val_mse: 1536.5210 - val_mae: 27.9273\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 537us/step - loss: 2538.1235 - mse: 2538.1223 - mae: 29.2212 - val_loss: 1540.1737 - val_mse: 1540.1737 - val_mae: 27.7387\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 541us/step - loss: 2430.2323 - mse: 2430.2317 - mae: 29.2263 - val_loss: 1534.0889 - val_mse: 1534.0886 - val_mae: 27.9972\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2531.2323 - mse: 2531.2322 - mae: 29.5421 - val_loss: 1540.9265 - val_mse: 1540.9265 - val_mae: 27.6622\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2528.1443 - mse: 2528.1443 - mae: 29.5390 - val_loss: 1536.9325 - val_mse: 1536.9326 - val_mae: 27.8518\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2472.8607 - mse: 2472.8601 - mae: 29.5548 - val_loss: 1533.6092 - val_mse: 1533.6091 - val_mae: 27.9873\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2478.6520 - mse: 2478.6521 - mae: 29.3138 - val_loss: 1536.5299 - val_mse: 1536.5300 - val_mae: 27.8105\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2474.9240 - mse: 2474.9250 - mae: 29.2572 - val_loss: 1535.2766 - val_mse: 1535.2766 - val_mae: 27.8415\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2489.3290 - mse: 2489.3289 - mae: 29.2809 - val_loss: 1532.4123 - val_mse: 1532.4124 - val_mae: 27.9204\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 501us/step - loss: 2522.6839 - mse: 2522.6838 - mae: 29.3962 - val_loss: 1532.3341 - val_mse: 1532.3340 - val_mae: 27.8999\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2505.7061 - mse: 2505.7063 - mae: 29.3950 - val_loss: 1535.5616 - val_mse: 1535.5619 - val_mae: 27.7679\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2557.3532 - mse: 2557.3530 - mae: 29.4469 - val_loss: 1534.4140 - val_mse: 1534.4139 - val_mae: 27.8861\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 490us/step - loss: 2536.8059 - mse: 2536.8049 - mae: 29.8054 - val_loss: 1532.8364 - val_mse: 1532.8364 - val_mae: 27.9028\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2506.7231 - mse: 2506.7229 - mae: 29.6094 - val_loss: 1541.1592 - val_mse: 1541.1592 - val_mae: 27.5165\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2531.5251 - mse: 2531.5254 - mae: 29.4608 - val_loss: 1531.7144 - val_mse: 1531.7144 - val_mae: 28.0038\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2512.1717 - mse: 2512.1724 - mae: 29.5608 - val_loss: 1536.7018 - val_mse: 1536.7019 - val_mae: 27.7620\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2546.0491 - mse: 2546.0493 - mae: 29.9917 - val_loss: 1533.8497 - val_mse: 1533.8499 - val_mae: 27.8864\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2484.2981 - mse: 2484.2981 - mae: 29.2369 - val_loss: 1544.0773 - val_mse: 1544.0774 - val_mae: 27.4746\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2532.3768 - mse: 2532.3772 - mae: 29.5512 - val_loss: 1534.8435 - val_mse: 1534.8435 - val_mae: 27.8881\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2467.8594 - mse: 2467.8599 - mae: 29.4541 - val_loss: 1539.1265 - val_mse: 1539.1266 - val_mae: 27.5937\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 522us/step - loss: 2507.8257 - mse: 2507.8269 - mae: 29.4829 - val_loss: 1536.7697 - val_mse: 1536.7697 - val_mae: 27.7101\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2594.4353 - mse: 2594.4355 - mae: 30.1983 - val_loss: 1536.0398 - val_mse: 1536.0397 - val_mae: 27.8111\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2513.9081 - mse: 2513.9089 - mae: 29.9407 - val_loss: 1533.2304 - val_mse: 1533.2306 - val_mae: 27.9503\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2540.3558 - mse: 2540.3562 - mae: 29.5600 - val_loss: 1541.7010 - val_mse: 1541.7008 - val_mae: 27.5749\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 536us/step - loss: 2466.9004 - mse: 2466.9011 - mae: 29.3894 - val_loss: 1542.6258 - val_mse: 1542.6259 - val_mae: 27.5061\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2532.1079 - mse: 2532.1079 - mae: 29.1575 - val_loss: 1534.1969 - val_mse: 1534.1971 - val_mae: 27.8457\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2472.2756 - mse: 2472.2756 - mae: 29.1299 - val_loss: 1535.1392 - val_mse: 1535.1390 - val_mae: 27.8045\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 522us/step - loss: 2461.3531 - mse: 2461.3530 - mae: 29.1968 - val_loss: 1529.8952 - val_mse: 1529.8953 - val_mae: 28.1370\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2406.2481 - mse: 2406.2483 - mae: 30.0607 - val_loss: 3701.0255 - val_mse: 3701.0259 - val_mae: 24.6831\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2408.9961 - mse: 2408.9961 - mae: 29.9111 - val_loss: 3702.1787 - val_mse: 3702.1787 - val_mae: 24.8804\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2323.3445 - mse: 2323.3450 - mae: 29.3457 - val_loss: 3703.3533 - val_mse: 3703.3530 - val_mae: 25.0453\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 539us/step - loss: 2436.6799 - mse: 2436.6804 - mae: 29.9326 - val_loss: 3701.2520 - val_mse: 3701.2524 - val_mae: 24.8313\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2418.3999 - mse: 2418.3992 - mae: 30.2087 - val_loss: 3699.8709 - val_mse: 3699.8708 - val_mae: 24.4757\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2389.9427 - mse: 2389.9434 - mae: 29.7783 - val_loss: 3702.6176 - val_mse: 3702.6174 - val_mae: 25.0060\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2430.2475 - mse: 2430.2476 - mae: 30.2499 - val_loss: 3702.5889 - val_mse: 3702.5891 - val_mae: 24.9619\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2395.7575 - mse: 2395.7571 - mae: 30.0854 - val_loss: 3700.8333 - val_mse: 3700.8330 - val_mae: 24.6523\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2396.3390 - mse: 2396.3391 - mae: 29.8642 - val_loss: 3702.1206 - val_mse: 3702.1208 - val_mae: 24.9326\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2393.4267 - mse: 2393.4268 - mae: 29.7615 - val_loss: 3702.9677 - val_mse: 3702.9678 - val_mae: 25.0505\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2391.7709 - mse: 2391.7712 - mae: 30.0564 - val_loss: 3703.9363 - val_mse: 3703.9365 - val_mae: 25.0901\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 520us/step - loss: 2463.9101 - mse: 2463.9104 - mae: 30.0261 - val_loss: 3699.5334 - val_mse: 3699.5327 - val_mae: 24.3919\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2378.0307 - mse: 2378.0300 - mae: 29.6343 - val_loss: 3701.6809 - val_mse: 3701.6812 - val_mae: 24.7878\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2377.3291 - mse: 2377.3298 - mae: 29.5238 - val_loss: 3699.5872 - val_mse: 3699.5869 - val_mae: 24.4089\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2384.0676 - mse: 2384.0679 - mae: 29.7861 - val_loss: 3701.1732 - val_mse: 3701.1736 - val_mae: 24.7023\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2407.3796 - mse: 2407.3794 - mae: 29.6608 - val_loss: 3699.5117 - val_mse: 3699.5117 - val_mae: 24.2050\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2397.2303 - mse: 2397.2312 - mae: 29.3883 - val_loss: 3700.3457 - val_mse: 3700.3467 - val_mae: 24.5346\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 525us/step - loss: 2378.6376 - mse: 2378.6370 - mae: 29.6038 - val_loss: 3702.8360 - val_mse: 3702.8367 - val_mae: 25.0204\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2375.4424 - mse: 2375.4429 - mae: 29.7997 - val_loss: 3701.8409 - val_mse: 3701.8403 - val_mae: 25.0020\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2406.5326 - mse: 2406.5320 - mae: 29.8961 - val_loss: 3704.2444 - val_mse: 3704.2446 - val_mae: 25.2705\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2399.2003 - mse: 2399.2002 - mae: 29.7864 - val_loss: 3700.5590 - val_mse: 3700.5596 - val_mae: 24.8081\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2374.5400 - mse: 2374.5393 - mae: 30.0535 - val_loss: 3698.1527 - val_mse: 3698.1519 - val_mae: 24.2820\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2431.8357 - mse: 2431.8354 - mae: 29.8226 - val_loss: 3699.7480 - val_mse: 3699.7480 - val_mae: 24.7696\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2376.3232 - mse: 2376.3225 - mae: 29.4222 - val_loss: 3699.8913 - val_mse: 3699.8916 - val_mae: 24.7866\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2390.6663 - mse: 2390.6658 - mae: 29.8491 - val_loss: 3698.5836 - val_mse: 3698.5837 - val_mae: 24.5158\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2386.1215 - mse: 2386.1221 - mae: 29.3236 - val_loss: 3699.8854 - val_mse: 3699.8845 - val_mae: 24.8202\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2380.4132 - mse: 2380.4136 - mae: 29.6638 - val_loss: 3698.9897 - val_mse: 3698.9890 - val_mae: 24.3779\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2432.5251 - mse: 2432.5251 - mae: 30.1534 - val_loss: 3699.4723 - val_mse: 3699.4714 - val_mae: 24.6071\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2398.3768 - mse: 2398.3770 - mae: 29.9549 - val_loss: 3702.9596 - val_mse: 3702.9600 - val_mae: 25.1387\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2402.1659 - mse: 2402.1658 - mae: 29.5941 - val_loss: 3701.4072 - val_mse: 3701.4072 - val_mae: 24.9435\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2398.5227 - mse: 2398.5225 - mae: 30.0373 - val_loss: 3702.8153 - val_mse: 3702.8152 - val_mae: 25.1716\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2430.8205 - mse: 2430.8206 - mae: 30.0878 - val_loss: 3701.7423 - val_mse: 3701.7422 - val_mae: 25.0073\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2431.2342 - mse: 2431.2334 - mae: 30.3665 - val_loss: 3700.6460 - val_mse: 3700.6470 - val_mae: 24.6965\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2368.3879 - mse: 2368.3889 - mae: 29.5726 - val_loss: 3706.2922 - val_mse: 3706.2935 - val_mae: 25.5877\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 513us/step - loss: 2382.9672 - mse: 2382.9675 - mae: 29.8139 - val_loss: 3700.7144 - val_mse: 3700.7139 - val_mae: 24.8573\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2358.4714 - mse: 2358.4714 - mae: 29.3637 - val_loss: 3700.5398 - val_mse: 3700.5398 - val_mae: 24.6683\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2435.7556 - mse: 2435.7551 - mae: 29.7631 - val_loss: 3700.2837 - val_mse: 3700.2832 - val_mae: 24.8037\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2367.1680 - mse: 2367.1680 - mae: 29.5331 - val_loss: 3698.4254 - val_mse: 3698.4250 - val_mae: 24.5530\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 540us/step - loss: 2400.3600 - mse: 2400.3596 - mae: 29.8684 - val_loss: 3700.6477 - val_mse: 3700.6487 - val_mae: 24.7712\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2334.6905 - mse: 2334.6907 - mae: 29.4411 - val_loss: 3706.2568 - val_mse: 3706.2566 - val_mae: 25.4627\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2393.4119 - mse: 2393.4126 - mae: 29.9222 - val_loss: 3701.7056 - val_mse: 3701.7058 - val_mae: 24.9474\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2372.0337 - mse: 2372.0339 - mae: 29.7570 - val_loss: 3702.5989 - val_mse: 3702.5981 - val_mae: 25.1241\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 521us/step - loss: 2389.0723 - mse: 2389.0720 - mae: 29.6675 - val_loss: 3704.8312 - val_mse: 3704.8320 - val_mae: 25.3658\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2380.7964 - mse: 2380.7971 - mae: 29.4752 - val_loss: 3701.6722 - val_mse: 3701.6716 - val_mae: 24.9030\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2412.3388 - mse: 2412.3384 - mae: 30.1227 - val_loss: 3699.8518 - val_mse: 3699.8521 - val_mae: 24.4208\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2379.4577 - mse: 2379.4583 - mae: 29.5055 - val_loss: 3700.8757 - val_mse: 3700.8755 - val_mae: 24.6780\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 534us/step - loss: 2412.9654 - mse: 2412.9658 - mae: 29.8926 - val_loss: 3700.1021 - val_mse: 3700.1021 - val_mae: 24.5637\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 518us/step - loss: 2323.1751 - mse: 2323.1743 - mae: 29.3021 - val_loss: 3702.3356 - val_mse: 3702.3359 - val_mae: 24.9154\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 485us/step - loss: 2387.7118 - mse: 2387.7107 - mae: 29.7640 - val_loss: 3703.4916 - val_mse: 3703.4917 - val_mae: 25.1589\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 517us/step - loss: 2361.8522 - mse: 2361.8530 - mae: 29.3480 - val_loss: 3703.1511 - val_mse: 3703.1516 - val_mae: 25.0739\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 645us/step - loss: 2404.1150 - mse: 2404.1152 - mae: 29.8295 - val_loss: 3705.7467 - val_mse: 3705.7463 - val_mae: 25.4104\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2369.3996 - mse: 2369.3992 - mae: 29.4350 - val_loss: 3704.1598 - val_mse: 3704.1597 - val_mae: 25.1607\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2398.8422 - mse: 2398.8420 - mae: 29.7180 - val_loss: 3701.4966 - val_mse: 3701.4966 - val_mae: 24.6432\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2348.1094 - mse: 2348.1091 - mae: 29.5472 - val_loss: 3702.9222 - val_mse: 3702.9216 - val_mae: 24.9403\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2344.5219 - mse: 2344.5220 - mae: 29.1625 - val_loss: 3702.7376 - val_mse: 3702.7375 - val_mae: 24.8308\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2434.8287 - mse: 2434.8286 - mae: 29.9397 - val_loss: 3702.3653 - val_mse: 3702.3647 - val_mae: 24.9146\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2373.6877 - mse: 2373.6875 - mae: 29.6858 - val_loss: 3702.2011 - val_mse: 3702.2014 - val_mae: 24.9990\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2431.3013 - mse: 2431.3013 - mae: 29.7305 - val_loss: 3701.1291 - val_mse: 3701.1289 - val_mae: 24.8234\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2403.3790 - mse: 2403.3784 - mae: 29.4898 - val_loss: 3702.4099 - val_mse: 3702.4104 - val_mae: 24.9937\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2384.4192 - mse: 2384.4192 - mae: 29.5697 - val_loss: 3702.7705 - val_mse: 3702.7710 - val_mae: 25.0132\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 522us/step - loss: 2371.3565 - mse: 2371.3562 - mae: 29.6434 - val_loss: 3703.1555 - val_mse: 3703.1560 - val_mae: 24.9103\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 502us/step - loss: 2431.1668 - mse: 2431.1663 - mae: 29.6214 - val_loss: 3704.7750 - val_mse: 3704.7737 - val_mae: 25.1359\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2359.8733 - mse: 2359.8738 - mae: 29.5372 - val_loss: 3702.1316 - val_mse: 3702.1309 - val_mae: 24.6930\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2356.6060 - mse: 2356.6055 - mae: 29.2619 - val_loss: 3702.1678 - val_mse: 3702.1672 - val_mae: 24.8304\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2389.1988 - mse: 2389.1992 - mae: 29.6732 - val_loss: 3701.1682 - val_mse: 3701.1680 - val_mae: 24.4795\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2373.7564 - mse: 2373.7568 - mae: 29.2742 - val_loss: 3705.2594 - val_mse: 3705.2593 - val_mae: 25.1838\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2390.8375 - mse: 2390.8381 - mae: 29.6644 - val_loss: 3704.0832 - val_mse: 3704.0823 - val_mae: 25.0346\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2373.3625 - mse: 2373.3616 - mae: 29.4725 - val_loss: 3704.9884 - val_mse: 3704.9885 - val_mae: 25.1699\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2318.7821 - mse: 2318.7822 - mae: 29.5801 - val_loss: 3706.3627 - val_mse: 3706.3628 - val_mae: 25.3852\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2381.1605 - mse: 2381.1611 - mae: 29.8771 - val_loss: 3703.4599 - val_mse: 3703.4595 - val_mae: 24.9744\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2385.0948 - mse: 2385.0945 - mae: 29.5365 - val_loss: 3700.6403 - val_mse: 3700.6401 - val_mae: 24.6046\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 530us/step - loss: 2385.2896 - mse: 2385.2896 - mae: 29.3608 - val_loss: 3701.3847 - val_mse: 3701.3853 - val_mae: 24.7157\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 541us/step - loss: 2415.1592 - mse: 2415.1584 - mae: 29.8075 - val_loss: 3701.2253 - val_mse: 3701.2256 - val_mae: 24.8930\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2370.1016 - mse: 2370.1008 - mae: 29.5877 - val_loss: 3702.2084 - val_mse: 3702.2087 - val_mae: 25.0668\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2368.3996 - mse: 2368.3992 - mae: 29.3527 - val_loss: 3702.8383 - val_mse: 3702.8384 - val_mae: 25.1653\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2415.3187 - mse: 2415.3186 - mae: 29.7374 - val_loss: 3701.4454 - val_mse: 3701.4458 - val_mae: 25.0633\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2377.2944 - mse: 2377.2939 - mae: 29.3776 - val_loss: 3702.5725 - val_mse: 3702.5720 - val_mae: 25.2046\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2398.4741 - mse: 2398.4751 - mae: 29.7437 - val_loss: 3699.3997 - val_mse: 3699.4001 - val_mae: 24.6043\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2391.3684 - mse: 2391.3687 - mae: 29.6400 - val_loss: 3705.1315 - val_mse: 3705.1316 - val_mae: 25.4548\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2340.0842 - mse: 2340.0842 - mae: 29.3619 - val_loss: 3699.8039 - val_mse: 3699.8035 - val_mae: 24.6524\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2781.9283 - mse: 2781.9294 - mae: 29.3637 - val_loss: 2491.1507 - val_mse: 2491.1511 - val_mae: 26.9806\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2750.1257 - mse: 2750.1257 - mae: 29.0138 - val_loss: 2500.0855 - val_mse: 2500.0854 - val_mae: 26.9728\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2796.0204 - mse: 2796.0210 - mae: 29.2858 - val_loss: 2492.9058 - val_mse: 2492.9060 - val_mae: 27.3248\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 537us/step - loss: 2741.5215 - mse: 2741.5210 - mae: 28.8535 - val_loss: 2488.7915 - val_mse: 2488.7913 - val_mae: 27.5040\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2768.3150 - mse: 2768.3152 - mae: 29.0026 - val_loss: 2495.6775 - val_mse: 2495.6775 - val_mae: 27.2604\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2766.7264 - mse: 2766.7266 - mae: 29.3207 - val_loss: 2492.3257 - val_mse: 2492.3254 - val_mae: 27.3200\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 496us/step - loss: 2768.4870 - mse: 2768.4866 - mae: 28.8241 - val_loss: 2506.4068 - val_mse: 2506.4062 - val_mae: 27.1737\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 537us/step - loss: 2745.6579 - mse: 2745.6577 - mae: 28.9599 - val_loss: 2501.6875 - val_mse: 2501.6875 - val_mae: 27.6757\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2741.9531 - mse: 2741.9529 - mae: 28.7298 - val_loss: 2499.3307 - val_mse: 2499.3306 - val_mae: 27.9806\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2809.6005 - mse: 2809.6006 - mae: 29.2758 - val_loss: 2499.4550 - val_mse: 2499.4556 - val_mae: 27.8579\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2773.2068 - mse: 2773.2068 - mae: 29.3844 - val_loss: 2507.1144 - val_mse: 2507.1140 - val_mae: 27.4269\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2820.6942 - mse: 2820.6943 - mae: 29.2311 - val_loss: 2511.4887 - val_mse: 2511.4890 - val_mae: 27.3262\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2750.4918 - mse: 2750.4910 - mae: 29.1792 - val_loss: 2501.6693 - val_mse: 2501.6694 - val_mae: 27.7237\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2759.5662 - mse: 2759.5664 - mae: 28.9919 - val_loss: 2503.1341 - val_mse: 2503.1345 - val_mae: 27.4599\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2729.6266 - mse: 2729.6267 - mae: 28.7993 - val_loss: 2506.5305 - val_mse: 2506.5308 - val_mae: 27.4371\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2786.4910 - mse: 2786.4912 - mae: 29.1847 - val_loss: 2512.1590 - val_mse: 2512.1592 - val_mae: 27.2966\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2744.0973 - mse: 2744.0972 - mae: 28.8183 - val_loss: 2502.8133 - val_mse: 2502.8140 - val_mae: 27.5805\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2771.5374 - mse: 2771.5374 - mae: 28.7822 - val_loss: 2509.3747 - val_mse: 2509.3748 - val_mae: 27.2956\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2728.9041 - mse: 2728.9045 - mae: 29.0278 - val_loss: 2502.7117 - val_mse: 2502.7117 - val_mae: 27.6710\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2770.3250 - mse: 2770.3254 - mae: 29.6219 - val_loss: 2520.5852 - val_mse: 2520.5857 - val_mae: 27.1401\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2737.8029 - mse: 2737.8032 - mae: 28.8469 - val_loss: 2504.9779 - val_mse: 2504.9780 - val_mae: 27.7862\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2788.0933 - mse: 2788.0930 - mae: 29.0744 - val_loss: 2507.7744 - val_mse: 2507.7742 - val_mae: 27.5618\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2782.2388 - mse: 2782.2385 - mae: 29.0384 - val_loss: 2510.1608 - val_mse: 2510.1609 - val_mae: 27.3737\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2833.2504 - mse: 2833.2502 - mae: 29.6057 - val_loss: 2516.5528 - val_mse: 2516.5525 - val_mae: 27.3574\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2731.6474 - mse: 2731.6467 - mae: 28.6024 - val_loss: 2510.7867 - val_mse: 2510.7869 - val_mae: 27.6808\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2743.7639 - mse: 2743.7639 - mae: 28.8146 - val_loss: 2505.0618 - val_mse: 2505.0615 - val_mae: 27.6283\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2762.3127 - mse: 2762.3130 - mae: 29.3638 - val_loss: 2506.2094 - val_mse: 2506.2092 - val_mae: 27.6651\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2724.7585 - mse: 2724.7578 - mae: 28.6734 - val_loss: 2510.3680 - val_mse: 2510.3679 - val_mae: 27.5565\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2751.2679 - mse: 2751.2673 - mae: 28.9416 - val_loss: 2498.9284 - val_mse: 2498.9287 - val_mae: 27.8774\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2730.6426 - mse: 2730.6426 - mae: 28.7620 - val_loss: 2503.4959 - val_mse: 2503.4963 - val_mae: 27.5399\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2693.2104 - mse: 2693.2104 - mae: 28.5606 - val_loss: 2498.4295 - val_mse: 2498.4297 - val_mae: 27.7890\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2759.1811 - mse: 2759.1816 - mae: 28.9976 - val_loss: 2506.3528 - val_mse: 2506.3530 - val_mae: 27.4018\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2787.1125 - mse: 2787.1116 - mae: 29.2862 - val_loss: 2505.5050 - val_mse: 2505.5049 - val_mae: 27.5299\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2672.3722 - mse: 2672.3735 - mae: 28.5432 - val_loss: 2507.7797 - val_mse: 2507.7798 - val_mae: 27.7878\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2780.8638 - mse: 2780.8647 - mae: 28.9069 - val_loss: 2506.5304 - val_mse: 2506.5303 - val_mae: 27.7730\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 508us/step - loss: 2769.5591 - mse: 2769.5583 - mae: 29.0712 - val_loss: 2507.4947 - val_mse: 2507.4946 - val_mae: 27.6910\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2752.4447 - mse: 2752.4448 - mae: 29.0163 - val_loss: 2516.9249 - val_mse: 2516.9248 - val_mae: 27.3151\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 538us/step - loss: 2713.4690 - mse: 2713.4695 - mae: 28.8449 - val_loss: 2505.1552 - val_mse: 2505.1553 - val_mae: 27.7969\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2740.1285 - mse: 2740.1289 - mae: 28.7217 - val_loss: 2511.0063 - val_mse: 2511.0056 - val_mae: 27.9674\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2721.6490 - mse: 2721.6499 - mae: 29.0022 - val_loss: 2513.9994 - val_mse: 2513.9995 - val_mae: 27.8396\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2739.3335 - mse: 2739.3335 - mae: 28.6494 - val_loss: 2520.6119 - val_mse: 2520.6118 - val_mae: 27.5630\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2765.5490 - mse: 2765.5493 - mae: 28.8593 - val_loss: 2514.7729 - val_mse: 2514.7729 - val_mae: 27.5812\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 546us/step - loss: 2753.1902 - mse: 2753.1899 - mae: 28.8395 - val_loss: 2514.1644 - val_mse: 2514.1643 - val_mae: 27.5457\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2753.3923 - mse: 2753.3918 - mae: 28.7582 - val_loss: 2514.4474 - val_mse: 2514.4470 - val_mae: 27.5578\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2734.8946 - mse: 2734.8948 - mae: 28.8499 - val_loss: 2509.8557 - val_mse: 2509.8560 - val_mae: 27.6036\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2718.7312 - mse: 2718.7305 - mae: 28.9241 - val_loss: 2509.2213 - val_mse: 2509.2209 - val_mae: 27.5890\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 533us/step - loss: 2783.1566 - mse: 2783.1572 - mae: 29.0004 - val_loss: 2507.8596 - val_mse: 2507.8596 - val_mae: 27.6802\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2730.6286 - mse: 2730.6287 - mae: 28.9602 - val_loss: 2506.7191 - val_mse: 2506.7190 - val_mae: 27.7113\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 468us/step - loss: 2720.2150 - mse: 2720.2144 - mae: 28.4652 - val_loss: 2512.6465 - val_mse: 2512.6467 - val_mae: 27.4341\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2768.5472 - mse: 2768.5469 - mae: 28.9112 - val_loss: 2520.3425 - val_mse: 2520.3428 - val_mae: 27.4028\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 536us/step - loss: 2768.0659 - mse: 2768.0662 - mae: 28.9713 - val_loss: 2507.8394 - val_mse: 2507.8386 - val_mae: 27.8418\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2793.0644 - mse: 2793.0652 - mae: 29.1114 - val_loss: 2504.3085 - val_mse: 2504.3086 - val_mae: 28.2180\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2799.9998 - mse: 2799.9995 - mae: 29.3880 - val_loss: 2510.2909 - val_mse: 2510.2908 - val_mae: 27.9208\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2736.1987 - mse: 2736.1992 - mae: 29.1292 - val_loss: 2511.8824 - val_mse: 2511.8821 - val_mae: 27.8900\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2779.2991 - mse: 2779.2998 - mae: 29.2359 - val_loss: 2503.4900 - val_mse: 2503.4900 - val_mae: 28.0888\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 536us/step - loss: 2762.5310 - mse: 2762.5312 - mae: 28.8250 - val_loss: 2510.8704 - val_mse: 2510.8706 - val_mae: 27.9024\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2755.7900 - mse: 2755.7900 - mae: 28.9403 - val_loss: 2509.9647 - val_mse: 2509.9639 - val_mae: 28.0078\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2711.9245 - mse: 2711.9241 - mae: 28.8255 - val_loss: 2511.8399 - val_mse: 2511.8394 - val_mae: 27.8448\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 483us/step - loss: 2715.4954 - mse: 2715.4954 - mae: 28.9326 - val_loss: 2512.7714 - val_mse: 2512.7720 - val_mae: 27.7806\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2735.7574 - mse: 2735.7581 - mae: 28.7649 - val_loss: 2510.5556 - val_mse: 2510.5559 - val_mae: 27.8110\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 547us/step - loss: 2728.2035 - mse: 2728.2034 - mae: 28.8230 - val_loss: 2520.8936 - val_mse: 2520.8933 - val_mae: 27.4597\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 523us/step - loss: 2768.2070 - mse: 2768.2065 - mae: 28.9780 - val_loss: 2509.0403 - val_mse: 2509.0405 - val_mae: 28.0584\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2714.4624 - mse: 2714.4622 - mae: 28.7113 - val_loss: 2516.7285 - val_mse: 2516.7288 - val_mae: 27.7744\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2732.8756 - mse: 2732.8750 - mae: 28.6911 - val_loss: 2518.9344 - val_mse: 2518.9343 - val_mae: 27.7087\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2702.1027 - mse: 2702.1018 - mae: 28.7648 - val_loss: 2519.5275 - val_mse: 2519.5276 - val_mae: 27.7041\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2728.9617 - mse: 2728.9612 - mae: 28.6053 - val_loss: 2514.4467 - val_mse: 2514.4463 - val_mae: 27.9589\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 525us/step - loss: 2697.2053 - mse: 2697.2058 - mae: 28.6981 - val_loss: 2519.6639 - val_mse: 2519.6643 - val_mae: 27.8534\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 539us/step - loss: 2681.0612 - mse: 2681.0601 - mae: 28.6399 - val_loss: 2521.2397 - val_mse: 2521.2402 - val_mae: 27.9088\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2722.8952 - mse: 2722.8943 - mae: 29.0250 - val_loss: 2520.2798 - val_mse: 2520.2798 - val_mae: 27.8474\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 534us/step - loss: 2746.0215 - mse: 2746.0212 - mae: 29.0629 - val_loss: 2529.3311 - val_mse: 2529.3315 - val_mae: 27.6856\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 503us/step - loss: 2679.2555 - mse: 2679.2563 - mae: 28.6294 - val_loss: 2525.2351 - val_mse: 2525.2349 - val_mae: 27.9698\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 450us/step - loss: 2778.3198 - mse: 2778.3191 - mae: 29.4106 - val_loss: 2530.8358 - val_mse: 2530.8357 - val_mae: 27.5466\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 419us/step - loss: 2748.0208 - mse: 2748.0208 - mae: 28.9552 - val_loss: 2526.8867 - val_mse: 2526.8870 - val_mae: 27.6549\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2748.6615 - mse: 2748.6609 - mae: 29.0591 - val_loss: 2523.9016 - val_mse: 2523.9016 - val_mae: 27.6790\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2766.6973 - mse: 2766.6978 - mae: 28.9596 - val_loss: 2526.9939 - val_mse: 2526.9939 - val_mae: 27.5390\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2728.7544 - mse: 2728.7539 - mae: 28.9099 - val_loss: 2518.9469 - val_mse: 2518.9473 - val_mae: 27.8872\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2716.4995 - mse: 2716.4995 - mae: 28.9325 - val_loss: 2526.0370 - val_mse: 2526.0376 - val_mae: 27.4132\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2779.4318 - mse: 2779.4321 - mae: 28.6593 - val_loss: 2519.4574 - val_mse: 2519.4573 - val_mae: 27.7219\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2707.5935 - mse: 2707.5928 - mae: 28.7221 - val_loss: 2524.4267 - val_mse: 2524.4265 - val_mae: 27.5170\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2713.3959 - mse: 2713.3958 - mae: 28.7614 - val_loss: 2523.3420 - val_mse: 2523.3418 - val_mae: 27.5644\n"
     ]
    }
   ],
   "source": [
    "# data set to append here (start with first feature added)\n",
    "X_ = X.loc[:,'PrevDay']\n",
    "\n",
    "# do first prediction\n",
    "X_.fillna(method = 'ffill', inplace = True)\n",
    "y.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "X_ = X_.astype('float64')\n",
    "X_ = X_.round(20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_, y, test_size = 0.15, shuffle = False)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# possible debug\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "def regressor_tunning(n_hidden = 5, \n",
    "                      n_neurons = 40, \n",
    "                      kernel_initializer = \"he_normal\",\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = n_neurons, input_dim = 1))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dropout(rate = 0.3))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(units = n_neurons))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.3))\n",
    "    model.add(Dense(units = 1, activation = 'linear'))\n",
    "    optimizer = optimizers.Adamax(lr = 0.001)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = splits)    \n",
    "regressor = regressor_tunning()\n",
    "\n",
    "# train model\n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "    y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "    regressor.fit(X_train_split, y_train_split,  \n",
    "                         shuffle = False, \n",
    "                         validation_split = 0.2,\n",
    "                         batch_size = 20, \n",
    "                         epochs = epochs)\n",
    "\n",
    "# make predictions and evaluate for all regions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS EVALUATION (1) for the whole test set\n",
    "# =============================================================================\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "# calculate metrics\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mae_error = mae(y_test, y_pred)\n",
    "\n",
    "# append to list\n",
    "rmse_gen.append(rmse_error)\n",
    "mae_gen.append(mae_error)\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS EVALUATION (2) on spike regions\n",
    "# =============================================================================\n",
    "\n",
    "# download spike indication binary set\n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "# append ot lists\n",
    "rmse_spi.append(rmse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "\n",
    "# =============================================================================\n",
    "# METRIC EVALUATION (3) on normal regions\n",
    "# =============================================================================\n",
    "\n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "# append to list\n",
    "rmse_nor.append(rmse_normal)\n",
    "mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply loop for the rest of the list of features with condition of improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 13328.6912 - mse: 13328.6914 - mae: 109.9012 - val_loss: 34613.4478 - val_mse: 34613.4492 - val_mae: 132.7116\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 13172.5818 - mse: 13172.5811 - mae: 109.1917 - val_loss: 34309.9562 - val_mse: 34309.9570 - val_mae: 131.5699\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 12727.3288 - mse: 12727.3301 - mae: 107.1292 - val_loss: 33350.7992 - val_mse: 33350.7969 - val_mae: 127.8952\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 11334.0678 - mse: 11334.0674 - mae: 100.5258 - val_loss: 30683.7009 - val_mse: 30683.6992 - val_mae: 117.0720\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 8188.1429 - mse: 8188.1431 - mae: 82.7140 - val_loss: 24728.8058 - val_mse: 24728.8066 - val_mae: 88.2368\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 559us/step - loss: 3479.0948 - mse: 3479.0942 - mae: 45.6153 - val_loss: 17944.3528 - val_mse: 17944.3516 - val_mae: 36.2923\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 2737.8988 - mse: 2737.8989 - mae: 37.3577 - val_loss: 17893.8765 - val_mse: 17893.8770 - val_mae: 36.0062\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 473us/step - loss: 2547.1818 - mse: 2547.1814 - mae: 35.5076 - val_loss: 17772.1322 - val_mse: 17772.1328 - val_mae: 35.3725\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2321.4703 - mse: 2321.4700 - mae: 34.3987 - val_loss: 17655.9209 - val_mse: 17655.9219 - val_mae: 35.0485\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2517.0511 - mse: 2517.0515 - mae: 36.0366 - val_loss: 17858.6708 - val_mse: 17858.6719 - val_mae: 35.8139\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2535.6247 - mse: 2535.6248 - mae: 35.9681 - val_loss: 18026.9404 - val_mse: 18026.9395 - val_mae: 36.8216\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2542.0569 - mse: 2542.0569 - mae: 36.0204 - val_loss: 17834.0013 - val_mse: 17834.0020 - val_mae: 35.6778\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 680us/step - loss: 2410.8322 - mse: 2410.8320 - mae: 34.5632 - val_loss: 17888.7371 - val_mse: 17888.7383 - val_mae: 35.9800\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2669.7038 - mse: 2669.7041 - mae: 37.0660 - val_loss: 18007.5042 - val_mse: 18007.5059 - val_mae: 36.6848\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2469.4759 - mse: 2469.4758 - mae: 35.9064 - val_loss: 17805.8574 - val_mse: 17805.8574 - val_mae: 35.5278\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 539us/step - loss: 2344.7159 - mse: 2344.7161 - mae: 34.9828 - val_loss: 17879.0122 - val_mse: 17879.0117 - val_mae: 35.9288\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 2417.4653 - mse: 2417.4656 - mae: 33.8734 - val_loss: 17787.5588 - val_mse: 17787.5586 - val_mae: 35.4392\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 2375.1461 - mse: 2375.1462 - mae: 34.8364 - val_loss: 17963.6504 - val_mse: 17963.6504 - val_mae: 36.4011\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2537.9074 - mse: 2537.9072 - mae: 35.7673 - val_loss: 17957.7949 - val_mse: 17957.7969 - val_mae: 36.3658\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2418.6217 - mse: 2418.6216 - mae: 34.5544 - val_loss: 17739.3416 - val_mse: 17739.3418 - val_mae: 35.2726\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 2466.4008 - mse: 2466.4006 - mae: 35.9162 - val_loss: 18044.4919 - val_mse: 18044.4922 - val_mae: 36.9278\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 563us/step - loss: 2119.7423 - mse: 2119.7422 - mae: 33.4039 - val_loss: 17873.5845 - val_mse: 17873.5859 - val_mae: 35.8974\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2502.1813 - mse: 2502.1809 - mae: 35.1580 - val_loss: 17805.1528 - val_mse: 17805.1543 - val_mae: 35.5218\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2310.9950 - mse: 2310.9951 - mae: 34.3779 - val_loss: 17771.8001 - val_mse: 17771.8008 - val_mae: 35.3805\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2297.1290 - mse: 2297.1287 - mae: 34.0899 - val_loss: 17607.7034 - val_mse: 17607.7051 - val_mae: 35.0888\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2374.8659 - mse: 2374.8657 - mae: 35.0916 - val_loss: 17823.3565 - val_mse: 17823.3574 - val_mae: 35.6182\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 2238.9017 - mse: 2238.9019 - mae: 33.4854 - val_loss: 17806.2081 - val_mse: 17806.2070 - val_mae: 35.5245\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2431.4301 - mse: 2431.4302 - mae: 34.2933 - val_loss: 17877.3012 - val_mse: 17877.3008 - val_mae: 35.9162\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 2384.6796 - mse: 2384.6794 - mae: 34.2681 - val_loss: 17777.5166 - val_mse: 17777.5156 - val_mae: 35.4128\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 565us/step - loss: 2526.7266 - mse: 2526.7266 - mae: 35.4305 - val_loss: 18126.7509 - val_mse: 18126.7500 - val_mae: 37.5007\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2275.9836 - mse: 2275.9834 - mae: 32.8749 - val_loss: 17734.9703 - val_mse: 17734.9688 - val_mae: 35.3036\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 692us/step - loss: 2255.3787 - mse: 2255.3784 - mae: 33.0146 - val_loss: 17833.4179 - val_mse: 17833.4180 - val_mae: 35.6712\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2372.5239 - mse: 2372.5237 - mae: 34.1313 - val_loss: 17801.2695 - val_mse: 17801.2695 - val_mae: 35.5114\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 692us/step - loss: 2254.9209 - mse: 2254.9209 - mae: 33.5891 - val_loss: 17628.1900 - val_mse: 17628.1895 - val_mae: 35.1697\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2178.4949 - mse: 2178.4946 - mae: 33.0482 - val_loss: 17656.2610 - val_mse: 17656.2617 - val_mae: 35.2104\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 642us/step - loss: 2132.7230 - mse: 2132.7227 - mae: 32.0593 - val_loss: 17776.7784 - val_mse: 17776.7773 - val_mae: 35.4310\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 706us/step - loss: 2092.3908 - mse: 2092.3909 - mae: 31.7223 - val_loss: 17701.8338 - val_mse: 17701.8340 - val_mae: 35.2785\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2277.1007 - mse: 2277.1008 - mae: 33.1102 - val_loss: 17719.8718 - val_mse: 17719.8711 - val_mae: 35.3118\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2182.5729 - mse: 2182.5730 - mae: 33.4564 - val_loss: 17793.8479 - val_mse: 17793.8457 - val_mae: 35.4994\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2243.1715 - mse: 2243.1716 - mae: 33.0771 - val_loss: 17793.9084 - val_mse: 17793.9082 - val_mae: 35.5050\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 2233.1842 - mse: 2233.1843 - mae: 33.1646 - val_loss: 17775.6469 - val_mse: 17775.6465 - val_mae: 35.4498\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2350.4904 - mse: 2350.4905 - mae: 34.5950 - val_loss: 17802.4691 - val_mse: 17802.4688 - val_mae: 35.5517\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2280.4028 - mse: 2280.4026 - mae: 33.3107 - val_loss: 17708.9588 - val_mse: 17708.9570 - val_mae: 35.3392\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 2312.8868 - mse: 2312.8867 - mae: 33.2731 - val_loss: 17869.4292 - val_mse: 17869.4297 - val_mae: 35.8711\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 2359.4855 - mse: 2359.4858 - mae: 34.9622 - val_loss: 17817.4562 - val_mse: 17817.4570 - val_mae: 35.6268\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2152.5169 - mse: 2152.5171 - mae: 32.4278 - val_loss: 17600.1357 - val_mse: 17600.1367 - val_mae: 35.2564\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2002.0156 - mse: 2002.0155 - mae: 31.3797 - val_loss: 17780.4721 - val_mse: 17780.4727 - val_mae: 35.4979\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 1973.9262 - mse: 1973.9260 - mae: 29.8964 - val_loss: 17659.2926 - val_mse: 17659.2930 - val_mae: 35.3155\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 480us/step - loss: 2257.2492 - mse: 2257.2493 - mae: 33.5371 - val_loss: 17785.1047 - val_mse: 17785.1035 - val_mae: 35.5262\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 2059.9009 - mse: 2059.9009 - mae: 31.7458 - val_loss: 17755.0194 - val_mse: 17755.0195 - val_mae: 35.4604\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2096.1794 - mse: 2096.1794 - mae: 31.2900 - val_loss: 17660.5026 - val_mse: 17660.5020 - val_mae: 35.3407\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2355.6523 - mse: 2355.6523 - mae: 34.1922 - val_loss: 17696.0983 - val_mse: 17696.0977 - val_mae: 35.3897\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 682us/step - loss: 2353.0165 - mse: 2353.0166 - mae: 33.4774 - val_loss: 17670.2617 - val_mse: 17670.2617 - val_mae: 35.3625\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2166.1717 - mse: 2166.1714 - mae: 31.5667 - val_loss: 17842.8277 - val_mse: 17842.8301 - val_mae: 35.7719\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 542us/step - loss: 2089.5462 - mse: 2089.5461 - mae: 31.8035 - val_loss: 17652.2810 - val_mse: 17652.2793 - val_mae: 35.3675\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 1936.9961 - mse: 1936.9958 - mae: 30.9634 - val_loss: 17883.9979 - val_mse: 17883.9980 - val_mae: 35.9698\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2035.6245 - mse: 2035.6245 - mae: 32.2382 - val_loss: 17614.8709 - val_mse: 17614.8711 - val_mae: 35.3626\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 2163.0523 - mse: 2163.0522 - mae: 32.4038 - val_loss: 17921.1864 - val_mse: 17921.1875 - val_mae: 36.1486\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 405us/step - loss: 2210.3145 - mse: 2210.3142 - mae: 32.1142 - val_loss: 17739.4152 - val_mse: 17739.4160 - val_mae: 35.4989\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 426us/step - loss: 2103.8337 - mse: 2103.8335 - mae: 31.5160 - val_loss: 17836.5441 - val_mse: 17836.5449 - val_mae: 35.7702\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 533us/step - loss: 2093.2723 - mse: 2093.2725 - mae: 30.4047 - val_loss: 17592.3952 - val_mse: 17592.3965 - val_mae: 35.3869\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 522us/step - loss: 2062.7180 - mse: 2062.7175 - mae: 31.1880 - val_loss: 17848.3159 - val_mse: 17848.3164 - val_mae: 35.8290\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 504us/step - loss: 2160.0171 - mse: 2160.0171 - mae: 31.7230 - val_loss: 17587.1749 - val_mse: 17587.1738 - val_mae: 35.4042\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 453us/step - loss: 2119.2843 - mse: 2119.2844 - mae: 31.0648 - val_loss: 17672.1549 - val_mse: 17672.1562 - val_mae: 35.4583\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 493us/step - loss: 1947.1029 - mse: 1947.1029 - mae: 30.9836 - val_loss: 17616.3630 - val_mse: 17616.3633 - val_mae: 35.4325\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2025.5488 - mse: 2025.5485 - mae: 30.7326 - val_loss: 17867.8601 - val_mse: 17867.8594 - val_mae: 35.9302\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 1934.5209 - mse: 1934.5206 - mae: 29.6429 - val_loss: 17482.0626 - val_mse: 17482.0625 - val_mae: 35.5778\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 2154.9165 - mse: 2154.9163 - mae: 33.1394 - val_loss: 17757.5655 - val_mse: 17757.5664 - val_mae: 35.5886\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 2018.5920 - mse: 2018.5920 - mae: 30.3045 - val_loss: 17767.0131 - val_mse: 17767.0117 - val_mae: 35.6097\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 1947.4157 - mse: 1947.4158 - mae: 30.9109 - val_loss: 17654.8106 - val_mse: 17654.8125 - val_mae: 35.4992\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 1881.7391 - mse: 1881.7390 - mae: 29.8310 - val_loss: 17712.2376 - val_mse: 17712.2363 - val_mae: 35.5480\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2032.8770 - mse: 2032.8770 - mae: 31.3162 - val_loss: 17753.5624 - val_mse: 17753.5625 - val_mae: 35.6114\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2156.5962 - mse: 2156.5962 - mae: 32.1279 - val_loss: 17610.4220 - val_mse: 17610.4219 - val_mae: 35.5155\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2040.2440 - mse: 2040.2440 - mae: 30.5485 - val_loss: 17648.4405 - val_mse: 17648.4395 - val_mae: 35.5252\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 780us/step - loss: 2055.0979 - mse: 2055.0977 - mae: 30.4914 - val_loss: 17618.5852 - val_mse: 17618.5840 - val_mae: 35.5307\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 1989.3972 - mse: 1989.3971 - mae: 30.3543 - val_loss: 17925.1118 - val_mse: 17925.1113 - val_mae: 36.2137\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 667us/step - loss: 2140.9173 - mse: 2140.9172 - mae: 31.1573 - val_loss: 17628.3584 - val_mse: 17628.3574 - val_mae: 35.5532\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 642us/step - loss: 1929.0835 - mse: 1929.0836 - mae: 31.1350 - val_loss: 17661.1210 - val_mse: 17661.1191 - val_mae: 35.5656\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 529us/step - loss: 1980.4584 - mse: 1980.4583 - mae: 30.4114 - val_loss: 17552.0904 - val_mse: 17552.0898 - val_mae: 35.6213\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 1904.4434 - mse: 1904.4434 - mae: 29.5975 - val_loss: 17748.7831 - val_mse: 17748.7812 - val_mae: 35.6579\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4332.5047 - mse: 4332.5034 - mae: 35.2873 - val_loss: 2058.5667 - val_mse: 2058.5664 - val_mae: 31.3942\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4218.2513 - mse: 4218.2510 - mae: 35.3479 - val_loss: 2206.6002 - val_mse: 2206.6003 - val_mae: 31.7067\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4411.8292 - mse: 4411.8286 - mae: 35.7024 - val_loss: 2164.1181 - val_mse: 2164.1182 - val_mae: 31.5775\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 544us/step - loss: 4134.7055 - mse: 4134.7056 - mae: 36.1849 - val_loss: 2221.4447 - val_mse: 2221.4446 - val_mae: 31.7400\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4206.6894 - mse: 4206.6895 - mae: 34.8356 - val_loss: 2191.9934 - val_mse: 2191.9932 - val_mae: 31.6494\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4353.8670 - mse: 4353.8677 - mae: 37.4892 - val_loss: 2329.9237 - val_mse: 2329.9238 - val_mae: 32.1001\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4263.4659 - mse: 4263.4658 - mae: 34.8943 - val_loss: 2242.4798 - val_mse: 2242.4797 - val_mae: 31.7956\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4173.5171 - mse: 4173.5171 - mae: 34.5372 - val_loss: 2223.0771 - val_mse: 2223.0769 - val_mae: 31.7332\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4399.2941 - mse: 4399.2939 - mae: 36.0105 - val_loss: 2322.6938 - val_mse: 2322.6941 - val_mae: 32.0656\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 4414.7317 - mse: 4414.7310 - mae: 35.8545 - val_loss: 2302.3999 - val_mse: 2302.3999 - val_mae: 31.9907\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 503us/step - loss: 4343.5327 - mse: 4343.5332 - mae: 34.7756 - val_loss: 2252.2929 - val_mse: 2252.2930 - val_mae: 31.8199\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 490us/step - loss: 4351.5094 - mse: 4351.5103 - mae: 35.1091 - val_loss: 2252.4277 - val_mse: 2252.4277 - val_mae: 31.8166\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4269.6120 - mse: 4269.6118 - mae: 35.7141 - val_loss: 2239.8499 - val_mse: 2239.8501 - val_mae: 31.7725\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4283.5700 - mse: 4283.5698 - mae: 35.6933 - val_loss: 2263.4432 - val_mse: 2263.4431 - val_mae: 31.8464\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4152.8920 - mse: 4152.8921 - mae: 35.2594 - val_loss: 2254.2970 - val_mse: 2254.2974 - val_mae: 31.8159\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4215.0748 - mse: 4215.0747 - mae: 34.6551 - val_loss: 2247.9303 - val_mse: 2247.9302 - val_mae: 31.7932\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4481.0628 - mse: 4481.0615 - mae: 35.8485 - val_loss: 2270.3557 - val_mse: 2270.3557 - val_mae: 31.8609\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4336.6779 - mse: 4336.6777 - mae: 35.1821 - val_loss: 2261.5846 - val_mse: 2261.5847 - val_mae: 31.8300\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 526us/step - loss: 4421.1408 - mse: 4421.1411 - mae: 35.6329 - val_loss: 2256.2817 - val_mse: 2256.2817 - val_mae: 31.8136\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4313.8406 - mse: 4313.8403 - mae: 34.4878 - val_loss: 2279.0251 - val_mse: 2279.0251 - val_mae: 31.8815\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 4273.0867 - mse: 4273.0864 - mae: 35.7428 - val_loss: 2204.5848 - val_mse: 2204.5845 - val_mae: 31.6606\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 521us/step - loss: 4277.1551 - mse: 4277.1548 - mae: 34.9396 - val_loss: 2237.1510 - val_mse: 2237.1509 - val_mae: 31.7573\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 4215.5778 - mse: 4215.5776 - mae: 36.1774 - val_loss: 2221.5472 - val_mse: 2221.5471 - val_mae: 31.7120\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4216.4053 - mse: 4216.4053 - mae: 34.5194 - val_loss: 2233.5767 - val_mse: 2233.5767 - val_mae: 31.7450\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4484.4790 - mse: 4484.4785 - mae: 35.3259 - val_loss: 2263.2997 - val_mse: 2263.2998 - val_mae: 31.8274\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4116.0176 - mse: 4116.0176 - mae: 34.9349 - val_loss: 2211.8302 - val_mse: 2211.8303 - val_mae: 31.6789\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4335.9844 - mse: 4335.9849 - mae: 35.4419 - val_loss: 2233.8326 - val_mse: 2233.8328 - val_mae: 31.7421\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4118.1519 - mse: 4118.1519 - mae: 34.8009 - val_loss: 2197.3339 - val_mse: 2197.3337 - val_mae: 31.6325\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4270.2736 - mse: 4270.2739 - mae: 35.2064 - val_loss: 2242.3361 - val_mse: 2242.3359 - val_mae: 31.7616\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 4333.3765 - mse: 4333.3765 - mae: 35.0973 - val_loss: 2321.0922 - val_mse: 2321.0923 - val_mae: 31.9996\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 569us/step - loss: 4325.8749 - mse: 4325.8750 - mae: 35.2334 - val_loss: 2242.6098 - val_mse: 2242.6099 - val_mae: 31.7595\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 454us/step - loss: 4119.9531 - mse: 4119.9526 - mae: 33.9180 - val_loss: 2233.3765 - val_mse: 2233.3762 - val_mae: 31.7326\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4351.2558 - mse: 4351.2563 - mae: 35.2827 - val_loss: 2329.1140 - val_mse: 2329.1140 - val_mae: 32.0210\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4273.6383 - mse: 4273.6377 - mae: 34.5899 - val_loss: 2235.8385 - val_mse: 2235.8384 - val_mae: 31.7381\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4081.2750 - mse: 4081.2756 - mae: 34.4167 - val_loss: 2250.5364 - val_mse: 2250.5366 - val_mae: 31.7788\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4010.6061 - mse: 4010.6060 - mae: 34.4538 - val_loss: 2205.3399 - val_mse: 2205.3398 - val_mae: 31.6519\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 4105.9792 - mse: 4105.9790 - mae: 34.2333 - val_loss: 2219.8451 - val_mse: 2219.8452 - val_mae: 31.6927\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 511us/step - loss: 4268.6658 - mse: 4268.6655 - mae: 34.4124 - val_loss: 2331.2924 - val_mse: 2331.2922 - val_mae: 32.0187\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 493us/step - loss: 4233.2502 - mse: 4233.2490 - mae: 33.7304 - val_loss: 2278.5842 - val_mse: 2278.5840 - val_mae: 31.8498\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 4281.3446 - mse: 4281.3442 - mae: 34.2197 - val_loss: 2263.3009 - val_mse: 2263.3010 - val_mae: 31.8071\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4124.3847 - mse: 4124.3843 - mae: 33.6240 - val_loss: 2277.7147 - val_mse: 2277.7148 - val_mae: 31.8451\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 4061.7363 - mse: 4061.7368 - mae: 33.6684 - val_loss: 2231.0438 - val_mse: 2231.0437 - val_mae: 31.7204\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 507us/step - loss: 4144.6429 - mse: 4144.6436 - mae: 35.7441 - val_loss: 2260.4273 - val_mse: 2260.4275 - val_mae: 31.7970\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 507us/step - loss: 4150.5958 - mse: 4150.5957 - mae: 34.9267 - val_loss: 2274.0022 - val_mse: 2274.0020 - val_mae: 31.8338\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4006.6189 - mse: 4006.6187 - mae: 34.4368 - val_loss: 2280.3907 - val_mse: 2280.3909 - val_mae: 31.8519\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 477us/step - loss: 4224.3865 - mse: 4224.3867 - mae: 34.8439 - val_loss: 2269.8658 - val_mse: 2269.8655 - val_mae: 31.8230\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 4268.1899 - mse: 4268.1890 - mae: 34.4538 - val_loss: 2332.3844 - val_mse: 2332.3843 - val_mae: 32.0121\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 522us/step - loss: 4133.9795 - mse: 4133.9795 - mae: 34.0301 - val_loss: 2259.8494 - val_mse: 2259.8499 - val_mae: 31.7978\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 482us/step - loss: 4158.9930 - mse: 4158.9932 - mae: 34.3174 - val_loss: 2320.4461 - val_mse: 2320.4463 - val_mae: 31.9725\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 3944.7036 - mse: 3944.7036 - mae: 32.9664 - val_loss: 2244.9274 - val_mse: 2244.9275 - val_mae: 31.7581\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4049.5326 - mse: 4049.5327 - mae: 34.0888 - val_loss: 2242.3046 - val_mse: 2242.3044 - val_mae: 31.7509\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4157.2262 - mse: 4157.2261 - mae: 35.0183 - val_loss: 2280.4622 - val_mse: 2280.4622 - val_mae: 31.8516\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4161.1226 - mse: 4161.1226 - mae: 33.6987 - val_loss: 2283.6743 - val_mse: 2283.6743 - val_mae: 31.8607\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4154.1934 - mse: 4154.1929 - mae: 34.1680 - val_loss: 2245.7026 - val_mse: 2245.7024 - val_mae: 31.7590\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4157.0713 - mse: 4157.0703 - mae: 34.8294 - val_loss: 2283.2963 - val_mse: 2283.2961 - val_mae: 31.8589\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4088.4010 - mse: 4088.4011 - mae: 34.7039 - val_loss: 2301.0414 - val_mse: 2301.0415 - val_mae: 31.9059\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4075.1112 - mse: 4075.1113 - mae: 32.9112 - val_loss: 2303.0832 - val_mse: 2303.0833 - val_mae: 31.9120\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4118.8141 - mse: 4118.8135 - mae: 33.8791 - val_loss: 2303.1930 - val_mse: 2303.1934 - val_mae: 31.9117\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 514us/step - loss: 4273.2969 - mse: 4273.2974 - mae: 35.7368 - val_loss: 2294.7359 - val_mse: 2294.7361 - val_mae: 31.8894\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4125.1319 - mse: 4125.1318 - mae: 33.1125 - val_loss: 2278.0104 - val_mse: 2278.0105 - val_mae: 31.8427\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 3972.8935 - mse: 3972.8940 - mae: 33.7206 - val_loss: 2228.7424 - val_mse: 2228.7422 - val_mae: 31.7094\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4248.1595 - mse: 4248.1597 - mae: 34.2138 - val_loss: 2295.4320 - val_mse: 2295.4319 - val_mae: 31.8906\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4161.8501 - mse: 4161.8501 - mae: 34.2303 - val_loss: 2224.4933 - val_mse: 2224.4932 - val_mae: 31.6984\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4182.0338 - mse: 4182.0337 - mae: 33.5441 - val_loss: 2248.3277 - val_mse: 2248.3279 - val_mae: 31.7630\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 547us/step - loss: 4219.7602 - mse: 4219.7598 - mae: 33.7694 - val_loss: 2275.7551 - val_mse: 2275.7554 - val_mae: 31.8381\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4233.5613 - mse: 4233.5610 - mae: 34.5719 - val_loss: 2326.7016 - val_mse: 2326.7014 - val_mae: 31.9812\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4154.9396 - mse: 4154.9395 - mae: 34.2172 - val_loss: 2340.5034 - val_mse: 2340.5032 - val_mae: 32.0225\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 547us/step - loss: 4086.1182 - mse: 4086.1187 - mae: 33.7592 - val_loss: 2244.3954 - val_mse: 2244.3953 - val_mae: 31.7538\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4200.5612 - mse: 4200.5610 - mae: 33.7857 - val_loss: 2309.9345 - val_mse: 2309.9346 - val_mae: 31.9330\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 525us/step - loss: 4074.8882 - mse: 4074.8892 - mae: 33.6732 - val_loss: 2228.0532 - val_mse: 2228.0530 - val_mae: 31.7067\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4083.7477 - mse: 4083.7480 - mae: 34.0217 - val_loss: 2262.2362 - val_mse: 2262.2363 - val_mae: 31.8008\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4166.4561 - mse: 4166.4561 - mae: 33.9045 - val_loss: 2238.4787 - val_mse: 2238.4785 - val_mae: 31.7350\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4319.2457 - mse: 4319.2456 - mae: 34.6978 - val_loss: 2329.2160 - val_mse: 2329.2161 - val_mae: 31.9886\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 676us/step - loss: 4059.1156 - mse: 4059.1160 - mae: 33.5319 - val_loss: 2268.4151 - val_mse: 2268.4155 - val_mae: 31.8203\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4137.0450 - mse: 4137.0454 - mae: 33.6454 - val_loss: 2338.1963 - val_mse: 2338.1963 - val_mae: 32.0148\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4156.8641 - mse: 4156.8643 - mae: 33.7723 - val_loss: 2267.7510 - val_mse: 2267.7510 - val_mae: 31.8185\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4071.9256 - mse: 4071.9253 - mae: 33.4699 - val_loss: 2303.0731 - val_mse: 2303.0732 - val_mae: 31.9154\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4007.2263 - mse: 4007.2258 - mae: 33.8019 - val_loss: 2324.3241 - val_mse: 2324.3242 - val_mae: 31.9718\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4195.7283 - mse: 4195.7280 - mae: 33.8880 - val_loss: 2273.1982 - val_mse: 2273.1982 - val_mae: 31.8318\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 662us/step - loss: 4134.4249 - mse: 4134.4248 - mae: 33.5110 - val_loss: 2278.3532 - val_mse: 2278.3530 - val_mae: 31.8477\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3404.1834 - mse: 3404.1833 - mae: 32.9518 - val_loss: 1430.6585 - val_mse: 1430.6588 - val_mae: 25.0207\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 512us/step - loss: 3537.8674 - mse: 3537.8679 - mae: 34.1718 - val_loss: 1431.0603 - val_mse: 1431.0599 - val_mae: 25.1047\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 467us/step - loss: 3452.5579 - mse: 3452.5579 - mae: 33.1033 - val_loss: 1432.1789 - val_mse: 1432.1788 - val_mae: 24.9930\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3507.2853 - mse: 3507.2847 - mae: 33.2433 - val_loss: 1439.6645 - val_mse: 1439.6644 - val_mae: 24.3816\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3388.5205 - mse: 3388.5193 - mae: 32.5323 - val_loss: 1433.7693 - val_mse: 1433.7692 - val_mae: 24.8181\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3520.5269 - mse: 3520.5264 - mae: 33.2155 - val_loss: 1433.0464 - val_mse: 1433.0465 - val_mae: 25.0347\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3390.5225 - mse: 3390.5225 - mae: 31.9210 - val_loss: 1433.6616 - val_mse: 1433.6617 - val_mae: 24.9909\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3506.6864 - mse: 3506.6863 - mae: 33.3515 - val_loss: 1434.4976 - val_mse: 1434.4977 - val_mae: 24.9243\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3423.9188 - mse: 3423.9187 - mae: 33.0181 - val_loss: 1434.1554 - val_mse: 1434.1554 - val_mae: 25.2481\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3461.5751 - mse: 3461.5750 - mae: 33.7842 - val_loss: 1437.1683 - val_mse: 1437.1681 - val_mae: 24.7748\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 522us/step - loss: 3449.3367 - mse: 3449.3379 - mae: 32.8133 - val_loss: 1435.8810 - val_mse: 1435.8812 - val_mae: 25.4454\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3407.2381 - mse: 3407.2385 - mae: 33.5331 - val_loss: 1437.0337 - val_mse: 1437.0336 - val_mae: 24.9886\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 445us/step - loss: 3308.9878 - mse: 3308.9875 - mae: 32.9261 - val_loss: 1437.0051 - val_mse: 1437.0051 - val_mae: 25.1838\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3441.9723 - mse: 3441.9727 - mae: 33.0015 - val_loss: 1439.2345 - val_mse: 1439.2346 - val_mae: 24.8618\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3347.9830 - mse: 3347.9822 - mae: 32.9726 - val_loss: 1437.6533 - val_mse: 1437.6532 - val_mae: 25.3591\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3404.1984 - mse: 3404.1990 - mae: 33.4963 - val_loss: 1438.3503 - val_mse: 1438.3505 - val_mae: 25.2287\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3459.2781 - mse: 3459.2773 - mae: 32.9396 - val_loss: 1443.4572 - val_mse: 1443.4573 - val_mae: 26.2584\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3316.6095 - mse: 3316.6094 - mae: 32.7428 - val_loss: 1439.5366 - val_mse: 1439.5364 - val_mae: 25.6373\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3426.6588 - mse: 3426.6580 - mae: 32.9876 - val_loss: 1440.1047 - val_mse: 1440.1047 - val_mae: 25.1649\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3421.0462 - mse: 3421.0461 - mae: 32.1750 - val_loss: 1443.5372 - val_mse: 1443.5375 - val_mae: 26.0986\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 532us/step - loss: 3545.2144 - mse: 3545.2141 - mae: 33.2913 - val_loss: 1442.9211 - val_mse: 1442.9211 - val_mae: 25.9234\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3292.7594 - mse: 3292.7598 - mae: 32.2078 - val_loss: 1447.6667 - val_mse: 1447.6665 - val_mae: 26.4363\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3387.8465 - mse: 3387.8462 - mae: 32.9287 - val_loss: 1443.5973 - val_mse: 1443.5974 - val_mae: 24.9659\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3320.0063 - mse: 3320.0066 - mae: 32.0873 - val_loss: 1443.3871 - val_mse: 1443.3871 - val_mae: 25.6799\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 514us/step - loss: 3329.9982 - mse: 3329.9988 - mae: 32.4388 - val_loss: 1445.7268 - val_mse: 1445.7268 - val_mae: 26.0739\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 507us/step - loss: 3383.8216 - mse: 3383.8220 - mae: 32.3556 - val_loss: 1444.0643 - val_mse: 1444.0642 - val_mae: 25.4180\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3361.6387 - mse: 3361.6384 - mae: 32.7900 - val_loss: 1445.3407 - val_mse: 1445.3407 - val_mae: 25.1175\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3514.9436 - mse: 3514.9438 - mae: 33.0965 - val_loss: 1446.1774 - val_mse: 1446.1775 - val_mae: 25.0392\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3338.5326 - mse: 3338.5327 - mae: 32.2632 - val_loss: 1445.9638 - val_mse: 1445.9639 - val_mae: 25.6599\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3356.0839 - mse: 3356.0835 - mae: 32.4082 - val_loss: 1448.9006 - val_mse: 1448.9005 - val_mae: 26.1548\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3332.6947 - mse: 3332.6943 - mae: 32.2576 - val_loss: 1446.8431 - val_mse: 1446.8433 - val_mae: 25.6421\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3351.1612 - mse: 3351.1616 - mae: 32.6953 - val_loss: 1447.6044 - val_mse: 1447.6045 - val_mae: 25.6396\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3418.1911 - mse: 3418.1907 - mae: 32.6483 - val_loss: 1448.4778 - val_mse: 1448.4778 - val_mae: 25.1835\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3355.1862 - mse: 3355.1855 - mae: 32.9082 - val_loss: 1454.4917 - val_mse: 1454.4918 - val_mae: 26.5323\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 540us/step - loss: 3439.9366 - mse: 3439.9375 - mae: 33.1613 - val_loss: 1448.9580 - val_mse: 1448.9580 - val_mae: 25.7384\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3285.9157 - mse: 3285.9158 - mae: 32.4605 - val_loss: 1449.2019 - val_mse: 1449.2019 - val_mae: 25.7147\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3323.8636 - mse: 3323.8638 - mae: 32.0537 - val_loss: 1449.3726 - val_mse: 1449.3724 - val_mae: 25.4986\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3441.3082 - mse: 3441.3081 - mae: 33.1891 - val_loss: 1450.1402 - val_mse: 1450.1403 - val_mae: 25.7973\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3456.7711 - mse: 3456.7715 - mae: 33.6157 - val_loss: 1451.3484 - val_mse: 1451.3486 - val_mae: 24.9904\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 507us/step - loss: 3434.9268 - mse: 3434.9268 - mae: 32.5709 - val_loss: 1450.9398 - val_mse: 1450.9397 - val_mae: 25.7863\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 514us/step - loss: 3409.1850 - mse: 3409.1853 - mae: 33.4721 - val_loss: 1450.8363 - val_mse: 1450.8362 - val_mae: 25.4896\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3370.1969 - mse: 3370.1978 - mae: 32.4702 - val_loss: 1455.3349 - val_mse: 1455.3348 - val_mae: 26.3810\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3453.3067 - mse: 3453.3064 - mae: 32.6123 - val_loss: 1453.0239 - val_mse: 1453.0239 - val_mae: 25.9553\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3321.2942 - mse: 3321.2944 - mae: 31.9413 - val_loss: 1452.2568 - val_mse: 1452.2568 - val_mae: 25.6673\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3357.1137 - mse: 3357.1138 - mae: 32.7798 - val_loss: 1452.4685 - val_mse: 1452.4686 - val_mae: 25.5260\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3384.7563 - mse: 3384.7561 - mae: 32.2644 - val_loss: 1454.9907 - val_mse: 1454.9908 - val_mae: 26.1405\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3423.9127 - mse: 3423.9124 - mae: 32.4851 - val_loss: 1454.0292 - val_mse: 1454.0291 - val_mae: 25.9232\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3317.4924 - mse: 3317.4922 - mae: 31.5540 - val_loss: 1461.5224 - val_mse: 1461.5223 - val_mae: 26.7778\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3363.6093 - mse: 3363.6094 - mae: 32.9048 - val_loss: 1454.2144 - val_mse: 1454.2144 - val_mae: 25.7311\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3317.4338 - mse: 3317.4338 - mae: 31.9069 - val_loss: 1454.7753 - val_mse: 1454.7754 - val_mae: 25.8002\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3306.6381 - mse: 3306.6382 - mae: 32.2377 - val_loss: 1456.3804 - val_mse: 1456.3804 - val_mae: 26.1341\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3313.4911 - mse: 3313.4910 - mae: 32.4139 - val_loss: 1454.9525 - val_mse: 1454.9525 - val_mae: 25.6864\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3345.6158 - mse: 3345.6157 - mae: 33.0950 - val_loss: 1456.2443 - val_mse: 1456.2443 - val_mae: 25.0734\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3313.8280 - mse: 3313.8281 - mae: 32.0203 - val_loss: 1455.6714 - val_mse: 1455.6715 - val_mae: 25.7805\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3294.5459 - mse: 3294.5459 - mae: 31.8779 - val_loss: 1459.0239 - val_mse: 1459.0240 - val_mae: 26.3748\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3343.2685 - mse: 3343.2688 - mae: 32.3923 - val_loss: 1455.4381 - val_mse: 1455.4380 - val_mae: 25.4434\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3285.0896 - mse: 3285.0906 - mae: 31.3414 - val_loss: 1457.2435 - val_mse: 1457.2435 - val_mae: 26.1223\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3395.3467 - mse: 3395.3472 - mae: 32.3420 - val_loss: 1457.3828 - val_mse: 1457.3827 - val_mae: 26.0845\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 519us/step - loss: 3389.0039 - mse: 3389.0039 - mae: 32.6337 - val_loss: 1457.1557 - val_mse: 1457.1558 - val_mae: 26.0462\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3277.1102 - mse: 3277.1106 - mae: 32.6716 - val_loss: 1463.9849 - val_mse: 1463.9847 - val_mae: 26.7985\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3347.4258 - mse: 3347.4250 - mae: 32.6233 - val_loss: 1460.4463 - val_mse: 1460.4464 - val_mae: 26.4136\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3336.8201 - mse: 3336.8191 - mae: 32.8632 - val_loss: 1457.3546 - val_mse: 1457.3546 - val_mae: 25.8268\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3176.1308 - mse: 3176.1313 - mae: 31.8050 - val_loss: 1457.4888 - val_mse: 1457.4888 - val_mae: 25.6673\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3308.2025 - mse: 3308.2026 - mae: 32.2425 - val_loss: 1458.0717 - val_mse: 1458.0717 - val_mae: 25.8800\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3287.9739 - mse: 3287.9736 - mae: 31.8752 - val_loss: 1457.8514 - val_mse: 1457.8514 - val_mae: 25.8583\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3353.3723 - mse: 3353.3718 - mae: 32.2842 - val_loss: 1458.2032 - val_mse: 1458.2034 - val_mae: 25.9381\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 662us/step - loss: 3314.1462 - mse: 3314.1455 - mae: 31.8058 - val_loss: 1457.6068 - val_mse: 1457.6068 - val_mae: 25.7426\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3318.8564 - mse: 3318.8569 - mae: 31.9793 - val_loss: 1457.8111 - val_mse: 1457.8110 - val_mae: 25.8273\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3242.2839 - mse: 3242.2842 - mae: 31.8281 - val_loss: 1457.4522 - val_mse: 1457.4521 - val_mae: 25.4716\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3410.4853 - mse: 3410.4856 - mae: 32.2055 - val_loss: 1459.8208 - val_mse: 1459.8208 - val_mae: 26.1760\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3344.1088 - mse: 3344.1086 - mae: 32.2834 - val_loss: 1461.1449 - val_mse: 1461.1449 - val_mae: 26.3340\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 535us/step - loss: 3320.7678 - mse: 3320.7676 - mae: 32.1751 - val_loss: 1458.5489 - val_mse: 1458.5488 - val_mae: 25.8165\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3269.6056 - mse: 3269.6055 - mae: 31.3351 - val_loss: 1464.0200 - val_mse: 1464.0203 - val_mae: 26.5985\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 497us/step - loss: 3392.1671 - mse: 3392.1665 - mae: 32.1113 - val_loss: 1459.6281 - val_mse: 1459.6281 - val_mae: 25.8660\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3286.8085 - mse: 3286.8093 - mae: 31.5747 - val_loss: 1461.2387 - val_mse: 1461.2388 - val_mae: 26.1771\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3309.7027 - mse: 3309.7026 - mae: 31.7943 - val_loss: 1462.6368 - val_mse: 1462.6368 - val_mae: 26.3725\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3387.7073 - mse: 3387.7070 - mae: 32.1842 - val_loss: 1460.5451 - val_mse: 1460.5452 - val_mae: 25.9366\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3307.7186 - mse: 3307.7188 - mae: 31.3603 - val_loss: 1460.5995 - val_mse: 1460.5992 - val_mae: 25.8241\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3275.8205 - mse: 3275.8198 - mae: 32.0029 - val_loss: 1465.4430 - val_mse: 1465.4430 - val_mae: 26.5858\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3304.6592 - mse: 3304.6592 - mae: 31.7921 - val_loss: 1464.7746 - val_mse: 1464.7745 - val_mae: 26.5145\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 541us/step - loss: 2926.0557 - mse: 2926.0562 - mae: 31.6269 - val_loss: 1069.2248 - val_mse: 1069.2250 - val_mae: 23.7762\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 519us/step - loss: 2972.5983 - mse: 2972.5986 - mae: 31.2844 - val_loss: 1072.7144 - val_mse: 1072.7144 - val_mae: 23.5309\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2933.2091 - mse: 2933.2092 - mae: 31.4627 - val_loss: 1069.6409 - val_mse: 1069.6409 - val_mae: 23.7068\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 541us/step - loss: 2983.5801 - mse: 2983.5803 - mae: 31.8275 - val_loss: 1073.4264 - val_mse: 1073.4264 - val_mae: 23.4415\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 3015.1525 - mse: 3015.1523 - mae: 30.9156 - val_loss: 1072.0195 - val_mse: 1072.0195 - val_mae: 23.4870\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2830.3769 - mse: 2830.3770 - mae: 31.0200 - val_loss: 1067.9115 - val_mse: 1067.9116 - val_mae: 23.7248\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2970.8350 - mse: 2970.8345 - mae: 31.2686 - val_loss: 1065.2049 - val_mse: 1065.2050 - val_mae: 24.1322\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 528us/step - loss: 2869.5522 - mse: 2869.5525 - mae: 30.8879 - val_loss: 1069.5744 - val_mse: 1069.5743 - val_mae: 23.5549\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 676us/step - loss: 2937.7594 - mse: 2937.7598 - mae: 31.2401 - val_loss: 1065.1853 - val_mse: 1065.1853 - val_mae: 23.9716\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2977.5108 - mse: 2977.5112 - mae: 31.2301 - val_loss: 1064.1587 - val_mse: 1064.1587 - val_mae: 24.2081\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2935.2103 - mse: 2935.2102 - mae: 31.1670 - val_loss: 1065.0622 - val_mse: 1065.0621 - val_mae: 23.8402\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 499us/step - loss: 2856.1028 - mse: 2856.1033 - mae: 31.0121 - val_loss: 1063.0415 - val_mse: 1063.0415 - val_mae: 24.2364\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2919.3222 - mse: 2919.3223 - mae: 31.1297 - val_loss: 1065.1511 - val_mse: 1065.1511 - val_mae: 23.7082\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 525us/step - loss: 2989.0639 - mse: 2989.0632 - mae: 31.6544 - val_loss: 1067.5577 - val_mse: 1067.5576 - val_mae: 23.4971\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2956.5965 - mse: 2956.5964 - mae: 31.2345 - val_loss: 1065.1941 - val_mse: 1065.1940 - val_mae: 23.6229\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 2993.5862 - mse: 2993.5867 - mae: 31.2435 - val_loss: 1066.8473 - val_mse: 1066.8473 - val_mae: 23.4710\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 689us/step - loss: 2921.5520 - mse: 2921.5515 - mae: 30.9459 - val_loss: 1063.2522 - val_mse: 1063.2521 - val_mae: 23.7219\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2889.9454 - mse: 2889.9451 - mae: 30.5554 - val_loss: 1061.7102 - val_mse: 1061.7101 - val_mae: 23.8917\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 526us/step - loss: 2935.7536 - mse: 2935.7539 - mae: 30.9659 - val_loss: 1060.9823 - val_mse: 1060.9822 - val_mae: 24.5207\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2847.2810 - mse: 2847.2805 - mae: 30.5243 - val_loss: 1060.9248 - val_mse: 1060.9247 - val_mae: 23.9978\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2968.5815 - mse: 2968.5811 - mae: 31.5534 - val_loss: 1067.4627 - val_mse: 1067.4626 - val_mae: 23.3631\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 641us/step - loss: 2962.0556 - mse: 2962.0562 - mae: 31.4045 - val_loss: 1066.9393 - val_mse: 1066.9392 - val_mae: 23.3682\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 3018.4671 - mse: 3018.4666 - mae: 31.5440 - val_loss: 1061.6425 - val_mse: 1061.6426 - val_mae: 23.7551\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2878.2962 - mse: 2878.2961 - mae: 30.9182 - val_loss: 1060.1383 - val_mse: 1060.1384 - val_mae: 23.8453\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 456us/step - loss: 2878.5613 - mse: 2878.5618 - mae: 30.9394 - val_loss: 1059.3245 - val_mse: 1059.3247 - val_mae: 23.9708\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2932.8628 - mse: 2932.8630 - mae: 31.6879 - val_loss: 1062.2484 - val_mse: 1062.2484 - val_mae: 23.5928\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2884.3346 - mse: 2884.3350 - mae: 30.7677 - val_loss: 1059.3200 - val_mse: 1059.3201 - val_mae: 23.8966\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2869.5190 - mse: 2869.5190 - mae: 31.0675 - val_loss: 1062.8037 - val_mse: 1062.8036 - val_mae: 23.5417\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2871.6258 - mse: 2871.6265 - mae: 30.8149 - val_loss: 1059.6874 - val_mse: 1059.6874 - val_mae: 23.8043\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2903.8692 - mse: 2903.8691 - mae: 30.8030 - val_loss: 1062.5823 - val_mse: 1062.5824 - val_mae: 23.5149\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2937.5623 - mse: 2937.5627 - mae: 31.2534 - val_loss: 1058.0970 - val_mse: 1058.0969 - val_mae: 24.1930\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2963.0054 - mse: 2963.0063 - mae: 30.8048 - val_loss: 1062.2410 - val_mse: 1062.2410 - val_mae: 23.4938\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2882.3875 - mse: 2882.3877 - mae: 30.8124 - val_loss: 1059.1319 - val_mse: 1059.1317 - val_mae: 23.7805\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2937.1542 - mse: 2937.1541 - mae: 31.2974 - val_loss: 1059.6834 - val_mse: 1059.6832 - val_mae: 23.7183\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2998.9144 - mse: 2998.9141 - mae: 31.9932 - val_loss: 1059.1025 - val_mse: 1059.1025 - val_mae: 23.7720\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2886.5506 - mse: 2886.5503 - mae: 31.1850 - val_loss: 1059.8756 - val_mse: 1059.8757 - val_mae: 23.6106\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 525us/step - loss: 2881.0827 - mse: 2881.0825 - mae: 30.8437 - val_loss: 1063.8813 - val_mse: 1063.8812 - val_mae: 23.3228\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 678us/step - loss: 2912.1194 - mse: 2912.1191 - mae: 31.1028 - val_loss: 1059.6407 - val_mse: 1059.6405 - val_mae: 23.5374\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2864.5887 - mse: 2864.5889 - mae: 30.9012 - val_loss: 1059.1273 - val_mse: 1059.1272 - val_mae: 23.5992\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2932.5192 - mse: 2932.5183 - mae: 30.8868 - val_loss: 1056.4224 - val_mse: 1056.4225 - val_mae: 23.8478\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2939.1148 - mse: 2939.1145 - mae: 30.8665 - val_loss: 1057.4472 - val_mse: 1057.4473 - val_mae: 23.7345\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2884.3532 - mse: 2884.3523 - mae: 31.2718 - val_loss: 1055.5559 - val_mse: 1055.5559 - val_mae: 24.0495\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2837.6494 - mse: 2837.6482 - mae: 31.2284 - val_loss: 1058.2098 - val_mse: 1058.2098 - val_mae: 23.6013\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2829.9902 - mse: 2829.9900 - mae: 30.7387 - val_loss: 1055.0753 - val_mse: 1055.0753 - val_mae: 23.8485\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 663us/step - loss: 2955.7022 - mse: 2955.7026 - mae: 31.1658 - val_loss: 1056.0630 - val_mse: 1056.0630 - val_mae: 23.7184\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2799.2439 - mse: 2799.2432 - mae: 30.4758 - val_loss: 1053.8831 - val_mse: 1053.8831 - val_mae: 23.9420\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2854.7615 - mse: 2854.7617 - mae: 30.3997 - val_loss: 1054.6929 - val_mse: 1054.6930 - val_mae: 23.8807\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2910.5715 - mse: 2910.5713 - mae: 31.0760 - val_loss: 1055.6908 - val_mse: 1055.6908 - val_mae: 23.6837\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2888.7186 - mse: 2888.7185 - mae: 30.7107 - val_loss: 1054.2737 - val_mse: 1054.2738 - val_mae: 23.8038\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2854.9231 - mse: 2854.9231 - mae: 30.7449 - val_loss: 1055.6898 - val_mse: 1055.6897 - val_mae: 23.5913\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2919.1578 - mse: 2919.1577 - mae: 31.1573 - val_loss: 1053.2316 - val_mse: 1053.2316 - val_mae: 23.7540\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2859.8076 - mse: 2859.8079 - mae: 30.8079 - val_loss: 1053.5888 - val_mse: 1053.5886 - val_mae: 23.7427\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2867.3122 - mse: 2867.3115 - mae: 30.9842 - val_loss: 1059.3142 - val_mse: 1059.3142 - val_mae: 23.2965\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2880.6722 - mse: 2880.6724 - mae: 30.3326 - val_loss: 1053.6232 - val_mse: 1053.6233 - val_mae: 23.6033\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2909.2069 - mse: 2909.2068 - mae: 30.9135 - val_loss: 1050.5583 - val_mse: 1050.5582 - val_mae: 24.0883\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2894.3425 - mse: 2894.3428 - mae: 30.7404 - val_loss: 1051.3902 - val_mse: 1051.3901 - val_mae: 23.8572\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2903.0493 - mse: 2903.0496 - mae: 31.0394 - val_loss: 1050.2310 - val_mse: 1050.2310 - val_mae: 24.0485\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2832.0576 - mse: 2832.0574 - mae: 30.8756 - val_loss: 1049.9472 - val_mse: 1049.9473 - val_mae: 24.0456\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2793.8706 - mse: 2793.8704 - mae: 30.6066 - val_loss: 1050.3901 - val_mse: 1050.3901 - val_mae: 23.7510\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2913.9998 - mse: 2913.9990 - mae: 31.0217 - val_loss: 1049.2313 - val_mse: 1049.2313 - val_mae: 23.9147\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2928.5595 - mse: 2928.5598 - mae: 31.0107 - val_loss: 1050.2631 - val_mse: 1050.2632 - val_mae: 23.7468\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2835.0751 - mse: 2835.0742 - mae: 30.8411 - val_loss: 1048.6521 - val_mse: 1048.6521 - val_mae: 23.9916\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2850.3662 - mse: 2850.3662 - mae: 30.7165 - val_loss: 1053.5832 - val_mse: 1053.5831 - val_mae: 23.3932\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2820.8903 - mse: 2820.8911 - mae: 30.2893 - val_loss: 1048.5554 - val_mse: 1048.5554 - val_mae: 23.7958\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2888.7191 - mse: 2888.7183 - mae: 31.0639 - val_loss: 1053.5469 - val_mse: 1053.5469 - val_mae: 23.3369\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2935.2851 - mse: 2935.2847 - mae: 30.5259 - val_loss: 1047.6778 - val_mse: 1047.6777 - val_mae: 24.0250\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2882.6217 - mse: 2882.6213 - mae: 30.6612 - val_loss: 1047.6208 - val_mse: 1047.6208 - val_mae: 24.2613\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2804.3141 - mse: 2804.3145 - mae: 30.2460 - val_loss: 1047.6881 - val_mse: 1047.6880 - val_mae: 23.8320\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2833.5791 - mse: 2833.5789 - mae: 30.2379 - val_loss: 1047.5110 - val_mse: 1047.5111 - val_mae: 23.7954\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2712.2928 - mse: 2712.2927 - mae: 30.1658 - val_loss: 1046.2511 - val_mse: 1046.2511 - val_mae: 23.8447\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 520us/step - loss: 2813.2943 - mse: 2813.2944 - mae: 30.2060 - val_loss: 1045.9038 - val_mse: 1045.9039 - val_mae: 23.8036\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2877.8219 - mse: 2877.8220 - mae: 30.4770 - val_loss: 1045.5495 - val_mse: 1045.5496 - val_mae: 23.7205\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 663us/step - loss: 2879.8709 - mse: 2879.8699 - mae: 30.4550 - val_loss: 1045.8019 - val_mse: 1045.8019 - val_mae: 23.6336\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2969.0303 - mse: 2969.0305 - mae: 30.5956 - val_loss: 1043.1209 - val_mse: 1043.1208 - val_mae: 24.0434\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2842.2460 - mse: 2842.2454 - mae: 30.8076 - val_loss: 1042.9376 - val_mse: 1042.9374 - val_mae: 24.2728\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2879.9189 - mse: 2879.9185 - mae: 30.6882 - val_loss: 1043.5953 - val_mse: 1043.5953 - val_mae: 23.6379\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2841.8832 - mse: 2841.8833 - mae: 30.4450 - val_loss: 1041.5937 - val_mse: 1041.5938 - val_mae: 23.8351\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 494us/step - loss: 2836.1214 - mse: 2836.1218 - mae: 29.7598 - val_loss: 1042.0606 - val_mse: 1042.0605 - val_mae: 23.7677\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2855.2094 - mse: 2855.2092 - mae: 30.0432 - val_loss: 1041.1901 - val_mse: 1041.1902 - val_mae: 23.7632\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2738.0775 - mse: 2738.0776 - mae: 29.8022 - val_loss: 1040.8081 - val_mse: 1040.8081 - val_mae: 23.6968\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2546.6801 - mse: 2546.6804 - mae: 29.5751 - val_loss: 1528.6889 - val_mse: 1528.6887 - val_mae: 27.6997\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2572.6569 - mse: 2572.6565 - mae: 30.1870 - val_loss: 1527.8000 - val_mse: 1527.8000 - val_mae: 27.6359\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2564.5127 - mse: 2564.5132 - mae: 29.8825 - val_loss: 1520.8455 - val_mse: 1520.8453 - val_mae: 27.8311\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2564.4046 - mse: 2564.4033 - mae: 29.6934 - val_loss: 1523.1937 - val_mse: 1523.1940 - val_mae: 27.6808\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2566.4885 - mse: 2566.4893 - mae: 29.7620 - val_loss: 1525.6586 - val_mse: 1525.6589 - val_mae: 27.5026\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2561.4995 - mse: 2561.4998 - mae: 29.8638 - val_loss: 1528.5222 - val_mse: 1528.5222 - val_mae: 27.3546\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2584.1638 - mse: 2584.1626 - mae: 30.6773 - val_loss: 1536.0796 - val_mse: 1536.0797 - val_mae: 27.0875\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2591.5250 - mse: 2591.5251 - mae: 29.6469 - val_loss: 1524.1214 - val_mse: 1524.1213 - val_mae: 27.3616\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2499.8521 - mse: 2499.8518 - mae: 29.5457 - val_loss: 1520.3486 - val_mse: 1520.3485 - val_mae: 27.4185\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2560.2029 - mse: 2560.2026 - mae: 30.0756 - val_loss: 1525.0768 - val_mse: 1525.0767 - val_mae: 27.2447\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2548.3442 - mse: 2548.3442 - mae: 29.7269 - val_loss: 1517.2559 - val_mse: 1517.2559 - val_mae: 27.4331\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2614.7632 - mse: 2614.7634 - mae: 30.0232 - val_loss: 1526.2693 - val_mse: 1526.2693 - val_mae: 27.1401\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2472.9228 - mse: 2472.9236 - mae: 29.4051 - val_loss: 1517.2616 - val_mse: 1517.2616 - val_mae: 27.3192\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2495.6080 - mse: 2495.6079 - mae: 29.5508 - val_loss: 1516.0463 - val_mse: 1516.0461 - val_mae: 27.3172\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2538.0985 - mse: 2538.0986 - mae: 29.9111 - val_loss: 1520.8805 - val_mse: 1520.8805 - val_mae: 27.1204\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2602.4060 - mse: 2602.4062 - mae: 30.0703 - val_loss: 1530.9872 - val_mse: 1530.9872 - val_mae: 26.8101\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 522us/step - loss: 2503.5473 - mse: 2503.5476 - mae: 29.7310 - val_loss: 1507.9571 - val_mse: 1507.9572 - val_mae: 27.3760\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2557.1233 - mse: 2557.1226 - mae: 29.6245 - val_loss: 1505.3626 - val_mse: 1505.3625 - val_mae: 27.3884\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2560.2222 - mse: 2560.2217 - mae: 29.8120 - val_loss: 1503.8507 - val_mse: 1503.8507 - val_mae: 27.3943\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2626.3186 - mse: 2626.3184 - mae: 29.9057 - val_loss: 1508.0570 - val_mse: 1508.0570 - val_mae: 27.2210\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2540.1324 - mse: 2540.1316 - mae: 29.2242 - val_loss: 1513.9571 - val_mse: 1513.9572 - val_mae: 27.0277\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2585.4241 - mse: 2585.4241 - mae: 29.7422 - val_loss: 1504.3117 - val_mse: 1504.3120 - val_mae: 27.2770\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2616.8800 - mse: 2616.8806 - mae: 29.9202 - val_loss: 1510.3624 - val_mse: 1510.3621 - val_mae: 27.0350\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2532.7327 - mse: 2532.7332 - mae: 29.3072 - val_loss: 1501.0027 - val_mse: 1501.0027 - val_mae: 27.2880\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2538.5671 - mse: 2538.5674 - mae: 29.6656 - val_loss: 1504.9462 - val_mse: 1504.9463 - val_mae: 27.1312\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 668us/step - loss: 2519.1096 - mse: 2519.1094 - mae: 29.3284 - val_loss: 1505.8749 - val_mse: 1505.8751 - val_mae: 27.0661\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2541.7945 - mse: 2541.7949 - mae: 29.6237 - val_loss: 1501.9989 - val_mse: 1501.9987 - val_mae: 27.1364\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2514.9027 - mse: 2514.9023 - mae: 29.4579 - val_loss: 1501.4366 - val_mse: 1501.4366 - val_mae: 27.1502\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2527.5430 - mse: 2527.5425 - mae: 29.2085 - val_loss: 1504.2005 - val_mse: 1504.2007 - val_mae: 27.0497\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2487.5299 - mse: 2487.5300 - mae: 29.5850 - val_loss: 1496.7122 - val_mse: 1496.7124 - val_mae: 27.2463\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2510.7274 - mse: 2510.7275 - mae: 29.3711 - val_loss: 1498.4674 - val_mse: 1498.4674 - val_mae: 27.1634\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2502.0880 - mse: 2502.0884 - mae: 29.2631 - val_loss: 1500.7452 - val_mse: 1500.7452 - val_mae: 27.0619\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2523.3185 - mse: 2523.3191 - mae: 29.5763 - val_loss: 1493.5119 - val_mse: 1493.5122 - val_mae: 27.2337\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2507.3203 - mse: 2507.3203 - mae: 28.7409 - val_loss: 1500.4631 - val_mse: 1500.4631 - val_mae: 26.9957\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2475.5028 - mse: 2475.5032 - mae: 28.9985 - val_loss: 1496.7515 - val_mse: 1496.7515 - val_mae: 27.0973\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 507us/step - loss: 2463.0087 - mse: 2463.0088 - mae: 29.2833 - val_loss: 1502.6529 - val_mse: 1502.6528 - val_mae: 26.9005\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 554us/step - loss: 2518.4486 - mse: 2518.4485 - mae: 29.5802 - val_loss: 1504.9792 - val_mse: 1504.9794 - val_mae: 26.8036\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2451.5162 - mse: 2451.5161 - mae: 29.2644 - val_loss: 1484.4764 - val_mse: 1484.4764 - val_mae: 27.3864\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2470.0923 - mse: 2470.0923 - mae: 29.6366 - val_loss: 1497.4517 - val_mse: 1497.4518 - val_mae: 26.9012\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2569.1385 - mse: 2569.1377 - mae: 29.8332 - val_loss: 1493.5023 - val_mse: 1493.5022 - val_mae: 26.9930\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 543us/step - loss: 2478.9431 - mse: 2478.9426 - mae: 28.9663 - val_loss: 1489.0244 - val_mse: 1489.0245 - val_mae: 27.1001\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2606.5913 - mse: 2606.5911 - mae: 29.8812 - val_loss: 1485.3127 - val_mse: 1485.3127 - val_mae: 27.1755\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2533.1666 - mse: 2533.1675 - mae: 29.0805 - val_loss: 1488.4379 - val_mse: 1488.4379 - val_mae: 27.0629\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2477.6857 - mse: 2477.6858 - mae: 29.7593 - val_loss: 1494.2344 - val_mse: 1494.2343 - val_mae: 26.8749\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 540us/step - loss: 2484.5108 - mse: 2484.5103 - mae: 29.0343 - val_loss: 1490.5852 - val_mse: 1490.5853 - val_mae: 26.9470\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2552.0572 - mse: 2552.0576 - mae: 29.3637 - val_loss: 1489.5392 - val_mse: 1489.5392 - val_mae: 26.9379\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 498us/step - loss: 2459.7681 - mse: 2459.7686 - mae: 29.4983 - val_loss: 1491.6577 - val_mse: 1491.6576 - val_mae: 26.8205\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2550.2062 - mse: 2550.2065 - mae: 29.4175 - val_loss: 1492.9639 - val_mse: 1492.9639 - val_mae: 26.7957\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2549.3258 - mse: 2549.3252 - mae: 29.3789 - val_loss: 1495.7584 - val_mse: 1495.7583 - val_mae: 26.7134\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 649us/step - loss: 2528.0489 - mse: 2528.0488 - mae: 29.4921 - val_loss: 1488.2881 - val_mse: 1488.2880 - val_mae: 26.9706\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2467.7551 - mse: 2467.7551 - mae: 29.3868 - val_loss: 1489.6807 - val_mse: 1489.6807 - val_mae: 26.9290\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2481.0074 - mse: 2481.0073 - mae: 29.4081 - val_loss: 1489.8380 - val_mse: 1489.8381 - val_mae: 26.8920\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2443.1777 - mse: 2443.1775 - mae: 29.0549 - val_loss: 1483.4036 - val_mse: 1483.4037 - val_mae: 27.0742\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2514.2774 - mse: 2514.2778 - mae: 29.3554 - val_loss: 1482.5954 - val_mse: 1482.5952 - val_mae: 27.0965\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2549.4459 - mse: 2549.4460 - mae: 29.4960 - val_loss: 1483.2698 - val_mse: 1483.2699 - val_mae: 27.0550\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2527.5620 - mse: 2527.5618 - mae: 29.8382 - val_loss: 1490.7763 - val_mse: 1490.7761 - val_mae: 26.8657\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2495.0561 - mse: 2495.0564 - mae: 29.1659 - val_loss: 1488.0888 - val_mse: 1488.0889 - val_mae: 26.9194\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2502.0086 - mse: 2502.0083 - mae: 29.2206 - val_loss: 1486.0937 - val_mse: 1486.0938 - val_mae: 26.9543\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2476.4761 - mse: 2476.4758 - mae: 29.2482 - val_loss: 1480.9918 - val_mse: 1480.9918 - val_mae: 27.1111\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 518us/step - loss: 2474.4452 - mse: 2474.4448 - mae: 28.9675 - val_loss: 1486.3608 - val_mse: 1486.3607 - val_mae: 26.9142\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2503.6862 - mse: 2503.6868 - mae: 29.4170 - val_loss: 1495.6675 - val_mse: 1495.6675 - val_mae: 26.6759\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2510.8169 - mse: 2510.8167 - mae: 29.1431 - val_loss: 1488.2309 - val_mse: 1488.2310 - val_mae: 26.8503\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 528us/step - loss: 2485.3786 - mse: 2485.3784 - mae: 29.0067 - val_loss: 1484.9016 - val_mse: 1484.9016 - val_mae: 26.9387\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2447.2666 - mse: 2447.2656 - mae: 29.3120 - val_loss: 1485.4098 - val_mse: 1485.4097 - val_mae: 26.9127\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2521.8656 - mse: 2521.8655 - mae: 29.4309 - val_loss: 1485.4620 - val_mse: 1485.4620 - val_mae: 26.8648\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2535.9253 - mse: 2535.9258 - mae: 29.3484 - val_loss: 1494.7873 - val_mse: 1494.7874 - val_mae: 26.5956\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2470.2377 - mse: 2470.2375 - mae: 29.3007 - val_loss: 1487.6076 - val_mse: 1487.6077 - val_mae: 26.7418\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 544us/step - loss: 2451.7219 - mse: 2451.7209 - mae: 28.6716 - val_loss: 1479.1847 - val_mse: 1479.1844 - val_mae: 26.9741\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2426.0630 - mse: 2426.0630 - mae: 28.8140 - val_loss: 1475.3988 - val_mse: 1475.3986 - val_mae: 27.1693\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2541.3054 - mse: 2541.3054 - mae: 29.5950 - val_loss: 1485.5690 - val_mse: 1485.5690 - val_mae: 26.8285\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2454.5041 - mse: 2454.5049 - mae: 28.9209 - val_loss: 1477.1899 - val_mse: 1477.1897 - val_mae: 27.1152\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2459.1466 - mse: 2459.1462 - mae: 29.1971 - val_loss: 1478.8832 - val_mse: 1478.8834 - val_mae: 27.0470\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2517.1533 - mse: 2517.1541 - mae: 29.3884 - val_loss: 1482.4222 - val_mse: 1482.4225 - val_mae: 26.8714\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2526.8993 - mse: 2526.8999 - mae: 29.6867 - val_loss: 1482.4726 - val_mse: 1482.4728 - val_mae: 26.8447\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2504.7538 - mse: 2504.7537 - mae: 29.4751 - val_loss: 1488.8182 - val_mse: 1488.8186 - val_mae: 26.6880\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2572.3626 - mse: 2572.3618 - mae: 29.4210 - val_loss: 1484.4894 - val_mse: 1484.4896 - val_mae: 26.8027\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2492.7587 - mse: 2492.7595 - mae: 29.2570 - val_loss: 1474.2444 - val_mse: 1474.2446 - val_mae: 27.1401\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2540.2236 - mse: 2540.2246 - mae: 29.5746 - val_loss: 1487.8195 - val_mse: 1487.8193 - val_mae: 26.7128\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2502.5197 - mse: 2502.5198 - mae: 29.3126 - val_loss: 1480.1978 - val_mse: 1480.1978 - val_mae: 26.9271\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2493.7544 - mse: 2493.7539 - mae: 29.2085 - val_loss: 1478.1153 - val_mse: 1478.1152 - val_mae: 26.9717\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2398.7505 - mse: 2398.7502 - mae: 29.5221 - val_loss: 3701.5393 - val_mse: 3701.5391 - val_mae: 23.8123\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2334.8624 - mse: 2334.8628 - mae: 28.8028 - val_loss: 3702.9122 - val_mse: 3702.9119 - val_mae: 24.0374\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2341.2646 - mse: 2341.2637 - mae: 29.3239 - val_loss: 3703.6919 - val_mse: 3703.6917 - val_mae: 24.4324\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2390.4727 - mse: 2390.4722 - mae: 29.8293 - val_loss: 3702.9350 - val_mse: 3702.9343 - val_mae: 24.1561\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 654us/step - loss: 2386.9934 - mse: 2386.9934 - mae: 29.6461 - val_loss: 3702.9755 - val_mse: 3702.9753 - val_mae: 23.7994\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2389.3494 - mse: 2389.3491 - mae: 29.3116 - val_loss: 3705.9586 - val_mse: 3705.9587 - val_mae: 24.3929\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2342.9594 - mse: 2342.9604 - mae: 29.2862 - val_loss: 3706.0564 - val_mse: 3706.0566 - val_mae: 24.1009\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2421.2825 - mse: 2421.2830 - mae: 29.5080 - val_loss: 3706.0205 - val_mse: 3706.0205 - val_mae: 23.7715\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2388.3635 - mse: 2388.3640 - mae: 29.4268 - val_loss: 3706.0345 - val_mse: 3706.0344 - val_mae: 23.8369\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2369.9530 - mse: 2369.9524 - mae: 29.6642 - val_loss: 3706.4220 - val_mse: 3706.4221 - val_mae: 23.9268\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2427.2272 - mse: 2427.2271 - mae: 29.5112 - val_loss: 3710.2769 - val_mse: 3710.2764 - val_mae: 24.5765\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2370.7018 - mse: 2370.7012 - mae: 29.3794 - val_loss: 3711.2995 - val_mse: 3711.2996 - val_mae: 24.9144\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2364.2918 - mse: 2364.2915 - mae: 29.4627 - val_loss: 3708.5196 - val_mse: 3708.5198 - val_mae: 24.5161\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 518us/step - loss: 2416.0310 - mse: 2416.0305 - mae: 29.4722 - val_loss: 3706.5391 - val_mse: 3706.5388 - val_mae: 24.1593\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2373.0645 - mse: 2373.0635 - mae: 29.6741 - val_loss: 3708.5133 - val_mse: 3708.5134 - val_mae: 24.4199\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2380.8980 - mse: 2380.8982 - mae: 29.7894 - val_loss: 3707.1257 - val_mse: 3707.1243 - val_mae: 24.1231\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 524us/step - loss: 2374.5989 - mse: 2374.5986 - mae: 29.4060 - val_loss: 3708.0975 - val_mse: 3708.0977 - val_mae: 24.2308\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2366.7690 - mse: 2366.7705 - mae: 29.5900 - val_loss: 3707.1187 - val_mse: 3707.1191 - val_mae: 24.0336\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2410.2167 - mse: 2410.2163 - mae: 29.7123 - val_loss: 3706.1425 - val_mse: 3706.1426 - val_mae: 23.9710\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2430.4927 - mse: 2430.4932 - mae: 29.7540 - val_loss: 3707.2706 - val_mse: 3707.2710 - val_mae: 23.9762\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2425.6578 - mse: 2425.6577 - mae: 29.7867 - val_loss: 3708.2509 - val_mse: 3708.2507 - val_mae: 24.1736\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2455.5983 - mse: 2455.5974 - mae: 29.6154 - val_loss: 3709.0212 - val_mse: 3709.0210 - val_mae: 24.1125\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2385.3743 - mse: 2385.3743 - mae: 29.6397 - val_loss: 3708.8956 - val_mse: 3708.8955 - val_mae: 24.3998\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2326.1825 - mse: 2326.1821 - mae: 29.3471 - val_loss: 3708.2806 - val_mse: 3708.2810 - val_mae: 23.7162\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2411.7705 - mse: 2411.7700 - mae: 29.6894 - val_loss: 3708.8260 - val_mse: 3708.8250 - val_mae: 23.6294\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2354.8226 - mse: 2354.8232 - mae: 29.1713 - val_loss: 3710.2235 - val_mse: 3710.2241 - val_mae: 24.1965\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2363.6306 - mse: 2363.6301 - mae: 29.5331 - val_loss: 3708.7891 - val_mse: 3708.7896 - val_mae: 23.8533\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2330.0807 - mse: 2330.0811 - mae: 29.5915 - val_loss: 3710.2112 - val_mse: 3710.2119 - val_mae: 24.3037\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 563us/step - loss: 2355.4073 - mse: 2355.4077 - mae: 29.7055 - val_loss: 3710.6684 - val_mse: 3710.6680 - val_mae: 24.2958\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2360.2207 - mse: 2360.2205 - mae: 29.0738 - val_loss: 3710.0354 - val_mse: 3710.0352 - val_mae: 24.1181\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 492us/step - loss: 2344.2906 - mse: 2344.2896 - mae: 29.1219 - val_loss: 3709.3939 - val_mse: 3709.3943 - val_mae: 24.3790\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2325.6618 - mse: 2325.6604 - mae: 29.6276 - val_loss: 3709.0388 - val_mse: 3709.0381 - val_mae: 24.0128\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 512us/step - loss: 2333.4973 - mse: 2333.4973 - mae: 29.1767 - val_loss: 3711.2492 - val_mse: 3711.2490 - val_mae: 24.5007\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2392.8942 - mse: 2392.8933 - mae: 29.4808 - val_loss: 3709.9589 - val_mse: 3709.9583 - val_mae: 24.0832\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2382.9311 - mse: 2382.9319 - mae: 29.8624 - val_loss: 3711.2187 - val_mse: 3711.2185 - val_mae: 24.2265\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2341.8939 - mse: 2341.8933 - mae: 29.2460 - val_loss: 3710.8677 - val_mse: 3710.8672 - val_mae: 24.1134\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2357.9966 - mse: 2357.9966 - mae: 29.0632 - val_loss: 3711.2289 - val_mse: 3711.2290 - val_mae: 24.0386\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2399.4575 - mse: 2399.4580 - mae: 29.4335 - val_loss: 3709.4657 - val_mse: 3709.4663 - val_mae: 23.7126\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2361.5919 - mse: 2361.5913 - mae: 29.2118 - val_loss: 3708.7634 - val_mse: 3708.7637 - val_mae: 23.7666\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2323.2745 - mse: 2323.2744 - mae: 29.0898 - val_loss: 3710.0990 - val_mse: 3710.0999 - val_mae: 24.1771\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 538us/step - loss: 2319.1664 - mse: 2319.1650 - mae: 28.9716 - val_loss: 3710.5050 - val_mse: 3710.5044 - val_mae: 24.0065\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2343.3512 - mse: 2343.3513 - mae: 29.3384 - val_loss: 3709.8374 - val_mse: 3709.8376 - val_mae: 24.2656\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2330.1632 - mse: 2330.1628 - mae: 29.1739 - val_loss: 3709.7017 - val_mse: 3709.7014 - val_mae: 23.9227\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2384.9681 - mse: 2384.9678 - mae: 29.3817 - val_loss: 3708.8456 - val_mse: 3708.8455 - val_mae: 24.0474\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2344.0766 - mse: 2344.0759 - mae: 29.4056 - val_loss: 3709.8710 - val_mse: 3709.8711 - val_mae: 24.4105\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2350.5222 - mse: 2350.5215 - mae: 29.5087 - val_loss: 3710.1832 - val_mse: 3710.1833 - val_mae: 24.3022\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 537us/step - loss: 2313.2634 - mse: 2313.2627 - mae: 29.0520 - val_loss: 3709.7487 - val_mse: 3709.7480 - val_mae: 24.1328\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2351.3582 - mse: 2351.3582 - mae: 29.1580 - val_loss: 3709.6894 - val_mse: 3709.6892 - val_mae: 24.0573\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2285.6973 - mse: 2285.6980 - mae: 28.6253 - val_loss: 3712.0899 - val_mse: 3712.0901 - val_mae: 24.6271\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2321.2388 - mse: 2321.2390 - mae: 29.0985 - val_loss: 3710.6354 - val_mse: 3710.6360 - val_mae: 23.9005\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2354.9950 - mse: 2354.9949 - mae: 29.1714 - val_loss: 3710.9598 - val_mse: 3710.9600 - val_mae: 24.0890\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2321.7317 - mse: 2321.7317 - mae: 29.0709 - val_loss: 3710.6043 - val_mse: 3710.6038 - val_mae: 24.1101\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2332.5094 - mse: 2332.5098 - mae: 29.0510 - val_loss: 3712.0989 - val_mse: 3712.0986 - val_mae: 24.5400\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2290.9053 - mse: 2290.9065 - mae: 29.2054 - val_loss: 3710.6110 - val_mse: 3710.6121 - val_mae: 24.0003\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2365.9035 - mse: 2365.9031 - mae: 28.9742 - val_loss: 3710.6222 - val_mse: 3710.6228 - val_mae: 24.4809\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2316.3833 - mse: 2316.3838 - mae: 29.3518 - val_loss: 3710.4429 - val_mse: 3710.4434 - val_mae: 23.9798\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2348.4982 - mse: 2348.4980 - mae: 29.3091 - val_loss: 3712.7599 - val_mse: 3712.7603 - val_mae: 24.5283\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 695us/step - loss: 2382.9391 - mse: 2382.9392 - mae: 29.4773 - val_loss: 3709.8711 - val_mse: 3709.8704 - val_mae: 24.2792\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2330.8608 - mse: 2330.8608 - mae: 29.3248 - val_loss: 3711.0113 - val_mse: 3711.0110 - val_mae: 24.3811\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2360.8098 - mse: 2360.8103 - mae: 29.6302 - val_loss: 3711.4988 - val_mse: 3711.4983 - val_mae: 24.2356\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 571us/step - loss: 2332.8413 - mse: 2332.8411 - mae: 29.3650 - val_loss: 3711.1042 - val_mse: 3711.1038 - val_mae: 23.9630\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2328.1071 - mse: 2328.1064 - mae: 29.1912 - val_loss: 3711.5985 - val_mse: 3711.5981 - val_mae: 24.3613\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2378.1076 - mse: 2378.1079 - mae: 29.3099 - val_loss: 3710.5069 - val_mse: 3710.5078 - val_mae: 23.6418\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2342.3500 - mse: 2342.3499 - mae: 29.0980 - val_loss: 3712.2777 - val_mse: 3712.2773 - val_mae: 24.4356\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 520us/step - loss: 2340.6399 - mse: 2340.6406 - mae: 29.3111 - val_loss: 3710.1195 - val_mse: 3710.1191 - val_mae: 24.2879\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2346.9912 - mse: 2346.9915 - mae: 29.0404 - val_loss: 3708.0630 - val_mse: 3708.0625 - val_mae: 24.1818\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2338.6932 - mse: 2338.6934 - mae: 29.1829 - val_loss: 3708.6354 - val_mse: 3708.6353 - val_mae: 24.2610\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 558us/step - loss: 2355.5957 - mse: 2355.5957 - mae: 29.2766 - val_loss: 3707.4958 - val_mse: 3707.4954 - val_mae: 23.7918\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2290.6349 - mse: 2290.6340 - mae: 28.5707 - val_loss: 3708.0633 - val_mse: 3708.0625 - val_mae: 24.2430\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2318.8001 - mse: 2318.7993 - mae: 29.2089 - val_loss: 3709.2929 - val_mse: 3709.2925 - val_mae: 24.4340\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2329.2104 - mse: 2329.2109 - mae: 29.0196 - val_loss: 3709.8155 - val_mse: 3709.8157 - val_mae: 24.4847\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2382.1562 - mse: 2382.1565 - mae: 29.5524 - val_loss: 3708.1299 - val_mse: 3708.1309 - val_mae: 24.1426\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2327.4831 - mse: 2327.4827 - mae: 29.0477 - val_loss: 3707.6199 - val_mse: 3707.6199 - val_mae: 24.2665\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2321.6031 - mse: 2321.6035 - mae: 28.9839 - val_loss: 3705.7939 - val_mse: 3705.7944 - val_mae: 23.9151\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2374.7843 - mse: 2374.7844 - mae: 29.2552 - val_loss: 3705.5803 - val_mse: 3705.5793 - val_mae: 23.8778\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2252.2302 - mse: 2252.2292 - mae: 28.6115 - val_loss: 3705.5401 - val_mse: 3705.5398 - val_mae: 24.0949\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2334.5540 - mse: 2334.5540 - mae: 28.8798 - val_loss: 3705.6072 - val_mse: 3705.6067 - val_mae: 24.1959\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2349.1159 - mse: 2349.1160 - mae: 28.9703 - val_loss: 3707.9940 - val_mse: 3707.9944 - val_mae: 24.6366\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2360.3644 - mse: 2360.3640 - mae: 29.2272 - val_loss: 3706.4509 - val_mse: 3706.4514 - val_mae: 24.1122\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2388.2752 - mse: 2388.2751 - mae: 29.2117 - val_loss: 3708.2265 - val_mse: 3708.2253 - val_mae: 24.5967\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2741.5854 - mse: 2741.5854 - mae: 28.8442 - val_loss: 2192.7341 - val_mse: 2192.7341 - val_mae: 26.7787\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 549us/step - loss: 2757.5709 - mse: 2757.5710 - mae: 28.9093 - val_loss: 2190.9743 - val_mse: 2190.9746 - val_mae: 26.7512\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2690.6988 - mse: 2690.6987 - mae: 28.4235 - val_loss: 2185.1813 - val_mse: 2185.1816 - val_mae: 26.7626\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2708.0948 - mse: 2708.0942 - mae: 28.3763 - val_loss: 2184.4449 - val_mse: 2184.4451 - val_mae: 26.7525\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2774.0418 - mse: 2774.0420 - mae: 28.7565 - val_loss: 2181.0830 - val_mse: 2181.0833 - val_mae: 26.6370\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2713.2514 - mse: 2713.2520 - mae: 28.5957 - val_loss: 2178.8852 - val_mse: 2178.8850 - val_mae: 26.5515\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2756.3582 - mse: 2756.3584 - mae: 28.9470 - val_loss: 2188.6669 - val_mse: 2188.6672 - val_mae: 26.5739\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2706.7917 - mse: 2706.7915 - mae: 28.7533 - val_loss: 2198.7155 - val_mse: 2198.7156 - val_mae: 26.5261\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2710.2431 - mse: 2710.2439 - mae: 28.4473 - val_loss: 2194.5858 - val_mse: 2194.5859 - val_mae: 26.6268\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2777.0111 - mse: 2777.0127 - mae: 28.4673 - val_loss: 2193.5249 - val_mse: 2193.5249 - val_mae: 26.8109\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2684.6275 - mse: 2684.6274 - mae: 28.1989 - val_loss: 2214.2475 - val_mse: 2214.2473 - val_mae: 26.2397\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 516us/step - loss: 2679.2183 - mse: 2679.2180 - mae: 28.2891 - val_loss: 2181.4438 - val_mse: 2181.4436 - val_mae: 26.9224\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2685.6267 - mse: 2685.6262 - mae: 28.4888 - val_loss: 2179.9622 - val_mse: 2179.9624 - val_mae: 26.7583\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 476us/step - loss: 2732.8386 - mse: 2732.8386 - mae: 28.7966 - val_loss: 2186.2037 - val_mse: 2186.2036 - val_mae: 26.7074\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2669.0389 - mse: 2669.0391 - mae: 27.9977 - val_loss: 2204.0328 - val_mse: 2204.0327 - val_mae: 26.6691\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2718.6556 - mse: 2718.6565 - mae: 28.4787 - val_loss: 2193.3711 - val_mse: 2193.3716 - val_mae: 27.0600\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2677.4234 - mse: 2677.4238 - mae: 28.5499 - val_loss: 2188.2522 - val_mse: 2188.2520 - val_mae: 27.1104\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2710.9245 - mse: 2710.9250 - mae: 28.6875 - val_loss: 2197.4872 - val_mse: 2197.4871 - val_mae: 26.5470\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2734.6127 - mse: 2734.6125 - mae: 28.6322 - val_loss: 2197.8780 - val_mse: 2197.8777 - val_mae: 26.7068\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2683.5620 - mse: 2683.5615 - mae: 28.1014 - val_loss: 2197.4437 - val_mse: 2197.4436 - val_mae: 26.7267\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 551us/step - loss: 2732.2765 - mse: 2732.2771 - mae: 28.5216 - val_loss: 2208.4711 - val_mse: 2208.4709 - val_mae: 26.7995\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2743.9821 - mse: 2743.9817 - mae: 28.5449 - val_loss: 2215.0539 - val_mse: 2215.0544 - val_mae: 26.5865\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 549us/step - loss: 2722.2718 - mse: 2722.2717 - mae: 28.4531 - val_loss: 2206.5352 - val_mse: 2206.5352 - val_mae: 26.9144\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2688.2744 - mse: 2688.2744 - mae: 28.2465 - val_loss: 2208.2341 - val_mse: 2208.2339 - val_mae: 26.9419\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2741.6323 - mse: 2741.6331 - mae: 28.7759 - val_loss: 2217.9072 - val_mse: 2217.9072 - val_mae: 26.8128\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2688.1539 - mse: 2688.1536 - mae: 28.3430 - val_loss: 2208.5430 - val_mse: 2208.5432 - val_mae: 26.7726\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2711.1204 - mse: 2711.1201 - mae: 28.6014 - val_loss: 2198.5505 - val_mse: 2198.5503 - val_mae: 27.0482\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2672.7445 - mse: 2672.7454 - mae: 28.2692 - val_loss: 2202.5866 - val_mse: 2202.5867 - val_mae: 26.9244\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2695.5086 - mse: 2695.5076 - mae: 28.6674 - val_loss: 2191.8632 - val_mse: 2191.8633 - val_mae: 27.0845\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2695.8727 - mse: 2695.8723 - mae: 28.5448 - val_loss: 2189.3309 - val_mse: 2189.3308 - val_mae: 26.9187\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2737.1862 - mse: 2737.1855 - mae: 28.4296 - val_loss: 2201.0658 - val_mse: 2201.0659 - val_mae: 26.5309\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2685.5871 - mse: 2685.5872 - mae: 28.4591 - val_loss: 2201.0624 - val_mse: 2201.0623 - val_mae: 26.9712\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2696.2114 - mse: 2696.2112 - mae: 28.3403 - val_loss: 2211.5296 - val_mse: 2211.5295 - val_mae: 26.7986\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2693.7885 - mse: 2693.7888 - mae: 28.5009 - val_loss: 2212.4581 - val_mse: 2212.4583 - val_mae: 26.8805\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2737.8884 - mse: 2737.8882 - mae: 28.7340 - val_loss: 2211.6516 - val_mse: 2211.6516 - val_mae: 26.9275\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 550us/step - loss: 2661.2700 - mse: 2661.2698 - mae: 28.3201 - val_loss: 2211.8391 - val_mse: 2211.8386 - val_mae: 26.8891\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2681.5212 - mse: 2681.5208 - mae: 28.3714 - val_loss: 2197.0310 - val_mse: 2197.0310 - val_mae: 27.1144\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2691.9041 - mse: 2691.9036 - mae: 28.3493 - val_loss: 2202.6580 - val_mse: 2202.6582 - val_mae: 26.9510\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2647.5939 - mse: 2647.5945 - mae: 28.3534 - val_loss: 2201.0438 - val_mse: 2201.0437 - val_mae: 27.1236\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2712.7085 - mse: 2712.7080 - mae: 28.5830 - val_loss: 2214.9637 - val_mse: 2214.9636 - val_mae: 26.7401\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2685.2280 - mse: 2685.2271 - mae: 28.3061 - val_loss: 2202.4180 - val_mse: 2202.4177 - val_mae: 27.0641\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2702.1611 - mse: 2702.1602 - mae: 28.2057 - val_loss: 2211.2458 - val_mse: 2211.2458 - val_mae: 26.8685\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2704.4870 - mse: 2704.4863 - mae: 28.1506 - val_loss: 2200.6966 - val_mse: 2200.6968 - val_mae: 26.9254\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2704.2836 - mse: 2704.2832 - mae: 28.4256 - val_loss: 2208.4151 - val_mse: 2208.4150 - val_mae: 26.6161\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2666.3542 - mse: 2666.3550 - mae: 28.6154 - val_loss: 2205.7803 - val_mse: 2205.7808 - val_mae: 26.8241\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2687.7198 - mse: 2687.7200 - mae: 28.4024 - val_loss: 2196.3142 - val_mse: 2196.3145 - val_mae: 27.0490\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2683.9945 - mse: 2683.9949 - mae: 28.6299 - val_loss: 2202.6041 - val_mse: 2202.6040 - val_mae: 27.0279\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2748.0860 - mse: 2748.0864 - mae: 28.5449 - val_loss: 2205.5603 - val_mse: 2205.5603 - val_mae: 27.1270\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2671.0905 - mse: 2671.0901 - mae: 28.0634 - val_loss: 2212.8866 - val_mse: 2212.8862 - val_mae: 26.6289\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2674.6826 - mse: 2674.6826 - mae: 28.3404 - val_loss: 2203.4436 - val_mse: 2203.4441 - val_mae: 26.9358\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2670.2624 - mse: 2670.2629 - mae: 27.9770 - val_loss: 2206.2367 - val_mse: 2206.2371 - val_mae: 26.9259\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2703.6292 - mse: 2703.6292 - mae: 28.3236 - val_loss: 2218.6209 - val_mse: 2218.6211 - val_mae: 26.7104\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2694.6662 - mse: 2694.6660 - mae: 28.2909 - val_loss: 2222.2245 - val_mse: 2222.2249 - val_mae: 26.7285\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2742.4705 - mse: 2742.4702 - mae: 28.5069 - val_loss: 2203.4878 - val_mse: 2203.4878 - val_mae: 26.8830\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2665.1906 - mse: 2665.1904 - mae: 28.2915 - val_loss: 2208.5086 - val_mse: 2208.5085 - val_mae: 26.7933\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2674.6043 - mse: 2674.6038 - mae: 27.9149 - val_loss: 2198.3277 - val_mse: 2198.3271 - val_mae: 27.0842\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2694.0195 - mse: 2694.0198 - mae: 28.5032 - val_loss: 2209.1785 - val_mse: 2209.1787 - val_mae: 26.9420\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2693.5616 - mse: 2693.5610 - mae: 28.4235 - val_loss: 2213.1040 - val_mse: 2213.1038 - val_mae: 26.8215\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2711.8022 - mse: 2711.8013 - mae: 28.6658 - val_loss: 2217.4235 - val_mse: 2217.4233 - val_mae: 26.7659\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2681.4775 - mse: 2681.4768 - mae: 28.2546 - val_loss: 2221.4312 - val_mse: 2221.4314 - val_mae: 26.9210\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 533us/step - loss: 2720.0470 - mse: 2720.0474 - mae: 28.7458 - val_loss: 2219.9920 - val_mse: 2219.9919 - val_mae: 26.7599\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2697.7799 - mse: 2697.7800 - mae: 28.3313 - val_loss: 2215.4663 - val_mse: 2215.4663 - val_mae: 26.7917\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2683.6475 - mse: 2683.6477 - mae: 28.1479 - val_loss: 2216.8014 - val_mse: 2216.8010 - val_mae: 26.9074\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2709.6425 - mse: 2709.6426 - mae: 28.6128 - val_loss: 2216.1721 - val_mse: 2216.1724 - val_mae: 26.7203\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 668us/step - loss: 2715.8217 - mse: 2715.8215 - mae: 28.5322 - val_loss: 2224.0994 - val_mse: 2224.0994 - val_mae: 26.5521\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2673.4204 - mse: 2673.4209 - mae: 28.4939 - val_loss: 2221.4999 - val_mse: 2221.4998 - val_mae: 26.8158\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2696.6652 - mse: 2696.6658 - mae: 28.1715 - val_loss: 2212.0385 - val_mse: 2212.0388 - val_mae: 26.9697\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2671.9995 - mse: 2671.9988 - mae: 27.8520 - val_loss: 2215.1016 - val_mse: 2215.1016 - val_mae: 27.1145\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 549us/step - loss: 2691.0572 - mse: 2691.0574 - mae: 28.5100 - val_loss: 2216.1701 - val_mse: 2216.1699 - val_mae: 26.8993\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2660.9053 - mse: 2660.9055 - mae: 27.9643 - val_loss: 2216.0648 - val_mse: 2216.0652 - val_mae: 26.9752\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 483us/step - loss: 2701.3922 - mse: 2701.3923 - mae: 28.6504 - val_loss: 2225.7640 - val_mse: 2225.7637 - val_mae: 26.6016\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 478us/step - loss: 2711.0427 - mse: 2711.0430 - mae: 28.2649 - val_loss: 2218.7505 - val_mse: 2218.7505 - val_mae: 27.0248\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 467us/step - loss: 2700.7244 - mse: 2700.7244 - mae: 28.5625 - val_loss: 2220.7299 - val_mse: 2220.7300 - val_mae: 26.9050\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2645.8521 - mse: 2645.8523 - mae: 28.1140 - val_loss: 2204.8204 - val_mse: 2204.8201 - val_mae: 27.2953\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2675.4836 - mse: 2675.4839 - mae: 28.3211 - val_loss: 2211.6437 - val_mse: 2211.6438 - val_mae: 27.2037\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2720.5175 - mse: 2720.5186 - mae: 28.5001 - val_loss: 2221.6163 - val_mse: 2221.6162 - val_mae: 26.9083\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2636.2918 - mse: 2636.2913 - mae: 27.8549 - val_loss: 2215.3373 - val_mse: 2215.3379 - val_mae: 27.0848\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2663.0837 - mse: 2663.0840 - mae: 28.2929 - val_loss: 2218.3782 - val_mse: 2218.3777 - val_mae: 26.8832\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2702.1731 - mse: 2702.1731 - mae: 28.1776 - val_loss: 2216.2486 - val_mse: 2216.2485 - val_mae: 26.8986\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2639.3655 - mse: 2639.3652 - mae: 28.1301 - val_loss: 2202.0167 - val_mse: 2202.0168 - val_mae: 26.9784\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 13337.1325 - mse: 13337.1328 - mae: 109.9395 - val_loss: 34628.0269 - val_mse: 34628.0273 - val_mae: 132.7650\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 609us/step - loss: 13198.2127 - mse: 13198.2119 - mae: 109.3100 - val_loss: 34345.2259 - val_mse: 34345.2266 - val_mae: 131.6979\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 12763.3016 - mse: 12763.3008 - mae: 107.3287 - val_loss: 33525.0352 - val_mse: 33525.0391 - val_mae: 128.5521\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 11607.4264 - mse: 11607.4258 - mae: 101.7856 - val_loss: 31273.0091 - val_mse: 31273.0078 - val_mae: 119.4893\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 8818.8366 - mse: 8818.8369 - mae: 86.7653 - val_loss: 26142.5967 - val_mse: 26142.5977 - val_mae: 95.6987\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 4175.9390 - mse: 4175.9395 - mae: 51.6629 - val_loss: 19057.3766 - val_mse: 19057.3789 - val_mae: 46.3041\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 2378.9052 - mse: 2378.9055 - mae: 35.6369 - val_loss: 17720.7025 - val_mse: 17720.7012 - val_mae: 36.9988\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 550us/step - loss: 2317.9182 - mse: 2317.9180 - mae: 34.3858 - val_loss: 17984.9139 - val_mse: 17984.9141 - val_mae: 37.8400\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2287.3624 - mse: 2287.3628 - mae: 33.7073 - val_loss: 17800.8592 - val_mse: 17800.8594 - val_mae: 37.1210\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2255.5909 - mse: 2255.5908 - mae: 34.6896 - val_loss: 17986.1797 - val_mse: 17986.1797 - val_mae: 37.8409\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 661us/step - loss: 2454.5161 - mse: 2454.5161 - mae: 35.1252 - val_loss: 18039.4984 - val_mse: 18039.4980 - val_mae: 38.1093\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 640us/step - loss: 2116.6253 - mse: 2116.6252 - mae: 32.3962 - val_loss: 17709.3570 - val_mse: 17709.3574 - val_mae: 36.9913\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2205.0943 - mse: 2205.0945 - mae: 33.6495 - val_loss: 18093.5032 - val_mse: 18093.5039 - val_mae: 38.4247\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 629us/step - loss: 2445.8912 - mse: 2445.8916 - mae: 34.5029 - val_loss: 17896.4051 - val_mse: 17896.4062 - val_mae: 37.4823\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 480us/step - loss: 2242.1071 - mse: 2242.1069 - mae: 32.6439 - val_loss: 17855.6375 - val_mse: 17855.6387 - val_mae: 37.3293\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 539us/step - loss: 1968.8253 - mse: 1968.8251 - mae: 31.3224 - val_loss: 17773.8060 - val_mse: 17773.8086 - val_mae: 37.0346\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2196.0070 - mse: 2196.0071 - mae: 33.2871 - val_loss: 17931.2667 - val_mse: 17931.2656 - val_mae: 37.6141\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 429us/step - loss: 2174.2613 - mse: 2174.2615 - mae: 33.0355 - val_loss: 17863.7992 - val_mse: 17863.7988 - val_mae: 37.3633\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 2157.3038 - mse: 2157.3037 - mae: 32.2121 - val_loss: 17773.4620 - val_mse: 17773.4629 - val_mae: 37.0340\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2200.4448 - mse: 2200.4453 - mae: 32.7602 - val_loss: 17877.5006 - val_mse: 17877.5020 - val_mae: 37.4136\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 2119.0417 - mse: 2119.0417 - mae: 31.8711 - val_loss: 17842.8846 - val_mse: 17842.8867 - val_mae: 37.2830\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 513us/step - loss: 2038.3824 - mse: 2038.3822 - mae: 32.0782 - val_loss: 17729.8346 - val_mse: 17729.8340 - val_mae: 36.9991\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 2216.0057 - mse: 2216.0059 - mae: 33.3372 - val_loss: 17814.4362 - val_mse: 17814.4355 - val_mae: 37.1720\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2144.0583 - mse: 2144.0583 - mae: 32.5946 - val_loss: 18089.1273 - val_mse: 18089.1270 - val_mae: 38.3977\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 2153.5805 - mse: 2153.5803 - mae: 32.4432 - val_loss: 17924.9378 - val_mse: 17924.9375 - val_mae: 37.5897\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 1939.4244 - mse: 1939.4246 - mae: 31.4960 - val_loss: 17738.4972 - val_mse: 17738.4961 - val_mae: 37.0028\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 1953.7195 - mse: 1953.7195 - mae: 32.4569 - val_loss: 17846.1802 - val_mse: 17846.1797 - val_mae: 37.2957\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 2136.9906 - mse: 2136.9907 - mae: 32.7267 - val_loss: 17734.1667 - val_mse: 17734.1680 - val_mae: 37.0009\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 2230.8227 - mse: 2230.8228 - mae: 33.2396 - val_loss: 17880.7745 - val_mse: 17880.7754 - val_mae: 37.4247\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 516us/step - loss: 2117.2455 - mse: 2117.2456 - mae: 32.5410 - val_loss: 17835.5269 - val_mse: 17835.5254 - val_mae: 37.2530\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2132.7761 - mse: 2132.7764 - mae: 31.4051 - val_loss: 17748.0517 - val_mse: 17748.0527 - val_mae: 37.0058\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 706us/step - loss: 2091.2670 - mse: 2091.2668 - mae: 31.4840 - val_loss: 17811.1084 - val_mse: 17811.1074 - val_mae: 37.1598\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 1995.1693 - mse: 1995.1694 - mae: 31.6035 - val_loss: 17808.4628 - val_mse: 17808.4629 - val_mae: 37.1512\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 1890.7625 - mse: 1890.7626 - mae: 30.5585 - val_loss: 17680.0017 - val_mse: 17680.0020 - val_mae: 37.0077\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2004.8571 - mse: 2004.8572 - mae: 30.9339 - val_loss: 17743.8297 - val_mse: 17743.8281 - val_mae: 37.0057\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2194.9604 - mse: 2194.9602 - mae: 32.5015 - val_loss: 17827.3975 - val_mse: 17827.3965 - val_mae: 37.2253\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 2039.6545 - mse: 2039.6544 - mae: 31.1209 - val_loss: 17846.8560 - val_mse: 17846.8574 - val_mae: 37.3014\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2164.9855 - mse: 2164.9856 - mae: 31.9484 - val_loss: 17727.8581 - val_mse: 17727.8574 - val_mae: 36.9996\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 1907.2021 - mse: 1907.2020 - mae: 30.1385 - val_loss: 17572.9905 - val_mse: 17572.9902 - val_mae: 37.2591\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2179.6075 - mse: 2179.6077 - mae: 32.8567 - val_loss: 17915.4434 - val_mse: 17915.4434 - val_mae: 37.5555\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 519us/step - loss: 2055.7176 - mse: 2055.7178 - mae: 30.5449 - val_loss: 17793.9212 - val_mse: 17793.9199 - val_mae: 37.0961\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 1937.4736 - mse: 1937.4736 - mae: 30.9516 - val_loss: 17737.0587 - val_mse: 17737.0586 - val_mae: 37.0050\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 1930.7613 - mse: 1930.7614 - mae: 30.7525 - val_loss: 17914.7578 - val_mse: 17914.7578 - val_mae: 37.5580\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 466us/step - loss: 2011.2254 - mse: 2011.2253 - mae: 30.3538 - val_loss: 17674.7360 - val_mse: 17674.7383 - val_mae: 37.0182\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 466us/step - loss: 1857.7242 - mse: 1857.7240 - mae: 30.0089 - val_loss: 17681.8821 - val_mse: 17681.8848 - val_mae: 37.0055\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2008.6855 - mse: 2008.6855 - mae: 31.0094 - val_loss: 17755.5967 - val_mse: 17755.5938 - val_mae: 37.0147\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 2059.7922 - mse: 2059.7922 - mae: 31.4521 - val_loss: 17827.3480 - val_mse: 17827.3477 - val_mae: 37.2280\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 1935.2308 - mse: 1935.2310 - mae: 29.9736 - val_loss: 17751.7812 - val_mse: 17751.7793 - val_mae: 37.0115\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 1994.3076 - mse: 1994.3073 - mae: 30.6270 - val_loss: 17793.1746 - val_mse: 17793.1738 - val_mae: 37.0969\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 1946.7829 - mse: 1946.7831 - mae: 30.3509 - val_loss: 17713.6557 - val_mse: 17713.6562 - val_mae: 36.9961\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2081.6785 - mse: 2081.6787 - mae: 30.3874 - val_loss: 17923.1646 - val_mse: 17923.1641 - val_mae: 37.5862\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 681us/step - loss: 2009.4576 - mse: 2009.4575 - mae: 30.8414 - val_loss: 17776.1998 - val_mse: 17776.2012 - val_mae: 37.0442\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 701us/step - loss: 1975.7888 - mse: 1975.7887 - mae: 30.9364 - val_loss: 17948.3684 - val_mse: 17948.3672 - val_mae: 37.6867\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 707us/step - loss: 1945.8900 - mse: 1945.8899 - mae: 29.6295 - val_loss: 17687.5497 - val_mse: 17687.5508 - val_mae: 37.0006\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 486us/step - loss: 1898.6901 - mse: 1898.6902 - mae: 30.8619 - val_loss: 17817.6596 - val_mse: 17817.6602 - val_mae: 37.1919\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 1951.8732 - mse: 1951.8732 - mae: 29.4993 - val_loss: 17743.8303 - val_mse: 17743.8320 - val_mae: 37.0085\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 735us/step - loss: 1945.8092 - mse: 1945.8090 - mae: 29.8733 - val_loss: 17900.1173 - val_mse: 17900.1172 - val_mae: 37.5035\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 719us/step - loss: 1942.8541 - mse: 1942.8540 - mae: 30.3448 - val_loss: 17755.7530 - val_mse: 17755.7539 - val_mae: 37.0152\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 1755.4927 - mse: 1755.4927 - mae: 28.2554 - val_loss: 17658.1886 - val_mse: 17658.1875 - val_mae: 37.0504\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 661us/step - loss: 1763.5175 - mse: 1763.5175 - mae: 28.4347 - val_loss: 17718.3113 - val_mse: 17718.3125 - val_mae: 36.9973\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 1828.3627 - mse: 1828.3627 - mae: 29.0590 - val_loss: 17678.4490 - val_mse: 17678.4473 - val_mae: 37.0107\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 1979.1634 - mse: 1979.1632 - mae: 31.0996 - val_loss: 17750.2715 - val_mse: 17750.2715 - val_mae: 37.0132\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 498us/step - loss: 1918.2050 - mse: 1918.2048 - mae: 30.9457 - val_loss: 17804.7817 - val_mse: 17804.7812 - val_mae: 37.1461\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 556us/step - loss: 1816.1576 - mse: 1816.1575 - mae: 28.1641 - val_loss: 17562.5117 - val_mse: 17562.5098 - val_mae: 37.2874\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 1849.9480 - mse: 1849.9482 - mae: 30.1876 - val_loss: 17670.0722 - val_mse: 17670.0723 - val_mae: 37.0282\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 1815.5989 - mse: 1815.5990 - mae: 28.5133 - val_loss: 17682.2155 - val_mse: 17682.2168 - val_mae: 37.0088\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 522us/step - loss: 1733.7297 - mse: 1733.7296 - mae: 28.6419 - val_loss: 17736.1800 - val_mse: 17736.1797 - val_mae: 37.0120\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 1910.3996 - mse: 1910.3997 - mae: 29.7419 - val_loss: 17798.7035 - val_mse: 17798.7031 - val_mae: 37.1333\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 1943.4026 - mse: 1943.4025 - mae: 29.9208 - val_loss: 17755.3254 - val_mse: 17755.3242 - val_mae: 37.0236\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 1965.1320 - mse: 1965.1320 - mae: 30.1964 - val_loss: 17766.6191 - val_mse: 17766.6172 - val_mae: 37.0316\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 744us/step - loss: 1945.0512 - mse: 1945.0513 - mae: 29.8727 - val_loss: 17745.4435 - val_mse: 17745.4434 - val_mae: 37.0164\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2042.2745 - mse: 2042.2745 - mae: 31.3940 - val_loss: 17850.0380 - val_mse: 17850.0371 - val_mae: 37.3288\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 1880.3103 - mse: 1880.3105 - mae: 29.5003 - val_loss: 17676.0135 - val_mse: 17676.0156 - val_mae: 37.0166\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 413us/step - loss: 1968.1325 - mse: 1968.1326 - mae: 29.8276 - val_loss: 17837.3180 - val_mse: 17837.3184 - val_mae: 37.2820\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 1869.1928 - mse: 1869.1926 - mae: 29.3136 - val_loss: 17674.8227 - val_mse: 17674.8242 - val_mae: 37.0189\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 527us/step - loss: 1956.2015 - mse: 1956.2017 - mae: 30.1574 - val_loss: 17761.5632 - val_mse: 17761.5645 - val_mae: 37.0261\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 1909.7665 - mse: 1909.7666 - mae: 29.7807 - val_loss: 17880.0177 - val_mse: 17880.0156 - val_mae: 37.4412\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 1918.6462 - mse: 1918.6461 - mae: 29.4965 - val_loss: 17797.7919 - val_mse: 17797.7930 - val_mae: 37.1267\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 565us/step - loss: 1784.9789 - mse: 1784.9788 - mae: 28.3962 - val_loss: 17562.4350 - val_mse: 17562.4336 - val_mae: 37.2872\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 535us/step - loss: 1961.0595 - mse: 1961.0593 - mae: 30.1073 - val_loss: 17693.6555 - val_mse: 17693.6562 - val_mae: 36.9990\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4071.7961 - mse: 4071.7959 - mae: 33.4464 - val_loss: 2184.1221 - val_mse: 2184.1223 - val_mae: 31.2418\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4300.6655 - mse: 4300.6650 - mae: 35.3417 - val_loss: 2311.1888 - val_mse: 2311.1890 - val_mae: 31.5802\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4198.6749 - mse: 4198.6748 - mae: 34.3686 - val_loss: 2338.4129 - val_mse: 2338.4128 - val_mae: 31.6653\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 422us/step - loss: 4090.6793 - mse: 4090.6790 - mae: 34.5167 - val_loss: 2287.0262 - val_mse: 2287.0264 - val_mae: 31.5152\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4005.4952 - mse: 4005.4951 - mae: 33.9434 - val_loss: 2323.9778 - val_mse: 2323.9778 - val_mae: 31.6265\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 653us/step - loss: 4276.9816 - mse: 4276.9814 - mae: 35.4661 - val_loss: 2395.7709 - val_mse: 2395.7708 - val_mae: 31.8429\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4222.9440 - mse: 4222.9448 - mae: 35.0535 - val_loss: 2343.9537 - val_mse: 2343.9536 - val_mae: 31.6908\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 510us/step - loss: 4170.4706 - mse: 4170.4712 - mae: 34.2372 - val_loss: 2383.2951 - val_mse: 2383.2952 - val_mae: 31.8030\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 503us/step - loss: 4139.0040 - mse: 4139.0039 - mae: 33.6918 - val_loss: 2328.7480 - val_mse: 2328.7483 - val_mae: 31.6509\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 530us/step - loss: 4290.5889 - mse: 4290.5889 - mae: 34.6746 - val_loss: 2362.3292 - val_mse: 2362.3293 - val_mae: 31.7500\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 449us/step - loss: 4104.3315 - mse: 4104.3325 - mae: 33.9026 - val_loss: 2262.2996 - val_mse: 2262.2993 - val_mae: 31.4659\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 518us/step - loss: 4020.3918 - mse: 4020.3916 - mae: 33.7621 - val_loss: 2281.7149 - val_mse: 2281.7151 - val_mae: 31.5221\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4288.1099 - mse: 4288.1099 - mae: 35.3342 - val_loss: 2356.3902 - val_mse: 2356.3901 - val_mae: 31.7375\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4087.3582 - mse: 4087.3591 - mae: 33.6693 - val_loss: 2342.7023 - val_mse: 2342.7024 - val_mae: 31.6983\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4096.1962 - mse: 4096.1958 - mae: 32.8933 - val_loss: 2318.0006 - val_mse: 2318.0007 - val_mae: 31.6257\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4130.1630 - mse: 4130.1626 - mae: 35.0498 - val_loss: 2303.3251 - val_mse: 2303.3250 - val_mae: 31.5811\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 455us/step - loss: 4198.3347 - mse: 4198.3350 - mae: 34.1128 - val_loss: 2298.8092 - val_mse: 2298.8093 - val_mae: 31.5716\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4143.5090 - mse: 4143.5093 - mae: 34.6550 - val_loss: 2348.5439 - val_mse: 2348.5437 - val_mae: 31.7209\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 486us/step - loss: 4237.2602 - mse: 4237.2607 - mae: 34.8106 - val_loss: 2307.2796 - val_mse: 2307.2798 - val_mae: 31.6009\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4057.8294 - mse: 4057.8296 - mae: 33.8874 - val_loss: 2270.6869 - val_mse: 2270.6870 - val_mae: 31.5003\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4032.1856 - mse: 4032.1860 - mae: 34.5316 - val_loss: 2307.7653 - val_mse: 2307.7649 - val_mae: 31.6018\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4301.7135 - mse: 4301.7139 - mae: 34.9335 - val_loss: 2390.5695 - val_mse: 2390.5696 - val_mae: 31.8466\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4170.1811 - mse: 4170.1812 - mae: 33.6995 - val_loss: 2361.0032 - val_mse: 2361.0032 - val_mae: 31.7622\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 4083.6944 - mse: 4083.6948 - mae: 33.6086 - val_loss: 2323.6328 - val_mse: 2323.6326 - val_mae: 31.6534\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4021.9940 - mse: 4021.9941 - mae: 33.6752 - val_loss: 2317.1502 - val_mse: 2317.1501 - val_mae: 31.6333\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4153.3628 - mse: 4153.3633 - mae: 34.1036 - val_loss: 2395.6074 - val_mse: 2395.6074 - val_mae: 31.8666\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4132.1096 - mse: 4132.1099 - mae: 33.2669 - val_loss: 2302.7612 - val_mse: 2302.7612 - val_mae: 31.5955\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 502us/step - loss: 4040.5463 - mse: 4040.5459 - mae: 34.4984 - val_loss: 2305.3601 - val_mse: 2305.3604 - val_mae: 31.6013\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4134.2420 - mse: 4134.2412 - mae: 33.7148 - val_loss: 2327.0894 - val_mse: 2327.0894 - val_mae: 31.6651\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4177.1351 - mse: 4177.1348 - mae: 34.1414 - val_loss: 2341.9461 - val_mse: 2341.9460 - val_mae: 31.7135\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 4154.3408 - mse: 4154.3408 - mae: 34.2116 - val_loss: 2306.9153 - val_mse: 2306.9153 - val_mae: 31.6121\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 660us/step - loss: 4108.7380 - mse: 4108.7378 - mae: 33.5368 - val_loss: 2345.2388 - val_mse: 2345.2390 - val_mae: 31.7290\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4094.8993 - mse: 4094.8992 - mae: 33.9650 - val_loss: 2312.8306 - val_mse: 2312.8303 - val_mae: 31.6322\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4244.9733 - mse: 4244.9741 - mae: 34.4058 - val_loss: 2290.4882 - val_mse: 2290.4880 - val_mae: 31.5717\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4265.5993 - mse: 4265.5991 - mae: 34.5951 - val_loss: 2359.7182 - val_mse: 2359.7183 - val_mae: 31.7699\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4138.4381 - mse: 4138.4385 - mae: 33.2905 - val_loss: 2269.9996 - val_mse: 2269.9998 - val_mae: 31.5175\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 709us/step - loss: 4231.2568 - mse: 4231.2563 - mae: 34.1278 - val_loss: 2313.2895 - val_mse: 2313.2898 - val_mae: 31.6354\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 657us/step - loss: 4227.3643 - mse: 4227.3652 - mae: 33.6174 - val_loss: 2298.2590 - val_mse: 2298.2590 - val_mae: 31.5928\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 622us/step - loss: 4089.4906 - mse: 4089.4907 - mae: 33.4376 - val_loss: 2251.1052 - val_mse: 2251.1052 - val_mae: 31.4622\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4157.8411 - mse: 4157.8408 - mae: 34.4188 - val_loss: 2319.5921 - val_mse: 2319.5920 - val_mae: 31.6532\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4202.8121 - mse: 4202.8120 - mae: 33.5121 - val_loss: 2367.4409 - val_mse: 2367.4409 - val_mae: 31.7952\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 699us/step - loss: 4031.1552 - mse: 4031.1545 - mae: 32.7336 - val_loss: 2324.6991 - val_mse: 2324.6990 - val_mae: 31.6737\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4153.9364 - mse: 4153.9365 - mae: 34.3937 - val_loss: 2385.0353 - val_mse: 2385.0354 - val_mae: 31.8529\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4043.9775 - mse: 4043.9771 - mae: 32.5196 - val_loss: 2275.0794 - val_mse: 2275.0796 - val_mae: 31.5431\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4009.2179 - mse: 4009.2180 - mae: 33.4656 - val_loss: 2363.0751 - val_mse: 2363.0750 - val_mae: 31.7886\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 3999.4485 - mse: 3999.4485 - mae: 33.3666 - val_loss: 2313.9103 - val_mse: 2313.9102 - val_mae: 31.6475\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4100.0692 - mse: 4100.0693 - mae: 33.7389 - val_loss: 2265.8975 - val_mse: 2265.8972 - val_mae: 31.5199\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4259.1477 - mse: 4259.1475 - mae: 33.5365 - val_loss: 2334.4055 - val_mse: 2334.4053 - val_mae: 31.7121\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4065.6453 - mse: 4065.6460 - mae: 33.5013 - val_loss: 2353.7743 - val_mse: 2353.7742 - val_mae: 31.7675\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4003.3574 - mse: 4003.3579 - mae: 32.9586 - val_loss: 2301.4691 - val_mse: 2301.4690 - val_mae: 31.6158\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 3968.6381 - mse: 3968.6387 - mae: 33.6878 - val_loss: 2267.0940 - val_mse: 2267.0940 - val_mae: 31.5194\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4203.6354 - mse: 4203.6348 - mae: 34.4039 - val_loss: 2346.4765 - val_mse: 2346.4766 - val_mae: 31.7494\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4068.2346 - mse: 4068.2341 - mae: 32.8069 - val_loss: 2332.6073 - val_mse: 2332.6074 - val_mae: 31.7116\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 3963.6976 - mse: 3963.6975 - mae: 33.8633 - val_loss: 2313.6559 - val_mse: 2313.6558 - val_mae: 31.6561\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4134.0817 - mse: 4134.0820 - mae: 33.5643 - val_loss: 2290.7987 - val_mse: 2290.7988 - val_mae: 31.5952\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4030.8732 - mse: 4030.8730 - mae: 33.1265 - val_loss: 2316.0060 - val_mse: 2316.0063 - val_mae: 31.6675\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 533us/step - loss: 4041.8079 - mse: 4041.8083 - mae: 33.4229 - val_loss: 2310.4391 - val_mse: 2310.4390 - val_mae: 31.6509\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 435us/step - loss: 4044.1488 - mse: 4044.1489 - mae: 33.8443 - val_loss: 2312.3005 - val_mse: 2312.3005 - val_mae: 31.6589\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 4151.6244 - mse: 4151.6240 - mae: 34.1024 - val_loss: 2372.3909 - val_mse: 2372.3909 - val_mae: 31.8321\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 4188.7580 - mse: 4188.7583 - mae: 33.8807 - val_loss: 2354.1701 - val_mse: 2354.1699 - val_mae: 31.7814\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4102.9052 - mse: 4102.9053 - mae: 32.7242 - val_loss: 2346.4948 - val_mse: 2346.4946 - val_mae: 31.7595\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 3989.5616 - mse: 3989.5615 - mae: 32.5723 - val_loss: 2331.1923 - val_mse: 2331.1924 - val_mae: 31.7149\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 504us/step - loss: 3940.1777 - mse: 3940.1777 - mae: 32.2039 - val_loss: 2275.3694 - val_mse: 2275.3694 - val_mae: 31.5615\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 515us/step - loss: 3960.3739 - mse: 3960.3735 - mae: 32.8322 - val_loss: 2356.6530 - val_mse: 2356.6531 - val_mae: 31.7930\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 3987.5992 - mse: 3987.5991 - mae: 33.2520 - val_loss: 2277.9879 - val_mse: 2277.9880 - val_mae: 31.5720\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 3994.3698 - mse: 3994.3699 - mae: 33.3900 - val_loss: 2338.3608 - val_mse: 2338.3608 - val_mae: 31.7404\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 554us/step - loss: 3864.1671 - mse: 3864.1667 - mae: 32.6893 - val_loss: 2280.0831 - val_mse: 2280.0830 - val_mae: 31.5790\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 3983.6109 - mse: 3983.6116 - mae: 31.9322 - val_loss: 2297.1904 - val_mse: 2297.1904 - val_mae: 31.6283\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 532us/step - loss: 4135.9794 - mse: 4135.9790 - mae: 32.8542 - val_loss: 2346.4909 - val_mse: 2346.4910 - val_mae: 31.7669\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 3942.7773 - mse: 3942.7776 - mae: 33.3598 - val_loss: 2316.6098 - val_mse: 2316.6099 - val_mae: 31.6787\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 3921.9536 - mse: 3921.9539 - mae: 32.7400 - val_loss: 2324.6896 - val_mse: 2324.6897 - val_mae: 31.7009\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4035.5947 - mse: 4035.5947 - mae: 33.4709 - val_loss: 2333.2057 - val_mse: 2333.2058 - val_mae: 31.7274\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 3996.4482 - mse: 3996.4485 - mae: 32.5993 - val_loss: 2296.0488 - val_mse: 2296.0491 - val_mae: 31.6261\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 3913.0003 - mse: 3913.0005 - mae: 32.9212 - val_loss: 2283.1764 - val_mse: 2283.1763 - val_mae: 31.5905\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 3880.6842 - mse: 3880.6838 - mae: 32.0626 - val_loss: 2295.9954 - val_mse: 2295.9956 - val_mae: 31.6253\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4179.2713 - mse: 4179.2710 - mae: 33.5514 - val_loss: 2334.7295 - val_mse: 2334.7295 - val_mae: 31.7329\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 639us/step - loss: 4162.1839 - mse: 4162.1841 - mae: 33.8664 - val_loss: 2331.5828 - val_mse: 2331.5828 - val_mae: 31.7255\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4132.0329 - mse: 4132.0327 - mae: 34.0978 - val_loss: 2365.4410 - val_mse: 2365.4409 - val_mae: 31.8213\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4031.5631 - mse: 4031.5630 - mae: 33.6341 - val_loss: 2354.9772 - val_mse: 2354.9773 - val_mae: 31.7919\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4076.3859 - mse: 4076.3862 - mae: 33.3277 - val_loss: 2335.5627 - val_mse: 2335.5627 - val_mae: 31.7343\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3344.7320 - mse: 3344.7324 - mae: 33.4467 - val_loss: 1464.6612 - val_mse: 1464.6614 - val_mae: 25.8291\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 514us/step - loss: 3354.7508 - mse: 3354.7512 - mae: 32.3567 - val_loss: 1464.1932 - val_mse: 1464.1932 - val_mae: 25.5588\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3430.5557 - mse: 3430.5552 - mae: 33.0324 - val_loss: 1466.9544 - val_mse: 1466.9543 - val_mae: 26.3119\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3431.0870 - mse: 3431.0876 - mae: 32.5255 - val_loss: 1467.0816 - val_mse: 1467.0817 - val_mae: 26.3510\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 521us/step - loss: 3469.6148 - mse: 3469.6155 - mae: 33.0667 - val_loss: 1467.9084 - val_mse: 1467.9084 - val_mae: 26.4763\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3341.3767 - mse: 3341.3760 - mae: 32.5400 - val_loss: 1463.3208 - val_mse: 1463.3208 - val_mae: 25.6309\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3470.8310 - mse: 3470.8311 - mae: 32.7741 - val_loss: 1463.4603 - val_mse: 1463.4604 - val_mae: 25.7864\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3306.7695 - mse: 3306.7700 - mae: 32.9303 - val_loss: 1466.8153 - val_mse: 1466.8153 - val_mae: 26.4009\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3327.2194 - mse: 3327.2195 - mae: 33.2352 - val_loss: 1463.2928 - val_mse: 1463.2930 - val_mae: 25.4433\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 688us/step - loss: 3383.4986 - mse: 3383.4980 - mae: 32.6028 - val_loss: 1463.2695 - val_mse: 1463.2694 - val_mae: 25.6628\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 523us/step - loss: 3461.6057 - mse: 3461.6060 - mae: 32.7081 - val_loss: 1463.3186 - val_mse: 1463.3185 - val_mae: 25.6926\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 484us/step - loss: 3510.1738 - mse: 3510.1736 - mae: 33.1599 - val_loss: 1463.1365 - val_mse: 1463.1364 - val_mae: 25.5574\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3235.6459 - mse: 3235.6458 - mae: 32.1909 - val_loss: 1464.4414 - val_mse: 1464.4413 - val_mae: 26.1147\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3269.6589 - mse: 3269.6582 - mae: 31.9608 - val_loss: 1463.2849 - val_mse: 1463.2848 - val_mae: 25.8461\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3375.9209 - mse: 3375.9214 - mae: 32.8589 - val_loss: 1462.6605 - val_mse: 1462.6606 - val_mae: 25.6627\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 534us/step - loss: 3207.9312 - mse: 3207.9307 - mae: 32.3096 - val_loss: 1463.7536 - val_mse: 1463.7537 - val_mae: 26.0237\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 493us/step - loss: 3274.9496 - mse: 3274.9485 - mae: 32.5751 - val_loss: 1462.6028 - val_mse: 1462.6028 - val_mae: 25.7277\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3280.7660 - mse: 3280.7659 - mae: 31.6947 - val_loss: 1466.3384 - val_mse: 1466.3384 - val_mae: 26.4310\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3372.4694 - mse: 3372.4695 - mae: 33.1028 - val_loss: 1462.4228 - val_mse: 1462.4229 - val_mae: 25.3786\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3382.3692 - mse: 3382.3687 - mae: 33.3000 - val_loss: 1461.7415 - val_mse: 1461.7413 - val_mae: 25.6227\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3378.8506 - mse: 3378.8511 - mae: 32.1045 - val_loss: 1461.5112 - val_mse: 1461.5114 - val_mae: 25.6049\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3422.3635 - mse: 3422.3635 - mae: 33.2552 - val_loss: 1461.7028 - val_mse: 1461.7028 - val_mae: 25.3362\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3250.7141 - mse: 3250.7141 - mae: 31.9590 - val_loss: 1461.6541 - val_mse: 1461.6541 - val_mae: 25.7908\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3373.1203 - mse: 3373.1211 - mae: 32.7253 - val_loss: 1463.9010 - val_mse: 1463.9011 - val_mae: 26.2145\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 723us/step - loss: 3411.5257 - mse: 3411.5256 - mae: 33.0349 - val_loss: 1461.7844 - val_mse: 1461.7843 - val_mae: 25.7774\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3329.7787 - mse: 3329.7788 - mae: 32.0358 - val_loss: 1463.6188 - val_mse: 1463.6190 - val_mae: 26.1718\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3355.6671 - mse: 3355.6680 - mae: 32.0714 - val_loss: 1461.4640 - val_mse: 1461.4639 - val_mae: 25.4632\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3251.0433 - mse: 3251.0427 - mae: 31.6423 - val_loss: 1461.2125 - val_mse: 1461.2125 - val_mae: 25.5200\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3199.4174 - mse: 3199.4180 - mae: 31.9786 - val_loss: 1463.3561 - val_mse: 1463.3561 - val_mae: 26.2166\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3307.6823 - mse: 3307.6816 - mae: 32.2862 - val_loss: 1460.8037 - val_mse: 1460.8038 - val_mae: 25.7062\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3283.0803 - mse: 3283.0803 - mae: 32.4222 - val_loss: 1462.2820 - val_mse: 1462.2822 - val_mae: 26.1070\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3414.5899 - mse: 3414.5901 - mae: 33.4901 - val_loss: 1460.4697 - val_mse: 1460.4700 - val_mae: 25.4406\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3221.0454 - mse: 3221.0452 - mae: 32.1199 - val_loss: 1460.3474 - val_mse: 1460.3474 - val_mae: 25.4304\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 674us/step - loss: 3253.3676 - mse: 3253.3677 - mae: 31.8625 - val_loss: 1462.8129 - val_mse: 1462.8130 - val_mae: 26.2170\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3375.5790 - mse: 3375.5784 - mae: 32.4909 - val_loss: 1461.8733 - val_mse: 1461.8733 - val_mae: 26.1013\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 660us/step - loss: 3192.4939 - mse: 3192.4937 - mae: 30.9905 - val_loss: 1462.3132 - val_mse: 1462.3131 - val_mae: 26.1726\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3479.3883 - mse: 3479.3879 - mae: 32.8089 - val_loss: 1460.1330 - val_mse: 1460.1328 - val_mae: 25.6968\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3326.7727 - mse: 3326.7725 - mae: 31.8078 - val_loss: 1461.6648 - val_mse: 1461.6648 - val_mae: 26.1201\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3260.2643 - mse: 3260.2646 - mae: 31.5017 - val_loss: 1464.1483 - val_mse: 1464.1484 - val_mae: 26.4520\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3337.4172 - mse: 3337.4167 - mae: 32.1791 - val_loss: 1460.6911 - val_mse: 1460.6910 - val_mae: 25.8680\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 515us/step - loss: 3310.0722 - mse: 3310.0723 - mae: 32.1101 - val_loss: 1461.8583 - val_mse: 1461.8584 - val_mae: 26.1080\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3274.1111 - mse: 3274.1111 - mae: 32.3547 - val_loss: 1460.9924 - val_mse: 1460.9923 - val_mae: 25.9659\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3316.5958 - mse: 3316.5952 - mae: 32.1728 - val_loss: 1460.2390 - val_mse: 1460.2391 - val_mae: 25.7622\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3286.8500 - mse: 3286.8499 - mae: 32.3293 - val_loss: 1459.6524 - val_mse: 1459.6525 - val_mae: 25.6223\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3321.1317 - mse: 3321.1321 - mae: 31.8302 - val_loss: 1464.3933 - val_mse: 1464.3934 - val_mae: 26.4892\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3302.7113 - mse: 3302.7124 - mae: 31.6070 - val_loss: 1461.1969 - val_mse: 1461.1969 - val_mae: 26.1234\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3341.8926 - mse: 3341.8923 - mae: 32.3052 - val_loss: 1459.9092 - val_mse: 1459.9091 - val_mae: 25.8382\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3317.1698 - mse: 3317.1707 - mae: 31.6930 - val_loss: 1460.6467 - val_mse: 1460.6467 - val_mae: 26.0199\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3353.3275 - mse: 3353.3274 - mae: 32.8326 - val_loss: 1459.2937 - val_mse: 1459.2938 - val_mae: 25.5632\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 532us/step - loss: 3223.0409 - mse: 3223.0417 - mae: 31.8798 - val_loss: 1469.2490 - val_mse: 1469.2491 - val_mae: 26.9463\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3299.1871 - mse: 3299.1873 - mae: 32.6217 - val_loss: 1459.2959 - val_mse: 1459.2959 - val_mae: 25.7204\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3235.9268 - mse: 3235.9265 - mae: 31.6808 - val_loss: 1461.3157 - val_mse: 1461.3157 - val_mae: 26.2010\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3382.3830 - mse: 3382.3835 - mae: 32.12 - 1s 581us/step - loss: 3346.6290 - mse: 3346.6299 - mae: 32.1469 - val_loss: 1459.3885 - val_mse: 1459.3885 - val_mae: 25.6728\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3326.9641 - mse: 3326.9636 - mae: 31.7227 - val_loss: 1460.4629 - val_mse: 1460.4629 - val_mae: 26.0065\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3333.2419 - mse: 3333.2412 - mae: 32.8051 - val_loss: 1459.1774 - val_mse: 1459.1775 - val_mae: 25.4539\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 667us/step - loss: 3313.6839 - mse: 3313.6843 - mae: 31.9815 - val_loss: 1466.3535 - val_mse: 1466.3534 - val_mae: 26.7174\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3263.2195 - mse: 3263.2200 - mae: 31.4166 - val_loss: 1461.2302 - val_mse: 1461.2302 - val_mae: 26.1845\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3267.1112 - mse: 3267.1121 - mae: 32.4283 - val_loss: 1458.9211 - val_mse: 1458.9211 - val_mae: 25.3888\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3359.4843 - mse: 3359.4854 - mae: 31.6725 - val_loss: 1458.8060 - val_mse: 1458.8062 - val_mae: 25.4962\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3326.4753 - mse: 3326.4746 - mae: 31.6492 - val_loss: 1459.4430 - val_mse: 1459.4430 - val_mae: 25.3491\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3364.6728 - mse: 3364.6729 - mae: 31.5934 - val_loss: 1459.0665 - val_mse: 1459.0665 - val_mae: 25.5071\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3339.5439 - mse: 3339.5442 - mae: 32.1991 - val_loss: 1461.5860 - val_mse: 1461.5861 - val_mae: 26.2348\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3289.7940 - mse: 3289.7932 - mae: 31.5265 - val_loss: 1460.6094 - val_mse: 1460.6093 - val_mae: 26.0884\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3171.2447 - mse: 3171.2444 - mae: 31.7253 - val_loss: 1460.0018 - val_mse: 1460.0018 - val_mae: 26.0339\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3275.1540 - mse: 3275.1536 - mae: 31.3919 - val_loss: 1461.1203 - val_mse: 1461.1204 - val_mae: 26.2556\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3246.3383 - mse: 3246.3384 - mae: 31.0491 - val_loss: 1462.1186 - val_mse: 1462.1187 - val_mae: 26.3961\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3239.5283 - mse: 3239.5273 - mae: 31.6472 - val_loss: 1458.2235 - val_mse: 1458.2236 - val_mae: 25.5630\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3352.5307 - mse: 3352.5308 - mae: 32.3877 - val_loss: 1458.3937 - val_mse: 1458.3936 - val_mae: 25.4659\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 533us/step - loss: 3372.2909 - mse: 3372.2905 - mae: 32.4997 - val_loss: 1458.1846 - val_mse: 1458.1847 - val_mae: 25.6245\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3278.1157 - mse: 3278.1160 - mae: 32.1494 - val_loss: 1458.3544 - val_mse: 1458.3545 - val_mae: 25.7955\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3383.3168 - mse: 3383.3162 - mae: 32.5128 - val_loss: 1457.9328 - val_mse: 1457.9330 - val_mae: 25.5589\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3249.6457 - mse: 3249.6458 - mae: 31.1482 - val_loss: 1459.6035 - val_mse: 1459.6034 - val_mae: 26.0371\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3460.0744 - mse: 3460.0750 - mae: 32.6898 - val_loss: 1459.6865 - val_mse: 1459.6865 - val_mae: 25.2009\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 636us/step - loss: 3261.2930 - mse: 3261.2930 - mae: 31.5812 - val_loss: 1459.0838 - val_mse: 1459.0839 - val_mae: 25.8257\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3222.5514 - mse: 3222.5518 - mae: 31.7634 - val_loss: 1461.2007 - val_mse: 1461.2007 - val_mae: 26.2165\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3274.8958 - mse: 3274.8960 - mae: 31.3226 - val_loss: 1460.8638 - val_mse: 1460.8638 - val_mae: 26.2300\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3217.0230 - mse: 3217.0234 - mae: 31.3202 - val_loss: 1459.5251 - val_mse: 1459.5250 - val_mae: 25.9671\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3149.6831 - mse: 3149.6833 - mae: 31.0908 - val_loss: 1462.3789 - val_mse: 1462.3790 - val_mae: 26.4193\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3268.4337 - mse: 3268.4331 - mae: 31.7714 - val_loss: 1459.3177 - val_mse: 1459.3179 - val_mae: 26.0025\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 543us/step - loss: 3170.5384 - mse: 3170.5386 - mae: 31.6104 - val_loss: 1459.3593 - val_mse: 1459.3593 - val_mae: 26.0006\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 3013.0328 - mse: 3013.0330 - mae: 31.9970 - val_loss: 1064.5672 - val_mse: 1064.5673 - val_mae: 24.2721\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 445us/step - loss: 2966.6879 - mse: 2966.6887 - mae: 31.1017 - val_loss: 1064.6783 - val_mse: 1064.6783 - val_mae: 24.1848\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2904.5315 - mse: 2904.5303 - mae: 31.4555 - val_loss: 1065.4942 - val_mse: 1065.4941 - val_mae: 23.9331\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2979.5308 - mse: 2979.5310 - mae: 31.1862 - val_loss: 1065.1576 - val_mse: 1065.1576 - val_mae: 24.0252\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2871.2238 - mse: 2871.2239 - mae: 31.0074 - val_loss: 1064.0932 - val_mse: 1064.0931 - val_mae: 24.5292\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 3016.4592 - mse: 3016.4592 - mae: 31.1898 - val_loss: 1066.7384 - val_mse: 1066.7386 - val_mae: 23.8139\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2974.8048 - mse: 2974.8052 - mae: 30.9108 - val_loss: 1063.8832 - val_mse: 1063.8832 - val_mae: 24.0755\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 503us/step - loss: 3004.3763 - mse: 3004.3762 - mae: 31.6350 - val_loss: 1065.6472 - val_mse: 1065.6472 - val_mae: 23.8615\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2889.0303 - mse: 2889.0303 - mae: 30.7175 - val_loss: 1063.9914 - val_mse: 1063.9913 - val_mae: 24.0157\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2920.0111 - mse: 2920.0117 - mae: 31.3243 - val_loss: 1064.7279 - val_mse: 1064.7279 - val_mae: 23.8900\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2888.6502 - mse: 2888.6494 - mae: 30.9998 - val_loss: 1063.2967 - val_mse: 1063.2968 - val_mae: 23.9640\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2932.0205 - mse: 2932.0200 - mae: 31.2831 - val_loss: 1064.4452 - val_mse: 1064.4453 - val_mae: 23.8273\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2966.3624 - mse: 2966.3628 - mae: 31.2876 - val_loss: 1063.1031 - val_mse: 1063.1030 - val_mae: 23.9712\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2954.5679 - mse: 2954.5686 - mae: 30.8253 - val_loss: 1064.4889 - val_mse: 1064.4889 - val_mae: 23.8112\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2942.2793 - mse: 2942.2791 - mae: 31.1187 - val_loss: 1065.0329 - val_mse: 1065.0328 - val_mae: 23.7337\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2883.6497 - mse: 2883.6504 - mae: 31.1974 - val_loss: 1064.6524 - val_mse: 1064.6525 - val_mae: 23.7741\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 541us/step - loss: 2874.3372 - mse: 2874.3376 - mae: 30.3724 - val_loss: 1061.4492 - val_mse: 1061.4491 - val_mae: 24.1533\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2917.3321 - mse: 2917.3323 - mae: 31.1077 - val_loss: 1064.6596 - val_mse: 1064.6595 - val_mae: 23.7514\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2963.8298 - mse: 2963.8303 - mae: 31.2252 - val_loss: 1065.1437 - val_mse: 1065.1437 - val_mae: 23.7241\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2980.1797 - mse: 2980.1797 - mae: 31.1589 - val_loss: 1065.2243 - val_mse: 1065.2244 - val_mae: 23.6430\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2964.6413 - mse: 2964.6423 - mae: 31.1184 - val_loss: 1063.5329 - val_mse: 1063.5330 - val_mae: 23.7229\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2931.2570 - mse: 2931.2571 - mae: 31.2806 - val_loss: 1059.4342 - val_mse: 1059.4342 - val_mae: 24.0538\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2861.0052 - mse: 2861.0059 - mae: 30.7949 - val_loss: 1058.6026 - val_mse: 1058.6027 - val_mae: 24.1470\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2950.4566 - mse: 2950.4563 - mae: 31.1181 - val_loss: 1058.6396 - val_mse: 1058.6395 - val_mae: 23.9527\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2908.5105 - mse: 2908.5095 - mae: 30.6944 - val_loss: 1060.4073 - val_mse: 1060.4075 - val_mae: 23.7752\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2930.3313 - mse: 2930.3315 - mae: 30.8015 - val_loss: 1060.7939 - val_mse: 1060.7939 - val_mae: 23.7548\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2853.8050 - mse: 2853.8054 - mae: 30.4398 - val_loss: 1058.0156 - val_mse: 1058.0156 - val_mae: 23.9957\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2958.2764 - mse: 2958.2766 - mae: 31.3061 - val_loss: 1063.2749 - val_mse: 1063.2750 - val_mae: 23.5331\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2869.5795 - mse: 2869.5798 - mae: 30.6832 - val_loss: 1057.6481 - val_mse: 1057.6481 - val_mae: 23.8977\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2839.5413 - mse: 2839.5417 - mae: 30.8011 - val_loss: 1056.8310 - val_mse: 1056.8309 - val_mae: 23.9423\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 700us/step - loss: 2963.4363 - mse: 2963.4351 - mae: 31.4342 - val_loss: 1058.8428 - val_mse: 1058.8428 - val_mae: 23.7377\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2945.5751 - mse: 2945.5757 - mae: 30.7485 - val_loss: 1060.3993 - val_mse: 1060.3993 - val_mae: 23.6315\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 688us/step - loss: 2820.8538 - mse: 2820.8535 - mae: 30.7497 - val_loss: 1059.9710 - val_mse: 1059.9711 - val_mae: 23.6578\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2876.6332 - mse: 2876.6331 - mae: 30.3464 - val_loss: 1055.3479 - val_mse: 1055.3479 - val_mae: 24.1916\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2926.5806 - mse: 2926.5798 - mae: 30.8910 - val_loss: 1056.3173 - val_mse: 1056.3174 - val_mae: 23.9746\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2893.4933 - mse: 2893.4941 - mae: 31.0451 - val_loss: 1055.9413 - val_mse: 1055.9413 - val_mae: 24.1318\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2879.9750 - mse: 2879.9741 - mae: 30.6904 - val_loss: 1056.0675 - val_mse: 1056.0674 - val_mae: 23.9655\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2953.5835 - mse: 2953.5830 - mae: 31.5358 - val_loss: 1055.7413 - val_mse: 1055.7413 - val_mae: 24.0236\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2861.3273 - mse: 2861.3267 - mae: 30.1360 - val_loss: 1054.6736 - val_mse: 1054.6737 - val_mae: 24.1615\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2867.6433 - mse: 2867.6438 - mae: 31.2882 - val_loss: 1055.4886 - val_mse: 1055.4885 - val_mae: 24.1041\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 503us/step - loss: 2849.3650 - mse: 2849.3650 - mae: 30.7346 - val_loss: 1055.0535 - val_mse: 1055.0536 - val_mae: 24.1551\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2942.0910 - mse: 2942.0916 - mae: 31.4034 - val_loss: 1054.8505 - val_mse: 1054.8506 - val_mae: 24.1727\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2932.2374 - mse: 2932.2378 - mae: 30.9385 - val_loss: 1055.9414 - val_mse: 1055.9414 - val_mae: 23.9904\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 3013.5104 - mse: 3013.5110 - mae: 30.9461 - val_loss: 1055.6964 - val_mse: 1055.6964 - val_mae: 24.0710\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 505us/step - loss: 2831.8890 - mse: 2831.8887 - mae: 30.9020 - val_loss: 1058.3814 - val_mse: 1058.3815 - val_mae: 23.7508\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2894.1571 - mse: 2894.1560 - mae: 30.6847 - val_loss: 1054.9090 - val_mse: 1054.9089 - val_mae: 24.1724\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2800.1602 - mse: 2800.1604 - mae: 30.6160 - val_loss: 1054.1536 - val_mse: 1054.1534 - val_mae: 24.1896\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2874.2619 - mse: 2874.2617 - mae: 30.5658 - val_loss: 1054.4452 - val_mse: 1054.4453 - val_mae: 24.0332\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 693us/step - loss: 2834.0239 - mse: 2834.0242 - mae: 30.5823 - val_loss: 1054.3074 - val_mse: 1054.3074 - val_mae: 23.9731\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2840.5695 - mse: 2840.5693 - mae: 30.8220 - val_loss: 1053.7771 - val_mse: 1053.7771 - val_mae: 24.0511\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 664us/step - loss: 2817.8498 - mse: 2817.8494 - mae: 30.6379 - val_loss: 1053.0825 - val_mse: 1053.0825 - val_mae: 24.1480\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2877.6962 - mse: 2877.6973 - mae: 30.7617 - val_loss: 1053.0691 - val_mse: 1053.0691 - val_mae: 24.0816\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2903.2358 - mse: 2903.2354 - mae: 30.8133 - val_loss: 1054.1412 - val_mse: 1054.1411 - val_mae: 23.9285\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2805.9118 - mse: 2805.9116 - mae: 30.6415 - val_loss: 1052.8734 - val_mse: 1052.8733 - val_mae: 24.0474\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2837.2780 - mse: 2837.2773 - mae: 30.4047 - val_loss: 1054.1787 - val_mse: 1054.1786 - val_mae: 23.8766\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2878.9326 - mse: 2878.9321 - mae: 30.3816 - val_loss: 1052.9907 - val_mse: 1052.9908 - val_mae: 23.9788\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2890.1592 - mse: 2890.1587 - mae: 30.9131 - val_loss: 1053.9684 - val_mse: 1053.9683 - val_mae: 23.8237\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2894.0847 - mse: 2894.0854 - mae: 30.4715 - val_loss: 1053.3195 - val_mse: 1053.3195 - val_mae: 23.8567\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2806.5258 - mse: 2806.5261 - mae: 30.3867 - val_loss: 1052.8809 - val_mse: 1052.8809 - val_mae: 23.9040\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2924.4887 - mse: 2924.4878 - mae: 31.1578 - val_loss: 1052.2902 - val_mse: 1052.2899 - val_mae: 23.9973\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2894.0120 - mse: 2894.0115 - mae: 30.6953 - val_loss: 1051.6359 - val_mse: 1051.6359 - val_mae: 24.2707\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 684us/step - loss: 2793.3815 - mse: 2793.3818 - mae: 30.0785 - val_loss: 1051.3672 - val_mse: 1051.3672 - val_mae: 24.2192\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2966.5410 - mse: 2966.5417 - mae: 31.3853 - val_loss: 1051.4043 - val_mse: 1051.4042 - val_mae: 24.1667\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 688us/step - loss: 2894.0373 - mse: 2894.0378 - mae: 30.6495 - val_loss: 1051.7500 - val_mse: 1051.7501 - val_mae: 24.0116\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2815.9361 - mse: 2815.9373 - mae: 30.4927 - val_loss: 1052.0073 - val_mse: 1052.0071 - val_mae: 24.0114\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2821.9386 - mse: 2821.9382 - mae: 30.4604 - val_loss: 1051.5515 - val_mse: 1051.5516 - val_mae: 24.2120\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2781.8242 - mse: 2781.8245 - mae: 30.2259 - val_loss: 1051.4128 - val_mse: 1051.4128 - val_mae: 24.3225\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2917.2736 - mse: 2917.2732 - mae: 31.6582 - val_loss: 1052.1085 - val_mse: 1052.1085 - val_mae: 24.1016\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2862.9210 - mse: 2862.9207 - mae: 30.8300 - val_loss: 1052.9690 - val_mse: 1052.9690 - val_mae: 23.9961\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2859.9011 - mse: 2859.9009 - mae: 30.3304 - val_loss: 1051.9852 - val_mse: 1051.9852 - val_mae: 24.2913\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2851.5969 - mse: 2851.5969 - mae: 30.5964 - val_loss: 1053.5593 - val_mse: 1053.5592 - val_mae: 23.9155\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 684us/step - loss: 2888.8432 - mse: 2888.8435 - mae: 30.5421 - val_loss: 1054.9215 - val_mse: 1054.9216 - val_mae: 23.7711\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2878.7356 - mse: 2878.7361 - mae: 30.4980 - val_loss: 1052.2529 - val_mse: 1052.2531 - val_mae: 24.0623\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2853.6439 - mse: 2853.6443 - mae: 30.6736 - val_loss: 1053.1213 - val_mse: 1053.1213 - val_mae: 23.9647\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2865.8777 - mse: 2865.8777 - mae: 30.3346 - val_loss: 1055.1456 - val_mse: 1055.1456 - val_mae: 23.7706\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2814.9889 - mse: 2814.9885 - mae: 30.2969 - val_loss: 1052.4661 - val_mse: 1052.4661 - val_mae: 24.5212\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2941.1062 - mse: 2941.1057 - mae: 31.0855 - val_loss: 1054.3085 - val_mse: 1054.3086 - val_mae: 23.8204\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 516us/step - loss: 2873.3954 - mse: 2873.3955 - mae: 30.4438 - val_loss: 1051.9730 - val_mse: 1051.9729 - val_mae: 24.0795\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2884.1244 - mse: 2884.1238 - mae: 30.5901 - val_loss: 1052.5504 - val_mse: 1052.5503 - val_mae: 23.8909\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2951.2102 - mse: 2951.2107 - mae: 30.7003 - val_loss: 1050.8929 - val_mse: 1050.8928 - val_mae: 24.3649\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2587.7773 - mse: 2587.7781 - mae: 30.2273 - val_loss: 1556.4548 - val_mse: 1556.4547 - val_mae: 27.3584\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2467.4817 - mse: 2467.4817 - mae: 29.2939 - val_loss: 1538.9302 - val_mse: 1538.9302 - val_mae: 27.9990\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2511.4753 - mse: 2511.4746 - mae: 29.7739 - val_loss: 1541.6400 - val_mse: 1541.6399 - val_mae: 27.8528\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2555.7970 - mse: 2555.7969 - mae: 29.9556 - val_loss: 1542.1866 - val_mse: 1542.1866 - val_mae: 27.8264\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2559.0004 - mse: 2559.0010 - mae: 30.2942 - val_loss: 1546.4606 - val_mse: 1546.4606 - val_mae: 27.6144\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2605.9275 - mse: 2605.9277 - mae: 30.1804 - val_loss: 1545.9193 - val_mse: 1545.9193 - val_mae: 27.6714\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2585.5786 - mse: 2585.5784 - mae: 29.5708 - val_loss: 1542.8737 - val_mse: 1542.8738 - val_mae: 27.7844\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2556.2414 - mse: 2556.2415 - mae: 30.0380 - val_loss: 1556.4360 - val_mse: 1556.4360 - val_mae: 27.3312\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2542.2451 - mse: 2542.2454 - mae: 29.9679 - val_loss: 1548.6408 - val_mse: 1548.6407 - val_mae: 27.5480\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2580.8432 - mse: 2580.8435 - mae: 29.9633 - val_loss: 1545.3957 - val_mse: 1545.3956 - val_mae: 27.6315\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2522.9241 - mse: 2522.9246 - mae: 29.7015 - val_loss: 1538.2050 - val_mse: 1538.2048 - val_mae: 27.9485\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 675us/step - loss: 2564.8814 - mse: 2564.8806 - mae: 30.1814 - val_loss: 1544.9303 - val_mse: 1544.9303 - val_mae: 27.6192\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2553.0968 - mse: 2553.0977 - mae: 29.7870 - val_loss: 1540.8665 - val_mse: 1540.8665 - val_mae: 27.8205\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2504.3303 - mse: 2504.3313 - mae: 29.7576 - val_loss: 1544.0743 - val_mse: 1544.0742 - val_mae: 27.6945\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 519us/step - loss: 2609.5427 - mse: 2609.5432 - mae: 30.0662 - val_loss: 1547.3184 - val_mse: 1547.3184 - val_mae: 27.5701\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2591.0248 - mse: 2591.0254 - mae: 30.1262 - val_loss: 1551.0824 - val_mse: 1551.0823 - val_mae: 27.4025\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 667us/step - loss: 2530.7651 - mse: 2530.7654 - mae: 29.5294 - val_loss: 1544.6276 - val_mse: 1544.6277 - val_mae: 27.6856\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 652us/step - loss: 2580.5635 - mse: 2580.5623 - mae: 30.2586 - val_loss: 1543.6559 - val_mse: 1543.6560 - val_mae: 27.6761\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 664us/step - loss: 2483.6797 - mse: 2483.6797 - mae: 29.6310 - val_loss: 1534.8215 - val_mse: 1534.8218 - val_mae: 28.0855\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2500.5401 - mse: 2500.5405 - mae: 29.2847 - val_loss: 1539.1345 - val_mse: 1539.1345 - val_mae: 27.8455\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2515.9263 - mse: 2515.9272 - mae: 29.7378 - val_loss: 1531.7560 - val_mse: 1531.7559 - val_mae: 28.2358\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2488.9607 - mse: 2488.9609 - mae: 29.5788 - val_loss: 1540.3173 - val_mse: 1540.3171 - val_mae: 27.7337\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2544.8494 - mse: 2544.8496 - mae: 29.5683 - val_loss: 1539.0616 - val_mse: 1539.0615 - val_mae: 27.7815\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 649us/step - loss: 2561.6538 - mse: 2561.6531 - mae: 30.0717 - val_loss: 1535.4791 - val_mse: 1535.4792 - val_mae: 27.9503\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2528.3217 - mse: 2528.3223 - mae: 29.3265 - val_loss: 1538.0134 - val_mse: 1538.0133 - val_mae: 27.8221\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2509.6421 - mse: 2509.6426 - mae: 29.2589 - val_loss: 1541.2445 - val_mse: 1541.2444 - val_mae: 27.6593\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2617.0222 - mse: 2617.0225 - mae: 30.4998 - val_loss: 1551.4588 - val_mse: 1551.4591 - val_mae: 27.3105\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2537.6482 - mse: 2537.6477 - mae: 29.3218 - val_loss: 1538.4759 - val_mse: 1538.4760 - val_mae: 27.7962\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2548.1630 - mse: 2548.1624 - mae: 29.8225 - val_loss: 1532.8678 - val_mse: 1532.8676 - val_mae: 28.0607\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 503us/step - loss: 2478.6821 - mse: 2478.6819 - mae: 29.4149 - val_loss: 1540.8684 - val_mse: 1540.8684 - val_mae: 27.7008\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2523.0448 - mse: 2523.0452 - mae: 29.8364 - val_loss: 1544.5834 - val_mse: 1544.5833 - val_mae: 27.5157\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2542.6675 - mse: 2542.6687 - mae: 29.8171 - val_loss: 1541.4557 - val_mse: 1541.4556 - val_mae: 27.6563\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2541.9523 - mse: 2541.9524 - mae: 29.4518 - val_loss: 1537.6315 - val_mse: 1537.6315 - val_mae: 27.7690\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2523.4398 - mse: 2523.4397 - mae: 29.6144 - val_loss: 1544.0025 - val_mse: 1544.0023 - val_mae: 27.5193\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2523.3292 - mse: 2523.3289 - mae: 29.4079 - val_loss: 1539.8235 - val_mse: 1539.8236 - val_mae: 27.7063\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 547us/step - loss: 2523.3779 - mse: 2523.3772 - mae: 29.6855 - val_loss: 1543.3639 - val_mse: 1543.3636 - val_mae: 27.5328\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2510.4611 - mse: 2510.4609 - mae: 29.5174 - val_loss: 1542.5054 - val_mse: 1542.5054 - val_mae: 27.5421\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 552us/step - loss: 2449.3689 - mse: 2449.3691 - mae: 29.2885 - val_loss: 1534.8700 - val_mse: 1534.8701 - val_mae: 27.8830\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2522.8869 - mse: 2522.8872 - mae: 29.8519 - val_loss: 1536.0805 - val_mse: 1536.0806 - val_mae: 27.8089\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2540.4205 - mse: 2540.4209 - mae: 29.8652 - val_loss: 1533.2806 - val_mse: 1533.2805 - val_mae: 27.9565\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2472.2768 - mse: 2472.2778 - mae: 29.4998 - val_loss: 1543.6979 - val_mse: 1543.6979 - val_mae: 27.4701\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2522.6360 - mse: 2522.6355 - mae: 29.3783 - val_loss: 1541.7532 - val_mse: 1541.7534 - val_mae: 27.5327\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2516.7493 - mse: 2516.7485 - mae: 29.4543 - val_loss: 1531.3122 - val_mse: 1531.3123 - val_mae: 28.0920\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 540us/step - loss: 2536.8463 - mse: 2536.8457 - mae: 29.8407 - val_loss: 1536.5654 - val_mse: 1536.5654 - val_mae: 27.7635\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2553.4823 - mse: 2553.4829 - mae: 29.6885 - val_loss: 1533.9240 - val_mse: 1533.9241 - val_mae: 27.9221\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2515.0054 - mse: 2515.0059 - mae: 29.8319 - val_loss: 1534.2382 - val_mse: 1534.2383 - val_mae: 27.9470\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2500.2703 - mse: 2500.2708 - mae: 29.1424 - val_loss: 1537.6967 - val_mse: 1537.6968 - val_mae: 27.7869\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2476.5331 - mse: 2476.5342 - mae: 29.5480 - val_loss: 1539.0784 - val_mse: 1539.0784 - val_mae: 27.7042\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2488.6159 - mse: 2488.6167 - mae: 29.8107 - val_loss: 1536.9779 - val_mse: 1536.9779 - val_mae: 27.8219\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2527.7545 - mse: 2527.7544 - mae: 29.8095 - val_loss: 1546.5985 - val_mse: 1546.5986 - val_mae: 27.4049\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2556.5342 - mse: 2556.5349 - mae: 29.6326 - val_loss: 1542.2201 - val_mse: 1542.2201 - val_mae: 27.4728\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 530us/step - loss: 2540.7616 - mse: 2540.7620 - mae: 29.7298 - val_loss: 1536.3223 - val_mse: 1536.3224 - val_mae: 27.7295\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2528.2813 - mse: 2528.2815 - mae: 29.8490 - val_loss: 1538.3797 - val_mse: 1538.3796 - val_mae: 27.6535\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2549.8700 - mse: 2549.8701 - mae: 29.5709 - val_loss: 1536.2443 - val_mse: 1536.2444 - val_mae: 27.7661\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2508.5946 - mse: 2508.5952 - mae: 29.7422 - val_loss: 1534.7267 - val_mse: 1534.7267 - val_mae: 27.8576\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2517.2816 - mse: 2517.2810 - mae: 29.4674 - val_loss: 1537.1522 - val_mse: 1537.1522 - val_mae: 27.7640\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2478.8939 - mse: 2478.8938 - mae: 29.5774 - val_loss: 1542.7700 - val_mse: 1542.7700 - val_mae: 27.5197\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 530us/step - loss: 2492.4664 - mse: 2492.4663 - mae: 29.5646 - val_loss: 1537.4465 - val_mse: 1537.4467 - val_mae: 27.7601\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2471.1628 - mse: 2471.1624 - mae: 29.3218 - val_loss: 1539.5470 - val_mse: 1539.5469 - val_mae: 27.6972\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2505.4767 - mse: 2505.4761 - mae: 29.0980 - val_loss: 1538.7605 - val_mse: 1538.7604 - val_mae: 27.7335\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2475.7440 - mse: 2475.7439 - mae: 29.3853 - val_loss: 1533.4808 - val_mse: 1533.4808 - val_mae: 27.9618\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2534.7771 - mse: 2534.7766 - mae: 29.6018 - val_loss: 1539.5061 - val_mse: 1539.5062 - val_mae: 27.6833\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2469.2446 - mse: 2469.2449 - mae: 29.3401 - val_loss: 1537.2713 - val_mse: 1537.2712 - val_mae: 27.7321\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2575.5702 - mse: 2575.5708 - mae: 29.9610 - val_loss: 1539.8845 - val_mse: 1539.8843 - val_mae: 27.6223\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 520us/step - loss: 2502.8891 - mse: 2502.8896 - mae: 29.1513 - val_loss: 1539.8494 - val_mse: 1539.8494 - val_mae: 27.6307\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2553.8677 - mse: 2553.8672 - mae: 29.4040 - val_loss: 1538.0413 - val_mse: 1538.0414 - val_mae: 27.7462\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2474.9729 - mse: 2474.9722 - mae: 29.3236 - val_loss: 1535.3318 - val_mse: 1535.3319 - val_mae: 27.8010\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2539.4164 - mse: 2539.4163 - mae: 29.7999 - val_loss: 1538.7246 - val_mse: 1538.7246 - val_mae: 27.6543\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2460.6747 - mse: 2460.6748 - mae: 29.0905 - val_loss: 1537.2179 - val_mse: 1537.2180 - val_mae: 27.7444\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2616.5941 - mse: 2616.5940 - mae: 30.0282 - val_loss: 1541.2881 - val_mse: 1541.2880 - val_mae: 27.5452\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2487.7200 - mse: 2487.7205 - mae: 29.3313 - val_loss: 1533.1659 - val_mse: 1533.1660 - val_mae: 27.8904\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2549.0146 - mse: 2549.0159 - mae: 29.6891 - val_loss: 1534.3836 - val_mse: 1534.3834 - val_mae: 27.7460\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2483.4829 - mse: 2483.4824 - mae: 29.6974 - val_loss: 1537.8001 - val_mse: 1537.8002 - val_mae: 27.5805\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2509.8144 - mse: 2509.8145 - mae: 29.5416 - val_loss: 1531.0854 - val_mse: 1531.0853 - val_mae: 27.9086\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 536us/step - loss: 2538.6255 - mse: 2538.6245 - mae: 29.5638 - val_loss: 1542.5503 - val_mse: 1542.5504 - val_mae: 27.5246\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2527.1446 - mse: 2527.1453 - mae: 29.5982 - val_loss: 1540.8101 - val_mse: 1540.8102 - val_mae: 27.5646\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2472.0345 - mse: 2472.0344 - mae: 29.0280 - val_loss: 1535.8194 - val_mse: 1535.8192 - val_mae: 27.8114\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2475.9082 - mse: 2475.9084 - mae: 29.2993 - val_loss: 1535.2024 - val_mse: 1535.2024 - val_mae: 27.8310\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2585.9704 - mse: 2585.9695 - mae: 29.7468 - val_loss: 1538.2464 - val_mse: 1538.2465 - val_mae: 27.7082\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 707us/step - loss: 2526.3234 - mse: 2526.3237 - mae: 29.6205 - val_loss: 1541.7248 - val_mse: 1541.7247 - val_mae: 27.5387\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2455.4928 - mse: 2455.4917 - mae: 30.0390 - val_loss: 3725.9928 - val_mse: 3725.9937 - val_mae: 24.5258\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2408.1281 - mse: 2408.1282 - mae: 29.8085 - val_loss: 3726.7721 - val_mse: 3726.7715 - val_mae: 24.8668\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2374.8212 - mse: 2374.8218 - mae: 29.7134 - val_loss: 3727.7374 - val_mse: 3727.7383 - val_mae: 24.8833\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2384.1878 - mse: 2384.1885 - mae: 29.6003 - val_loss: 3726.2190 - val_mse: 3726.2195 - val_mae: 24.7910\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2427.8581 - mse: 2427.8577 - mae: 29.8070 - val_loss: 3726.9474 - val_mse: 3726.9470 - val_mae: 25.3857\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2386.8699 - mse: 2386.8704 - mae: 29.7140 - val_loss: 3724.4071 - val_mse: 3724.4070 - val_mae: 25.1262\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2414.1649 - mse: 2414.1650 - mae: 29.8614 - val_loss: 3722.6219 - val_mse: 3722.6218 - val_mae: 24.9319\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2396.8405 - mse: 2396.8406 - mae: 29.9168 - val_loss: 3721.7390 - val_mse: 3721.7385 - val_mae: 24.9638\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2425.1731 - mse: 2425.1746 - mae: 29.8865 - val_loss: 3719.2641 - val_mse: 3719.2637 - val_mae: 24.6479\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2453.2221 - mse: 2453.2219 - mae: 30.1595 - val_loss: 3719.9931 - val_mse: 3719.9929 - val_mae: 24.9493\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2431.3700 - mse: 2431.3706 - mae: 29.8181 - val_loss: 3718.5966 - val_mse: 3718.5974 - val_mae: 24.7031\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2409.7441 - mse: 2409.7446 - mae: 29.7869 - val_loss: 3719.3759 - val_mse: 3719.3755 - val_mae: 25.1715\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2445.4029 - mse: 2445.4028 - mae: 30.3163 - val_loss: 3715.9742 - val_mse: 3715.9736 - val_mae: 24.7522\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2376.2164 - mse: 2376.2163 - mae: 29.8550 - val_loss: 3715.0250 - val_mse: 3715.0259 - val_mae: 24.7752\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2371.1129 - mse: 2371.1128 - mae: 29.6341 - val_loss: 3714.1452 - val_mse: 3714.1455 - val_mae: 24.6145\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2446.3578 - mse: 2446.3582 - mae: 29.9120 - val_loss: 3718.1500 - val_mse: 3718.1499 - val_mae: 25.2802\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2389.4993 - mse: 2389.5000 - mae: 29.9345 - val_loss: 3715.5251 - val_mse: 3715.5254 - val_mae: 24.8889\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2365.4162 - mse: 2365.4158 - mae: 29.4285 - val_loss: 3717.0906 - val_mse: 3717.0903 - val_mae: 25.2488\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2418.0159 - mse: 2418.0156 - mae: 30.1092 - val_loss: 3714.9389 - val_mse: 3714.9377 - val_mae: 25.0472\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2402.6533 - mse: 2402.6528 - mae: 29.7337 - val_loss: 3711.9693 - val_mse: 3711.9695 - val_mae: 24.7498\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2432.0045 - mse: 2432.0037 - mae: 29.8002 - val_loss: 3711.4135 - val_mse: 3711.4133 - val_mae: 24.7634\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2415.3096 - mse: 2415.3101 - mae: 30.1116 - val_loss: 3709.2324 - val_mse: 3709.2322 - val_mae: 24.4510\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2345.3321 - mse: 2345.3318 - mae: 29.5328 - val_loss: 3710.7223 - val_mse: 3710.7222 - val_mae: 24.9196\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2423.1387 - mse: 2423.1387 - mae: 29.7413 - val_loss: 3712.0215 - val_mse: 3712.0220 - val_mae: 25.2314\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2355.3974 - mse: 2355.3970 - mae: 29.4506 - val_loss: 3711.5173 - val_mse: 3711.5181 - val_mae: 25.2819\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2424.1932 - mse: 2424.1934 - mae: 30.2008 - val_loss: 3709.0459 - val_mse: 3709.0461 - val_mae: 24.8178\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2370.9104 - mse: 2370.9094 - mae: 29.8194 - val_loss: 3709.9562 - val_mse: 3709.9551 - val_mae: 25.0225\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2359.7036 - mse: 2359.7036 - mae: 29.5932 - val_loss: 3709.1134 - val_mse: 3709.1130 - val_mae: 24.9635\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2395.4214 - mse: 2395.4209 - mae: 30.0270 - val_loss: 3706.8318 - val_mse: 3706.8328 - val_mae: 24.7179\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2385.2325 - mse: 2385.2322 - mae: 29.5871 - val_loss: 3709.7585 - val_mse: 3709.7581 - val_mae: 25.1574\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2401.9184 - mse: 2401.9180 - mae: 29.8106 - val_loss: 3706.2328 - val_mse: 3706.2327 - val_mae: 24.6423\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 507us/step - loss: 2330.1911 - mse: 2330.1917 - mae: 29.5042 - val_loss: 3709.1149 - val_mse: 3709.1147 - val_mae: 25.2245\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2406.7078 - mse: 2406.7073 - mae: 29.6046 - val_loss: 3707.6759 - val_mse: 3707.6763 - val_mae: 25.1028\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2401.7072 - mse: 2401.7073 - mae: 29.4074 - val_loss: 3708.3301 - val_mse: 3708.3301 - val_mae: 25.1240\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2389.1099 - mse: 2389.1099 - mae: 29.8165 - val_loss: 3705.4568 - val_mse: 3705.4565 - val_mae: 24.7490\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2452.3785 - mse: 2452.3789 - mae: 29.8661 - val_loss: 3702.8270 - val_mse: 3702.8267 - val_mae: 23.9876\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2396.0805 - mse: 2396.0806 - mae: 29.5109 - val_loss: 3705.3727 - val_mse: 3705.3718 - val_mae: 24.8515\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 468us/step - loss: 2393.5224 - mse: 2393.5222 - mae: 29.4585 - val_loss: 3705.3997 - val_mse: 3705.3992 - val_mae: 24.9384\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2352.3368 - mse: 2352.3364 - mae: 29.5201 - val_loss: 3702.8206 - val_mse: 3702.8201 - val_mae: 24.4014\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2395.3237 - mse: 2395.3232 - mae: 29.3183 - val_loss: 3705.5009 - val_mse: 3705.5012 - val_mae: 25.0596\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2397.1540 - mse: 2397.1543 - mae: 29.8854 - val_loss: 3703.1075 - val_mse: 3703.1082 - val_mae: 24.8775\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2410.3907 - mse: 2410.3906 - mae: 29.4561 - val_loss: 3701.4996 - val_mse: 3701.4990 - val_mae: 24.5718\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2413.2918 - mse: 2413.2925 - mae: 29.7490 - val_loss: 3701.2049 - val_mse: 3701.2046 - val_mae: 24.7516\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2433.2746 - mse: 2433.2734 - mae: 30.0410 - val_loss: 3700.5789 - val_mse: 3700.5793 - val_mae: 24.7108\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2372.5716 - mse: 2372.5720 - mae: 29.4132 - val_loss: 3702.3680 - val_mse: 3702.3674 - val_mae: 24.9284\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 506us/step - loss: 2339.3565 - mse: 2339.3564 - mae: 29.3245 - val_loss: 3701.6310 - val_mse: 3701.6316 - val_mae: 24.8421\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2343.1513 - mse: 2343.1511 - mae: 29.9712 - val_loss: 3700.0436 - val_mse: 3700.0439 - val_mae: 24.6904\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2406.9764 - mse: 2406.9758 - mae: 29.6567 - val_loss: 3699.7356 - val_mse: 3699.7358 - val_mae: 24.5057\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2407.9366 - mse: 2407.9370 - mae: 29.6004 - val_loss: 3700.1589 - val_mse: 3700.1587 - val_mae: 24.6473\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 539us/step - loss: 2438.1880 - mse: 2438.1877 - mae: 29.8726 - val_loss: 3698.4333 - val_mse: 3698.4331 - val_mae: 24.3740\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2400.0843 - mse: 2400.0842 - mae: 29.4258 - val_loss: 3700.9404 - val_mse: 3700.9402 - val_mae: 25.0708\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2407.7789 - mse: 2407.7786 - mae: 29.8271 - val_loss: 3697.8333 - val_mse: 3697.8337 - val_mae: 24.5717\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2369.4674 - mse: 2369.4678 - mae: 29.1435 - val_loss: 3702.7607 - val_mse: 3702.7598 - val_mae: 25.3052\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2339.2108 - mse: 2339.2112 - mae: 29.4709 - val_loss: 3696.7870 - val_mse: 3696.7876 - val_mae: 24.3609\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2338.4028 - mse: 2338.4023 - mae: 29.1074 - val_loss: 3700.6881 - val_mse: 3700.6877 - val_mae: 25.1290\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 532us/step - loss: 2346.5025 - mse: 2346.5022 - mae: 29.3495 - val_loss: 3697.2802 - val_mse: 3697.2795 - val_mae: 24.6710\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2393.7674 - mse: 2393.7673 - mae: 29.7512 - val_loss: 3697.0916 - val_mse: 3697.0911 - val_mae: 24.6981\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2400.5717 - mse: 2400.5715 - mae: 29.7154 - val_loss: 3696.2096 - val_mse: 3696.2100 - val_mae: 24.6614\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2369.1681 - mse: 2369.1689 - mae: 29.6067 - val_loss: 3698.5100 - val_mse: 3698.5098 - val_mae: 25.1067\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2360.8316 - mse: 2360.8311 - mae: 29.5684 - val_loss: 3694.3859 - val_mse: 3694.3860 - val_mae: 24.6310\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2351.9994 - mse: 2351.9983 - mae: 29.4986 - val_loss: 3694.8529 - val_mse: 3694.8535 - val_mae: 24.7851\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2346.4195 - mse: 2346.4197 - mae: 29.6485 - val_loss: 3696.8819 - val_mse: 3696.8816 - val_mae: 25.0637\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2351.5022 - mse: 2351.5024 - mae: 29.5681 - val_loss: 3696.7969 - val_mse: 3696.7969 - val_mae: 25.0614\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2371.9449 - mse: 2371.9441 - mae: 29.3726 - val_loss: 3693.8116 - val_mse: 3693.8113 - val_mae: 24.5769\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2418.9490 - mse: 2418.9490 - mae: 30.0093 - val_loss: 3696.4487 - val_mse: 3696.4492 - val_mae: 25.0763\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2342.0952 - mse: 2342.0950 - mae: 29.4341 - val_loss: 3695.6248 - val_mse: 3695.6255 - val_mae: 25.0321\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2397.5700 - mse: 2397.5701 - mae: 29.8737 - val_loss: 3694.8325 - val_mse: 3694.8328 - val_mae: 24.9204\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2366.9899 - mse: 2366.9897 - mae: 29.3384 - val_loss: 3696.1855 - val_mse: 3696.1853 - val_mae: 25.1812\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2383.4830 - mse: 2383.4832 - mae: 29.2718 - val_loss: 3697.9835 - val_mse: 3697.9827 - val_mae: 25.3534\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2405.5039 - mse: 2405.5046 - mae: 29.9371 - val_loss: 3690.9375 - val_mse: 3690.9380 - val_mae: 24.4563\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2383.8645 - mse: 2383.8640 - mae: 29.2540 - val_loss: 3691.0971 - val_mse: 3691.0972 - val_mae: 24.4157\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2395.6435 - mse: 2395.6438 - mae: 29.8366 - val_loss: 3690.5277 - val_mse: 3690.5281 - val_mae: 24.4572\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2392.4191 - mse: 2392.4194 - mae: 29.5320 - val_loss: 3691.6523 - val_mse: 3691.6526 - val_mae: 24.7843\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2368.6236 - mse: 2368.6240 - mae: 29.7780 - val_loss: 3692.2124 - val_mse: 3692.2119 - val_mae: 24.8225\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2364.7965 - mse: 2364.7966 - mae: 29.5901 - val_loss: 3690.6102 - val_mse: 3690.6118 - val_mae: 24.4017\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2337.2504 - mse: 2337.2517 - mae: 29.3833 - val_loss: 3691.2615 - val_mse: 3691.2610 - val_mae: 24.6089\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2440.9996 - mse: 2441.0002 - mae: 29.7961 - val_loss: 3691.1683 - val_mse: 3691.1685 - val_mae: 24.5136\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2399.1721 - mse: 2399.1716 - mae: 29.4028 - val_loss: 3691.3336 - val_mse: 3691.3333 - val_mae: 24.7505\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2342.5292 - mse: 2342.5305 - mae: 29.4372 - val_loss: 3690.4596 - val_mse: 3690.4600 - val_mae: 24.5499\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2402.3698 - mse: 2402.3704 - mae: 29.8195 - val_loss: 3689.6263 - val_mse: 3689.6257 - val_mae: 24.3853\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 668us/step - loss: 2726.8659 - mse: 2726.8647 - mae: 29.1405 - val_loss: 2458.5701 - val_mse: 2458.5701 - val_mae: 27.5060\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2719.1418 - mse: 2719.1423 - mae: 29.0567 - val_loss: 2472.2045 - val_mse: 2472.2043 - val_mae: 27.0805\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2770.2804 - mse: 2770.2803 - mae: 29.2741 - val_loss: 2478.8735 - val_mse: 2478.8738 - val_mae: 27.4510\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 670us/step - loss: 2779.2450 - mse: 2779.2441 - mae: 28.9865 - val_loss: 2478.9983 - val_mse: 2478.9985 - val_mae: 27.4452\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 661us/step - loss: 2714.5534 - mse: 2714.5535 - mae: 29.0240 - val_loss: 2467.2826 - val_mse: 2467.2827 - val_mae: 27.9793\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2778.4702 - mse: 2778.4709 - mae: 29.2888 - val_loss: 2476.1594 - val_mse: 2476.1597 - val_mae: 27.7044\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2722.4773 - mse: 2722.4773 - mae: 29.2080 - val_loss: 2483.2471 - val_mse: 2483.2473 - val_mae: 27.5498\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2760.6546 - mse: 2760.6548 - mae: 28.9455 - val_loss: 2481.4894 - val_mse: 2481.4895 - val_mae: 27.6281\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2723.7652 - mse: 2723.7656 - mae: 28.7252 - val_loss: 2496.8112 - val_mse: 2496.8108 - val_mae: 26.9711\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2792.7452 - mse: 2792.7451 - mae: 29.3104 - val_loss: 2493.2346 - val_mse: 2493.2346 - val_mae: 26.9748\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2724.0415 - mse: 2724.0420 - mae: 28.5446 - val_loss: 2483.9964 - val_mse: 2483.9966 - val_mae: 27.4012\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2730.6190 - mse: 2730.6199 - mae: 29.1485 - val_loss: 2486.1947 - val_mse: 2486.1951 - val_mae: 27.4646\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2743.7646 - mse: 2743.7639 - mae: 28.9485 - val_loss: 2494.6447 - val_mse: 2494.6443 - val_mae: 26.9298\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2740.0068 - mse: 2740.0066 - mae: 29.3295 - val_loss: 2494.1085 - val_mse: 2494.1086 - val_mae: 27.3799\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2809.0629 - mse: 2809.0625 - mae: 29.3582 - val_loss: 2494.2066 - val_mse: 2494.2068 - val_mae: 27.2116\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2739.6967 - mse: 2739.6973 - mae: 28.6888 - val_loss: 2489.0636 - val_mse: 2489.0635 - val_mae: 27.3832\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2768.3889 - mse: 2768.3894 - mae: 28.9969 - val_loss: 2482.5058 - val_mse: 2482.5056 - val_mae: 27.4344\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2769.6259 - mse: 2769.6265 - mae: 29.3828 - val_loss: 2499.3639 - val_mse: 2499.3640 - val_mae: 27.0245\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2745.6188 - mse: 2745.6184 - mae: 28.9394 - val_loss: 2501.4605 - val_mse: 2501.4604 - val_mae: 26.8751\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 666us/step - loss: 2762.8981 - mse: 2762.8979 - mae: 28.9936 - val_loss: 2491.9891 - val_mse: 2491.9888 - val_mae: 27.5034\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 668us/step - loss: 2745.8859 - mse: 2745.8860 - mae: 28.8612 - val_loss: 2497.4443 - val_mse: 2497.4443 - val_mae: 27.3749\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 663us/step - loss: 2731.8408 - mse: 2731.8411 - mae: 28.9695 - val_loss: 2496.5925 - val_mse: 2496.5925 - val_mae: 27.2572\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 669us/step - loss: 2735.5207 - mse: 2735.5205 - mae: 29.0089 - val_loss: 2508.9287 - val_mse: 2508.9290 - val_mae: 27.2805\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2718.3896 - mse: 2718.3899 - mae: 28.7230 - val_loss: 2508.9044 - val_mse: 2508.9043 - val_mae: 27.1824\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2780.0218 - mse: 2780.0220 - mae: 29.1026 - val_loss: 2507.7738 - val_mse: 2507.7742 - val_mae: 27.0996\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2707.1410 - mse: 2707.1409 - mae: 28.8535 - val_loss: 2487.8181 - val_mse: 2487.8181 - val_mae: 27.9671\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2764.7081 - mse: 2764.7083 - mae: 29.2007 - val_loss: 2496.0921 - val_mse: 2496.0920 - val_mae: 27.2596\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2679.5797 - mse: 2679.5803 - mae: 28.5101 - val_loss: 2494.2168 - val_mse: 2494.2168 - val_mae: 27.3058\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2734.2484 - mse: 2734.2478 - mae: 28.9202 - val_loss: 2496.3217 - val_mse: 2496.3220 - val_mae: 27.0660\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2720.3901 - mse: 2720.3906 - mae: 29.0760 - val_loss: 2490.5050 - val_mse: 2490.5049 - val_mae: 27.2639\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2719.2758 - mse: 2719.2761 - mae: 28.7863 - val_loss: 2492.3063 - val_mse: 2492.3062 - val_mae: 27.3265\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2725.5031 - mse: 2725.5039 - mae: 29.1208 - val_loss: 2494.5633 - val_mse: 2494.5630 - val_mae: 27.1726\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2750.6574 - mse: 2750.6575 - mae: 29.0872 - val_loss: 2494.0178 - val_mse: 2494.0176 - val_mae: 27.1961\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2772.7848 - mse: 2772.7854 - mae: 29.0946 - val_loss: 2489.6081 - val_mse: 2489.6079 - val_mae: 27.1289\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 679us/step - loss: 2711.6166 - mse: 2711.6165 - mae: 29.0246 - val_loss: 2488.9957 - val_mse: 2488.9954 - val_mae: 27.2642\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2744.0219 - mse: 2744.0217 - mae: 28.7791 - val_loss: 2482.5807 - val_mse: 2482.5808 - val_mae: 27.7236\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2740.9625 - mse: 2740.9634 - mae: 29.0147 - val_loss: 2491.8318 - val_mse: 2491.8318 - val_mae: 27.3344\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2714.6624 - mse: 2714.6624 - mae: 28.7272 - val_loss: 2493.2105 - val_mse: 2493.2109 - val_mae: 27.2553\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2692.5484 - mse: 2692.5481 - mae: 28.5680 - val_loss: 2490.3025 - val_mse: 2490.3027 - val_mae: 27.2832\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 674us/step - loss: 2753.3500 - mse: 2753.3503 - mae: 28.9781 - val_loss: 2489.9974 - val_mse: 2489.9971 - val_mae: 27.4653\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2734.0376 - mse: 2734.0376 - mae: 28.8745 - val_loss: 2488.9177 - val_mse: 2488.9177 - val_mae: 27.6233\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2795.0230 - mse: 2795.0232 - mae: 29.2828 - val_loss: 2488.2235 - val_mse: 2488.2236 - val_mae: 27.4719\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2708.5319 - mse: 2708.5325 - mae: 28.7020 - val_loss: 2484.5161 - val_mse: 2484.5161 - val_mae: 27.5959\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 667us/step - loss: 2725.7945 - mse: 2725.7949 - mae: 28.7395 - val_loss: 2487.0554 - val_mse: 2487.0549 - val_mae: 27.5479\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2721.1018 - mse: 2721.1021 - mae: 28.7473 - val_loss: 2490.7843 - val_mse: 2490.7844 - val_mae: 27.3477\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2747.3167 - mse: 2747.3169 - mae: 28.8264 - val_loss: 2488.7542 - val_mse: 2488.7534 - val_mae: 27.7954\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2732.4166 - mse: 2732.4163 - mae: 29.1546 - val_loss: 2486.8635 - val_mse: 2486.8635 - val_mae: 27.5592\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2741.8065 - mse: 2741.8066 - mae: 28.7582 - val_loss: 2489.3135 - val_mse: 2489.3137 - val_mae: 27.3571\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2687.1716 - mse: 2687.1716 - mae: 28.3713 - val_loss: 2483.7119 - val_mse: 2483.7119 - val_mae: 27.4886\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2767.8393 - mse: 2767.8391 - mae: 28.8821 - val_loss: 2488.4842 - val_mse: 2488.4839 - val_mae: 27.3956\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2748.1893 - mse: 2748.1892 - mae: 29.1444 - val_loss: 2485.6466 - val_mse: 2485.6460 - val_mae: 27.5847\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2693.2741 - mse: 2693.2737 - mae: 28.6254 - val_loss: 2481.0569 - val_mse: 2481.0574 - val_mae: 27.8793\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2737.6200 - mse: 2737.6201 - mae: 28.9643 - val_loss: 2505.0849 - val_mse: 2505.0850 - val_mae: 26.9531\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2763.0943 - mse: 2763.0942 - mae: 29.0032 - val_loss: 2496.2325 - val_mse: 2496.2329 - val_mae: 27.4384\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2749.5069 - mse: 2749.5059 - mae: 28.7254 - val_loss: 2493.4630 - val_mse: 2493.4634 - val_mae: 27.5117\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 526us/step - loss: 2740.2729 - mse: 2740.2727 - mae: 28.9708 - val_loss: 2490.1406 - val_mse: 2490.1414 - val_mae: 27.4945\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2705.7137 - mse: 2705.7134 - mae: 28.9575 - val_loss: 2491.0168 - val_mse: 2491.0166 - val_mae: 27.1820\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2728.5168 - mse: 2728.5159 - mae: 28.4349 - val_loss: 2486.4553 - val_mse: 2486.4556 - val_mae: 27.2698\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2730.9404 - mse: 2730.9399 - mae: 29.1703 - val_loss: 2483.6673 - val_mse: 2483.6672 - val_mae: 27.6441\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2761.3694 - mse: 2761.3694 - mae: 28.9242 - val_loss: 2482.7422 - val_mse: 2482.7429 - val_mae: 27.6241\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2698.6185 - mse: 2698.6184 - mae: 28.6863 - val_loss: 2484.8219 - val_mse: 2484.8220 - val_mae: 27.4987\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2740.7109 - mse: 2740.7117 - mae: 29.0004 - val_loss: 2491.7398 - val_mse: 2491.7400 - val_mae: 27.4035\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2727.9401 - mse: 2727.9395 - mae: 28.9489 - val_loss: 2494.0060 - val_mse: 2494.0056 - val_mae: 27.3203\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2735.7651 - mse: 2735.7661 - mae: 28.6827 - val_loss: 2488.3295 - val_mse: 2488.3291 - val_mae: 27.4675\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2735.6146 - mse: 2735.6147 - mae: 29.0867 - val_loss: 2479.1642 - val_mse: 2479.1643 - val_mae: 27.6494\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2728.6341 - mse: 2728.6345 - mae: 29.0090 - val_loss: 2481.1914 - val_mse: 2481.1914 - val_mae: 27.6703\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 396us/step - loss: 2704.1665 - mse: 2704.1658 - mae: 28.8126 - val_loss: 2487.4536 - val_mse: 2487.4541 - val_mae: 27.4593\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 392us/step - loss: 2774.6174 - mse: 2774.6169 - mae: 29.0639 - val_loss: 2488.8075 - val_mse: 2488.8074 - val_mae: 27.5328\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 495us/step - loss: 2740.5713 - mse: 2740.5708 - mae: 28.5542 - val_loss: 2493.5789 - val_mse: 2493.5786 - val_mae: 27.2936\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 458us/step - loss: 2785.7335 - mse: 2785.7334 - mae: 29.1664 - val_loss: 2494.7416 - val_mse: 2494.7417 - val_mae: 27.3581\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2736.0853 - mse: 2736.0852 - mae: 28.7132 - val_loss: 2494.8833 - val_mse: 2494.8833 - val_mae: 27.2659\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2743.4811 - mse: 2743.4810 - mae: 28.6987 - val_loss: 2490.7459 - val_mse: 2490.7463 - val_mae: 27.4873\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2683.1595 - mse: 2683.1604 - mae: 28.3336 - val_loss: 2490.3925 - val_mse: 2490.3928 - val_mae: 27.6333\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2772.2801 - mse: 2772.2798 - mae: 29.0296 - val_loss: 2497.7584 - val_mse: 2497.7585 - val_mae: 27.2740\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2738.2800 - mse: 2738.2800 - mae: 28.7452 - val_loss: 2490.6316 - val_mse: 2490.6321 - val_mae: 27.5237\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2706.1605 - mse: 2706.1604 - mae: 28.5528 - val_loss: 2493.8926 - val_mse: 2493.8926 - val_mae: 27.3836\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2707.0363 - mse: 2707.0356 - mae: 28.7510 - val_loss: 2495.4375 - val_mse: 2495.4375 - val_mae: 27.2340\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2766.9272 - mse: 2766.9263 - mae: 28.4852 - val_loss: 2494.5372 - val_mse: 2494.5378 - val_mae: 27.4164\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2733.5477 - mse: 2733.5466 - mae: 28.6378 - val_loss: 2501.0505 - val_mse: 2501.0508 - val_mae: 27.2180\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2772.8876 - mse: 2772.8877 - mae: 28.8587 - val_loss: 2498.3885 - val_mse: 2498.3882 - val_mae: 27.1744\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 13322.9753 - mse: 13322.9766 - mae: 109.8771 - val_loss: 34605.3993 - val_mse: 34605.3984 - val_mae: 132.6810\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 689us/step - loss: 13125.9833 - mse: 13125.9834 - mae: 108.9997 - val_loss: 34238.5260 - val_mse: 34238.5273 - val_mae: 131.2977\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 642us/step - loss: 12522.3696 - mse: 12522.3691 - mae: 106.2322 - val_loss: 33108.2263 - val_mse: 33108.2266 - val_mae: 126.9421\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 685us/step - loss: 10805.7932 - mse: 10805.7939 - mae: 97.6996 - val_loss: 30162.6859 - val_mse: 30162.6855 - val_mae: 114.8102\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 7276.0842 - mse: 7276.0845 - mae: 76.3379 - val_loss: 23852.2910 - val_mse: 23852.2930 - val_mae: 82.9653\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 3140.6240 - mse: 3140.6240 - mae: 42.2357 - val_loss: 18378.2753 - val_mse: 18378.2754 - val_mae: 44.0752\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 467us/step - loss: 2676.4867 - mse: 2676.4871 - mae: 38.0191 - val_loss: 18507.0502 - val_mse: 18507.0508 - val_mae: 44.7208\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2989.1305 - mse: 2989.1304 - mae: 40.4093 - val_loss: 18889.0913 - val_mse: 18889.0898 - val_mae: 47.1591\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 651us/step - loss: 2714.1391 - mse: 2714.1394 - mae: 37.9888 - val_loss: 18953.1079 - val_mse: 18953.1074 - val_mae: 47.5809\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 720us/step - loss: 2647.2422 - mse: 2647.2422 - mae: 36.5816 - val_loss: 18702.3000 - val_mse: 18702.3008 - val_mae: 45.8225\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 642us/step - loss: 2528.9129 - mse: 2528.9128 - mae: 36.6432 - val_loss: 18637.7582 - val_mse: 18637.7578 - val_mae: 45.3778\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2663.8363 - mse: 2663.8359 - mae: 37.4914 - val_loss: 18731.2104 - val_mse: 18731.2109 - val_mae: 45.9903\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2760.0390 - mse: 2760.0391 - mae: 38.2819 - val_loss: 18850.2005 - val_mse: 18850.2012 - val_mae: 46.7945\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2545.9571 - mse: 2545.9570 - mae: 35.8697 - val_loss: 18612.8589 - val_mse: 18612.8594 - val_mae: 45.1521\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2829.1192 - mse: 2829.1191 - mae: 38.1788 - val_loss: 18421.1948 - val_mse: 18421.1934 - val_mae: 44.0458\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2809.2364 - mse: 2809.2363 - mae: 38.9936 - val_loss: 18709.8099 - val_mse: 18709.8105 - val_mae: 45.7542\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 488us/step - loss: 2717.2898 - mse: 2717.2898 - mae: 37.0485 - val_loss: 18679.3904 - val_mse: 18679.3926 - val_mae: 45.5188\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 457us/step - loss: 2486.6394 - mse: 2486.6392 - mae: 36.0386 - val_loss: 18476.7787 - val_mse: 18476.7793 - val_mae: 44.2532\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 2164.2319 - mse: 2164.2319 - mae: 33.3126 - val_loss: 18406.3453 - val_mse: 18406.3438 - val_mae: 43.8595\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 2630.1282 - mse: 2630.1284 - mae: 36.4600 - val_loss: 18327.0917 - val_mse: 18327.0918 - val_mae: 43.4035\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 707us/step - loss: 2773.5646 - mse: 2773.5645 - mae: 36.6535 - val_loss: 18507.0589 - val_mse: 18507.0586 - val_mae: 44.3379\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2570.4401 - mse: 2570.4402 - mae: 36.7280 - val_loss: 18554.2435 - val_mse: 18554.2441 - val_mae: 44.5855\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2523.1222 - mse: 2523.1223 - mae: 36.2350 - val_loss: 18708.0154 - val_mse: 18708.0156 - val_mae: 45.5936\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2600.5745 - mse: 2600.5747 - mae: 36.3455 - val_loss: 18807.3347 - val_mse: 18807.3340 - val_mae: 46.2738\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 623us/step - loss: 2199.5548 - mse: 2199.5547 - mae: 33.8366 - val_loss: 18544.5875 - val_mse: 18544.5859 - val_mae: 44.4385\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 677us/step - loss: 2514.4717 - mse: 2514.4714 - mae: 35.9131 - val_loss: 18445.6502 - val_mse: 18445.6523 - val_mae: 43.8423\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 651us/step - loss: 2360.4547 - mse: 2360.4546 - mae: 34.5615 - val_loss: 18281.7630 - val_mse: 18281.7637 - val_mae: 42.9275\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2511.0933 - mse: 2511.0933 - mae: 36.4528 - val_loss: 18542.6433 - val_mse: 18542.6445 - val_mae: 44.3462\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2418.5398 - mse: 2418.5398 - mae: 35.3503 - val_loss: 18487.4285 - val_mse: 18487.4297 - val_mae: 43.9775\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2312.4870 - mse: 2312.4868 - mae: 35.1175 - val_loss: 18415.4656 - val_mse: 18415.4648 - val_mae: 43.5501\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 700us/step - loss: 2342.2534 - mse: 2342.2534 - mae: 35.0251 - val_loss: 18165.9825 - val_mse: 18165.9824 - val_mae: 42.2078\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 676us/step - loss: 2329.1749 - mse: 2329.1750 - mae: 34.0985 - val_loss: 18219.9026 - val_mse: 18219.9043 - val_mae: 42.4391\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 709us/step - loss: 2326.9295 - mse: 2326.9292 - mae: 34.3933 - val_loss: 18623.4936 - val_mse: 18623.4941 - val_mae: 44.7562\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2240.4730 - mse: 2240.4729 - mae: 33.8834 - val_loss: 18269.3654 - val_mse: 18269.3633 - val_mae: 42.6275\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 675us/step - loss: 2191.6222 - mse: 2191.6223 - mae: 33.7540 - val_loss: 18242.0241 - val_mse: 18242.0234 - val_mae: 42.4493\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 609us/step - loss: 2381.7317 - mse: 2381.7314 - mae: 34.6928 - val_loss: 18490.0237 - val_mse: 18490.0234 - val_mae: 43.7826\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 659us/step - loss: 2277.8249 - mse: 2277.8247 - mae: 33.8536 - val_loss: 18298.8114 - val_mse: 18298.8125 - val_mae: 42.6807\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 698us/step - loss: 2261.6031 - mse: 2261.6035 - mae: 34.2779 - val_loss: 18286.6773 - val_mse: 18286.6777 - val_mae: 42.5840\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 2373.1869 - mse: 2373.1870 - mae: 35.1512 - val_loss: 18571.6635 - val_mse: 18571.6641 - val_mae: 44.2445\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2139.3372 - mse: 2139.3372 - mae: 32.3020 - val_loss: 18298.5974 - val_mse: 18298.5977 - val_mae: 42.5960\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 2227.0804 - mse: 2227.0806 - mae: 34.0569 - val_loss: 18474.3635 - val_mse: 18474.3613 - val_mae: 43.5561\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2484.1635 - mse: 2484.1638 - mae: 35.0699 - val_loss: 18434.7156 - val_mse: 18434.7168 - val_mae: 43.2889\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 545us/step - loss: 2380.1218 - mse: 2380.1221 - mae: 34.6286 - val_loss: 18357.6592 - val_mse: 18357.6602 - val_mae: 42.8264\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2182.1543 - mse: 2182.1541 - mae: 32.2296 - val_loss: 18194.8999 - val_mse: 18194.9004 - val_mae: 41.9165\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2144.4644 - mse: 2144.4644 - mae: 32.8113 - val_loss: 18351.5076 - val_mse: 18351.5078 - val_mae: 42.7339\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2250.3336 - mse: 2250.3340 - mae: 34.2331 - val_loss: 18203.4082 - val_mse: 18203.4082 - val_mae: 41.8894\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2077.8937 - mse: 2077.8938 - mae: 32.2426 - val_loss: 18194.8026 - val_mse: 18194.8027 - val_mae: 41.8134\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2222.4283 - mse: 2222.4282 - mae: 32.7193 - val_loss: 18269.2974 - val_mse: 18269.2988 - val_mae: 42.1821\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 2285.7128 - mse: 2285.7129 - mae: 33.6610 - val_loss: 18365.9417 - val_mse: 18365.9414 - val_mae: 42.6872\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 556us/step - loss: 2182.0037 - mse: 2182.0037 - mae: 32.5619 - val_loss: 18158.3473 - val_mse: 18158.3477 - val_mae: 41.5286\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 2009.0591 - mse: 2009.0592 - mae: 31.1462 - val_loss: 18160.6189 - val_mse: 18160.6172 - val_mae: 41.5088\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2277.2453 - mse: 2277.2454 - mae: 33.2193 - val_loss: 18275.4658 - val_mse: 18275.4668 - val_mae: 42.0898\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2087.2888 - mse: 2087.2888 - mae: 31.0907 - val_loss: 17989.3041 - val_mse: 17989.3047 - val_mae: 40.6567\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 2085.2028 - mse: 2085.2026 - mae: 32.8476 - val_loss: 18262.7955 - val_mse: 18262.7969 - val_mae: 41.9629\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2202.3490 - mse: 2202.3491 - mae: 33.4995 - val_loss: 18090.6969 - val_mse: 18090.6973 - val_mae: 41.0347\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 1878.8988 - mse: 1878.8987 - mae: 30.1391 - val_loss: 17942.4986 - val_mse: 17942.4980 - val_mae: 40.3897\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2008.7596 - mse: 2008.7594 - mae: 31.4844 - val_loss: 18034.0367 - val_mse: 18034.0352 - val_mae: 40.7106\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 609us/step - loss: 1915.9859 - mse: 1915.9858 - mae: 31.1420 - val_loss: 18041.9137 - val_mse: 18041.9141 - val_mae: 40.7130\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2098.3891 - mse: 2098.3889 - mae: 31.8416 - val_loss: 18142.5931 - val_mse: 18142.5918 - val_mae: 41.1697\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 1911.4569 - mse: 1911.4572 - mae: 30.9604 - val_loss: 18211.0433 - val_mse: 18211.0430 - val_mae: 41.4843\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2344.1201 - mse: 2344.1204 - mae: 33.8183 - val_loss: 18114.4739 - val_mse: 18114.4727 - val_mae: 40.9580\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 530us/step - loss: 1965.3516 - mse: 1965.3513 - mae: 31.1557 - val_loss: 17945.7647 - val_mse: 17945.7656 - val_mae: 40.1688\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 2117.3548 - mse: 2117.3547 - mae: 32.3735 - val_loss: 18009.9281 - val_mse: 18009.9297 - val_mae: 40.3959\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2046.3353 - mse: 2046.3352 - mae: 31.2578 - val_loss: 18116.9931 - val_mse: 18116.9961 - val_mae: 40.8703\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 1937.9377 - mse: 1937.9379 - mae: 30.1004 - val_loss: 17961.7969 - val_mse: 17961.7969 - val_mae: 40.1251\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2011.9610 - mse: 2011.9612 - mae: 31.7140 - val_loss: 18271.2704 - val_mse: 18271.2715 - val_mae: 41.6378\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2131.9558 - mse: 2131.9561 - mae: 31.7929 - val_loss: 18187.5603 - val_mse: 18187.5625 - val_mae: 41.1313\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 2108.6774 - mse: 2108.6775 - mae: 31.5322 - val_loss: 18036.2626 - val_mse: 18036.2637 - val_mae: 40.3407\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2030.7893 - mse: 2030.7896 - mae: 30.8749 - val_loss: 18033.2296 - val_mse: 18033.2305 - val_mae: 40.2933\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 695us/step - loss: 1905.8711 - mse: 1905.8712 - mae: 30.1189 - val_loss: 18018.2024 - val_mse: 18018.2012 - val_mae: 40.1926\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 2028.6994 - mse: 2028.6990 - mae: 30.5368 - val_loss: 17999.5941 - val_mse: 17999.5938 - val_mae: 40.0771\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 1937.1130 - mse: 1937.1129 - mae: 31.0838 - val_loss: 18048.7239 - val_mse: 18048.7246 - val_mae: 40.2766\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 2004.2577 - mse: 2004.2574 - mae: 31.5601 - val_loss: 18080.5257 - val_mse: 18080.5254 - val_mae: 40.4036\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 1887.1203 - mse: 1887.1201 - mae: 31.2131 - val_loss: 17891.5593 - val_mse: 17891.5625 - val_mae: 39.5617\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 1886.3165 - mse: 1886.3164 - mae: 30.3424 - val_loss: 17954.2357 - val_mse: 17954.2363 - val_mae: 39.7625\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 2116.5922 - mse: 2116.5923 - mae: 31.4788 - val_loss: 18020.6515 - val_mse: 18020.6523 - val_mae: 40.0170\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2106.4326 - mse: 2106.4321 - mae: 31.4531 - val_loss: 18093.9634 - val_mse: 18093.9648 - val_mae: 40.3364\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 666us/step - loss: 2023.1874 - mse: 2023.1873 - mae: 32.0129 - val_loss: 18226.4281 - val_mse: 18226.4277 - val_mae: 41.0002\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 1890.1543 - mse: 1890.1541 - mae: 30.0755 - val_loss: 17735.6251 - val_mse: 17735.6270 - val_mae: 39.1349\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 2073.0927 - mse: 2073.0925 - mae: 31.0413 - val_loss: 18145.6391 - val_mse: 18145.6387 - val_mae: 40.5018\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4259.3804 - mse: 4259.3804 - mae: 35.8364 - val_loss: 2194.7124 - val_mse: 2194.7124 - val_mae: 31.8317\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4019.6747 - mse: 4019.6750 - mae: 35.0719 - val_loss: 2233.7431 - val_mse: 2233.7432 - val_mae: 31.9244\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4304.9935 - mse: 4304.9937 - mae: 35.9018 - val_loss: 2298.0387 - val_mse: 2298.0388 - val_mae: 32.0941\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4137.2750 - mse: 4137.2749 - mae: 34.9873 - val_loss: 2261.8488 - val_mse: 2261.8486 - val_mae: 31.9891\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 517us/step - loss: 4304.8572 - mse: 4304.8574 - mae: 35.8670 - val_loss: 2319.8857 - val_mse: 2319.8857 - val_mae: 32.1408\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4390.3151 - mse: 4390.3149 - mae: 34.9815 - val_loss: 2346.9949 - val_mse: 2346.9951 - val_mae: 32.2029\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4215.8850 - mse: 4215.8853 - mae: 34.9881 - val_loss: 2307.9868 - val_mse: 2307.9868 - val_mae: 32.0970\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 689us/step - loss: 4378.6418 - mse: 4378.6421 - mae: 36.3668 - val_loss: 2273.8737 - val_mse: 2273.8738 - val_mae: 32.0037\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4302.9137 - mse: 4302.9131 - mae: 34.9740 - val_loss: 2306.2035 - val_mse: 2306.2036 - val_mae: 32.0845\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 553us/step - loss: 4212.3114 - mse: 4212.3115 - mae: 34.8207 - val_loss: 2241.5111 - val_mse: 2241.5110 - val_mae: 31.9133\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4243.4014 - mse: 4243.4014 - mae: 36.0502 - val_loss: 2313.8837 - val_mse: 2313.8838 - val_mae: 32.0924\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4287.8358 - mse: 4287.8354 - mae: 34.9731 - val_loss: 2295.2322 - val_mse: 2295.2322 - val_mae: 32.0422\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 429us/step - loss: 4206.2028 - mse: 4206.2017 - mae: 35.2015 - val_loss: 2349.2013 - val_mse: 2349.2009 - val_mae: 32.1661\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4227.0446 - mse: 4227.0444 - mae: 35.3875 - val_loss: 2373.6333 - val_mse: 2373.6335 - val_mae: 32.2204\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 448us/step - loss: 4339.0875 - mse: 4339.0869 - mae: 35.1038 - val_loss: 2354.6673 - val_mse: 2354.6672 - val_mae: 32.1681\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 506us/step - loss: 4019.8430 - mse: 4019.8435 - mae: 33.9052 - val_loss: 2270.0064 - val_mse: 2270.0063 - val_mae: 31.9589\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4222.8790 - mse: 4222.8789 - mae: 35.1903 - val_loss: 2294.2432 - val_mse: 2294.2429 - val_mae: 32.0180\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4090.2938 - mse: 4090.2944 - mae: 34.4533 - val_loss: 2273.9888 - val_mse: 2273.9888 - val_mae: 31.9622\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 553us/step - loss: 4235.3228 - mse: 4235.3223 - mae: 34.1712 - val_loss: 2285.9407 - val_mse: 2285.9407 - val_mae: 31.9898\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4170.1221 - mse: 4170.1221 - mae: 34.1373 - val_loss: 2306.0467 - val_mse: 2306.0464 - val_mae: 32.0365\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 526us/step - loss: 4339.2342 - mse: 4339.2339 - mae: 35.0588 - val_loss: 2338.4477 - val_mse: 2338.4475 - val_mae: 32.1079\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4213.1731 - mse: 4213.1733 - mae: 34.5762 - val_loss: 2268.1241 - val_mse: 2268.1240 - val_mae: 31.9329\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 680us/step - loss: 4166.1532 - mse: 4166.1528 - mae: 34.7345 - val_loss: 2258.2526 - val_mse: 2258.2524 - val_mae: 31.9014\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 4178.4039 - mse: 4178.4043 - mae: 34.7606 - val_loss: 2281.2476 - val_mse: 2281.2478 - val_mae: 31.9570\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 4174.9404 - mse: 4174.9404 - mae: 34.2121 - val_loss: 2303.6788 - val_mse: 2303.6787 - val_mae: 32.0049\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 695us/step - loss: 4180.4386 - mse: 4180.4385 - mae: 33.9234 - val_loss: 2284.0591 - val_mse: 2284.0593 - val_mae: 31.9507\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 4126.2549 - mse: 4126.2544 - mae: 35.3654 - val_loss: 2311.8988 - val_mse: 2311.8987 - val_mae: 32.0126\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 4044.6232 - mse: 4044.6228 - mae: 34.5253 - val_loss: 2285.0322 - val_mse: 2285.0322 - val_mae: 31.9427\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 4129.6412 - mse: 4129.6411 - mae: 34.5069 - val_loss: 2273.2394 - val_mse: 2273.2393 - val_mae: 31.9092\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4035.0401 - mse: 4035.0403 - mae: 33.8617 - val_loss: 2245.9989 - val_mse: 2245.9990 - val_mae: 31.8320\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 4047.6934 - mse: 4047.6934 - mae: 33.6116 - val_loss: 2265.6078 - val_mse: 2265.6079 - val_mae: 31.8815\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4163.2041 - mse: 4163.2041 - mae: 34.7921 - val_loss: 2289.1076 - val_mse: 2289.1077 - val_mae: 31.9364\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4124.9002 - mse: 4124.9004 - mae: 33.7943 - val_loss: 2277.1982 - val_mse: 2277.1985 - val_mae: 31.9013\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4257.7298 - mse: 4257.7300 - mae: 34.3922 - val_loss: 2311.8060 - val_mse: 2311.8059 - val_mae: 31.9813\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4131.3193 - mse: 4131.3193 - mae: 34.5981 - val_loss: 2291.7593 - val_mse: 2291.7593 - val_mae: 31.9307\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4252.2012 - mse: 4252.2012 - mae: 33.5948 - val_loss: 2278.5980 - val_mse: 2278.5981 - val_mae: 31.8941\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 4103.8774 - mse: 4103.8774 - mae: 34.1199 - val_loss: 2320.1317 - val_mse: 2320.1316 - val_mae: 31.9917\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 4168.3406 - mse: 4168.3408 - mae: 33.6969 - val_loss: 2297.8904 - val_mse: 2297.8904 - val_mae: 31.9338\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 691us/step - loss: 4106.8915 - mse: 4106.8916 - mae: 33.8712 - val_loss: 2327.4201 - val_mse: 2327.4202 - val_mae: 32.0045\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4043.9731 - mse: 4043.9727 - mae: 33.8796 - val_loss: 2294.6003 - val_mse: 2294.6003 - val_mae: 31.9214\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 4028.0680 - mse: 4028.0681 - mae: 33.2722 - val_loss: 2265.2063 - val_mse: 2265.2063 - val_mae: 31.8447\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4086.9883 - mse: 4086.9883 - mae: 33.5718 - val_loss: 2314.9373 - val_mse: 2314.9373 - val_mae: 31.9630\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 698us/step - loss: 4130.9121 - mse: 4130.9116 - mae: 34.2465 - val_loss: 2333.1317 - val_mse: 2333.1316 - val_mae: 32.0042\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 478us/step - loss: 4049.2445 - mse: 4049.2446 - mae: 32.9440 - val_loss: 2265.1533 - val_mse: 2265.1533 - val_mae: 31.8283\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 639us/step - loss: 4100.7525 - mse: 4100.7529 - mae: 33.3038 - val_loss: 2336.1552 - val_mse: 2336.1550 - val_mae: 31.9996\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4099.2992 - mse: 4099.2988 - mae: 33.4002 - val_loss: 2268.5372 - val_mse: 2268.5371 - val_mae: 31.8268\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4043.0404 - mse: 4043.0408 - mae: 33.9127 - val_loss: 2301.6126 - val_mse: 2301.6125 - val_mae: 31.9047\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 700us/step - loss: 4043.6878 - mse: 4043.6875 - mae: 33.3412 - val_loss: 2281.1850 - val_mse: 2281.1848 - val_mae: 31.8513\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4009.6378 - mse: 4009.6375 - mae: 32.9364 - val_loss: 2301.9128 - val_mse: 2301.9128 - val_mae: 31.8998\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 3883.1845 - mse: 3883.1843 - mae: 33.6401 - val_loss: 2343.6443 - val_mse: 2343.6445 - val_mae: 32.0020\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4119.0842 - mse: 4119.0845 - mae: 33.5719 - val_loss: 2318.6664 - val_mse: 2318.6663 - val_mae: 31.9378\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 502us/step - loss: 3830.6872 - mse: 3830.6868 - mae: 32.5108 - val_loss: 2280.2195 - val_mse: 2280.2195 - val_mae: 31.8387\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 507us/step - loss: 4076.7051 - mse: 4076.7053 - mae: 33.9319 - val_loss: 2314.8425 - val_mse: 2314.8425 - val_mae: 31.9272\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4065.8614 - mse: 4065.8611 - mae: 33.9857 - val_loss: 2307.4902 - val_mse: 2307.4900 - val_mae: 31.9070\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4150.2987 - mse: 4150.2993 - mae: 34.3925 - val_loss: 2316.6732 - val_mse: 2316.6733 - val_mae: 31.9269\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4013.1841 - mse: 4013.1841 - mae: 32.9778 - val_loss: 2312.2913 - val_mse: 2312.2915 - val_mae: 31.9126\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4068.3504 - mse: 4068.3503 - mae: 32.9900 - val_loss: 2284.6437 - val_mse: 2284.6438 - val_mae: 31.8376\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4110.1770 - mse: 4110.1772 - mae: 33.7890 - val_loss: 2271.5319 - val_mse: 2271.5317 - val_mae: 31.8007\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 544us/step - loss: 4020.5082 - mse: 4020.5085 - mae: 33.0591 - val_loss: 2326.3266 - val_mse: 2326.3264 - val_mae: 31.9387\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 3934.1661 - mse: 3934.1660 - mae: 32.5278 - val_loss: 2289.2092 - val_mse: 2289.2092 - val_mae: 31.8391\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 506us/step - loss: 4220.7625 - mse: 4220.7627 - mae: 33.5400 - val_loss: 2284.8323 - val_mse: 2284.8325 - val_mae: 31.8250\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4142.9816 - mse: 4142.9814 - mae: 33.5481 - val_loss: 2312.8635 - val_mse: 2312.8635 - val_mae: 31.8974\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 506us/step - loss: 4136.9318 - mse: 4136.9316 - mae: 33.4646 - val_loss: 2285.4440 - val_mse: 2285.4443 - val_mae: 31.8220\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4191.1187 - mse: 4191.1187 - mae: 33.7359 - val_loss: 2333.1334 - val_mse: 2333.1333 - val_mae: 31.9405\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 4144.1933 - mse: 4144.1929 - mae: 33.7819 - val_loss: 2327.6744 - val_mse: 2327.6743 - val_mae: 31.9206\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4238.1706 - mse: 4238.1694 - mae: 34.1691 - val_loss: 2379.5307 - val_mse: 2379.5308 - val_mae: 32.0630\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4117.7337 - mse: 4117.7339 - mae: 33.3147 - val_loss: 2339.6013 - val_mse: 2339.6013 - val_mae: 31.9472\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 680us/step - loss: 3885.3374 - mse: 3885.3381 - mae: 32.4648 - val_loss: 2276.4860 - val_mse: 2276.4858 - val_mae: 31.7797\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4176.6713 - mse: 4176.6704 - mae: 34.1059 - val_loss: 2313.0486 - val_mse: 2313.0486 - val_mae: 31.8726\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 509us/step - loss: 3996.7176 - mse: 3996.7178 - mae: 32.5973 - val_loss: 2260.0872 - val_mse: 2260.0869 - val_mae: 31.7287\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 533us/step - loss: 4096.5895 - mse: 4096.5898 - mae: 34.0008 - val_loss: 2331.5483 - val_mse: 2331.5483 - val_mae: 31.9158\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 4071.1842 - mse: 4071.1841 - mae: 32.5898 - val_loss: 2297.4513 - val_mse: 2297.4512 - val_mae: 31.8240\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 527us/step - loss: 4094.8292 - mse: 4094.8303 - mae: 32.6875 - val_loss: 2301.5777 - val_mse: 2301.5774 - val_mae: 31.8288\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 3942.0933 - mse: 3942.0933 - mae: 32.6618 - val_loss: 2302.6291 - val_mse: 2302.6292 - val_mae: 31.8280\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4092.2566 - mse: 4092.2568 - mae: 32.6790 - val_loss: 2287.8409 - val_mse: 2287.8408 - val_mae: 31.7849\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 3987.8455 - mse: 3987.8450 - mae: 33.2797 - val_loss: 2299.5555 - val_mse: 2299.5554 - val_mae: 31.8126\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 3862.1244 - mse: 3862.1238 - mae: 32.4394 - val_loss: 2328.4558 - val_mse: 2328.4561 - val_mae: 31.8828\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 4105.9455 - mse: 4105.9458 - mae: 33.5074 - val_loss: 2357.4856 - val_mse: 2357.4858 - val_mae: 31.9594\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 3946.6919 - mse: 3946.6919 - mae: 32.6310 - val_loss: 2324.4276 - val_mse: 2324.4277 - val_mae: 31.8662\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4024.8978 - mse: 4024.8977 - mae: 33.0597 - val_loss: 2315.6676 - val_mse: 2315.6677 - val_mae: 31.8434\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3459.3290 - mse: 3459.3289 - mae: 33.5176 - val_loss: 1474.7546 - val_mse: 1474.7545 - val_mae: 25.2820\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3383.9761 - mse: 3383.9758 - mae: 33.2340 - val_loss: 1473.6436 - val_mse: 1473.6436 - val_mae: 25.8038\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3455.4911 - mse: 3455.4910 - mae: 32.9131 - val_loss: 1473.8883 - val_mse: 1473.8882 - val_mae: 25.3952\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3313.1726 - mse: 3313.1726 - mae: 31.9792 - val_loss: 1474.6060 - val_mse: 1474.6060 - val_mae: 26.1612\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3450.6720 - mse: 3450.6726 - mae: 32.9420 - val_loss: 1473.0861 - val_mse: 1473.0861 - val_mae: 25.6067\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3377.9672 - mse: 3377.9666 - mae: 32.8785 - val_loss: 1473.1046 - val_mse: 1473.1045 - val_mae: 25.8250\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3373.1138 - mse: 3373.1140 - mae: 31.7081 - val_loss: 1472.9060 - val_mse: 1472.9061 - val_mae: 25.6464\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3438.5066 - mse: 3438.5061 - mae: 32.7247 - val_loss: 1472.9844 - val_mse: 1472.9845 - val_mae: 25.8254\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3370.0203 - mse: 3370.0205 - mae: 32.9922 - val_loss: 1474.1679 - val_mse: 1474.1680 - val_mae: 26.1648\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 481us/step - loss: 3293.3945 - mse: 3293.3945 - mae: 31.8332 - val_loss: 1472.6012 - val_mse: 1472.6014 - val_mae: 25.6782\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 524us/step - loss: 3207.9874 - mse: 3207.9866 - mae: 31.9266 - val_loss: 1476.0538 - val_mse: 1476.0536 - val_mae: 26.4607\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3379.1041 - mse: 3379.1040 - mae: 33.0363 - val_loss: 1473.1270 - val_mse: 1473.1270 - val_mae: 25.9415\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3495.2058 - mse: 3495.2063 - mae: 33.0517 - val_loss: 1473.0141 - val_mse: 1473.0140 - val_mae: 25.8390\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3289.5051 - mse: 3289.5046 - mae: 31.9295 - val_loss: 1476.8467 - val_mse: 1476.8466 - val_mae: 26.5420\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3442.1869 - mse: 3442.1865 - mae: 32.7881 - val_loss: 1472.7891 - val_mse: 1472.7893 - val_mae: 25.6846\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3380.3711 - mse: 3380.3708 - mae: 32.1570 - val_loss: 1475.5083 - val_mse: 1475.5083 - val_mae: 26.3716\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 718us/step - loss: 3299.5890 - mse: 3299.5889 - mae: 32.1419 - val_loss: 1475.6988 - val_mse: 1475.6987 - val_mae: 26.3891\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3293.9887 - mse: 3293.9890 - mae: 32.6234 - val_loss: 1477.2831 - val_mse: 1477.2831 - val_mae: 26.5776\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 540us/step - loss: 3367.1407 - mse: 3367.1404 - mae: 32.9812 - val_loss: 1473.2637 - val_mse: 1473.2637 - val_mae: 25.9167\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3411.9425 - mse: 3411.9424 - mae: 32.6440 - val_loss: 1472.9817 - val_mse: 1472.9816 - val_mae: 25.5816\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3377.6853 - mse: 3377.6853 - mae: 32.1419 - val_loss: 1473.1384 - val_mse: 1473.1384 - val_mae: 25.7905\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3263.9763 - mse: 3263.9763 - mae: 33.3040 - val_loss: 1473.3085 - val_mse: 1473.3085 - val_mae: 25.7086\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3385.0027 - mse: 3385.0029 - mae: 32.1806 - val_loss: 1473.3363 - val_mse: 1473.3364 - val_mae: 25.7721\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 515us/step - loss: 3333.1496 - mse: 3333.1501 - mae: 32.7003 - val_loss: 1473.7972 - val_mse: 1473.7971 - val_mae: 26.0387\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3317.6317 - mse: 3317.6318 - mae: 32.8882 - val_loss: 1475.8814 - val_mse: 1475.8813 - val_mae: 26.4515\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3295.9307 - mse: 3295.9312 - mae: 32.1271 - val_loss: 1472.8263 - val_mse: 1472.8260 - val_mae: 25.7030\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 520us/step - loss: 3279.4794 - mse: 3279.4800 - mae: 31.5235 - val_loss: 1473.5480 - val_mse: 1473.5480 - val_mae: 25.9408\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 540us/step - loss: 3268.7595 - mse: 3268.7595 - mae: 31.5704 - val_loss: 1478.5739 - val_mse: 1478.5739 - val_mae: 26.7399\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3293.4327 - mse: 3293.4324 - mae: 31.4375 - val_loss: 1474.0141 - val_mse: 1474.0140 - val_mae: 26.0773\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3358.1151 - mse: 3358.1157 - mae: 32.3342 - val_loss: 1473.8973 - val_mse: 1473.8972 - val_mae: 26.0811\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3328.2934 - mse: 3328.2937 - mae: 31.9843 - val_loss: 1475.0396 - val_mse: 1475.0396 - val_mae: 26.2829\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3306.2418 - mse: 3306.2407 - mae: 31.8718 - val_loss: 1473.4158 - val_mse: 1473.4158 - val_mae: 25.8191\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3259.3364 - mse: 3259.3357 - mae: 31.9708 - val_loss: 1475.4558 - val_mse: 1475.4557 - val_mae: 26.3186\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 543us/step - loss: 3218.2613 - mse: 3218.2605 - mae: 31.4282 - val_loss: 1476.2396 - val_mse: 1476.2396 - val_mae: 26.4459\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3355.5315 - mse: 3355.5315 - mae: 32.3574 - val_loss: 1474.8586 - val_mse: 1474.8588 - val_mae: 26.2192\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3326.7517 - mse: 3326.7515 - mae: 31.7923 - val_loss: 1477.9971 - val_mse: 1477.9972 - val_mae: 26.6788\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3230.5592 - mse: 3230.5596 - mae: 31.9242 - val_loss: 1473.4502 - val_mse: 1473.4503 - val_mae: 25.7292\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 653us/step - loss: 3380.3099 - mse: 3380.3101 - mae: 32.3309 - val_loss: 1473.4140 - val_mse: 1473.4139 - val_mae: 25.7785\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3402.1066 - mse: 3402.1067 - mae: 32.6671 - val_loss: 1475.2470 - val_mse: 1475.2469 - val_mae: 25.2191\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 534us/step - loss: 3256.5834 - mse: 3256.5833 - mae: 31.9470 - val_loss: 1473.8463 - val_mse: 1473.8463 - val_mae: 25.8448\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3261.4166 - mse: 3261.4170 - mae: 31.7127 - val_loss: 1473.9705 - val_mse: 1473.9705 - val_mae: 25.8949\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3286.3136 - mse: 3286.3135 - mae: 32.0586 - val_loss: 1473.7171 - val_mse: 1473.7170 - val_mae: 25.6062\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3397.1657 - mse: 3397.1655 - mae: 31.9230 - val_loss: 1475.1876 - val_mse: 1475.1875 - val_mae: 25.2377\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 657us/step - loss: 3426.6960 - mse: 3426.6968 - mae: 32.2080 - val_loss: 1474.2546 - val_mse: 1474.2546 - val_mae: 25.9397\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3256.5850 - mse: 3256.5859 - mae: 31.5733 - val_loss: 1474.4804 - val_mse: 1474.4805 - val_mae: 25.9931\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3307.2529 - mse: 3307.2534 - mae: 31.7903 - val_loss: 1474.1030 - val_mse: 1474.1031 - val_mae: 25.5603\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3257.0073 - mse: 3257.0078 - mae: 31.7717 - val_loss: 1475.0883 - val_mse: 1475.0883 - val_mae: 26.1572\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3355.0255 - mse: 3355.0251 - mae: 32.2941 - val_loss: 1474.6612 - val_mse: 1474.6613 - val_mae: 26.0126\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3313.8918 - mse: 3313.8914 - mae: 31.7402 - val_loss: 1474.7217 - val_mse: 1474.7218 - val_mae: 25.4229\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3416.8673 - mse: 3416.8677 - mae: 32.5075 - val_loss: 1474.1941 - val_mse: 1474.1942 - val_mae: 25.6947\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3229.7686 - mse: 3229.7688 - mae: 31.3038 - val_loss: 1477.8582 - val_mse: 1477.8579 - val_mae: 26.5998\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 435us/step - loss: 3250.7504 - mse: 3250.7498 - mae: 31.8392 - val_loss: 1474.6066 - val_mse: 1474.6064 - val_mae: 26.0142\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3279.5028 - mse: 3279.5027 - mae: 32.1237 - val_loss: 1476.0063 - val_mse: 1476.0063 - val_mae: 26.3393\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3278.6985 - mse: 3278.6992 - mae: 31.4047 - val_loss: 1474.2548 - val_mse: 1474.2549 - val_mae: 25.6302\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3341.4489 - mse: 3341.4495 - mae: 31.9517 - val_loss: 1474.6853 - val_mse: 1474.6853 - val_mae: 25.9169\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3239.3862 - mse: 3239.3860 - mae: 30.5035 - val_loss: 1479.3813 - val_mse: 1479.3812 - val_mae: 26.7304\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3266.3071 - mse: 3266.3074 - mae: 30.9498 - val_loss: 1474.8500 - val_mse: 1474.8500 - val_mae: 25.9503\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3354.6123 - mse: 3354.6118 - mae: 31.7331 - val_loss: 1476.0050 - val_mse: 1476.0049 - val_mae: 26.2044\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3264.0229 - mse: 3264.0227 - mae: 31.2169 - val_loss: 1476.0893 - val_mse: 1476.0892 - val_mae: 26.1439\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3449.1038 - mse: 3449.1040 - mae: 32.1505 - val_loss: 1475.7957 - val_mse: 1475.7958 - val_mae: 25.8542\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3348.0492 - mse: 3348.0496 - mae: 32.3372 - val_loss: 1475.5945 - val_mse: 1475.5946 - val_mae: 25.7560\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3204.1068 - mse: 3204.1060 - mae: 31.4087 - val_loss: 1476.7407 - val_mse: 1476.7405 - val_mae: 26.2592\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3202.3025 - mse: 3202.3027 - mae: 31.6588 - val_loss: 1475.5081 - val_mse: 1475.5081 - val_mae: 25.9583\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3289.8305 - mse: 3289.8311 - mae: 31.6157 - val_loss: 1476.1195 - val_mse: 1476.1195 - val_mae: 26.1541\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 491us/step - loss: 3380.6984 - mse: 3380.6982 - mae: 32.0658 - val_loss: 1475.5830 - val_mse: 1475.5829 - val_mae: 25.6499\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3148.4272 - mse: 3148.4275 - mae: 31.0029 - val_loss: 1482.1043 - val_mse: 1482.1042 - val_mae: 26.9085\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3121.3115 - mse: 3121.3118 - mae: 30.9346 - val_loss: 1476.9417 - val_mse: 1476.9419 - val_mae: 26.2546\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 487us/step - loss: 3233.9724 - mse: 3233.9717 - mae: 31.5203 - val_loss: 1475.9539 - val_mse: 1475.9537 - val_mae: 25.9054\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3206.8774 - mse: 3206.8777 - mae: 31.5578 - val_loss: 1477.2707 - val_mse: 1477.2709 - val_mae: 26.2729\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3247.9873 - mse: 3247.9871 - mae: 31.6993 - val_loss: 1476.6639 - val_mse: 1476.6639 - val_mae: 26.1330\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 660us/step - loss: 3271.3526 - mse: 3271.3525 - mae: 30.9804 - val_loss: 1475.7859 - val_mse: 1475.7858 - val_mae: 25.7129\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3166.4443 - mse: 3166.4441 - mae: 31.6640 - val_loss: 1478.7836 - val_mse: 1478.7838 - val_mae: 26.5582\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 510us/step - loss: 3316.8904 - mse: 3316.8896 - mae: 31.5257 - val_loss: 1475.7120 - val_mse: 1475.7119 - val_mae: 25.8085\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3331.3454 - mse: 3331.3455 - mae: 31.5725 - val_loss: 1475.7594 - val_mse: 1475.7595 - val_mae: 25.6881\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3243.2655 - mse: 3243.2651 - mae: 30.8448 - val_loss: 1476.9695 - val_mse: 1476.9694 - val_mae: 26.2627\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3244.8246 - mse: 3244.8262 - mae: 30.8816 - val_loss: 1475.6515 - val_mse: 1475.6514 - val_mae: 25.7199\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3280.4268 - mse: 3280.4270 - mae: 32.1948 - val_loss: 1476.4362 - val_mse: 1476.4362 - val_mae: 26.1411\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3209.9355 - mse: 3209.9351 - mae: 31.3638 - val_loss: 1477.3888 - val_mse: 1477.3887 - val_mae: 26.3065\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3293.3266 - mse: 3293.3257 - mae: 31.8655 - val_loss: 1476.9296 - val_mse: 1476.9297 - val_mae: 26.2559\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3133.3095 - mse: 3133.3098 - mae: 30.7521 - val_loss: 1475.9911 - val_mse: 1475.9911 - val_mae: 25.9090\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 500us/step - loss: 2885.6914 - mse: 2885.6921 - mae: 31.0811 - val_loss: 1075.0200 - val_mse: 1075.0200 - val_mae: 24.0872\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2913.1824 - mse: 2913.1824 - mae: 30.5866 - val_loss: 1076.4317 - val_mse: 1076.4316 - val_mae: 23.8904\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2948.5550 - mse: 2948.5552 - mae: 31.3648 - val_loss: 1077.0343 - val_mse: 1077.0344 - val_mae: 23.8457\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2929.6475 - mse: 2929.6482 - mae: 31.1815 - val_loss: 1074.0497 - val_mse: 1074.0497 - val_mae: 24.1491\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2966.3350 - mse: 2966.3352 - mae: 31.4690 - val_loss: 1077.6664 - val_mse: 1077.6664 - val_mae: 23.7940\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2932.2911 - mse: 2932.2908 - mae: 30.4855 - val_loss: 1074.8832 - val_mse: 1074.8832 - val_mae: 24.0739\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2803.3836 - mse: 2803.3831 - mae: 30.4027 - val_loss: 1075.3397 - val_mse: 1075.3397 - val_mae: 24.0221\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2848.9718 - mse: 2848.9717 - mae: 30.7922 - val_loss: 1074.4272 - val_mse: 1074.4272 - val_mae: 24.1956\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2866.1616 - mse: 2866.1621 - mae: 31.2124 - val_loss: 1076.0258 - val_mse: 1076.0258 - val_mae: 23.9789\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2879.7072 - mse: 2879.7070 - mae: 31.0430 - val_loss: 1075.1272 - val_mse: 1075.1272 - val_mae: 24.0782\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2991.2725 - mse: 2991.2729 - mae: 31.5650 - val_loss: 1076.8282 - val_mse: 1076.8281 - val_mae: 23.8993\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2834.4670 - mse: 2834.4668 - mae: 30.5690 - val_loss: 1075.0085 - val_mse: 1075.0085 - val_mae: 24.0767\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2873.3962 - mse: 2873.3955 - mae: 31.0110 - val_loss: 1076.2389 - val_mse: 1076.2389 - val_mae: 23.8555\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 656us/step - loss: 2903.3845 - mse: 2903.3848 - mae: 30.7604 - val_loss: 1073.2531 - val_mse: 1073.2532 - val_mae: 24.2400\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2839.0328 - mse: 2839.0330 - mae: 30.7050 - val_loss: 1072.7762 - val_mse: 1072.7761 - val_mae: 24.1730\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2974.3216 - mse: 2974.3220 - mae: 31.1992 - val_loss: 1074.3301 - val_mse: 1074.3301 - val_mae: 23.9473\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 537us/step - loss: 2875.1144 - mse: 2875.1147 - mae: 30.6576 - val_loss: 1077.5740 - val_mse: 1077.5739 - val_mae: 23.7118\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2985.1756 - mse: 2985.1760 - mae: 31.1464 - val_loss: 1078.5610 - val_mse: 1078.5610 - val_mae: 23.6473\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 553us/step - loss: 2860.8172 - mse: 2860.8176 - mae: 31.1033 - val_loss: 1076.0087 - val_mse: 1076.0089 - val_mae: 23.7895\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2957.7393 - mse: 2957.7395 - mae: 30.9129 - val_loss: 1076.2771 - val_mse: 1076.2771 - val_mae: 23.7544\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2893.4530 - mse: 2893.4526 - mae: 30.6513 - val_loss: 1073.6626 - val_mse: 1073.6626 - val_mae: 23.9565\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 640us/step - loss: 2902.3369 - mse: 2902.3374 - mae: 30.7212 - val_loss: 1071.5801 - val_mse: 1071.5801 - val_mae: 24.2797\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2915.9778 - mse: 2915.9775 - mae: 31.1396 - val_loss: 1074.3290 - val_mse: 1074.3289 - val_mae: 23.9269\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 656us/step - loss: 2836.4309 - mse: 2836.4302 - mae: 30.5475 - val_loss: 1076.2101 - val_mse: 1076.2102 - val_mae: 23.7970\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 718us/step - loss: 2877.1402 - mse: 2877.1401 - mae: 30.4973 - val_loss: 1081.0779 - val_mse: 1081.0779 - val_mae: 23.5567\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2873.7156 - mse: 2873.7148 - mae: 30.5370 - val_loss: 1072.9799 - val_mse: 1072.9799 - val_mae: 24.1343\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 671us/step - loss: 2821.4730 - mse: 2821.4729 - mae: 30.4522 - val_loss: 1072.6566 - val_mse: 1072.6566 - val_mae: 24.1357\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2900.5622 - mse: 2900.5625 - mae: 30.7105 - val_loss: 1073.8405 - val_mse: 1073.8405 - val_mae: 23.9249\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2894.4640 - mse: 2894.4639 - mae: 31.1524 - val_loss: 1073.1671 - val_mse: 1073.1672 - val_mae: 24.0257\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2913.9930 - mse: 2913.9924 - mae: 30.5790 - val_loss: 1072.0096 - val_mse: 1072.0096 - val_mae: 24.2802\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2887.3454 - mse: 2887.3452 - mae: 30.8176 - val_loss: 1072.2910 - val_mse: 1072.2911 - val_mae: 24.2712\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2935.0180 - mse: 2935.0183 - mae: 31.3399 - val_loss: 1082.6258 - val_mse: 1082.6259 - val_mae: 23.5020\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2847.3968 - mse: 2847.3965 - mae: 29.8605 - val_loss: 1074.0203 - val_mse: 1074.0201 - val_mae: 24.0455\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 514us/step - loss: 2856.2993 - mse: 2856.2988 - mae: 30.6522 - val_loss: 1076.1672 - val_mse: 1076.1671 - val_mae: 23.8527\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2884.0549 - mse: 2884.0549 - mae: 30.6095 - val_loss: 1074.0799 - val_mse: 1074.0800 - val_mae: 24.0777\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2905.2790 - mse: 2905.2788 - mae: 31.0435 - val_loss: 1075.5894 - val_mse: 1075.5895 - val_mae: 23.9009\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2804.1454 - mse: 2804.1455 - mae: 30.1044 - val_loss: 1074.4807 - val_mse: 1074.4807 - val_mae: 23.9561\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 649us/step - loss: 2891.8417 - mse: 2891.8430 - mae: 30.4233 - val_loss: 1074.5919 - val_mse: 1074.5918 - val_mae: 23.9926\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2892.7171 - mse: 2892.7166 - mae: 30.8324 - val_loss: 1075.3305 - val_mse: 1075.3307 - val_mae: 23.9005\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2830.9439 - mse: 2830.9443 - mae: 30.4322 - val_loss: 1074.6031 - val_mse: 1074.6030 - val_mae: 23.9582\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2858.2093 - mse: 2858.2087 - mae: 30.8063 - val_loss: 1074.2651 - val_mse: 1074.2651 - val_mae: 24.0173\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2978.5048 - mse: 2978.5051 - mae: 30.4978 - val_loss: 1073.9107 - val_mse: 1073.9106 - val_mae: 24.0288\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2876.4944 - mse: 2876.4944 - mae: 30.4962 - val_loss: 1073.8988 - val_mse: 1073.8988 - val_mae: 24.5823\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2802.9723 - mse: 2802.9722 - mae: 30.3590 - val_loss: 1074.1675 - val_mse: 1074.1675 - val_mae: 24.0495\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2827.5018 - mse: 2827.5024 - mae: 30.4712 - val_loss: 1073.4371 - val_mse: 1073.4371 - val_mae: 24.1953\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 676us/step - loss: 2879.4900 - mse: 2879.4910 - mae: 30.7553 - val_loss: 1073.0193 - val_mse: 1073.0194 - val_mae: 24.1885\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2788.3915 - mse: 2788.3926 - mae: 29.7990 - val_loss: 1072.7366 - val_mse: 1072.7366 - val_mae: 24.2978\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2846.9920 - mse: 2846.9912 - mae: 30.2776 - val_loss: 1072.5256 - val_mse: 1072.5255 - val_mae: 24.4110\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 677us/step - loss: 2890.1581 - mse: 2890.1582 - mae: 30.2656 - val_loss: 1072.7679 - val_mse: 1072.7679 - val_mae: 24.3849\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2882.7485 - mse: 2882.7488 - mae: 30.7307 - val_loss: 1073.0859 - val_mse: 1073.0861 - val_mae: 24.2306\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2821.8150 - mse: 2821.8147 - mae: 30.2775 - val_loss: 1073.1636 - val_mse: 1073.1636 - val_mae: 24.4155\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2948.1865 - mse: 2948.1863 - mae: 30.8739 - val_loss: 1073.4051 - val_mse: 1073.4050 - val_mae: 24.1819\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2859.9205 - mse: 2859.9207 - mae: 30.2626 - val_loss: 1073.5797 - val_mse: 1073.5797 - val_mae: 24.1463\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2869.0408 - mse: 2869.0398 - mae: 30.7923 - val_loss: 1073.8157 - val_mse: 1073.8158 - val_mae: 24.1585\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2887.4539 - mse: 2887.4543 - mae: 30.7471 - val_loss: 1073.6315 - val_mse: 1073.6315 - val_mae: 24.1442\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2948.7675 - mse: 2948.7676 - mae: 31.0304 - val_loss: 1074.4891 - val_mse: 1074.4889 - val_mae: 24.0203\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2827.1638 - mse: 2827.1633 - mae: 30.4731 - val_loss: 1073.5182 - val_mse: 1073.5183 - val_mae: 24.5407\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2834.5553 - mse: 2834.5549 - mae: 30.1836 - val_loss: 1073.6218 - val_mse: 1073.6217 - val_mae: 24.2017\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 689us/step - loss: 2742.3322 - mse: 2742.3325 - mae: 29.9792 - val_loss: 1073.7579 - val_mse: 1073.7579 - val_mae: 24.4258\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 662us/step - loss: 2929.1618 - mse: 2929.1616 - mae: 30.8765 - val_loss: 1073.7430 - val_mse: 1073.7429 - val_mae: 24.4577\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 677us/step - loss: 2907.6464 - mse: 2907.6477 - mae: 30.8258 - val_loss: 1074.9291 - val_mse: 1074.9290 - val_mae: 24.0427\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 680us/step - loss: 2849.6785 - mse: 2849.6780 - mae: 30.3503 - val_loss: 1073.2773 - val_mse: 1073.2773 - val_mae: 24.4207\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2762.0711 - mse: 2762.0706 - mae: 29.9880 - val_loss: 1073.1718 - val_mse: 1073.1718 - val_mae: 24.2811\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2773.3841 - mse: 2773.3840 - mae: 30.4046 - val_loss: 1073.3409 - val_mse: 1073.3409 - val_mae: 24.4343\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2844.9189 - mse: 2844.9180 - mae: 30.3036 - val_loss: 1073.2119 - val_mse: 1073.2119 - val_mae: 24.3452\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2896.4800 - mse: 2896.4795 - mae: 30.4192 - val_loss: 1073.4116 - val_mse: 1073.4117 - val_mae: 24.4493\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2773.0374 - mse: 2773.0376 - mae: 29.8724 - val_loss: 1073.5671 - val_mse: 1073.5673 - val_mae: 24.5580\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2804.9613 - mse: 2804.9619 - mae: 30.0212 - val_loss: 1074.4371 - val_mse: 1074.4371 - val_mae: 24.1150\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2871.7476 - mse: 2871.7483 - mae: 30.1257 - val_loss: 1074.8968 - val_mse: 1074.8969 - val_mae: 24.0877\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2869.5573 - mse: 2869.5576 - mae: 30.3649 - val_loss: 1073.8050 - val_mse: 1073.8049 - val_mae: 24.2624\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2902.2853 - mse: 2902.2856 - mae: 30.4088 - val_loss: 1073.3197 - val_mse: 1073.3196 - val_mae: 24.4057\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2858.7078 - mse: 2858.7083 - mae: 30.6246 - val_loss: 1074.8898 - val_mse: 1074.8898 - val_mae: 23.9890\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2945.5819 - mse: 2945.5818 - mae: 30.8745 - val_loss: 1076.9100 - val_mse: 1076.9099 - val_mae: 23.8564\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2827.6394 - mse: 2827.6389 - mae: 29.6348 - val_loss: 1073.8862 - val_mse: 1073.8861 - val_mae: 24.5393\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 510us/step - loss: 2862.0357 - mse: 2862.0364 - mae: 30.4935 - val_loss: 1075.3351 - val_mse: 1075.3352 - val_mae: 23.9945\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 641us/step - loss: 2814.4638 - mse: 2814.4634 - mae: 30.0013 - val_loss: 1072.8572 - val_mse: 1072.8572 - val_mae: 24.2876\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 640us/step - loss: 2865.7333 - mse: 2865.7336 - mae: 30.5792 - val_loss: 1072.5053 - val_mse: 1072.5052 - val_mae: 24.2791\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2883.0625 - mse: 2883.0630 - mae: 30.0713 - val_loss: 1072.1497 - val_mse: 1072.1495 - val_mae: 24.2403\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2778.6467 - mse: 2778.6467 - mae: 30.1877 - val_loss: 1072.0537 - val_mse: 1072.0536 - val_mae: 24.4642\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2787.8090 - mse: 2787.8088 - mae: 30.2448 - val_loss: 1072.2529 - val_mse: 1072.2531 - val_mae: 24.1680\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2565.5918 - mse: 2565.5923 - mae: 30.0920 - val_loss: 1552.4889 - val_mse: 1552.4886 - val_mae: 28.5727\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2516.1827 - mse: 2516.1821 - mae: 29.8473 - val_loss: 1557.3964 - val_mse: 1557.3965 - val_mae: 28.1865\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 545us/step - loss: 2512.8223 - mse: 2512.8230 - mae: 29.5333 - val_loss: 1558.3486 - val_mse: 1558.3484 - val_mae: 28.1465\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2559.0680 - mse: 2559.0681 - mae: 29.7767 - val_loss: 1553.1886 - val_mse: 1553.1888 - val_mae: 28.4665\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2515.9271 - mse: 2515.9268 - mae: 29.7712 - val_loss: 1557.4353 - val_mse: 1557.4353 - val_mae: 28.1688\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2527.4352 - mse: 2527.4358 - mae: 29.2731 - val_loss: 1558.8505 - val_mse: 1558.8507 - val_mae: 28.0786\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2572.6548 - mse: 2572.6550 - mae: 29.7730 - val_loss: 1554.8443 - val_mse: 1554.8444 - val_mae: 28.3497\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2561.5277 - mse: 2561.5278 - mae: 29.6055 - val_loss: 1559.3301 - val_mse: 1559.3301 - val_mae: 28.0920\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2602.1470 - mse: 2602.1479 - mae: 29.8034 - val_loss: 1553.3226 - val_mse: 1553.3225 - val_mae: 28.4069\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2515.6575 - mse: 2515.6584 - mae: 29.4810 - val_loss: 1553.1820 - val_mse: 1553.1820 - val_mae: 28.4056\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2494.8392 - mse: 2494.8394 - mae: 29.2969 - val_loss: 1550.2407 - val_mse: 1550.2406 - val_mae: 28.6391\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2577.3550 - mse: 2577.3547 - mae: 29.5720 - val_loss: 1556.9594 - val_mse: 1556.9594 - val_mae: 28.1585\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2575.7823 - mse: 2575.7825 - mae: 29.9505 - val_loss: 1554.7491 - val_mse: 1554.7490 - val_mae: 28.2807\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2567.1538 - mse: 2567.1523 - mae: 29.5460 - val_loss: 1552.7173 - val_mse: 1552.7174 - val_mae: 28.4154\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 520us/step - loss: 2485.5630 - mse: 2485.5632 - mae: 29.5325 - val_loss: 1552.7351 - val_mse: 1552.7351 - val_mae: 28.4098\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2564.4140 - mse: 2564.4153 - mae: 29.9100 - val_loss: 1556.6115 - val_mse: 1556.6117 - val_mae: 28.1717\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2522.7311 - mse: 2522.7307 - mae: 29.6856 - val_loss: 1562.2755 - val_mse: 1562.2756 - val_mae: 27.8818\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - ETA: 0s - loss: 2563.2101 - mse: 2563.2100 - mae: 29.45 - 1s 553us/step - loss: 2490.4440 - mse: 2490.4436 - mae: 29.1138 - val_loss: 1555.7189 - val_mse: 1555.7190 - val_mae: 28.1726\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2513.3150 - mse: 2513.3154 - mae: 29.3881 - val_loss: 1548.9514 - val_mse: 1548.9513 - val_mae: 28.6120\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2581.9528 - mse: 2581.9519 - mae: 30.0521 - val_loss: 1553.3529 - val_mse: 1553.3530 - val_mae: 28.2266\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2631.1174 - mse: 2631.1174 - mae: 30.0995 - val_loss: 1556.6888 - val_mse: 1556.6890 - val_mae: 28.0339\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2489.6334 - mse: 2489.6343 - mae: 29.5053 - val_loss: 1556.0950 - val_mse: 1556.0951 - val_mae: 28.0977\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2568.3070 - mse: 2568.3071 - mae: 29.5270 - val_loss: 1557.4334 - val_mse: 1557.4333 - val_mae: 27.9873\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 682us/step - loss: 2505.6249 - mse: 2505.6245 - mae: 29.1266 - val_loss: 1546.2518 - val_mse: 1546.2515 - val_mae: 28.7801\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2477.5501 - mse: 2477.5498 - mae: 29.4928 - val_loss: 1547.6655 - val_mse: 1547.6655 - val_mae: 28.5872\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2507.1015 - mse: 2507.1013 - mae: 29.6254 - val_loss: 1547.4280 - val_mse: 1547.4282 - val_mae: 28.6310\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2523.0011 - mse: 2523.0007 - mae: 29.7383 - val_loss: 1549.2909 - val_mse: 1549.2909 - val_mae: 28.4471\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2560.1540 - mse: 2560.1536 - mae: 29.5467 - val_loss: 1546.9316 - val_mse: 1546.9318 - val_mae: 28.6671\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2569.7001 - mse: 2569.7007 - mae: 29.8375 - val_loss: 1548.1528 - val_mse: 1548.1530 - val_mae: 28.4812\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2608.7041 - mse: 2608.7051 - mae: 30.3234 - val_loss: 1548.8287 - val_mse: 1548.8286 - val_mae: 28.4075\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 685us/step - loss: 2564.1917 - mse: 2564.1912 - mae: 29.6643 - val_loss: 1547.9732 - val_mse: 1547.9730 - val_mae: 28.5123\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2527.7732 - mse: 2527.7732 - mae: 29.4380 - val_loss: 1546.5232 - val_mse: 1546.5232 - val_mae: 28.6602\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 552us/step - loss: 2451.0922 - mse: 2451.0928 - mae: 29.2541 - val_loss: 1547.7193 - val_mse: 1547.7189 - val_mae: 28.5341\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2573.4025 - mse: 2573.4028 - mae: 29.6134 - val_loss: 1552.1537 - val_mse: 1552.1538 - val_mae: 28.2011\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 669us/step - loss: 2581.0242 - mse: 2581.0244 - mae: 29.9684 - val_loss: 1553.4083 - val_mse: 1553.4083 - val_mae: 28.1386\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 668us/step - loss: 2488.3917 - mse: 2488.3921 - mae: 29.1157 - val_loss: 1553.1850 - val_mse: 1553.1852 - val_mae: 28.1166\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 671us/step - loss: 2552.9503 - mse: 2552.9490 - mae: 29.8000 - val_loss: 1551.4107 - val_mse: 1551.4109 - val_mae: 28.1858\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2523.8225 - mse: 2523.8232 - mae: 29.5622 - val_loss: 1548.9207 - val_mse: 1548.9207 - val_mae: 28.3514\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2548.8143 - mse: 2548.8149 - mae: 29.6095 - val_loss: 1545.7336 - val_mse: 1545.7336 - val_mae: 28.5760\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2544.6104 - mse: 2544.6099 - mae: 29.7671 - val_loss: 1549.7414 - val_mse: 1549.7415 - val_mae: 28.2830\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2587.5364 - mse: 2587.5369 - mae: 29.5182 - val_loss: 1550.2849 - val_mse: 1550.2852 - val_mae: 28.2528\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2536.7713 - mse: 2536.7717 - mae: 29.6621 - val_loss: 1546.0600 - val_mse: 1546.0601 - val_mae: 28.5592\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2545.1675 - mse: 2545.1680 - mae: 29.7340 - val_loss: 1553.7859 - val_mse: 1553.7858 - val_mae: 28.0259\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2536.5117 - mse: 2536.5120 - mae: 29.3705 - val_loss: 1545.7039 - val_mse: 1545.7037 - val_mae: 28.5402\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2506.8673 - mse: 2506.8660 - mae: 29.3241 - val_loss: 1551.6623 - val_mse: 1551.6622 - val_mae: 28.1308\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2521.4889 - mse: 2521.4890 - mae: 29.3199 - val_loss: 1549.0127 - val_mse: 1549.0127 - val_mae: 28.3187\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2465.4058 - mse: 2465.4062 - mae: 29.0085 - val_loss: 1547.6819 - val_mse: 1547.6819 - val_mae: 28.3949\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2476.9499 - mse: 2476.9495 - mae: 28.9622 - val_loss: 1544.7330 - val_mse: 1544.7328 - val_mae: 28.6580\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2541.2958 - mse: 2541.2961 - mae: 29.8017 - val_loss: 1546.0471 - val_mse: 1546.0470 - val_mae: 28.5199\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 652us/step - loss: 2526.8898 - mse: 2526.8901 - mae: 29.6455 - val_loss: 1547.3674 - val_mse: 1547.3674 - val_mae: 28.3721\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2452.0982 - mse: 2452.0981 - mae: 29.2201 - val_loss: 1547.0774 - val_mse: 1547.0774 - val_mae: 28.3761\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2596.2402 - mse: 2596.2402 - mae: 30.1082 - val_loss: 1545.1123 - val_mse: 1545.1123 - val_mae: 28.5228\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2481.4608 - mse: 2481.4612 - mae: 29.3881 - val_loss: 1547.7603 - val_mse: 1547.7604 - val_mae: 28.2893\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2558.3783 - mse: 2558.3779 - mae: 29.7283 - val_loss: 1549.9981 - val_mse: 1549.9979 - val_mae: 28.1355\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2505.8101 - mse: 2505.8108 - mae: 29.2863 - val_loss: 1545.6198 - val_mse: 1545.6200 - val_mae: 28.3970\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2521.2636 - mse: 2521.2637 - mae: 29.5748 - val_loss: 1546.3618 - val_mse: 1546.3618 - val_mae: 28.3279\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 682us/step - loss: 2551.4556 - mse: 2551.4561 - mae: 29.8716 - val_loss: 1543.3974 - val_mse: 1543.3976 - val_mae: 28.5652\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 686us/step - loss: 2475.0712 - mse: 2475.0708 - mae: 29.2063 - val_loss: 1542.8815 - val_mse: 1542.8816 - val_mae: 28.5367\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2441.0948 - mse: 2441.0955 - mae: 28.8358 - val_loss: 1541.5923 - val_mse: 1541.5923 - val_mae: 28.6517\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2503.4448 - mse: 2503.4451 - mae: 29.6394 - val_loss: 1545.6198 - val_mse: 1545.6198 - val_mae: 28.2994\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2507.8766 - mse: 2507.8767 - mae: 29.6609 - val_loss: 1543.7437 - val_mse: 1543.7437 - val_mae: 28.4051\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2394.0545 - mse: 2394.0542 - mae: 29.0208 - val_loss: 1541.0816 - val_mse: 1541.0814 - val_mae: 28.6385\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2495.9697 - mse: 2495.9697 - mae: 29.1118 - val_loss: 1539.5005 - val_mse: 1539.5004 - val_mae: 28.7285\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2490.1340 - mse: 2490.1328 - mae: 29.2396 - val_loss: 1540.9644 - val_mse: 1540.9646 - val_mae: 28.5763\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2468.0951 - mse: 2468.0950 - mae: 28.8004 - val_loss: 1542.9588 - val_mse: 1542.9587 - val_mae: 28.3972\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2561.0121 - mse: 2561.0120 - mae: 29.4123 - val_loss: 1545.9939 - val_mse: 1545.9940 - val_mae: 28.2090\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2479.4782 - mse: 2479.4790 - mae: 29.0546 - val_loss: 1543.8567 - val_mse: 1543.8567 - val_mae: 28.3186\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2567.5375 - mse: 2567.5378 - mae: 29.3075 - val_loss: 1544.5312 - val_mse: 1544.5312 - val_mae: 28.2657\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2480.2783 - mse: 2480.2788 - mae: 29.2610 - val_loss: 1544.0273 - val_mse: 1544.0275 - val_mae: 28.3229\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2485.6715 - mse: 2485.6721 - mae: 29.3314 - val_loss: 1542.7086 - val_mse: 1542.7086 - val_mae: 28.4577\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2509.6752 - mse: 2509.6753 - mae: 29.3819 - val_loss: 1542.1031 - val_mse: 1542.1031 - val_mae: 28.4954\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2496.8683 - mse: 2496.8682 - mae: 29.3395 - val_loss: 1541.1599 - val_mse: 1541.1598 - val_mae: 28.5488\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2529.4521 - mse: 2529.4519 - mae: 29.6459 - val_loss: 1542.7672 - val_mse: 1542.7672 - val_mae: 28.3534\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2498.8760 - mse: 2498.8752 - mae: 29.6442 - val_loss: 1539.8467 - val_mse: 1539.8470 - val_mae: 28.6210\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2522.1817 - mse: 2522.1816 - mae: 29.3966 - val_loss: 1541.6827 - val_mse: 1541.6826 - val_mae: 28.4466\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2471.0391 - mse: 2471.0393 - mae: 29.3877 - val_loss: 1542.4691 - val_mse: 1542.4691 - val_mae: 28.3758\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2500.5080 - mse: 2500.5076 - mae: 29.0428 - val_loss: 1541.4880 - val_mse: 1541.4879 - val_mae: 28.4586\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 544us/step - loss: 2424.4990 - mse: 2424.4995 - mae: 28.9327 - val_loss: 1539.7258 - val_mse: 1539.7257 - val_mae: 28.5664\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 535us/step - loss: 2559.4155 - mse: 2559.4158 - mae: 29.3704 - val_loss: 1543.4853 - val_mse: 1543.4852 - val_mae: 28.2533\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2479.2730 - mse: 2479.2725 - mae: 28.9644 - val_loss: 1537.3183 - val_mse: 1537.3184 - val_mae: 28.9649\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2382.6911 - mse: 2382.6914 - mae: 29.7853 - val_loss: 3695.8625 - val_mse: 3695.8618 - val_mae: 24.4648\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2387.9098 - mse: 2387.9104 - mae: 29.5224 - val_loss: 3697.5725 - val_mse: 3697.5713 - val_mae: 24.9388\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 662us/step - loss: 2415.6882 - mse: 2415.6877 - mae: 29.7344 - val_loss: 3695.9379 - val_mse: 3695.9385 - val_mae: 24.6718\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2349.2984 - mse: 2349.2981 - mae: 29.4971 - val_loss: 3695.4355 - val_mse: 3695.4358 - val_mae: 24.5801\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2436.1669 - mse: 2436.1667 - mae: 29.6331 - val_loss: 3693.8510 - val_mse: 3693.8513 - val_mae: 24.4139\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2420.7646 - mse: 2420.7654 - mae: 29.7254 - val_loss: 3694.4320 - val_mse: 3694.4316 - val_mae: 24.6328\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2412.6229 - mse: 2412.6230 - mae: 29.7919 - val_loss: 3692.6632 - val_mse: 3692.6619 - val_mae: 24.1936\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2366.8266 - mse: 2366.8267 - mae: 29.2521 - val_loss: 3694.9058 - val_mse: 3694.9062 - val_mae: 24.7026\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2434.8338 - mse: 2434.8340 - mae: 29.8589 - val_loss: 3693.4915 - val_mse: 3693.4912 - val_mae: 24.2891\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2365.0102 - mse: 2365.0103 - mae: 29.5091 - val_loss: 3694.3614 - val_mse: 3694.3618 - val_mae: 24.6173\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2396.9729 - mse: 2396.9729 - mae: 29.4018 - val_loss: 3694.0986 - val_mse: 3694.0986 - val_mae: 24.6745\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2371.6686 - mse: 2371.6687 - mae: 29.7019 - val_loss: 3692.7031 - val_mse: 3692.7031 - val_mae: 24.3403\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2328.5899 - mse: 2328.5898 - mae: 29.3190 - val_loss: 3694.1881 - val_mse: 3694.1887 - val_mae: 24.8317\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2445.7429 - mse: 2445.7434 - mae: 29.9539 - val_loss: 3694.9251 - val_mse: 3694.9248 - val_mae: 24.9968\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 525us/step - loss: 2329.3192 - mse: 2329.3184 - mae: 29.7413 - val_loss: 3693.2653 - val_mse: 3693.2646 - val_mae: 24.7395\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2402.9595 - mse: 2402.9600 - mae: 29.6959 - val_loss: 3692.9248 - val_mse: 3692.9253 - val_mae: 24.7616\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2401.9763 - mse: 2401.9753 - mae: 29.4329 - val_loss: 3691.4724 - val_mse: 3691.4727 - val_mae: 24.4648\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2390.1313 - mse: 2390.1313 - mae: 29.6101 - val_loss: 3691.3809 - val_mse: 3691.3799 - val_mae: 24.3513\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2430.4534 - mse: 2430.4524 - mae: 29.8175 - val_loss: 3692.0529 - val_mse: 3692.0535 - val_mae: 24.4950\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2374.5114 - mse: 2374.5112 - mae: 29.4736 - val_loss: 3694.9075 - val_mse: 3694.9072 - val_mae: 25.0867\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2397.7011 - mse: 2397.7007 - mae: 29.6814 - val_loss: 3694.6716 - val_mse: 3694.6714 - val_mae: 25.0845\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 533us/step - loss: 2362.4911 - mse: 2362.4915 - mae: 29.6783 - val_loss: 3692.4927 - val_mse: 3692.4929 - val_mae: 24.8076\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2355.6795 - mse: 2355.6799 - mae: 29.5497 - val_loss: 3694.0363 - val_mse: 3694.0361 - val_mae: 25.0299\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2395.7486 - mse: 2395.7485 - mae: 29.2796 - val_loss: 3692.9742 - val_mse: 3692.9741 - val_mae: 24.8269\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2356.4678 - mse: 2356.4670 - mae: 29.5996 - val_loss: 3691.0905 - val_mse: 3691.0894 - val_mae: 24.5294\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2372.1979 - mse: 2372.1982 - mae: 30.0328 - val_loss: 3690.8683 - val_mse: 3690.8682 - val_mae: 24.5193\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2372.3228 - mse: 2372.3225 - mae: 29.6946 - val_loss: 3690.7214 - val_mse: 3690.7219 - val_mae: 24.6098\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2353.2989 - mse: 2353.2988 - mae: 29.6671 - val_loss: 3690.7340 - val_mse: 3690.7336 - val_mae: 24.6051\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2335.9510 - mse: 2335.9514 - mae: 29.0603 - val_loss: 3689.6895 - val_mse: 3689.6902 - val_mae: 24.2199\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2406.2701 - mse: 2406.2700 - mae: 29.7237 - val_loss: 3691.0175 - val_mse: 3691.0173 - val_mae: 24.5141\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2405.0383 - mse: 2405.0381 - mae: 29.6006 - val_loss: 3692.5082 - val_mse: 3692.5083 - val_mae: 24.8201\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 670us/step - loss: 2395.1198 - mse: 2395.1199 - mae: 29.5308 - val_loss: 3691.3553 - val_mse: 3691.3562 - val_mae: 24.7742\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 634us/step - loss: 2367.8635 - mse: 2367.8630 - mae: 29.1978 - val_loss: 3690.6485 - val_mse: 3690.6487 - val_mae: 24.7329\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2358.2189 - mse: 2358.2188 - mae: 29.5646 - val_loss: 3689.3608 - val_mse: 3689.3608 - val_mae: 24.4821\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2342.4557 - mse: 2342.4558 - mae: 29.6332 - val_loss: 3690.1252 - val_mse: 3690.1252 - val_mae: 24.6941\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2336.9133 - mse: 2336.9136 - mae: 29.4045 - val_loss: 3689.3269 - val_mse: 3689.3269 - val_mae: 24.4899\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 546us/step - loss: 2375.5259 - mse: 2375.5259 - mae: 29.6004 - val_loss: 3689.0840 - val_mse: 3689.0842 - val_mae: 24.2942\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2388.7066 - mse: 2388.7065 - mae: 29.3402 - val_loss: 3694.8634 - val_mse: 3694.8638 - val_mae: 25.3151\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2384.4711 - mse: 2384.4714 - mae: 29.8068 - val_loss: 3689.9999 - val_mse: 3690.0002 - val_mae: 24.6742\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2389.1525 - mse: 2389.1528 - mae: 29.2980 - val_loss: 3687.9943 - val_mse: 3687.9944 - val_mae: 24.0897\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2349.8319 - mse: 2349.8311 - mae: 29.2844 - val_loss: 3688.4459 - val_mse: 3688.4451 - val_mae: 24.5093\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2340.8850 - mse: 2340.8850 - mae: 29.4503 - val_loss: 3687.4199 - val_mse: 3687.4194 - val_mae: 24.3884\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2426.7185 - mse: 2426.7175 - mae: 29.7712 - val_loss: 3686.5073 - val_mse: 3686.5078 - val_mae: 24.2233\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2328.1191 - mse: 2328.1189 - mae: 29.1393 - val_loss: 3688.4975 - val_mse: 3688.4980 - val_mae: 24.6048\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2339.7778 - mse: 2339.7778 - mae: 29.1088 - val_loss: 3688.6375 - val_mse: 3688.6379 - val_mae: 24.6359\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2346.4483 - mse: 2346.4485 - mae: 29.2985 - val_loss: 3688.4311 - val_mse: 3688.4299 - val_mae: 24.6787\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 686us/step - loss: 2403.8957 - mse: 2403.8962 - mae: 29.6955 - val_loss: 3687.8920 - val_mse: 3687.8916 - val_mae: 24.6348\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2351.5431 - mse: 2351.5442 - mae: 29.5543 - val_loss: 3687.5041 - val_mse: 3687.5039 - val_mae: 24.4057\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2352.0377 - mse: 2352.0371 - mae: 29.1869 - val_loss: 3690.4524 - val_mse: 3690.4519 - val_mae: 24.9575\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2315.9509 - mse: 2315.9497 - mae: 29.1150 - val_loss: 3689.9606 - val_mse: 3689.9600 - val_mae: 24.9056\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 538us/step - loss: 2394.5495 - mse: 2394.5498 - mae: 29.5268 - val_loss: 3688.1878 - val_mse: 3688.1877 - val_mae: 24.5608\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 524us/step - loss: 2375.6763 - mse: 2375.6770 - mae: 29.6564 - val_loss: 3688.2452 - val_mse: 3688.2461 - val_mae: 24.3863\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 546us/step - loss: 2362.7564 - mse: 2362.7561 - mae: 29.5419 - val_loss: 3690.0620 - val_mse: 3690.0620 - val_mae: 24.8070\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2358.5172 - mse: 2358.5173 - mae: 29.3184 - val_loss: 3689.6823 - val_mse: 3689.6824 - val_mae: 24.9808\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2349.7784 - mse: 2349.7788 - mae: 29.1889 - val_loss: 3691.1277 - val_mse: 3691.1282 - val_mae: 25.2002\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2321.4412 - mse: 2321.4417 - mae: 29.2058 - val_loss: 3687.0954 - val_mse: 3687.0950 - val_mae: 24.7038\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2408.6169 - mse: 2408.6167 - mae: 29.5434 - val_loss: 3687.2798 - val_mse: 3687.2795 - val_mae: 24.6257\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2374.8286 - mse: 2374.8281 - mae: 29.7080 - val_loss: 3687.5807 - val_mse: 3687.5813 - val_mae: 24.8404\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 546us/step - loss: 2356.6858 - mse: 2356.6853 - mae: 29.4089 - val_loss: 3685.5786 - val_mse: 3685.5791 - val_mae: 24.4800\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2364.0946 - mse: 2364.0945 - mae: 29.5687 - val_loss: 3685.8195 - val_mse: 3685.8196 - val_mae: 24.4730\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2349.6638 - mse: 2349.6641 - mae: 29.0605 - val_loss: 3686.7634 - val_mse: 3686.7634 - val_mae: 24.7207\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2360.2940 - mse: 2360.2935 - mae: 29.5448 - val_loss: 3687.5033 - val_mse: 3687.5029 - val_mae: 24.6247\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 571us/step - loss: 2335.7485 - mse: 2335.7495 - mae: 29.4920 - val_loss: 3688.4423 - val_mse: 3688.4429 - val_mae: 24.8914\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 571us/step - loss: 2396.7779 - mse: 2396.7791 - mae: 29.5978 - val_loss: 3684.7201 - val_mse: 3684.7205 - val_mae: 24.2540\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2341.4430 - mse: 2341.4434 - mae: 29.5129 - val_loss: 3685.7733 - val_mse: 3685.7742 - val_mae: 24.6169\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2369.8995 - mse: 2369.8997 - mae: 29.3183 - val_loss: 3686.2569 - val_mse: 3686.2571 - val_mae: 24.6503\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2317.9234 - mse: 2317.9224 - mae: 29.2370 - val_loss: 3688.2161 - val_mse: 3688.2156 - val_mae: 24.9936\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2399.5754 - mse: 2399.5752 - mae: 29.4003 - val_loss: 3683.6955 - val_mse: 3683.6956 - val_mae: 24.3200\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2323.7194 - mse: 2323.7200 - mae: 29.3764 - val_loss: 3685.9385 - val_mse: 3685.9377 - val_mae: 24.8184\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2325.7403 - mse: 2325.7402 - mae: 29.3391 - val_loss: 3684.5297 - val_mse: 3684.5291 - val_mae: 24.5953\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2401.8389 - mse: 2401.8396 - mae: 29.8177 - val_loss: 3684.8189 - val_mse: 3684.8184 - val_mae: 24.6186\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2384.1418 - mse: 2384.1423 - mae: 29.1191 - val_loss: 3684.3626 - val_mse: 3684.3630 - val_mae: 24.5564\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2297.2490 - mse: 2297.2495 - mae: 28.9200 - val_loss: 3687.3606 - val_mse: 3687.3601 - val_mae: 24.9624\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2384.1822 - mse: 2384.1824 - mae: 29.4998 - val_loss: 3684.4763 - val_mse: 3684.4758 - val_mae: 24.6574\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2416.4703 - mse: 2416.4700 - mae: 29.8086 - val_loss: 3683.3140 - val_mse: 3683.3142 - val_mae: 24.5703\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 682us/step - loss: 2342.1991 - mse: 2342.1997 - mae: 29.2921 - val_loss: 3686.3050 - val_mse: 3686.3049 - val_mae: 24.9125\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2364.6881 - mse: 2364.6885 - mae: 29.3789 - val_loss: 3685.4402 - val_mse: 3685.4404 - val_mae: 24.8020\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 541us/step - loss: 2330.6338 - mse: 2330.6333 - mae: 29.2772 - val_loss: 3685.3832 - val_mse: 3685.3843 - val_mae: 24.8128\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2360.4056 - mse: 2360.4055 - mae: 29.4263 - val_loss: 3686.6885 - val_mse: 3686.6885 - val_mae: 24.8920\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2373.2894 - mse: 2373.2898 - mae: 29.2588 - val_loss: 3685.3244 - val_mse: 3685.3242 - val_mae: 24.6193\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2743.3075 - mse: 2743.3083 - mae: 29.0850 - val_loss: 2426.6636 - val_mse: 2426.6633 - val_mae: 26.7970\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2759.5279 - mse: 2759.5283 - mae: 28.9611 - val_loss: 2415.0395 - val_mse: 2415.0396 - val_mae: 27.1516\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2735.4269 - mse: 2735.4268 - mae: 28.8154 - val_loss: 2407.7892 - val_mse: 2407.7893 - val_mae: 27.2812\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2756.7896 - mse: 2756.7903 - mae: 29.0598 - val_loss: 2417.6576 - val_mse: 2417.6575 - val_mae: 27.3006\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2757.4388 - mse: 2757.4385 - mae: 28.7278 - val_loss: 2419.3916 - val_mse: 2419.3918 - val_mae: 27.1448\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2743.0180 - mse: 2743.0178 - mae: 28.6933 - val_loss: 2428.5774 - val_mse: 2428.5769 - val_mae: 26.9872\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2710.2442 - mse: 2710.2451 - mae: 28.3493 - val_loss: 2433.1763 - val_mse: 2433.1763 - val_mae: 26.8545\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2771.8797 - mse: 2771.8804 - mae: 28.6427 - val_loss: 2423.9828 - val_mse: 2423.9832 - val_mae: 27.2085\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2740.3094 - mse: 2740.3091 - mae: 28.7455 - val_loss: 2420.0543 - val_mse: 2420.0542 - val_mae: 27.4748\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 545us/step - loss: 2731.6921 - mse: 2731.6929 - mae: 28.8116 - val_loss: 2433.0774 - val_mse: 2433.0771 - val_mae: 27.2549\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2748.1298 - mse: 2748.1292 - mae: 28.5282 - val_loss: 2439.5779 - val_mse: 2439.5779 - val_mae: 27.2399\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2720.9208 - mse: 2720.9209 - mae: 28.6381 - val_loss: 2440.4914 - val_mse: 2440.4919 - val_mae: 27.0768\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2735.7770 - mse: 2735.7778 - mae: 28.8046 - val_loss: 2440.1913 - val_mse: 2440.1914 - val_mae: 27.3381\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2777.0758 - mse: 2777.0764 - mae: 28.9992 - val_loss: 2431.4153 - val_mse: 2431.4150 - val_mae: 27.5683\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2755.9417 - mse: 2755.9421 - mae: 29.0311 - val_loss: 2429.0353 - val_mse: 2429.0352 - val_mae: 27.4884\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2742.0481 - mse: 2742.0481 - mae: 28.4787 - val_loss: 2428.5964 - val_mse: 2428.5964 - val_mae: 27.3376\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2748.2203 - mse: 2748.2197 - mae: 28.9869 - val_loss: 2435.1517 - val_mse: 2435.1516 - val_mae: 27.2043\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2752.6788 - mse: 2752.6792 - mae: 28.7778 - val_loss: 2438.4552 - val_mse: 2438.4553 - val_mae: 27.4457\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2745.1926 - mse: 2745.1931 - mae: 28.7480 - val_loss: 2441.2806 - val_mse: 2441.2803 - val_mae: 27.1075\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2835.2784 - mse: 2835.2781 - mae: 29.1927 - val_loss: 2440.3085 - val_mse: 2440.3079 - val_mae: 27.2537\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 545us/step - loss: 2729.8334 - mse: 2729.8333 - mae: 28.4001 - val_loss: 2440.8339 - val_mse: 2440.8337 - val_mae: 27.0935\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2800.0983 - mse: 2800.0981 - mae: 28.9009 - val_loss: 2436.9347 - val_mse: 2436.9348 - val_mae: 27.1558\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2736.1115 - mse: 2736.1123 - mae: 28.7518 - val_loss: 2436.2115 - val_mse: 2436.2117 - val_mae: 27.3830\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 545us/step - loss: 2709.1451 - mse: 2709.1450 - mae: 28.5968 - val_loss: 2439.2662 - val_mse: 2439.2664 - val_mae: 27.3950\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 551us/step - loss: 2688.6415 - mse: 2688.6416 - mae: 28.5210 - val_loss: 2439.9131 - val_mse: 2439.9131 - val_mae: 27.3597\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2713.6797 - mse: 2713.6792 - mae: 28.7497 - val_loss: 2444.3088 - val_mse: 2444.3086 - val_mae: 27.3203\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 555us/step - loss: 2734.2970 - mse: 2734.2964 - mae: 28.6255 - val_loss: 2442.3372 - val_mse: 2442.3376 - val_mae: 27.3603\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2731.4557 - mse: 2731.4551 - mae: 28.8617 - val_loss: 2434.5905 - val_mse: 2434.5903 - val_mae: 27.5773\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 536us/step - loss: 2722.2024 - mse: 2722.2014 - mae: 28.4938 - val_loss: 2423.2442 - val_mse: 2423.2439 - val_mae: 27.3964\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2721.2136 - mse: 2721.2139 - mae: 28.7321 - val_loss: 2417.9987 - val_mse: 2417.9985 - val_mae: 27.3621\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2674.1237 - mse: 2674.1240 - mae: 28.3600 - val_loss: 2415.3190 - val_mse: 2415.3191 - val_mae: 27.6595\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2696.4077 - mse: 2696.4080 - mae: 28.4066 - val_loss: 2425.8692 - val_mse: 2425.8689 - val_mae: 27.4426\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2698.5124 - mse: 2698.5120 - mae: 28.7131 - val_loss: 2429.7383 - val_mse: 2429.7385 - val_mae: 27.4197\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2726.3918 - mse: 2726.3926 - mae: 28.6962 - val_loss: 2432.2671 - val_mse: 2432.2666 - val_mae: 27.5012\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 685us/step - loss: 2718.5015 - mse: 2718.5012 - mae: 28.4490 - val_loss: 2428.5178 - val_mse: 2428.5178 - val_mae: 27.5171\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 679us/step - loss: 2713.3714 - mse: 2713.3718 - mae: 28.8254 - val_loss: 2426.1098 - val_mse: 2426.1096 - val_mae: 27.4986\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2756.5488 - mse: 2756.5493 - mae: 29.0878 - val_loss: 2426.7245 - val_mse: 2426.7249 - val_mae: 27.5748\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2719.9903 - mse: 2719.9900 - mae: 28.5694 - val_loss: 2421.4450 - val_mse: 2421.4453 - val_mae: 27.2638\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2736.4179 - mse: 2736.4180 - mae: 28.9775 - val_loss: 2433.9945 - val_mse: 2433.9949 - val_mae: 26.9831\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2697.2925 - mse: 2697.2927 - mae: 28.6500 - val_loss: 2423.5963 - val_mse: 2423.5964 - val_mae: 27.1198\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2742.2691 - mse: 2742.2688 - mae: 28.5852 - val_loss: 2418.0245 - val_mse: 2418.0247 - val_mae: 27.6466\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2757.6131 - mse: 2757.6128 - mae: 28.7436 - val_loss: 2433.7156 - val_mse: 2433.7156 - val_mae: 27.2439\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 550us/step - loss: 2744.5708 - mse: 2744.5725 - mae: 28.7970 - val_loss: 2436.1672 - val_mse: 2436.1672 - val_mae: 26.8943\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2746.0430 - mse: 2746.0430 - mae: 28.7788 - val_loss: 2433.2728 - val_mse: 2433.2727 - val_mae: 27.1364\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2768.2911 - mse: 2768.2910 - mae: 29.0896 - val_loss: 2435.5485 - val_mse: 2435.5481 - val_mae: 27.5040\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2652.6314 - mse: 2652.6313 - mae: 28.1046 - val_loss: 2431.8235 - val_mse: 2431.8235 - val_mae: 27.4947\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2714.4100 - mse: 2714.4102 - mae: 28.6408 - val_loss: 2432.1216 - val_mse: 2432.1221 - val_mae: 27.2888\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2711.4540 - mse: 2711.4534 - mae: 28.6555 - val_loss: 2424.5314 - val_mse: 2424.5310 - val_mae: 27.2359\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2703.3089 - mse: 2703.3096 - mae: 28.5139 - val_loss: 2424.3938 - val_mse: 2424.3936 - val_mae: 27.2925\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2714.5955 - mse: 2714.5962 - mae: 28.7847 - val_loss: 2425.6063 - val_mse: 2425.6064 - val_mae: 27.4677\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2711.7992 - mse: 2711.7986 - mae: 28.6414 - val_loss: 2429.8445 - val_mse: 2429.8442 - val_mae: 27.4438\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 669us/step - loss: 2631.4961 - mse: 2631.4954 - mae: 28.1491 - val_loss: 2432.1287 - val_mse: 2432.1287 - val_mae: 27.3325\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2713.4487 - mse: 2713.4490 - mae: 28.7754 - val_loss: 2433.5489 - val_mse: 2433.5488 - val_mae: 27.6054\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2670.8705 - mse: 2670.8711 - mae: 28.7599 - val_loss: 2442.4049 - val_mse: 2442.4050 - val_mae: 27.6002\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2708.0672 - mse: 2708.0671 - mae: 28.6268 - val_loss: 2443.7574 - val_mse: 2443.7578 - val_mae: 27.3259\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2725.0451 - mse: 2725.0444 - mae: 28.7382 - val_loss: 2439.6874 - val_mse: 2439.6870 - val_mae: 27.5811\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2716.3913 - mse: 2716.3914 - mae: 28.7820 - val_loss: 2435.9503 - val_mse: 2435.9507 - val_mae: 27.4437\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2717.9143 - mse: 2717.9136 - mae: 28.7501 - val_loss: 2444.1969 - val_mse: 2444.1968 - val_mae: 27.3347\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2729.0796 - mse: 2729.0789 - mae: 28.8759 - val_loss: 2444.7679 - val_mse: 2444.7678 - val_mae: 27.5982\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2703.1452 - mse: 2703.1462 - mae: 28.6306 - val_loss: 2457.2589 - val_mse: 2457.2588 - val_mae: 27.4668\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2726.4442 - mse: 2726.4446 - mae: 28.5204 - val_loss: 2454.7602 - val_mse: 2454.7600 - val_mae: 27.3862\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2634.8723 - mse: 2634.8721 - mae: 28.4994 - val_loss: 2456.2068 - val_mse: 2456.2065 - val_mae: 27.5005\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2707.1319 - mse: 2707.1323 - mae: 28.3653 - val_loss: 2449.1332 - val_mse: 2449.1331 - val_mae: 27.9222\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2780.0412 - mse: 2780.0413 - mae: 28.8745 - val_loss: 2448.6726 - val_mse: 2448.6724 - val_mae: 27.5416\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 670us/step - loss: 2705.2263 - mse: 2705.2263 - mae: 28.6412 - val_loss: 2435.2533 - val_mse: 2435.2537 - val_mae: 27.7413\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 506us/step - loss: 2693.6543 - mse: 2693.6550 - mae: 28.5916 - val_loss: 2439.6342 - val_mse: 2439.6345 - val_mae: 27.4034\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 493us/step - loss: 2736.5348 - mse: 2736.5347 - mae: 28.5270 - val_loss: 2440.9369 - val_mse: 2440.9373 - val_mae: 27.4331\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 480us/step - loss: 2758.8133 - mse: 2758.8127 - mae: 28.8849 - val_loss: 2442.7575 - val_mse: 2442.7576 - val_mae: 27.6367\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 487us/step - loss: 2682.5093 - mse: 2682.5100 - mae: 28.2808 - val_loss: 2438.1290 - val_mse: 2438.1294 - val_mae: 27.7265\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 489us/step - loss: 2710.3392 - mse: 2710.3381 - mae: 28.3801 - val_loss: 2430.6511 - val_mse: 2430.6511 - val_mae: 27.4000\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2728.7273 - mse: 2728.7278 - mae: 28.7691 - val_loss: 2431.0130 - val_mse: 2431.0129 - val_mae: 27.3668\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2717.5870 - mse: 2717.5872 - mae: 28.5491 - val_loss: 2443.3790 - val_mse: 2443.3794 - val_mae: 27.2479\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2741.0997 - mse: 2741.0999 - mae: 29.2151 - val_loss: 2432.5435 - val_mse: 2432.5439 - val_mae: 27.4378\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2752.0503 - mse: 2752.0505 - mae: 28.8321 - val_loss: 2435.7131 - val_mse: 2435.7126 - val_mae: 27.3492\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2728.3189 - mse: 2728.3179 - mae: 28.6376 - val_loss: 2435.2079 - val_mse: 2435.2083 - val_mae: 27.1087\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2688.8829 - mse: 2688.8826 - mae: 28.3033 - val_loss: 2438.1506 - val_mse: 2438.1509 - val_mae: 27.3026\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2698.5661 - mse: 2698.5667 - mae: 28.8708 - val_loss: 2442.9641 - val_mse: 2442.9644 - val_mae: 27.3511\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 683us/step - loss: 2662.4586 - mse: 2662.4583 - mae: 28.4870 - val_loss: 2446.5581 - val_mse: 2446.5586 - val_mae: 27.5982\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2679.2846 - mse: 2679.2849 - mae: 28.3413 - val_loss: 2434.8621 - val_mse: 2434.8623 - val_mae: 27.6630\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2684.4520 - mse: 2684.4512 - mae: 28.7679 - val_loss: 2435.1733 - val_mse: 2435.1731 - val_mae: 27.5664\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 13341.2435 - mse: 13341.2441 - mae: 109.9588 - val_loss: 34644.8142 - val_mse: 34644.8164 - val_mae: 132.8308\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 705us/step - loss: 13238.9703 - mse: 13238.9697 - mae: 109.4969 - val_loss: 34444.0337 - val_mse: 34444.0312 - val_mae: 132.0847\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 12961.0049 - mse: 12961.0049 - mae: 108.2195 - val_loss: 33894.0127 - val_mse: 33894.0117 - val_mae: 130.0255\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 12231.5203 - mse: 12231.5225 - mae: 104.8975 - val_loss: 32499.7802 - val_mse: 32499.7793 - val_mae: 124.6601\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 459us/step - loss: 10423.2662 - mse: 10423.2656 - mae: 95.7094 - val_loss: 29271.1120 - val_mse: 29271.1113 - val_mae: 111.2623\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 7149.2544 - mse: 7149.2539 - mae: 75.4006 - val_loss: 23175.6034 - val_mse: 23175.6035 - val_mae: 80.1362\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 527us/step - loss: 3565.7550 - mse: 3565.7549 - mae: 45.5998 - val_loss: 17578.5166 - val_mse: 17578.5156 - val_mae: 40.5454\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 459us/step - loss: 2618.2991 - mse: 2618.2993 - mae: 36.8715 - val_loss: 17367.0902 - val_mse: 17367.0898 - val_mae: 39.6276\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2600.8717 - mse: 2600.8718 - mae: 37.5000 - val_loss: 17628.2540 - val_mse: 17628.2539 - val_mae: 40.6204\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2557.2304 - mse: 2557.2305 - mae: 36.4291 - val_loss: 17689.4334 - val_mse: 17689.4316 - val_mae: 40.8684\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 455us/step - loss: 2927.7345 - mse: 2927.7349 - mae: 38.9407 - val_loss: 17764.0987 - val_mse: 17764.0977 - val_mae: 41.2288\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 460us/step - loss: 2587.6452 - mse: 2587.6453 - mae: 36.6872 - val_loss: 17806.5161 - val_mse: 17806.5156 - val_mae: 41.4044\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 594us/step - loss: 2588.5425 - mse: 2588.5422 - mae: 37.6819 - val_loss: 17587.4255 - val_mse: 17587.4258 - val_mae: 40.1080\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 703us/step - loss: 2528.5317 - mse: 2528.5315 - mae: 35.2246 - val_loss: 17517.1304 - val_mse: 17517.1309 - val_mae: 39.7606\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2380.6858 - mse: 2380.6860 - mae: 35.8748 - val_loss: 17671.2957 - val_mse: 17671.2969 - val_mae: 40.3848\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2458.2365 - mse: 2458.2366 - mae: 35.9017 - val_loss: 17578.1791 - val_mse: 17578.1797 - val_mae: 39.8570\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 664us/step - loss: 2540.7103 - mse: 2540.7102 - mae: 36.3115 - val_loss: 17742.5489 - val_mse: 17742.5508 - val_mae: 40.6372\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2418.6860 - mse: 2418.6860 - mae: 36.3329 - val_loss: 17528.7590 - val_mse: 17528.7598 - val_mae: 39.5398\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 2499.9788 - mse: 2499.9785 - mae: 36.8622 - val_loss: 17639.1484 - val_mse: 17639.1484 - val_mae: 39.9470\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2629.9269 - mse: 2629.9268 - mae: 37.2523 - val_loss: 17715.7838 - val_mse: 17715.7832 - val_mae: 40.2760\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 2490.2082 - mse: 2490.2080 - mae: 35.7704 - val_loss: 17386.6612 - val_mse: 17386.6602 - val_mae: 38.9132\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2541.0155 - mse: 2541.0154 - mae: 36.2470 - val_loss: 17620.2399 - val_mse: 17620.2402 - val_mae: 39.6351\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 728us/step - loss: 2413.6704 - mse: 2413.6702 - mae: 35.2257 - val_loss: 17716.3892 - val_mse: 17716.3887 - val_mae: 40.0537\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2237.1780 - mse: 2237.1777 - mae: 33.7208 - val_loss: 17512.6995 - val_mse: 17512.6992 - val_mae: 39.1041\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2496.9908 - mse: 2496.9907 - mae: 35.0706 - val_loss: 17537.2584 - val_mse: 17537.2578 - val_mae: 39.1352\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2471.6688 - mse: 2471.6692 - mae: 35.3699 - val_loss: 17576.7918 - val_mse: 17576.7930 - val_mae: 39.2109\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 2485.6024 - mse: 2485.6028 - mae: 35.5583 - val_loss: 17596.3209 - val_mse: 17596.3203 - val_mae: 39.2210\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2300.2331 - mse: 2300.2332 - mae: 34.0778 - val_loss: 17630.6916 - val_mse: 17630.6914 - val_mae: 39.3148\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 676us/step - loss: 2296.6378 - mse: 2296.6379 - mae: 34.6780 - val_loss: 17646.0270 - val_mse: 17646.0254 - val_mae: 39.3209\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2163.1681 - mse: 2163.1680 - mae: 33.7026 - val_loss: 17424.7458 - val_mse: 17424.7480 - val_mae: 38.5931\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 524us/step - loss: 2256.5429 - mse: 2256.5430 - mae: 34.1790 - val_loss: 17653.4819 - val_mse: 17653.4824 - val_mae: 39.2547\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2387.4120 - mse: 2387.4121 - mae: 35.2897 - val_loss: 17550.2266 - val_mse: 17550.2266 - val_mae: 38.8350\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 715us/step - loss: 2328.7688 - mse: 2328.7688 - mae: 34.1973 - val_loss: 17655.3631 - val_mse: 17655.3633 - val_mae: 39.1369\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2232.6757 - mse: 2232.6760 - mae: 33.2670 - val_loss: 17580.9824 - val_mse: 17580.9824 - val_mae: 38.8333\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 640us/step - loss: 2251.6441 - mse: 2251.6443 - mae: 33.0934 - val_loss: 17615.8304 - val_mse: 17615.8301 - val_mae: 38.8867\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 519us/step - loss: 2249.1636 - mse: 2249.1638 - mae: 32.7628 - val_loss: 17528.3078 - val_mse: 17528.3086 - val_mae: 38.5957\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 645us/step - loss: 2299.1537 - mse: 2299.1538 - mae: 32.9749 - val_loss: 17582.9162 - val_mse: 17582.9160 - val_mae: 38.6875\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 658us/step - loss: 2041.1206 - mse: 2041.1207 - mae: 32.3623 - val_loss: 17470.0513 - val_mse: 17470.0527 - val_mae: 38.3744\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 713us/step - loss: 2337.2830 - mse: 2337.2830 - mae: 34.2139 - val_loss: 17468.2125 - val_mse: 17468.2148 - val_mae: 38.3392\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2380.9246 - mse: 2380.9246 - mae: 33.6297 - val_loss: 17735.8648 - val_mse: 17735.8633 - val_mae: 39.1535\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 730us/step - loss: 2083.8075 - mse: 2083.8076 - mae: 32.8709 - val_loss: 17562.5416 - val_mse: 17562.5430 - val_mae: 38.4785\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 528us/step - loss: 2096.6930 - mse: 2096.6931 - mae: 32.1243 - val_loss: 17470.2204 - val_mse: 17470.2207 - val_mae: 38.2363\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 2180.6836 - mse: 2180.6836 - mae: 33.0797 - val_loss: 17548.3851 - val_mse: 17548.3848 - val_mae: 38.3680\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 453us/step - loss: 2104.9747 - mse: 2104.9751 - mae: 32.9323 - val_loss: 17578.2374 - val_mse: 17578.2363 - val_mae: 38.3903\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 436us/step - loss: 2284.1844 - mse: 2284.1843 - mae: 32.7953 - val_loss: 17669.7146 - val_mse: 17669.7148 - val_mae: 38.6198\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 434us/step - loss: 2166.3628 - mse: 2166.3628 - mae: 33.2790 - val_loss: 17510.1622 - val_mse: 17510.1621 - val_mae: 38.1772\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 490us/step - loss: 2156.7313 - mse: 2156.7312 - mae: 31.7552 - val_loss: 17460.6474 - val_mse: 17460.6484 - val_mae: 38.0439\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2175.9109 - mse: 2175.9106 - mae: 32.9653 - val_loss: 17543.4589 - val_mse: 17543.4570 - val_mae: 38.1813\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 1995.6073 - mse: 1995.6072 - mae: 31.3236 - val_loss: 17536.0820 - val_mse: 17536.0801 - val_mae: 38.1341\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2303.6404 - mse: 2303.6404 - mae: 33.0056 - val_loss: 17685.3227 - val_mse: 17685.3223 - val_mae: 38.4738\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 1958.7940 - mse: 1958.7938 - mae: 31.0632 - val_loss: 17572.4818 - val_mse: 17572.4824 - val_mae: 38.1445\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2129.7509 - mse: 2129.7507 - mae: 32.1195 - val_loss: 17575.6691 - val_mse: 17575.6680 - val_mae: 38.1216\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 677us/step - loss: 2026.9845 - mse: 2026.9845 - mae: 31.6649 - val_loss: 17607.0949 - val_mse: 17607.0938 - val_mae: 38.1538\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 840us/step - loss: 2009.1232 - mse: 2009.1233 - mae: 31.4335 - val_loss: 17504.4018 - val_mse: 17504.4023 - val_mae: 37.9018\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 702us/step - loss: 2132.6071 - mse: 2132.6072 - mae: 31.4920 - val_loss: 17540.4463 - val_mse: 17540.4473 - val_mae: 37.9401\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 1998.9027 - mse: 1998.9026 - mae: 31.6988 - val_loss: 17635.4779 - val_mse: 17635.4785 - val_mae: 38.1090\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2205.8665 - mse: 2205.8665 - mae: 32.4623 - val_loss: 17371.6615 - val_mse: 17371.6602 - val_mae: 37.5255\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 1842.9325 - mse: 1842.9326 - mae: 31.2252 - val_loss: 17502.5194 - val_mse: 17502.5195 - val_mae: 37.7745\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2138.4360 - mse: 2138.4360 - mae: 31.6125 - val_loss: 17672.9260 - val_mse: 17672.9238 - val_mae: 38.0945\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2104.4684 - mse: 2104.4685 - mae: 31.6886 - val_loss: 17615.2957 - val_mse: 17615.2969 - val_mae: 37.9468\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2174.0869 - mse: 2174.0869 - mae: 33.1191 - val_loss: 17629.3226 - val_mse: 17629.3242 - val_mae: 37.9520\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2073.5267 - mse: 2073.5266 - mae: 30.5108 - val_loss: 17677.3861 - val_mse: 17677.3887 - val_mae: 38.0189\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 516us/step - loss: 2009.7811 - mse: 2009.7811 - mae: 32.0699 - val_loss: 17690.0160 - val_mse: 17690.0137 - val_mae: 38.0294\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 666us/step - loss: 1932.2162 - mse: 1932.2161 - mae: 29.6920 - val_loss: 17574.8689 - val_mse: 17574.8672 - val_mae: 37.7526\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 523us/step - loss: 1895.1619 - mse: 1895.1619 - mae: 30.8335 - val_loss: 17502.9128 - val_mse: 17502.9141 - val_mae: 37.5797\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2090.9999 - mse: 2090.9998 - mae: 30.8142 - val_loss: 17636.5460 - val_mse: 17636.5469 - val_mae: 37.8379\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 1930.0695 - mse: 1930.0697 - mae: 30.0011 - val_loss: 17443.4101 - val_mse: 17443.4102 - val_mae: 37.4053\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 654us/step - loss: 2038.1497 - mse: 2038.1495 - mae: 32.0587 - val_loss: 17570.8735 - val_mse: 17570.8730 - val_mae: 37.6431\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 1920.0516 - mse: 1920.0514 - mae: 30.1316 - val_loss: 17656.2096 - val_mse: 17656.2070 - val_mae: 37.7951\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 2147.9675 - mse: 2147.9673 - mae: 32.4031 - val_loss: 17728.6928 - val_mse: 17728.6934 - val_mae: 37.9592\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 1951.5462 - mse: 1951.5463 - mae: 30.7739 - val_loss: 17675.0375 - val_mse: 17675.0352 - val_mae: 37.7966\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2039.0115 - mse: 2039.0116 - mae: 32.0670 - val_loss: 17778.5694 - val_mse: 17778.5703 - val_mae: 38.0870\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 629us/step - loss: 1848.4506 - mse: 1848.4508 - mae: 29.8666 - val_loss: 17553.2160 - val_mse: 17553.2168 - val_mae: 37.5026\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 1864.4279 - mse: 1864.4280 - mae: 29.5560 - val_loss: 17601.7986 - val_mse: 17601.7988 - val_mae: 37.5793\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 722us/step - loss: 1972.5344 - mse: 1972.5345 - mae: 31.1027 - val_loss: 17732.5033 - val_mse: 17732.5020 - val_mae: 37.8648\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 1913.2518 - mse: 1913.2520 - mae: 29.5541 - val_loss: 17631.4438 - val_mse: 17631.4434 - val_mae: 37.6073\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 1922.7344 - mse: 1922.7345 - mae: 30.1774 - val_loss: 17529.9110 - val_mse: 17529.9102 - val_mae: 37.3607\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 1993.0253 - mse: 1993.0253 - mae: 30.1667 - val_loss: 17766.0398 - val_mse: 17766.0371 - val_mae: 37.9087\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2083.3108 - mse: 2083.3108 - mae: 30.3852 - val_loss: 17737.1465 - val_mse: 17737.1465 - val_mae: 37.7767\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 1927.3287 - mse: 1927.3286 - mae: 29.5377 - val_loss: 17563.3224 - val_mse: 17563.3223 - val_mae: 37.3646\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 643us/step - loss: 4052.6157 - mse: 4052.6155 - mae: 34.6771 - val_loss: 2278.3994 - val_mse: 2278.3992 - val_mae: 30.8523\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4391.7959 - mse: 4391.7959 - mae: 36.1538 - val_loss: 2495.2673 - val_mse: 2495.2676 - val_mae: 31.8871\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4338.0024 - mse: 4338.0024 - mae: 34.6662 - val_loss: 2495.5194 - val_mse: 2495.5195 - val_mae: 31.8900\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4176.9602 - mse: 4176.9604 - mae: 33.8887 - val_loss: 2394.9436 - val_mse: 2394.9436 - val_mae: 31.3776\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4092.9328 - mse: 4092.9329 - mae: 34.5326 - val_loss: 2366.4460 - val_mse: 2366.4460 - val_mae: 31.2449\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4197.2091 - mse: 4197.2095 - mae: 36.1071 - val_loss: 2433.9351 - val_mse: 2433.9351 - val_mae: 31.5720\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4292.6942 - mse: 4292.6938 - mae: 35.9667 - val_loss: 2391.9600 - val_mse: 2391.9602 - val_mae: 31.3675\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4156.2877 - mse: 4156.2876 - mae: 34.1273 - val_loss: 2405.9631 - val_mse: 2405.9626 - val_mae: 31.4319\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4312.7441 - mse: 4312.7437 - mae: 35.4921 - val_loss: 2448.3660 - val_mse: 2448.3660 - val_mae: 31.6367\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 721us/step - loss: 4135.8174 - mse: 4135.8179 - mae: 34.4214 - val_loss: 2395.6690 - val_mse: 2395.6689 - val_mae: 31.3820\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4173.3796 - mse: 4173.3789 - mae: 34.4381 - val_loss: 2362.8115 - val_mse: 2362.8115 - val_mae: 31.2352\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 476us/step - loss: 4115.0623 - mse: 4115.0630 - mae: 33.6873 - val_loss: 2317.9139 - val_mse: 2317.9141 - val_mae: 31.0343\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 4275.5766 - mse: 4275.5762 - mae: 35.0300 - val_loss: 2492.4350 - val_mse: 2492.4351 - val_mae: 31.8826\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 517us/step - loss: 4142.9154 - mse: 4142.9150 - mae: 34.0215 - val_loss: 2406.1383 - val_mse: 2406.1379 - val_mae: 31.4454\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4123.4219 - mse: 4123.4219 - mae: 33.6844 - val_loss: 2393.2087 - val_mse: 2393.2085 - val_mae: 31.3845\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4219.5044 - mse: 4219.5039 - mae: 35.2944 - val_loss: 2370.3105 - val_mse: 2370.3103 - val_mae: 31.2880\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4257.8635 - mse: 4257.8638 - mae: 35.1603 - val_loss: 2488.0441 - val_mse: 2488.0439 - val_mae: 31.8713\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 660us/step - loss: 4217.8091 - mse: 4217.8091 - mae: 34.3825 - val_loss: 2414.6875 - val_mse: 2414.6875 - val_mae: 31.5109\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 3974.7917 - mse: 3974.7913 - mae: 33.8869 - val_loss: 2363.0287 - val_mse: 2363.0288 - val_mae: 31.2789\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4162.3473 - mse: 4162.3472 - mae: 35.3397 - val_loss: 2396.5619 - val_mse: 2396.5620 - val_mae: 31.4271\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4239.1467 - mse: 4239.1470 - mae: 34.9160 - val_loss: 2465.6974 - val_mse: 2465.6973 - val_mae: 31.7600\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4099.3831 - mse: 4099.3828 - mae: 34.3230 - val_loss: 2454.6960 - val_mse: 2454.6960 - val_mae: 31.7075\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 4073.4517 - mse: 4073.4519 - mae: 33.4743 - val_loss: 2430.1741 - val_mse: 2430.1741 - val_mae: 31.5932\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4218.2121 - mse: 4218.2114 - mae: 34.2086 - val_loss: 2407.7221 - val_mse: 2407.7219 - val_mae: 31.4910\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4370.0406 - mse: 4370.0405 - mae: 35.1846 - val_loss: 2449.8352 - val_mse: 2449.8350 - val_mae: 31.6968\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4201.6625 - mse: 4201.6621 - mae: 34.1221 - val_loss: 2449.8855 - val_mse: 2449.8857 - val_mae: 31.6984\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 3919.8325 - mse: 3919.8330 - mae: 33.4323 - val_loss: 2389.2758 - val_mse: 2389.2756 - val_mae: 31.4182\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 4157.2279 - mse: 4157.2280 - mae: 34.9858 - val_loss: 2427.6612 - val_mse: 2427.6611 - val_mae: 31.5963\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 737us/step - loss: 4161.5356 - mse: 4161.5356 - mae: 33.9826 - val_loss: 2426.9416 - val_mse: 2426.9417 - val_mae: 31.5960\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4184.1891 - mse: 4184.1890 - mae: 34.6522 - val_loss: 2409.3514 - val_mse: 2409.3513 - val_mae: 31.5094\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 660us/step - loss: 3919.4341 - mse: 3919.4338 - mae: 33.3177 - val_loss: 2366.9509 - val_mse: 2366.9509 - val_mae: 31.3227\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 3949.6031 - mse: 3949.6030 - mae: 34.0494 - val_loss: 2443.0841 - val_mse: 2443.0842 - val_mae: 31.6667\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4033.8790 - mse: 4033.8787 - mae: 33.8686 - val_loss: 2463.5380 - val_mse: 2463.5381 - val_mae: 31.7590\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4092.0020 - mse: 4092.0022 - mae: 33.7304 - val_loss: 2399.1537 - val_mse: 2399.1536 - val_mae: 31.4578\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4222.4241 - mse: 4222.4248 - mae: 34.4904 - val_loss: 2424.7202 - val_mse: 2424.7202 - val_mae: 31.5789\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4011.7964 - mse: 4011.7961 - mae: 33.3719 - val_loss: 2437.0922 - val_mse: 2437.0920 - val_mae: 31.6408\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4109.1012 - mse: 4109.1011 - mae: 33.5039 - val_loss: 2407.4911 - val_mse: 2407.4910 - val_mae: 31.5043\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4211.6179 - mse: 4211.6177 - mae: 33.6302 - val_loss: 2427.4097 - val_mse: 2427.4099 - val_mae: 31.5981\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4038.0182 - mse: 4038.0181 - mae: 34.0360 - val_loss: 2400.3585 - val_mse: 2400.3582 - val_mae: 31.4754\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 767us/step - loss: 3939.8496 - mse: 3939.8496 - mae: 32.5936 - val_loss: 2328.2844 - val_mse: 2328.2842 - val_mae: 31.1582\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4033.8934 - mse: 4033.8931 - mae: 34.4652 - val_loss: 2354.9705 - val_mse: 2354.9705 - val_mae: 31.2723\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 3812.7447 - mse: 3812.7446 - mae: 32.9138 - val_loss: 2355.6096 - val_mse: 2355.6099 - val_mae: 31.2730\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4005.9459 - mse: 4005.9465 - mae: 33.6975 - val_loss: 2426.5918 - val_mse: 2426.5918 - val_mae: 31.5885\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4101.3214 - mse: 4101.3213 - mae: 33.5431 - val_loss: 2396.4548 - val_mse: 2396.4548 - val_mae: 31.4497\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4069.6878 - mse: 4069.6875 - mae: 34.3336 - val_loss: 2389.4226 - val_mse: 2389.4229 - val_mae: 31.4195\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 3983.5144 - mse: 3983.5146 - mae: 32.4933 - val_loss: 2420.6057 - val_mse: 2420.6057 - val_mae: 31.5602\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 3972.9400 - mse: 3972.9399 - mae: 32.7775 - val_loss: 2403.6778 - val_mse: 2403.6777 - val_mae: 31.4795\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 4087.1099 - mse: 4087.1101 - mae: 34.0325 - val_loss: 2379.3744 - val_mse: 2379.3743 - val_mae: 31.3719\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4032.1810 - mse: 4032.1812 - mae: 34.2508 - val_loss: 2410.3220 - val_mse: 2410.3223 - val_mae: 31.5096\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 3943.8362 - mse: 3943.8357 - mae: 33.8931 - val_loss: 2420.2310 - val_mse: 2420.2310 - val_mae: 31.5526\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 521us/step - loss: 4098.4302 - mse: 4098.4297 - mae: 33.8433 - val_loss: 2415.0749 - val_mse: 2415.0747 - val_mae: 31.5276\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 453us/step - loss: 4035.6321 - mse: 4035.6323 - mae: 34.1160 - val_loss: 2388.4150 - val_mse: 2388.4150 - val_mae: 31.4077\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 454us/step - loss: 4050.1536 - mse: 4050.1536 - mae: 32.2971 - val_loss: 2349.7812 - val_mse: 2349.7810 - val_mae: 31.2365\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4079.0591 - mse: 4079.0588 - mae: 32.7771 - val_loss: 2379.5331 - val_mse: 2379.5332 - val_mae: 31.3641\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4190.5591 - mse: 4190.5586 - mae: 34.2975 - val_loss: 2434.7804 - val_mse: 2434.7805 - val_mae: 31.6189\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 3950.9311 - mse: 3950.9316 - mae: 32.4575 - val_loss: 2343.1752 - val_mse: 2343.1755 - val_mae: 31.2104\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 3936.4792 - mse: 3936.4788 - mae: 33.4886 - val_loss: 2395.2084 - val_mse: 2395.2083 - val_mae: 31.4302\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4101.5997 - mse: 4101.5996 - mae: 34.5609 - val_loss: 2397.9645 - val_mse: 2397.9644 - val_mae: 31.4431\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 680us/step - loss: 3992.9636 - mse: 3992.9636 - mae: 32.9888 - val_loss: 2422.1262 - val_mse: 2422.1262 - val_mae: 31.5615\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 3995.4791 - mse: 3995.4785 - mae: 32.4468 - val_loss: 2346.9005 - val_mse: 2346.9001 - val_mae: 31.2262\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4069.8814 - mse: 4069.8811 - mae: 33.8373 - val_loss: 2383.3381 - val_mse: 2383.3381 - val_mae: 31.3838\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4012.1859 - mse: 4012.1860 - mae: 34.0731 - val_loss: 2401.0918 - val_mse: 2401.0918 - val_mae: 31.4664\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4061.0285 - mse: 4061.0288 - mae: 33.5806 - val_loss: 2442.8105 - val_mse: 2442.8105 - val_mae: 31.6666\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4089.2468 - mse: 4089.2468 - mae: 33.5803 - val_loss: 2423.8689 - val_mse: 2423.8689 - val_mae: 31.5826\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 3919.1298 - mse: 3919.1299 - mae: 34.1691 - val_loss: 2412.6237 - val_mse: 2412.6238 - val_mae: 31.5373\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 3894.0384 - mse: 3894.0393 - mae: 31.8033 - val_loss: 2370.2116 - val_mse: 2370.2117 - val_mae: 31.3562\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4008.8195 - mse: 4008.8191 - mae: 33.2631 - val_loss: 2393.8882 - val_mse: 2393.8879 - val_mae: 31.4604\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4118.7110 - mse: 4118.7104 - mae: 33.5220 - val_loss: 2426.2321 - val_mse: 2426.2319 - val_mae: 31.6065\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 3846.8989 - mse: 3846.8992 - mae: 32.9311 - val_loss: 2358.8980 - val_mse: 2358.8979 - val_mae: 31.3157\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 3982.1867 - mse: 3982.1863 - mae: 33.1938 - val_loss: 2359.4426 - val_mse: 2359.4424 - val_mae: 31.3190\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4030.6035 - mse: 4030.6042 - mae: 33.2497 - val_loss: 2365.3823 - val_mse: 2365.3823 - val_mae: 31.3385\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 3950.5315 - mse: 3950.5315 - mae: 33.1244 - val_loss: 2407.8866 - val_mse: 2407.8867 - val_mae: 31.5179\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 4012.1368 - mse: 4012.1370 - mae: 33.2898 - val_loss: 2439.8913 - val_mse: 2439.8914 - val_mae: 31.6618\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4089.4754 - mse: 4089.4751 - mae: 32.7213 - val_loss: 2395.2590 - val_mse: 2395.2590 - val_mae: 31.4613\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 662us/step - loss: 3986.5412 - mse: 3986.5400 - mae: 32.8362 - val_loss: 2348.1800 - val_mse: 2348.1799 - val_mae: 31.2625\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 454us/step - loss: 3865.1300 - mse: 3865.1304 - mae: 33.1537 - val_loss: 2372.4729 - val_mse: 2372.4729 - val_mae: 31.3647\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 484us/step - loss: 3974.4365 - mse: 3974.4363 - mae: 33.0799 - val_loss: 2435.5067 - val_mse: 2435.5071 - val_mae: 31.6422\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4079.4683 - mse: 4079.4680 - mae: 33.0451 - val_loss: 2334.6754 - val_mse: 2334.6755 - val_mae: 31.2036\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 3995.0505 - mse: 3995.0503 - mae: 33.2855 - val_loss: 2353.9841 - val_mse: 2353.9839 - val_mae: 31.2955\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 507us/step - loss: 4108.7804 - mse: 4108.7803 - mae: 33.1125 - val_loss: 2426.4963 - val_mse: 2426.4966 - val_mae: 31.6113\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3379.8186 - mse: 3379.8188 - mae: 33.3716 - val_loss: 1483.9677 - val_mse: 1483.9675 - val_mae: 25.2171\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3392.4921 - mse: 3392.4919 - mae: 32.7937 - val_loss: 1484.8986 - val_mse: 1484.8986 - val_mae: 25.1531\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3498.6492 - mse: 3498.6499 - mae: 33.5831 - val_loss: 1486.1351 - val_mse: 1486.1353 - val_mae: 25.0737\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 703us/step - loss: 3416.0952 - mse: 3416.0950 - mae: 33.1563 - val_loss: 1493.2818 - val_mse: 1493.2819 - val_mae: 24.7681\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3287.1308 - mse: 3287.1304 - mae: 32.7298 - val_loss: 1482.9639 - val_mse: 1482.9639 - val_mae: 25.2135\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3405.7247 - mse: 3405.7244 - mae: 33.3048 - val_loss: 1481.2208 - val_mse: 1481.2207 - val_mae: 25.3319\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3480.1767 - mse: 3480.1763 - mae: 32.9921 - val_loss: 1484.5463 - val_mse: 1484.5461 - val_mae: 25.1228\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3503.5868 - mse: 3503.5867 - mae: 33.0343 - val_loss: 1480.3975 - val_mse: 1480.3973 - val_mae: 25.3899\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3380.4070 - mse: 3380.4070 - mae: 33.0593 - val_loss: 1481.2303 - val_mse: 1481.2301 - val_mae: 25.3077\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3382.0286 - mse: 3382.0288 - mae: 32.2043 - val_loss: 1481.1504 - val_mse: 1481.1504 - val_mae: 25.3121\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3420.4106 - mse: 3420.4104 - mae: 33.1293 - val_loss: 1483.6329 - val_mse: 1483.6331 - val_mae: 25.1570\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3308.7393 - mse: 3308.7390 - mae: 31.9424 - val_loss: 1478.4874 - val_mse: 1478.4874 - val_mae: 25.5914\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3431.5013 - mse: 3431.5010 - mae: 32.4741 - val_loss: 1482.6444 - val_mse: 1482.6444 - val_mae: 25.2125\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3446.1404 - mse: 3446.1401 - mae: 32.5041 - val_loss: 1478.3902 - val_mse: 1478.3903 - val_mae: 25.5563\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3351.2834 - mse: 3351.2827 - mae: 32.1810 - val_loss: 1477.6778 - val_mse: 1477.6779 - val_mae: 25.6306\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3293.5508 - mse: 3293.5505 - mae: 32.4277 - val_loss: 1476.6999 - val_mse: 1476.6998 - val_mae: 25.8275\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3419.1360 - mse: 3419.1365 - mae: 33.0732 - val_loss: 1483.7415 - val_mse: 1483.7415 - val_mae: 25.0857\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 533us/step - loss: 3396.1622 - mse: 3396.1619 - mae: 32.1071 - val_loss: 1476.7836 - val_mse: 1476.7834 - val_mae: 25.8452\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3384.2622 - mse: 3384.2620 - mae: 33.1940 - val_loss: 1477.5724 - val_mse: 1477.5724 - val_mae: 26.3271\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3420.5074 - mse: 3420.5073 - mae: 32.7400 - val_loss: 1483.6209 - val_mse: 1483.6208 - val_mae: 25.0958\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3234.1983 - mse: 3234.1992 - mae: 31.7127 - val_loss: 1479.2388 - val_mse: 1479.2388 - val_mae: 25.3937\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3437.6871 - mse: 3437.6877 - mae: 33.4920 - val_loss: 1479.1752 - val_mse: 1479.1753 - val_mae: 25.3922\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3270.0615 - mse: 3270.0618 - mae: 31.7468 - val_loss: 1481.5627 - val_mse: 1481.5626 - val_mae: 25.2227\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3426.7589 - mse: 3426.7585 - mae: 32.9552 - val_loss: 1479.3041 - val_mse: 1479.3040 - val_mae: 25.4084\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 692us/step - loss: 3392.6413 - mse: 3392.6418 - mae: 32.4120 - val_loss: 1477.1643 - val_mse: 1477.1641 - val_mae: 26.1941\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3379.0741 - mse: 3379.0742 - mae: 32.2032 - val_loss: 1476.8860 - val_mse: 1476.8862 - val_mae: 26.0526\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 686us/step - loss: 3391.2483 - mse: 3391.2495 - mae: 32.6127 - val_loss: 1483.6246 - val_mse: 1483.6245 - val_mae: 25.1147\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3298.0286 - mse: 3298.0288 - mae: 31.6400 - val_loss: 1476.7510 - val_mse: 1476.7510 - val_mae: 25.9698\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3355.0917 - mse: 3355.0920 - mae: 32.1784 - val_loss: 1477.0196 - val_mse: 1477.0195 - val_mae: 26.1068\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 633us/step - loss: 3400.4270 - mse: 3400.4265 - mae: 32.6577 - val_loss: 1480.4664 - val_mse: 1480.4664 - val_mae: 25.3298\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3316.8069 - mse: 3316.8083 - mae: 32.1568 - val_loss: 1478.1339 - val_mse: 1478.1340 - val_mae: 25.5298\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3164.0846 - mse: 3164.0842 - mae: 31.9454 - val_loss: 1478.1398 - val_mse: 1478.1399 - val_mae: 25.5400\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3335.3253 - mse: 3335.3252 - mae: 32.2368 - val_loss: 1482.6915 - val_mse: 1482.6915 - val_mae: 25.1680\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3264.5488 - mse: 3264.5491 - mae: 31.7682 - val_loss: 1480.9754 - val_mse: 1480.9756 - val_mae: 25.2834\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3335.6852 - mse: 3335.6853 - mae: 32.8250 - val_loss: 1479.4086 - val_mse: 1479.4088 - val_mae: 25.3914\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3304.5023 - mse: 3304.5012 - mae: 32.3107 - val_loss: 1476.8880 - val_mse: 1476.8881 - val_mae: 25.8131\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3232.9251 - mse: 3232.9255 - mae: 32.1321 - val_loss: 1476.7419 - val_mse: 1476.7418 - val_mae: 26.0600\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 544us/step - loss: 3453.6353 - mse: 3453.6355 - mae: 33.0136 - val_loss: 1495.0126 - val_mse: 1495.0127 - val_mae: 24.6518\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3284.9901 - mse: 3284.9912 - mae: 31.8297 - val_loss: 1477.1299 - val_mse: 1477.1299 - val_mae: 25.7123\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 663us/step - loss: 3443.9689 - mse: 3443.9692 - mae: 32.7614 - val_loss: 1476.8215 - val_mse: 1476.8215 - val_mae: 25.8823\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3367.6751 - mse: 3367.6748 - mae: 32.2337 - val_loss: 1476.9216 - val_mse: 1476.9218 - val_mae: 25.8098\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3358.8822 - mse: 3358.8831 - mae: 32.2706 - val_loss: 1477.0106 - val_mse: 1477.0105 - val_mae: 25.7971\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3325.8631 - mse: 3325.8628 - mae: 32.0410 - val_loss: 1482.5393 - val_mse: 1482.5393 - val_mae: 25.1670\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 474us/step - loss: 3179.3450 - mse: 3179.3445 - mae: 30.8910 - val_loss: 1477.1591 - val_mse: 1477.1593 - val_mae: 25.7770\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 502us/step - loss: 3314.5950 - mse: 3314.5952 - mae: 32.2736 - val_loss: 1479.5755 - val_mse: 1479.5753 - val_mae: 25.4010\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3345.3180 - mse: 3345.3176 - mae: 32.1377 - val_loss: 1477.1476 - val_mse: 1477.1476 - val_mae: 26.0783\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3343.3374 - mse: 3343.3364 - mae: 31.9261 - val_loss: 1480.8853 - val_mse: 1480.8854 - val_mae: 25.3098\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3210.4864 - mse: 3210.4858 - mae: 31.9546 - val_loss: 1477.5107 - val_mse: 1477.5107 - val_mae: 25.7338\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3378.2693 - mse: 3378.2700 - mae: 32.2138 - val_loss: 1482.2247 - val_mse: 1482.2247 - val_mae: 25.1896\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3370.9517 - mse: 3370.9512 - mae: 32.7154 - val_loss: 1477.0464 - val_mse: 1477.0465 - val_mae: 25.7972\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3354.8376 - mse: 3354.8379 - mae: 32.2539 - val_loss: 1478.2800 - val_mse: 1478.2800 - val_mae: 25.5327\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 683us/step - loss: 3338.4611 - mse: 3338.4602 - mae: 32.3862 - val_loss: 1477.8466 - val_mse: 1477.8466 - val_mae: 25.5751\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3287.5511 - mse: 3287.5500 - mae: 32.0051 - val_loss: 1480.3574 - val_mse: 1480.3574 - val_mae: 25.3121\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3258.9876 - mse: 3258.9883 - mae: 31.1739 - val_loss: 1477.7480 - val_mse: 1477.7482 - val_mae: 25.5881\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3234.3764 - mse: 3234.3762 - mae: 31.9421 - val_loss: 1478.7207 - val_mse: 1478.7207 - val_mae: 25.4609\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3128.4710 - mse: 3128.4717 - mae: 31.2264 - val_loss: 1477.3778 - val_mse: 1477.3777 - val_mae: 25.6302\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3383.3186 - mse: 3383.3181 - mae: 32.6776 - val_loss: 1477.5328 - val_mse: 1477.5330 - val_mae: 25.5901\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3314.7204 - mse: 3314.7207 - mae: 31.8343 - val_loss: 1478.5135 - val_mse: 1478.5135 - val_mae: 25.4580\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3295.2322 - mse: 3295.2322 - mae: 32.8834 - val_loss: 1481.0805 - val_mse: 1481.0803 - val_mae: 25.2174\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3310.6996 - mse: 3310.6997 - mae: 32.1224 - val_loss: 1480.1656 - val_mse: 1480.1656 - val_mae: 25.2926\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 671us/step - loss: 3198.2003 - mse: 3198.2000 - mae: 30.8483 - val_loss: 1478.5324 - val_mse: 1478.5322 - val_mae: 25.4311\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3315.1791 - mse: 3315.1782 - mae: 32.1431 - val_loss: 1479.4176 - val_mse: 1479.4175 - val_mae: 25.3387\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3307.7837 - mse: 3307.7832 - mae: 32.0820 - val_loss: 1477.5424 - val_mse: 1477.5425 - val_mae: 25.5661\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3317.2173 - mse: 3317.2178 - mae: 31.3517 - val_loss: 1476.8670 - val_mse: 1476.8669 - val_mae: 25.6851\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 676us/step - loss: 3305.4387 - mse: 3305.4385 - mae: 31.7577 - val_loss: 1476.1265 - val_mse: 1476.1263 - val_mae: 25.9021\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3219.6704 - mse: 3219.6702 - mae: 32.0305 - val_loss: 1477.2540 - val_mse: 1477.2538 - val_mae: 25.5575\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3344.9338 - mse: 3344.9341 - mae: 32.2056 - val_loss: 1478.4400 - val_mse: 1478.4401 - val_mae: 25.4133\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3224.7790 - mse: 3224.7788 - mae: 31.6266 - val_loss: 1476.5616 - val_mse: 1476.5615 - val_mae: 25.6984\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3204.6113 - mse: 3204.6111 - mae: 31.4993 - val_loss: 1476.2127 - val_mse: 1476.2128 - val_mae: 26.1212\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3342.7833 - mse: 3342.7837 - mae: 32.2444 - val_loss: 1476.1252 - val_mse: 1476.1251 - val_mae: 25.7930\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3379.3170 - mse: 3379.3174 - mae: 32.4247 - val_loss: 1476.9816 - val_mse: 1476.9816 - val_mae: 25.6058\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3262.1898 - mse: 3262.1897 - mae: 31.5784 - val_loss: 1480.7772 - val_mse: 1480.7772 - val_mae: 25.2296\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3221.6032 - mse: 3221.6040 - mae: 31.1215 - val_loss: 1475.9692 - val_mse: 1475.9691 - val_mae: 25.8768\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3333.0982 - mse: 3333.0974 - mae: 32.1470 - val_loss: 1482.6954 - val_mse: 1482.6953 - val_mae: 25.0871\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3388.1815 - mse: 3388.1819 - mae: 31.7681 - val_loss: 1477.5870 - val_mse: 1477.5869 - val_mae: 25.4884\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 689us/step - loss: 3236.2684 - mse: 3236.2683 - mae: 31.6992 - val_loss: 1476.3668 - val_mse: 1476.3669 - val_mae: 26.2009\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 694us/step - loss: 3228.7196 - mse: 3228.7190 - mae: 31.5065 - val_loss: 1476.3246 - val_mse: 1476.3247 - val_mae: 25.7600\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3211.9598 - mse: 3211.9612 - mae: 31.3411 - val_loss: 1475.8406 - val_mse: 1475.8407 - val_mae: 25.9106\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3290.0279 - mse: 3290.0273 - mae: 32.1051 - val_loss: 1478.7461 - val_mse: 1478.7460 - val_mae: 25.3467\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 473us/step - loss: 3264.3176 - mse: 3264.3176 - mae: 32.2865 - val_loss: 1475.7761 - val_mse: 1475.7761 - val_mae: 25.8352\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 551us/step - loss: 3015.1047 - mse: 3015.1045 - mae: 31.9381 - val_loss: 1101.1222 - val_mse: 1101.1222 - val_mae: 23.7995\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2839.0153 - mse: 2839.0159 - mae: 30.5701 - val_loss: 1102.4083 - val_mse: 1102.4082 - val_mae: 23.7254\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2976.6536 - mse: 2976.6538 - mae: 31.3329 - val_loss: 1099.9774 - val_mse: 1099.9773 - val_mae: 23.7815\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 673us/step - loss: 2931.4304 - mse: 2931.4304 - mae: 30.8173 - val_loss: 1096.4221 - val_mse: 1096.4221 - val_mae: 23.8806\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2883.4392 - mse: 2883.4397 - mae: 31.0808 - val_loss: 1093.6037 - val_mse: 1093.6035 - val_mae: 24.0183\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2927.4868 - mse: 2927.4861 - mae: 31.4395 - val_loss: 1100.3760 - val_mse: 1100.3759 - val_mae: 23.6897\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 548us/step - loss: 2824.5658 - mse: 2824.5659 - mae: 31.5090 - val_loss: 1093.2914 - val_mse: 1093.2914 - val_mae: 24.0348\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2971.3760 - mse: 2971.3757 - mae: 31.4611 - val_loss: 1092.5439 - val_mse: 1092.5439 - val_mae: 24.0791\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2936.1989 - mse: 2936.1980 - mae: 31.0592 - val_loss: 1098.3545 - val_mse: 1098.3545 - val_mae: 23.7402\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2910.6472 - mse: 2910.6477 - mae: 30.8429 - val_loss: 1095.6274 - val_mse: 1095.6273 - val_mae: 23.8385\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2985.6426 - mse: 2985.6426 - mae: 31.0723 - val_loss: 1090.7529 - val_mse: 1090.7531 - val_mae: 24.1842\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 501us/step - loss: 2952.9592 - mse: 2952.9587 - mae: 31.5228 - val_loss: 1096.3359 - val_mse: 1096.3359 - val_mae: 23.7829\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2954.9240 - mse: 2954.9243 - mae: 31.3275 - val_loss: 1093.3307 - val_mse: 1093.3308 - val_mae: 23.8811\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2805.5301 - mse: 2805.5308 - mae: 30.9418 - val_loss: 1090.4879 - val_mse: 1090.4878 - val_mae: 24.0593\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2942.3262 - mse: 2942.3271 - mae: 31.2605 - val_loss: 1092.5216 - val_mse: 1092.5219 - val_mae: 23.9028\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2839.8678 - mse: 2839.8677 - mae: 30.5110 - val_loss: 1092.1245 - val_mse: 1092.1245 - val_mae: 23.9368\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2981.5981 - mse: 2981.5989 - mae: 31.7293 - val_loss: 1097.2398 - val_mse: 1097.2399 - val_mae: 23.6941\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 529us/step - loss: 2865.1477 - mse: 2865.1479 - mae: 30.9284 - val_loss: 1097.7245 - val_mse: 1097.7246 - val_mae: 23.6889\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2863.5488 - mse: 2863.5498 - mae: 31.2220 - val_loss: 1095.2564 - val_mse: 1095.2563 - val_mae: 23.7588\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2858.9140 - mse: 2858.9143 - mae: 30.9287 - val_loss: 1094.9633 - val_mse: 1094.9633 - val_mae: 23.7645\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2860.2341 - mse: 2860.2334 - mae: 31.2997 - val_loss: 1095.5495 - val_mse: 1095.5494 - val_mae: 23.7425\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2899.6515 - mse: 2899.6516 - mae: 30.9575 - val_loss: 1090.6848 - val_mse: 1090.6849 - val_mae: 24.0079\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2883.9559 - mse: 2883.9548 - mae: 30.9349 - val_loss: 1088.3407 - val_mse: 1088.3407 - val_mae: 24.2422\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2879.2496 - mse: 2879.2485 - mae: 31.0122 - val_loss: 1096.5158 - val_mse: 1096.5159 - val_mae: 23.6943\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2794.4078 - mse: 2794.4082 - mae: 30.4580 - val_loss: 1089.2279 - val_mse: 1089.2278 - val_mae: 24.1098\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 651us/step - loss: 2954.0027 - mse: 2954.0022 - mae: 31.7941 - val_loss: 1092.0173 - val_mse: 1092.0172 - val_mae: 23.8811\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2832.6909 - mse: 2832.6909 - mae: 30.4790 - val_loss: 1096.7738 - val_mse: 1096.7737 - val_mae: 23.6816\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2823.6763 - mse: 2823.6768 - mae: 30.5478 - val_loss: 1091.3839 - val_mse: 1091.3839 - val_mae: 23.9056\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2924.0715 - mse: 2924.0718 - mae: 31.1219 - val_loss: 1093.5241 - val_mse: 1093.5239 - val_mae: 23.8011\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2836.3408 - mse: 2836.3403 - mae: 30.7008 - val_loss: 1089.7079 - val_mse: 1089.7078 - val_mae: 24.0378\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 659us/step - loss: 2909.1491 - mse: 2909.1489 - mae: 30.8107 - val_loss: 1088.7857 - val_mse: 1088.7856 - val_mae: 24.1179\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2866.1459 - mse: 2866.1465 - mae: 31.3103 - val_loss: 1094.2454 - val_mse: 1094.2456 - val_mae: 23.7779\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2937.8716 - mse: 2937.8716 - mae: 30.9225 - val_loss: 1093.5327 - val_mse: 1093.5327 - val_mae: 23.8073\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 3008.7556 - mse: 3008.7551 - mae: 31.4523 - val_loss: 1093.0278 - val_mse: 1093.0278 - val_mae: 23.8230\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2812.4806 - mse: 2812.4810 - mae: 30.7982 - val_loss: 1092.1097 - val_mse: 1092.1099 - val_mae: 23.8455\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 539us/step - loss: 2940.8709 - mse: 2940.8723 - mae: 31.2054 - val_loss: 1093.3805 - val_mse: 1093.3805 - val_mae: 23.7784\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2926.5242 - mse: 2926.5254 - mae: 31.1601 - val_loss: 1089.1176 - val_mse: 1089.1176 - val_mae: 24.0181\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2900.5069 - mse: 2900.5068 - mae: 30.9860 - val_loss: 1091.0173 - val_mse: 1091.0172 - val_mae: 23.8687\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2864.2710 - mse: 2864.2703 - mae: 30.3876 - val_loss: 1088.6171 - val_mse: 1088.6171 - val_mae: 24.0352\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2881.7538 - mse: 2881.7527 - mae: 30.4801 - val_loss: 1087.5709 - val_mse: 1087.5708 - val_mae: 24.1349\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2934.5745 - mse: 2934.5737 - mae: 31.6295 - val_loss: 1097.6409 - val_mse: 1097.6409 - val_mae: 23.6238\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2872.3759 - mse: 2872.3755 - mae: 30.0961 - val_loss: 1092.4059 - val_mse: 1092.4058 - val_mae: 23.7878\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2904.0532 - mse: 2904.0530 - mae: 30.6714 - val_loss: 1092.6525 - val_mse: 1092.6526 - val_mae: 23.7791\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2881.6225 - mse: 2881.6223 - mae: 30.7862 - val_loss: 1087.3816 - val_mse: 1087.3815 - val_mae: 24.1193\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2868.5129 - mse: 2868.5137 - mae: 30.5916 - val_loss: 1095.7332 - val_mse: 1095.7333 - val_mae: 23.6678\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2849.2459 - mse: 2849.2461 - mae: 30.0585 - val_loss: 1086.6125 - val_mse: 1086.6124 - val_mae: 24.2117\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 536us/step - loss: 2883.4208 - mse: 2883.4204 - mae: 30.2958 - val_loss: 1091.1178 - val_mse: 1091.1177 - val_mae: 23.8134\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2821.3657 - mse: 2821.3655 - mae: 30.2787 - val_loss: 1085.9139 - val_mse: 1085.9138 - val_mae: 24.3115\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2837.9068 - mse: 2837.9065 - mae: 30.8304 - val_loss: 1089.8849 - val_mse: 1089.8849 - val_mae: 23.9027\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2853.5810 - mse: 2853.5815 - mae: 30.6934 - val_loss: 1087.9097 - val_mse: 1087.9098 - val_mae: 24.1195\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2830.1482 - mse: 2830.1492 - mae: 30.4318 - val_loss: 1088.3152 - val_mse: 1088.3152 - val_mae: 24.0430\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 649us/step - loss: 2810.8002 - mse: 2810.7998 - mae: 30.3472 - val_loss: 1088.6570 - val_mse: 1088.6571 - val_mae: 24.0270\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2946.2025 - mse: 2946.2026 - mae: 30.8093 - val_loss: 1088.3909 - val_mse: 1088.3907 - val_mae: 24.0490\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2892.4432 - mse: 2892.4434 - mae: 30.6155 - val_loss: 1093.2817 - val_mse: 1093.2819 - val_mae: 23.7564\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 694us/step - loss: 2910.0320 - mse: 2910.0322 - mae: 30.7915 - val_loss: 1091.1443 - val_mse: 1091.1442 - val_mae: 23.8518\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2850.0001 - mse: 2850.0000 - mae: 30.3450 - val_loss: 1089.4039 - val_mse: 1089.4039 - val_mae: 23.9515\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2905.2820 - mse: 2905.2822 - mae: 31.1989 - val_loss: 1090.4017 - val_mse: 1090.4017 - val_mae: 23.8951\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2848.2150 - mse: 2848.2151 - mae: 30.3262 - val_loss: 1086.9076 - val_mse: 1086.9076 - val_mae: 24.2161\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2888.0503 - mse: 2888.0513 - mae: 31.1245 - val_loss: 1092.0636 - val_mse: 1092.0636 - val_mae: 23.8204\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2862.5499 - mse: 2862.5505 - mae: 30.8497 - val_loss: 1086.7719 - val_mse: 1086.7720 - val_mae: 24.1669\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2897.9761 - mse: 2897.9756 - mae: 30.7732 - val_loss: 1088.5004 - val_mse: 1088.5004 - val_mae: 24.0001\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2828.7320 - mse: 2828.7319 - mae: 30.2012 - val_loss: 1086.1248 - val_mse: 1086.1248 - val_mae: 24.2920\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 641us/step - loss: 2810.1179 - mse: 2810.1174 - mae: 30.4808 - val_loss: 1089.2905 - val_mse: 1089.2903 - val_mae: 23.9226\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2914.2471 - mse: 2914.2473 - mae: 30.4706 - val_loss: 1087.8988 - val_mse: 1087.8988 - val_mae: 23.9874\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2820.0708 - mse: 2820.0708 - mae: 30.4864 - val_loss: 1085.2054 - val_mse: 1085.2054 - val_mae: 24.3097\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2862.0308 - mse: 2862.0303 - mae: 30.2405 - val_loss: 1085.8459 - val_mse: 1085.8458 - val_mae: 24.1814\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2915.7831 - mse: 2915.7827 - mae: 31.1544 - val_loss: 1088.0193 - val_mse: 1088.0194 - val_mae: 23.9552\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2939.5854 - mse: 2939.5857 - mae: 30.9355 - val_loss: 1087.5015 - val_mse: 1087.5016 - val_mae: 24.0332\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 678us/step - loss: 2879.7038 - mse: 2879.7031 - mae: 30.6120 - val_loss: 1086.1359 - val_mse: 1086.1359 - val_mae: 24.1866\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2837.4004 - mse: 2837.4009 - mae: 30.2383 - val_loss: 1093.2653 - val_mse: 1093.2653 - val_mae: 23.7073\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2855.7334 - mse: 2855.7339 - mae: 30.5027 - val_loss: 1086.2266 - val_mse: 1086.2266 - val_mae: 24.0171\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2869.2126 - mse: 2869.2126 - mae: 30.8392 - val_loss: 1089.3440 - val_mse: 1089.3441 - val_mae: 23.8083\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2871.2719 - mse: 2871.2720 - mae: 30.8787 - val_loss: 1086.9855 - val_mse: 1086.9855 - val_mae: 23.9576\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2863.7495 - mse: 2863.7493 - mae: 30.1562 - val_loss: 1085.1244 - val_mse: 1085.1243 - val_mae: 24.0837\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2900.7303 - mse: 2900.7300 - mae: 30.8291 - val_loss: 1083.5405 - val_mse: 1083.5405 - val_mae: 24.3018\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2845.9458 - mse: 2845.9453 - mae: 31.1359 - val_loss: 1089.3750 - val_mse: 1089.3750 - val_mae: 23.7519\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2875.9453 - mse: 2875.9463 - mae: 30.6918 - val_loss: 1086.2963 - val_mse: 1086.2964 - val_mae: 23.8741\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2835.4697 - mse: 2835.4707 - mae: 30.3828 - val_loss: 1085.8928 - val_mse: 1085.8928 - val_mae: 23.8803\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2801.7480 - mse: 2801.7483 - mae: 30.2943 - val_loss: 1084.0918 - val_mse: 1084.0918 - val_mae: 23.9845\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2843.8915 - mse: 2843.8916 - mae: 30.0165 - val_loss: 1081.3226 - val_mse: 1081.3225 - val_mae: 24.5760\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2547.4234 - mse: 2547.4231 - mae: 29.9326 - val_loss: 1563.7504 - val_mse: 1563.7502 - val_mae: 28.0881\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2543.1341 - mse: 2543.1333 - mae: 30.0141 - val_loss: 1568.6714 - val_mse: 1568.6715 - val_mae: 27.8644\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2546.3861 - mse: 2546.3855 - mae: 29.8228 - val_loss: 1569.4448 - val_mse: 1569.4448 - val_mae: 27.8151\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 688us/step - loss: 2560.6898 - mse: 2560.6899 - mae: 30.1247 - val_loss: 1566.2432 - val_mse: 1566.2432 - val_mae: 27.9074\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2537.8253 - mse: 2537.8257 - mae: 30.0119 - val_loss: 1570.5047 - val_mse: 1570.5049 - val_mae: 27.7634\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2547.0320 - mse: 2547.0320 - mae: 30.0735 - val_loss: 1573.4978 - val_mse: 1573.4978 - val_mae: 27.6453\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2567.0572 - mse: 2567.0579 - mae: 29.8815 - val_loss: 1562.4630 - val_mse: 1562.4630 - val_mae: 28.0617\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2581.8368 - mse: 2581.8367 - mae: 29.9889 - val_loss: 1568.0253 - val_mse: 1568.0253 - val_mae: 27.8289\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2590.2763 - mse: 2590.2754 - mae: 29.8243 - val_loss: 1559.4493 - val_mse: 1559.4491 - val_mae: 28.1861\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2529.8852 - mse: 2529.8860 - mae: 29.6700 - val_loss: 1567.5239 - val_mse: 1567.5239 - val_mae: 27.8139\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 514us/step - loss: 2601.2811 - mse: 2601.2808 - mae: 30.2519 - val_loss: 1565.7128 - val_mse: 1565.7128 - val_mae: 27.8632\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 501us/step - loss: 2547.3015 - mse: 2547.3015 - mae: 29.6475 - val_loss: 1567.2234 - val_mse: 1567.2235 - val_mae: 27.7915\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2577.6314 - mse: 2577.6313 - mae: 29.9798 - val_loss: 1557.7611 - val_mse: 1557.7610 - val_mae: 28.1749\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2618.7386 - mse: 2618.7383 - mae: 30.4445 - val_loss: 1575.4620 - val_mse: 1575.4620 - val_mae: 27.4823\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 494us/step - loss: 2523.0333 - mse: 2523.0339 - mae: 29.7316 - val_loss: 1574.1781 - val_mse: 1574.1782 - val_mae: 27.4996\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2527.0865 - mse: 2527.0869 - mae: 29.7517 - val_loss: 1561.6218 - val_mse: 1561.6218 - val_mae: 27.9201\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2479.5856 - mse: 2479.5854 - mae: 29.6088 - val_loss: 1558.0144 - val_mse: 1558.0143 - val_mae: 28.0614\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2493.9940 - mse: 2493.9944 - mae: 29.6960 - val_loss: 1564.2556 - val_mse: 1564.2559 - val_mae: 27.7881\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2512.1201 - mse: 2512.1204 - mae: 29.7564 - val_loss: 1563.1369 - val_mse: 1563.1368 - val_mae: 27.8128\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2518.0559 - mse: 2518.0562 - mae: 29.8597 - val_loss: 1562.3054 - val_mse: 1562.3054 - val_mae: 27.8397\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2610.5833 - mse: 2610.5820 - mae: 29.9846 - val_loss: 1559.2417 - val_mse: 1559.2417 - val_mae: 27.9578\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2484.5356 - mse: 2484.5352 - mae: 29.2442 - val_loss: 1550.9190 - val_mse: 1550.9191 - val_mae: 28.4288\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2542.1675 - mse: 2542.1670 - mae: 29.7658 - val_loss: 1555.4057 - val_mse: 1555.4056 - val_mae: 28.0869\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2635.3184 - mse: 2635.3179 - mae: 30.4759 - val_loss: 1572.2794 - val_mse: 1572.2795 - val_mae: 27.4575\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2496.9347 - mse: 2496.9348 - mae: 29.5892 - val_loss: 1558.5719 - val_mse: 1558.5718 - val_mae: 27.9253\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2566.7584 - mse: 2566.7585 - mae: 30.2005 - val_loss: 1561.8031 - val_mse: 1561.8031 - val_mae: 27.7795\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2620.5601 - mse: 2620.5603 - mae: 30.4070 - val_loss: 1564.6629 - val_mse: 1564.6628 - val_mae: 27.6685\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2610.3589 - mse: 2610.3577 - mae: 30.0057 - val_loss: 1563.5646 - val_mse: 1563.5646 - val_mae: 27.7027\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2605.4853 - mse: 2605.4851 - mae: 30.1073 - val_loss: 1566.4267 - val_mse: 1566.4264 - val_mae: 27.6080\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2549.8375 - mse: 2549.8384 - mae: 29.9011 - val_loss: 1561.0859 - val_mse: 1561.0858 - val_mae: 27.7899\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2519.2173 - mse: 2519.2175 - mae: 29.9506 - val_loss: 1556.7340 - val_mse: 1556.7340 - val_mae: 27.9089\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 537us/step - loss: 2487.9234 - mse: 2487.9233 - mae: 29.9704 - val_loss: 1560.9226 - val_mse: 1560.9226 - val_mae: 27.7477\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2503.1623 - mse: 2503.1624 - mae: 29.9098 - val_loss: 1557.4420 - val_mse: 1557.4419 - val_mae: 27.8472\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2472.3537 - mse: 2472.3538 - mae: 29.3956 - val_loss: 1553.2637 - val_mse: 1553.2635 - val_mae: 28.0171\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 675us/step - loss: 2430.7669 - mse: 2430.7664 - mae: 29.1762 - val_loss: 1553.4786 - val_mse: 1553.4785 - val_mae: 27.9747\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2532.1708 - mse: 2532.1714 - mae: 29.8655 - val_loss: 1560.2921 - val_mse: 1560.2921 - val_mae: 27.6594\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2554.6917 - mse: 2554.6917 - mae: 29.9176 - val_loss: 1557.5962 - val_mse: 1557.5963 - val_mae: 27.7557\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2539.9105 - mse: 2539.9102 - mae: 29.5095 - val_loss: 1557.8440 - val_mse: 1557.8440 - val_mae: 27.7879\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2539.9980 - mse: 2539.9971 - mae: 29.6482 - val_loss: 1552.2657 - val_mse: 1552.2659 - val_mae: 28.0211\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 659us/step - loss: 2543.6676 - mse: 2543.6677 - mae: 29.5064 - val_loss: 1562.3604 - val_mse: 1562.3605 - val_mae: 27.6084\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2522.1276 - mse: 2522.1267 - mae: 29.7070 - val_loss: 1553.9755 - val_mse: 1553.9755 - val_mae: 27.8539\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2531.1299 - mse: 2531.1306 - mae: 29.4877 - val_loss: 1552.1480 - val_mse: 1552.1479 - val_mae: 27.8729\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 666us/step - loss: 2490.1367 - mse: 2490.1379 - mae: 29.2215 - val_loss: 1545.7204 - val_mse: 1545.7206 - val_mae: 28.1553\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2528.3061 - mse: 2528.3062 - mae: 29.5916 - val_loss: 1554.4332 - val_mse: 1554.4332 - val_mae: 27.7264\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2499.1503 - mse: 2499.1506 - mae: 29.4558 - val_loss: 1548.2794 - val_mse: 1548.2794 - val_mae: 27.9015\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2513.7158 - mse: 2513.7158 - mae: 29.6518 - val_loss: 1554.0166 - val_mse: 1554.0164 - val_mae: 27.5991\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2671.4917 - mse: 2671.4927 - mae: 30.2226 - val_loss: 1550.8203 - val_mse: 1550.8204 - val_mae: 27.7210\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 552us/step - loss: 2477.1799 - mse: 2477.1792 - mae: 29.4962 - val_loss: 1544.7942 - val_mse: 1544.7942 - val_mae: 28.0437\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 518us/step - loss: 2550.7869 - mse: 2550.7876 - mae: 29.5363 - val_loss: 1548.3255 - val_mse: 1548.3254 - val_mae: 27.8868\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2535.6348 - mse: 2535.6338 - mae: 30.0931 - val_loss: 1559.6704 - val_mse: 1559.6703 - val_mae: 27.4552\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2551.5475 - mse: 2551.5471 - mae: 29.4162 - val_loss: 1550.9257 - val_mse: 1550.9258 - val_mae: 27.6965\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2466.1824 - mse: 2466.1816 - mae: 29.6040 - val_loss: 1546.5225 - val_mse: 1546.5226 - val_mae: 27.8219\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2460.9321 - mse: 2460.9316 - mae: 29.8237 - val_loss: 1544.2675 - val_mse: 1544.2673 - val_mae: 27.9448\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 710us/step - loss: 2516.8991 - mse: 2516.8992 - mae: 29.5788 - val_loss: 1543.3960 - val_mse: 1543.3961 - val_mae: 27.8988\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2497.9665 - mse: 2497.9661 - mae: 29.7430 - val_loss: 1547.1730 - val_mse: 1547.1731 - val_mae: 27.7335\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 549us/step - loss: 2485.5446 - mse: 2485.5454 - mae: 29.3276 - val_loss: 1543.9038 - val_mse: 1543.9039 - val_mae: 27.8259\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2500.8674 - mse: 2500.8684 - mae: 29.7769 - val_loss: 1547.3225 - val_mse: 1547.3226 - val_mae: 27.6077\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2508.2895 - mse: 2508.2900 - mae: 29.4446 - val_loss: 1550.2790 - val_mse: 1550.2791 - val_mae: 27.4702\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2550.4311 - mse: 2550.4316 - mae: 29.9807 - val_loss: 1548.3230 - val_mse: 1548.3231 - val_mae: 27.5870\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2440.8258 - mse: 2440.8264 - mae: 29.2365 - val_loss: 1543.3800 - val_mse: 1543.3802 - val_mae: 27.7304\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2536.3536 - mse: 2536.3535 - mae: 29.5438 - val_loss: 1545.4590 - val_mse: 1545.4591 - val_mae: 27.6253\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2541.9218 - mse: 2541.9226 - mae: 29.6172 - val_loss: 1546.1788 - val_mse: 1546.1788 - val_mae: 27.5947\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2494.5958 - mse: 2494.5952 - mae: 29.6293 - val_loss: 1542.0605 - val_mse: 1542.0605 - val_mae: 27.7606\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2518.3152 - mse: 2518.3157 - mae: 29.1818 - val_loss: 1541.7563 - val_mse: 1541.7561 - val_mae: 27.8210\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2525.8321 - mse: 2525.8315 - mae: 30.0645 - val_loss: 1543.6594 - val_mse: 1543.6595 - val_mae: 27.6823\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2583.3547 - mse: 2583.3547 - mae: 29.6737 - val_loss: 1541.2359 - val_mse: 1541.2361 - val_mae: 27.7533\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2481.5284 - mse: 2481.5288 - mae: 29.8206 - val_loss: 1538.9707 - val_mse: 1538.9708 - val_mae: 27.8419\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2510.2167 - mse: 2510.2173 - mae: 29.7326 - val_loss: 1544.2606 - val_mse: 1544.2607 - val_mae: 27.5902\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2389.4030 - mse: 2389.4033 - mae: 28.9275 - val_loss: 1542.1813 - val_mse: 1542.1810 - val_mae: 27.6568\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2496.7205 - mse: 2496.7200 - mae: 29.8588 - val_loss: 1539.5004 - val_mse: 1539.5002 - val_mae: 27.7662\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2525.1880 - mse: 2525.1873 - mae: 29.9384 - val_loss: 1546.8259 - val_mse: 1546.8258 - val_mae: 27.4612\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2474.2786 - mse: 2474.2788 - mae: 29.0119 - val_loss: 1538.4924 - val_mse: 1538.4924 - val_mae: 27.7808\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2514.7563 - mse: 2514.7559 - mae: 29.1882 - val_loss: 1540.4465 - val_mse: 1540.4465 - val_mae: 27.7286\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2446.1371 - mse: 2446.1367 - mae: 29.2962 - val_loss: 1541.8109 - val_mse: 1541.8110 - val_mae: 27.6676\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2549.0568 - mse: 2549.0576 - mae: 29.7947 - val_loss: 1541.3757 - val_mse: 1541.3756 - val_mae: 27.6810\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 683us/step - loss: 2494.5825 - mse: 2494.5828 - mae: 29.4980 - val_loss: 1549.1860 - val_mse: 1549.1860 - val_mae: 27.3588\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2510.7300 - mse: 2510.7297 - mae: 29.4064 - val_loss: 1544.6411 - val_mse: 1544.6411 - val_mae: 27.5388\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 656us/step - loss: 2538.9795 - mse: 2538.9797 - mae: 29.3572 - val_loss: 1543.7658 - val_mse: 1543.7659 - val_mae: 27.5883\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2497.8380 - mse: 2497.8379 - mae: 29.5567 - val_loss: 1543.2820 - val_mse: 1543.2819 - val_mae: 27.5712\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2501.0874 - mse: 2501.0869 - mae: 29.4219 - val_loss: 1544.8536 - val_mse: 1544.8535 - val_mae: 27.5393\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2447.5076 - mse: 2447.5081 - mae: 30.0863 - val_loss: 3681.0039 - val_mse: 3681.0044 - val_mae: 24.5048\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 682us/step - loss: 2400.5277 - mse: 2400.5278 - mae: 29.7586 - val_loss: 3682.1130 - val_mse: 3682.1135 - val_mae: 24.6745\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2376.4858 - mse: 2376.4856 - mae: 29.8697 - val_loss: 3681.9577 - val_mse: 3681.9575 - val_mae: 24.6021\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2409.5231 - mse: 2409.5234 - mae: 29.9609 - val_loss: 3682.9324 - val_mse: 3682.9326 - val_mae: 24.9296\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2410.6787 - mse: 2410.6785 - mae: 29.8004 - val_loss: 3680.4243 - val_mse: 3680.4241 - val_mae: 24.5672\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2393.5004 - mse: 2393.5000 - mae: 29.6507 - val_loss: 3678.9002 - val_mse: 3678.9006 - val_mae: 24.2967\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2408.0335 - mse: 2408.0327 - mae: 29.5653 - val_loss: 3680.8658 - val_mse: 3680.8657 - val_mae: 24.8105\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2402.2573 - mse: 2402.2573 - mae: 29.6987 - val_loss: 3678.1823 - val_mse: 3678.1816 - val_mae: 24.1802\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2403.3893 - mse: 2403.3892 - mae: 29.9546 - val_loss: 3679.1754 - val_mse: 3679.1748 - val_mae: 24.4390\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2404.8653 - mse: 2404.8660 - mae: 29.9476 - val_loss: 3679.1608 - val_mse: 3679.1606 - val_mae: 24.5555\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2403.1226 - mse: 2403.1230 - mae: 29.7660 - val_loss: 3681.2792 - val_mse: 3681.2798 - val_mae: 24.8929\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 524us/step - loss: 2340.8471 - mse: 2340.8472 - mae: 29.2057 - val_loss: 3679.3824 - val_mse: 3679.3828 - val_mae: 24.5766\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2390.2506 - mse: 2390.2507 - mae: 29.6405 - val_loss: 3677.6046 - val_mse: 3677.6050 - val_mae: 24.4199\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2419.0721 - mse: 2419.0730 - mae: 29.8945 - val_loss: 3678.1262 - val_mse: 3678.1262 - val_mae: 24.4447\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 696us/step - loss: 2340.9190 - mse: 2340.9189 - mae: 29.5367 - val_loss: 3678.3675 - val_mse: 3678.3667 - val_mae: 24.7823\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2409.5105 - mse: 2409.5100 - mae: 29.9780 - val_loss: 3676.9689 - val_mse: 3676.9683 - val_mae: 24.6327\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2374.4183 - mse: 2374.4189 - mae: 29.7508 - val_loss: 3675.9847 - val_mse: 3675.9844 - val_mae: 24.4128\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2456.6076 - mse: 2456.6074 - mae: 29.7394 - val_loss: 3675.1470 - val_mse: 3675.1465 - val_mae: 24.1794\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2393.5403 - mse: 2393.5403 - mae: 29.3038 - val_loss: 3676.5802 - val_mse: 3676.5803 - val_mae: 24.3754\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2395.6916 - mse: 2395.6924 - mae: 29.5872 - val_loss: 3676.8415 - val_mse: 3676.8413 - val_mae: 24.6166\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2382.2002 - mse: 2382.1997 - mae: 29.5363 - val_loss: 3679.0973 - val_mse: 3679.0977 - val_mae: 24.9074\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 552us/step - loss: 2413.0753 - mse: 2413.0750 - mae: 29.9517 - val_loss: 3676.1940 - val_mse: 3676.1948 - val_mae: 24.5488\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2424.3250 - mse: 2424.3257 - mae: 29.6543 - val_loss: 3679.0254 - val_mse: 3679.0247 - val_mae: 24.9176\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 673us/step - loss: 2371.8505 - mse: 2371.8506 - mae: 29.4721 - val_loss: 3676.1408 - val_mse: 3676.1406 - val_mae: 24.7479\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 523us/step - loss: 2409.7050 - mse: 2409.7056 - mae: 29.9797 - val_loss: 3672.6649 - val_mse: 3672.6646 - val_mae: 24.1912\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2410.7066 - mse: 2410.7065 - mae: 30.0634 - val_loss: 3675.3469 - val_mse: 3675.3464 - val_mae: 24.8908\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2444.4695 - mse: 2444.4695 - mae: 29.6780 - val_loss: 3673.6466 - val_mse: 3673.6462 - val_mae: 24.3344\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2442.8909 - mse: 2442.8906 - mae: 29.8210 - val_loss: 3673.5821 - val_mse: 3673.5823 - val_mae: 23.9789\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2442.8904 - mse: 2442.8906 - mae: 29.6180 - val_loss: 3678.3489 - val_mse: 3678.3486 - val_mae: 25.2973\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - ETA: 0s - loss: 2348.9843 - mse: 2348.9839 - mae: 29.49 - 2s 581us/step - loss: 2371.2145 - mse: 2371.2141 - mae: 29.7534 - val_loss: 3673.2935 - val_mse: 3673.2935 - val_mae: 24.5649\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2360.0683 - mse: 2360.0688 - mae: 29.3684 - val_loss: 3671.7192 - val_mse: 3671.7192 - val_mae: 24.4475\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2370.2250 - mse: 2370.2249 - mae: 29.8237 - val_loss: 3670.9372 - val_mse: 3670.9375 - val_mae: 24.2488\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2373.8182 - mse: 2373.8179 - mae: 29.1738 - val_loss: 3674.9288 - val_mse: 3674.9287 - val_mae: 25.0430\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2393.2069 - mse: 2393.2063 - mae: 30.0550 - val_loss: 3671.9100 - val_mse: 3671.9094 - val_mae: 24.4548\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 674us/step - loss: 2367.8125 - mse: 2367.8125 - mae: 29.5195 - val_loss: 3675.8789 - val_mse: 3675.8792 - val_mae: 25.0004\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2388.5274 - mse: 2388.5273 - mae: 29.7891 - val_loss: 3672.9194 - val_mse: 3672.9194 - val_mae: 24.3730\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2333.0096 - mse: 2333.0093 - mae: 29.0024 - val_loss: 3674.4808 - val_mse: 3674.4800 - val_mae: 25.0240\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2399.7015 - mse: 2399.7024 - mae: 30.0439 - val_loss: 3672.8734 - val_mse: 3672.8735 - val_mae: 24.8440\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2370.3661 - mse: 2370.3667 - mae: 29.5024 - val_loss: 3670.5073 - val_mse: 3670.5066 - val_mae: 24.5490\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 665us/step - loss: 2409.2384 - mse: 2409.2385 - mae: 29.9117 - val_loss: 3670.4032 - val_mse: 3670.4033 - val_mae: 24.6108\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2387.0705 - mse: 2387.0706 - mae: 29.6584 - val_loss: 3669.0595 - val_mse: 3669.0593 - val_mae: 24.4312\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2394.7692 - mse: 2394.7690 - mae: 29.4424 - val_loss: 3669.8322 - val_mse: 3669.8320 - val_mae: 24.5021\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 671us/step - loss: 2391.4150 - mse: 2391.4146 - mae: 29.9135 - val_loss: 3670.2361 - val_mse: 3670.2356 - val_mae: 24.8276\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 669us/step - loss: 2334.0180 - mse: 2334.0183 - mae: 29.7551 - val_loss: 3667.2958 - val_mse: 3667.2966 - val_mae: 24.3149\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2374.0846 - mse: 2374.0840 - mae: 29.5725 - val_loss: 3669.2194 - val_mse: 3669.2190 - val_mae: 24.4549\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2384.0742 - mse: 2384.0737 - mae: 29.3883 - val_loss: 3671.0729 - val_mse: 3671.0732 - val_mae: 24.8916\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2399.2414 - mse: 2399.2412 - mae: 29.6946 - val_loss: 3672.2555 - val_mse: 3672.2554 - val_mae: 25.1074\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2378.6603 - mse: 2378.6614 - mae: 29.7398 - val_loss: 3668.5019 - val_mse: 3668.5012 - val_mae: 24.3656\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2333.9278 - mse: 2333.9275 - mae: 29.2529 - val_loss: 3667.9726 - val_mse: 3667.9727 - val_mae: 24.4975\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2441.6263 - mse: 2441.6262 - mae: 29.5858 - val_loss: 3668.1606 - val_mse: 3668.1599 - val_mae: 24.6056\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2380.1038 - mse: 2380.1040 - mae: 29.6289 - val_loss: 3667.8095 - val_mse: 3667.8088 - val_mae: 24.4641\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2362.2856 - mse: 2362.2854 - mae: 29.5736 - val_loss: 3667.7156 - val_mse: 3667.7158 - val_mae: 24.4665\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2347.0034 - mse: 2347.0027 - mae: 29.6072 - val_loss: 3667.5153 - val_mse: 3667.5154 - val_mae: 24.1100\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2356.3238 - mse: 2356.3228 - mae: 29.2698 - val_loss: 3669.1379 - val_mse: 3669.1389 - val_mae: 24.5848\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2398.7280 - mse: 2398.7283 - mae: 29.9237 - val_loss: 3667.6070 - val_mse: 3667.6064 - val_mae: 24.0794\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 668us/step - loss: 2345.2156 - mse: 2345.2156 - mae: 29.6904 - val_loss: 3669.3984 - val_mse: 3669.3989 - val_mae: 24.2106\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2373.2588 - mse: 2373.2588 - mae: 29.3661 - val_loss: 3668.9200 - val_mse: 3668.9197 - val_mae: 24.6206\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2396.0027 - mse: 2396.0032 - mae: 29.7008 - val_loss: 3670.8121 - val_mse: 3670.8120 - val_mae: 24.9235\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2370.6426 - mse: 2370.6433 - mae: 29.5764 - val_loss: 3666.9329 - val_mse: 3666.9338 - val_mae: 24.1638\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2376.0805 - mse: 2376.0808 - mae: 29.3115 - val_loss: 3666.4302 - val_mse: 3666.4304 - val_mae: 24.5854\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2385.1851 - mse: 2385.1843 - mae: 29.6916 - val_loss: 3666.1246 - val_mse: 3666.1245 - val_mae: 24.6303\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2397.7652 - mse: 2397.7651 - mae: 29.6602 - val_loss: 3665.1201 - val_mse: 3665.1191 - val_mae: 24.5322\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2347.5187 - mse: 2347.5193 - mae: 29.4496 - val_loss: 3664.0382 - val_mse: 3664.0381 - val_mae: 24.4479\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2420.1119 - mse: 2420.1121 - mae: 29.6472 - val_loss: 3664.1120 - val_mse: 3664.1125 - val_mae: 24.1880\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2398.3794 - mse: 2398.3796 - mae: 29.3529 - val_loss: 3664.4630 - val_mse: 3664.4624 - val_mae: 24.3972\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 546us/step - loss: 2402.9653 - mse: 2402.9653 - mae: 29.5331 - val_loss: 3664.2980 - val_mse: 3664.2981 - val_mae: 24.4194\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2403.1369 - mse: 2403.1377 - mae: 29.6142 - val_loss: 3666.3046 - val_mse: 3666.3044 - val_mae: 24.5165\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2397.8538 - mse: 2397.8535 - mae: 29.8178 - val_loss: 3666.7928 - val_mse: 3666.7932 - val_mae: 24.4777\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2363.8574 - mse: 2363.8572 - mae: 29.5121 - val_loss: 3666.2073 - val_mse: 3666.2073 - val_mae: 24.6571\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2342.9042 - mse: 2342.9038 - mae: 29.3712 - val_loss: 3666.9377 - val_mse: 3666.9380 - val_mae: 25.0228\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2349.4585 - mse: 2349.4590 - mae: 29.5049 - val_loss: 3664.8863 - val_mse: 3664.8860 - val_mae: 24.6509\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2389.3026 - mse: 2389.3022 - mae: 30.1087 - val_loss: 3662.5785 - val_mse: 3662.5784 - val_mae: 24.3793\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2357.1712 - mse: 2357.1716 - mae: 29.1819 - val_loss: 3664.8324 - val_mse: 3664.8320 - val_mae: 24.2010\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2419.2841 - mse: 2419.2839 - mae: 29.8390 - val_loss: 3666.4869 - val_mse: 3666.4873 - val_mae: 24.4030\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2378.8950 - mse: 2378.8950 - mae: 29.6147 - val_loss: 3667.6958 - val_mse: 3667.6958 - val_mae: 24.7550\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 634us/step - loss: 2354.0852 - mse: 2354.0845 - mae: 29.3465 - val_loss: 3669.3899 - val_mse: 3669.3901 - val_mae: 25.0559\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 552us/step - loss: 2378.3110 - mse: 2378.3103 - mae: 29.7574 - val_loss: 3666.0252 - val_mse: 3666.0254 - val_mae: 24.5683\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2414.9529 - mse: 2414.9519 - mae: 29.9096 - val_loss: 3664.0462 - val_mse: 3664.0461 - val_mae: 24.3067\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 558us/step - loss: 2301.3373 - mse: 2301.3367 - mae: 29.0406 - val_loss: 3663.9101 - val_mse: 3663.9104 - val_mae: 24.3938\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2307.1031 - mse: 2307.1030 - mae: 29.2118 - val_loss: 3665.2126 - val_mse: 3665.2124 - val_mae: 24.5947\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2745.3097 - mse: 2745.3103 - mae: 29.2174 - val_loss: 2482.9756 - val_mse: 2482.9756 - val_mae: 26.8557\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2734.0841 - mse: 2734.0845 - mae: 29.0532 - val_loss: 2499.5697 - val_mse: 2499.5696 - val_mae: 26.8732\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2744.7873 - mse: 2744.7876 - mae: 28.9936 - val_loss: 2501.2668 - val_mse: 2501.2671 - val_mae: 26.8617\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2802.3444 - mse: 2802.3450 - mae: 29.0953 - val_loss: 2498.3975 - val_mse: 2498.3977 - val_mae: 27.0732\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2778.8945 - mse: 2778.8950 - mae: 29.1127 - val_loss: 2502.1562 - val_mse: 2502.1562 - val_mae: 27.2010\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2716.5671 - mse: 2716.5669 - mae: 28.9991 - val_loss: 2504.0859 - val_mse: 2504.0859 - val_mae: 27.0692\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2736.4219 - mse: 2736.4224 - mae: 29.0460 - val_loss: 2506.8633 - val_mse: 2506.8630 - val_mae: 27.0691\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2774.9507 - mse: 2774.9504 - mae: 29.1128 - val_loss: 2506.7266 - val_mse: 2506.7271 - val_mae: 27.0772\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2742.4997 - mse: 2742.4995 - mae: 28.9368 - val_loss: 2502.2097 - val_mse: 2502.2100 - val_mae: 27.0403\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2745.7917 - mse: 2745.7922 - mae: 28.8439 - val_loss: 2496.8136 - val_mse: 2496.8130 - val_mae: 27.2918\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2758.2134 - mse: 2758.2139 - mae: 28.9839 - val_loss: 2496.6771 - val_mse: 2496.6772 - val_mae: 27.3829\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2786.7991 - mse: 2786.7988 - mae: 29.1998 - val_loss: 2503.2923 - val_mse: 2503.2922 - val_mae: 27.1259\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2714.3213 - mse: 2714.3218 - mae: 29.0591 - val_loss: 2509.7758 - val_mse: 2509.7751 - val_mae: 26.8863\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2743.5752 - mse: 2743.5757 - mae: 29.0634 - val_loss: 2509.9103 - val_mse: 2509.9102 - val_mae: 26.9128\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2698.7570 - mse: 2698.7583 - mae: 29.1711 - val_loss: 2499.0666 - val_mse: 2499.0664 - val_mae: 27.3343\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2693.8301 - mse: 2693.8308 - mae: 28.8527 - val_loss: 2503.7525 - val_mse: 2503.7527 - val_mae: 26.9219\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 659us/step - loss: 2754.3445 - mse: 2754.3450 - mae: 29.0649 - val_loss: 2499.0052 - val_mse: 2499.0051 - val_mae: 27.2426\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 671us/step - loss: 2726.5035 - mse: 2726.5039 - mae: 29.1328 - val_loss: 2507.8998 - val_mse: 2507.8997 - val_mae: 26.7219\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2771.5131 - mse: 2771.5142 - mae: 28.9460 - val_loss: 2517.9863 - val_mse: 2517.9866 - val_mae: 26.5089\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2730.7838 - mse: 2730.7842 - mae: 28.8827 - val_loss: 2502.9423 - val_mse: 2502.9421 - val_mae: 27.0786\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 523us/step - loss: 2757.2663 - mse: 2757.2661 - mae: 28.9296 - val_loss: 2506.8537 - val_mse: 2506.8540 - val_mae: 26.7087\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 537us/step - loss: 2745.1997 - mse: 2745.2000 - mae: 29.1137 - val_loss: 2501.0332 - val_mse: 2501.0334 - val_mae: 26.7892\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 666us/step - loss: 2730.2114 - mse: 2730.2122 - mae: 29.0297 - val_loss: 2495.1318 - val_mse: 2495.1318 - val_mae: 27.0668\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2782.8707 - mse: 2782.8708 - mae: 28.9289 - val_loss: 2496.5349 - val_mse: 2496.5349 - val_mae: 26.9277\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2789.9206 - mse: 2789.9211 - mae: 28.9252 - val_loss: 2508.8306 - val_mse: 2508.8306 - val_mae: 26.5550\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2735.8921 - mse: 2735.8926 - mae: 28.9702 - val_loss: 2493.5725 - val_mse: 2493.5723 - val_mae: 27.3921\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2742.9867 - mse: 2742.9873 - mae: 29.2624 - val_loss: 2512.0616 - val_mse: 2512.0610 - val_mae: 26.7112\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 670us/step - loss: 2697.5111 - mse: 2697.5110 - mae: 28.6977 - val_loss: 2505.1243 - val_mse: 2505.1248 - val_mae: 26.9114\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2736.4335 - mse: 2736.4326 - mae: 28.6855 - val_loss: 2501.9215 - val_mse: 2501.9214 - val_mae: 26.9577\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2734.4779 - mse: 2734.4773 - mae: 28.9291 - val_loss: 2496.9832 - val_mse: 2496.9839 - val_mae: 26.9646\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 679us/step - loss: 2730.5376 - mse: 2730.5378 - mae: 29.0472 - val_loss: 2493.3819 - val_mse: 2493.3818 - val_mae: 27.0953\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2749.5864 - mse: 2749.5864 - mae: 29.1403 - val_loss: 2492.1764 - val_mse: 2492.1765 - val_mae: 26.9615\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2759.6259 - mse: 2759.6255 - mae: 29.2551 - val_loss: 2496.5505 - val_mse: 2496.5503 - val_mae: 26.8918\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2726.3120 - mse: 2726.3127 - mae: 28.5072 - val_loss: 2491.0927 - val_mse: 2491.0933 - val_mae: 27.2274\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2714.0894 - mse: 2714.0894 - mae: 28.9272 - val_loss: 2487.3384 - val_mse: 2487.3381 - val_mae: 26.9244\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 551us/step - loss: 2721.1373 - mse: 2721.1375 - mae: 29.0857 - val_loss: 2492.7217 - val_mse: 2492.7222 - val_mae: 26.9955\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2745.8749 - mse: 2745.8750 - mae: 28.7611 - val_loss: 2493.1262 - val_mse: 2493.1267 - val_mae: 26.8998\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2719.3099 - mse: 2719.3098 - mae: 28.5672 - val_loss: 2491.3718 - val_mse: 2491.3718 - val_mae: 27.0989\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 696us/step - loss: 2705.5197 - mse: 2705.5200 - mae: 28.7360 - val_loss: 2491.3120 - val_mse: 2491.3125 - val_mae: 26.9523\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2779.2369 - mse: 2779.2371 - mae: 29.0746 - val_loss: 2484.4633 - val_mse: 2484.4634 - val_mae: 27.3015\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2788.9607 - mse: 2788.9619 - mae: 29.1002 - val_loss: 2492.9378 - val_mse: 2492.9380 - val_mae: 26.9124\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2729.1670 - mse: 2729.1675 - mae: 28.6372 - val_loss: 2491.8149 - val_mse: 2491.8147 - val_mae: 26.8232\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2692.7582 - mse: 2692.7578 - mae: 28.8337 - val_loss: 2484.4358 - val_mse: 2484.4358 - val_mae: 27.3261\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2793.3096 - mse: 2793.3093 - mae: 28.9373 - val_loss: 2496.9461 - val_mse: 2496.9460 - val_mae: 26.9510\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2754.5037 - mse: 2754.5042 - mae: 29.0112 - val_loss: 2495.2991 - val_mse: 2495.2993 - val_mae: 26.8137\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2736.5887 - mse: 2736.5889 - mae: 28.6065 - val_loss: 2491.0053 - val_mse: 2491.0051 - val_mae: 27.0434\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2767.0763 - mse: 2767.0764 - mae: 29.0256 - val_loss: 2496.8955 - val_mse: 2496.8958 - val_mae: 26.7725\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2707.9860 - mse: 2707.9866 - mae: 28.4916 - val_loss: 2486.9913 - val_mse: 2486.9915 - val_mae: 27.1693\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2721.8787 - mse: 2721.8784 - mae: 28.8080 - val_loss: 2486.7619 - val_mse: 2486.7620 - val_mae: 27.2922\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2752.3433 - mse: 2752.3425 - mae: 28.8945 - val_loss: 2488.5284 - val_mse: 2488.5286 - val_mae: 26.9934\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2710.7529 - mse: 2710.7532 - mae: 28.9604 - val_loss: 2486.7633 - val_mse: 2486.7634 - val_mae: 26.9537\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 472us/step - loss: 2690.9989 - mse: 2690.9990 - mae: 28.8440 - val_loss: 2481.1288 - val_mse: 2481.1289 - val_mae: 27.0836\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2662.8906 - mse: 2662.8906 - mae: 28.8261 - val_loss: 2481.9232 - val_mse: 2481.9233 - val_mae: 27.1161\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2756.0348 - mse: 2756.0347 - mae: 28.7135 - val_loss: 2486.2406 - val_mse: 2486.2407 - val_mae: 26.8981\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2738.6868 - mse: 2738.6875 - mae: 29.0347 - val_loss: 2488.2094 - val_mse: 2488.2090 - val_mae: 27.1782\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2720.5310 - mse: 2720.5315 - mae: 29.0861 - val_loss: 2485.1902 - val_mse: 2485.1907 - val_mae: 27.2312\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2738.5553 - mse: 2738.5554 - mae: 28.4727 - val_loss: 2489.7257 - val_mse: 2489.7251 - val_mae: 26.8707\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2801.6918 - mse: 2801.6921 - mae: 29.2919 - val_loss: 2488.4297 - val_mse: 2488.4294 - val_mae: 26.8427\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 554us/step - loss: 2736.2366 - mse: 2736.2363 - mae: 29.0372 - val_loss: 2486.4487 - val_mse: 2486.4490 - val_mae: 26.9680\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2715.0174 - mse: 2715.0176 - mae: 28.6934 - val_loss: 2484.3805 - val_mse: 2484.3801 - val_mae: 26.8024\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 675us/step - loss: 2709.1718 - mse: 2709.1707 - mae: 28.7487 - val_loss: 2482.0045 - val_mse: 2482.0046 - val_mae: 27.0647\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2740.2392 - mse: 2740.2393 - mae: 28.9983 - val_loss: 2485.0836 - val_mse: 2485.0837 - val_mae: 26.6730\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2717.5471 - mse: 2717.5469 - mae: 28.7565 - val_loss: 2477.3199 - val_mse: 2477.3203 - val_mae: 27.1125\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2719.4985 - mse: 2719.4995 - mae: 28.8976 - val_loss: 2478.6718 - val_mse: 2478.6716 - val_mae: 27.0721\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 655us/step - loss: 2709.5600 - mse: 2709.5598 - mae: 28.6381 - val_loss: 2477.7330 - val_mse: 2477.7329 - val_mae: 27.0652\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2651.3185 - mse: 2651.3179 - mae: 28.7986 - val_loss: 2470.6882 - val_mse: 2470.6887 - val_mae: 27.2163\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2678.1769 - mse: 2678.1770 - mae: 28.2522 - val_loss: 2469.3825 - val_mse: 2469.3826 - val_mae: 26.9568\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2716.0130 - mse: 2716.0132 - mae: 28.9496 - val_loss: 2472.4804 - val_mse: 2472.4805 - val_mae: 26.8474\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2764.1178 - mse: 2764.1189 - mae: 29.1060 - val_loss: 2475.5641 - val_mse: 2475.5642 - val_mae: 26.9441\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 494us/step - loss: 2712.0262 - mse: 2712.0256 - mae: 29.0063 - val_loss: 2473.9438 - val_mse: 2473.9438 - val_mae: 27.2005\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 378us/step - loss: 2717.3966 - mse: 2717.3960 - mae: 28.3175 - val_loss: 2476.0520 - val_mse: 2476.0515 - val_mae: 27.0660\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 453us/step - loss: 2691.9221 - mse: 2691.9221 - mae: 28.3947 - val_loss: 2479.0313 - val_mse: 2479.0312 - val_mae: 26.9440\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 456us/step - loss: 2723.8461 - mse: 2723.8467 - mae: 29.0278 - val_loss: 2484.8856 - val_mse: 2484.8855 - val_mae: 26.8739\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 493us/step - loss: 2743.2520 - mse: 2743.2524 - mae: 28.5652 - val_loss: 2482.0097 - val_mse: 2482.0095 - val_mae: 26.6978\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 676us/step - loss: 2748.8210 - mse: 2748.8213 - mae: 28.8018 - val_loss: 2480.6908 - val_mse: 2480.6909 - val_mae: 27.2535\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2678.6379 - mse: 2678.6379 - mae: 28.4054 - val_loss: 2477.4229 - val_mse: 2477.4231 - val_mae: 27.2586\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2739.3995 - mse: 2739.3999 - mae: 29.0169 - val_loss: 2482.0740 - val_mse: 2482.0742 - val_mae: 26.9574\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2709.0344 - mse: 2709.0347 - mae: 28.4857 - val_loss: 2477.8671 - val_mse: 2477.8672 - val_mae: 27.0777\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2707.7339 - mse: 2707.7344 - mae: 28.4334 - val_loss: 2470.2014 - val_mse: 2470.2019 - val_mae: 27.4115\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2727.5931 - mse: 2727.5930 - mae: 28.7163 - val_loss: 2473.8927 - val_mse: 2473.8928 - val_mae: 26.9751\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 13330.8561 - mse: 13330.8564 - mae: 109.9109 - val_loss: 34622.3998 - val_mse: 34622.3984 - val_mae: 132.7479\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 711us/step - loss: 13197.4643 - mse: 13197.4629 - mae: 109.3122 - val_loss: 34371.1482 - val_mse: 34371.1445 - val_mae: 131.8129\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 12836.9548 - mse: 12836.9541 - mae: 107.6731 - val_loss: 33675.7238 - val_mse: 33675.7227 - val_mae: 129.1934\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 11847.4983 - mse: 11847.4971 - mae: 102.9847 - val_loss: 31799.5558 - val_mse: 31799.5566 - val_mae: 121.8535\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 673us/step - loss: 9444.7030 - mse: 9444.7031 - mae: 90.2719 - val_loss: 27479.2664 - val_mse: 27479.2656 - val_mae: 102.9975\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 5266.4728 - mse: 5266.4722 - mae: 61.0138 - val_loss: 20405.9238 - val_mse: 20405.9238 - val_mae: 60.6621\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 654us/step - loss: 2832.9720 - mse: 2832.9719 - mae: 37.8882 - val_loss: 17973.4722 - val_mse: 17973.4707 - val_mae: 39.0316\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 651us/step - loss: 2561.1999 - mse: 2561.1997 - mae: 36.9347 - val_loss: 18203.7563 - val_mse: 18203.7578 - val_mae: 40.7216\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 2758.3648 - mse: 2758.3647 - mae: 37.0955 - val_loss: 18101.6775 - val_mse: 18101.6777 - val_mae: 39.8370\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2861.2025 - mse: 2861.2024 - mae: 38.0546 - val_loss: 18303.0493 - val_mse: 18303.0508 - val_mae: 41.4277\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 2735.1169 - mse: 2735.1167 - mae: 37.9171 - val_loss: 18196.9801 - val_mse: 18196.9785 - val_mae: 40.5016\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 2482.0511 - mse: 2482.0510 - mae: 35.7686 - val_loss: 18215.6384 - val_mse: 18215.6387 - val_mae: 40.6024\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2596.3249 - mse: 2596.3250 - mae: 36.3678 - val_loss: 18124.5870 - val_mse: 18124.5879 - val_mae: 39.8121\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 628us/step - loss: 2888.7376 - mse: 2888.7375 - mae: 38.7792 - val_loss: 18151.4407 - val_mse: 18151.4375 - val_mae: 39.9785\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 2528.8766 - mse: 2528.8765 - mae: 36.1434 - val_loss: 18162.4494 - val_mse: 18162.4492 - val_mae: 40.0186\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2438.5553 - mse: 2438.5554 - mae: 35.8291 - val_loss: 18001.3615 - val_mse: 18001.3613 - val_mae: 38.7487\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2441.2957 - mse: 2441.2957 - mae: 35.0762 - val_loss: 18057.0808 - val_mse: 18057.0820 - val_mae: 39.1014\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 2651.4019 - mse: 2651.4019 - mae: 35.9624 - val_loss: 18342.9260 - val_mse: 18342.9238 - val_mae: 41.3893\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2353.8661 - mse: 2353.8662 - mae: 35.1822 - val_loss: 18122.2888 - val_mse: 18122.2871 - val_mae: 39.4860\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2473.9741 - mse: 2473.9741 - mae: 34.6270 - val_loss: 17935.1535 - val_mse: 17935.1562 - val_mae: 38.0682\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 654us/step - loss: 2457.0057 - mse: 2457.0059 - mae: 35.4631 - val_loss: 18009.4612 - val_mse: 18009.4629 - val_mae: 38.5585\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2342.8054 - mse: 2342.8054 - mae: 33.7612 - val_loss: 17919.9320 - val_mse: 17919.9297 - val_mae: 37.8929\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2391.2726 - mse: 2391.2722 - mae: 34.5968 - val_loss: 17977.0211 - val_mse: 17977.0195 - val_mae: 38.2346\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2511.9150 - mse: 2511.9150 - mae: 35.9635 - val_loss: 18044.6769 - val_mse: 18044.6777 - val_mae: 38.6904\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 2462.7992 - mse: 2462.7993 - mae: 34.3181 - val_loss: 17974.1472 - val_mse: 17974.1484 - val_mae: 38.1498\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2292.2905 - mse: 2292.2905 - mae: 33.1739 - val_loss: 17934.9878 - val_mse: 17934.9883 - val_mae: 37.8547\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2202.3403 - mse: 2202.3403 - mae: 32.6238 - val_loss: 17876.7267 - val_mse: 17876.7266 - val_mae: 37.4817\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 2360.5421 - mse: 2360.5420 - mae: 34.4748 - val_loss: 17833.5042 - val_mse: 17833.5039 - val_mae: 37.2202\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2620.9210 - mse: 2620.9211 - mae: 37.5033 - val_loss: 18043.3642 - val_mse: 18043.3633 - val_mae: 38.5120\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2457.7926 - mse: 2457.7927 - mae: 34.3962 - val_loss: 17996.3604 - val_mse: 17996.3594 - val_mae: 38.1434\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2409.6721 - mse: 2409.6721 - mae: 35.0786 - val_loss: 17973.2485 - val_mse: 17973.2480 - val_mae: 37.9592\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2418.1179 - mse: 2418.1179 - mae: 35.3839 - val_loss: 17947.7502 - val_mse: 17947.7500 - val_mae: 37.7615\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2212.1972 - mse: 2212.1975 - mae: 31.9927 - val_loss: 17938.9305 - val_mse: 17938.9297 - val_mae: 37.6816\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 2383.5088 - mse: 2383.5088 - mae: 34.6525 - val_loss: 17994.2505 - val_mse: 17994.2480 - val_mae: 38.0212\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 746us/step - loss: 2372.3498 - mse: 2372.3501 - mae: 35.0553 - val_loss: 18123.2954 - val_mse: 18123.2949 - val_mae: 38.8495\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 2338.1975 - mse: 2338.1978 - mae: 33.6217 - val_loss: 17914.1961 - val_mse: 17914.1953 - val_mae: 37.4456\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2305.1969 - mse: 2305.1965 - mae: 33.3525 - val_loss: 17854.4303 - val_mse: 17854.4297 - val_mae: 37.0662\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 659us/step - loss: 2321.6034 - mse: 2321.6038 - mae: 33.9144 - val_loss: 17847.9101 - val_mse: 17847.9102 - val_mae: 37.0068\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2317.9024 - mse: 2317.9026 - mae: 33.2977 - val_loss: 17946.8942 - val_mse: 17946.8965 - val_mae: 37.5835\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 700us/step - loss: 2234.4432 - mse: 2234.4436 - mae: 33.2117 - val_loss: 17853.9262 - val_mse: 17853.9277 - val_mae: 36.9910\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 519us/step - loss: 2132.4388 - mse: 2132.4390 - mae: 32.6263 - val_loss: 17790.9147 - val_mse: 17790.9141 - val_mae: 36.6652\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 455us/step - loss: 2206.7465 - mse: 2206.7466 - mae: 33.6976 - val_loss: 17983.2538 - val_mse: 17983.2539 - val_mae: 37.7570\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2290.5901 - mse: 2290.5903 - mae: 32.7947 - val_loss: 17880.3203 - val_mse: 17880.3203 - val_mae: 37.0723\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 730us/step - loss: 1976.4928 - mse: 1976.4930 - mae: 31.0392 - val_loss: 17809.2080 - val_mse: 17809.2070 - val_mae: 36.6958\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 691us/step - loss: 2180.8657 - mse: 2180.8655 - mae: 32.5718 - val_loss: 17868.7477 - val_mse: 17868.7461 - val_mae: 36.9665\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 677us/step - loss: 2059.4534 - mse: 2059.4534 - mae: 30.9714 - val_loss: 17762.3433 - val_mse: 17762.3438 - val_mae: 36.4876\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 2101.0769 - mse: 2101.0769 - mae: 32.1129 - val_loss: 17691.2792 - val_mse: 17691.2793 - val_mae: 36.2540\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 594us/step - loss: 2216.1532 - mse: 2216.1531 - mae: 33.3651 - val_loss: 18041.0332 - val_mse: 18041.0312 - val_mae: 38.0157\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 2228.2432 - mse: 2228.2429 - mae: 33.3147 - val_loss: 17711.7189 - val_mse: 17711.7188 - val_mae: 36.2978\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 1924.5624 - mse: 1924.5624 - mae: 30.7687 - val_loss: 17797.6044 - val_mse: 17797.6035 - val_mae: 36.5825\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 799us/step - loss: 2161.8805 - mse: 2161.8806 - mae: 32.7874 - val_loss: 17906.4828 - val_mse: 17906.4844 - val_mae: 37.1030\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2007.3812 - mse: 2007.3811 - mae: 31.2572 - val_loss: 17790.0378 - val_mse: 17790.0371 - val_mae: 36.5416\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2126.8633 - mse: 2126.8635 - mae: 32.3451 - val_loss: 17780.4526 - val_mse: 17780.4512 - val_mae: 36.4997\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2093.0349 - mse: 2093.0349 - mae: 31.8168 - val_loss: 17840.7286 - val_mse: 17840.7305 - val_mae: 36.7387\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 1976.5596 - mse: 1976.5594 - mae: 30.8185 - val_loss: 17653.8797 - val_mse: 17653.8809 - val_mae: 36.0849\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2059.7668 - mse: 2059.7668 - mae: 31.8935 - val_loss: 17869.6432 - val_mse: 17869.6426 - val_mae: 36.8679\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 1915.9752 - mse: 1915.9752 - mae: 29.7380 - val_loss: 17931.6595 - val_mse: 17931.6602 - val_mae: 37.2206\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 1994.6516 - mse: 1994.6516 - mae: 30.5671 - val_loss: 17674.1741 - val_mse: 17674.1738 - val_mae: 36.1284\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 1981.6973 - mse: 1981.6975 - mae: 30.7018 - val_loss: 17779.0227 - val_mse: 17779.0215 - val_mae: 36.4510\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 2004.2353 - mse: 2004.2352 - mae: 30.4316 - val_loss: 17920.4364 - val_mse: 17920.4355 - val_mae: 37.1254\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 1927.1359 - mse: 1927.1359 - mae: 30.2492 - val_loss: 17860.1733 - val_mse: 17860.1738 - val_mae: 36.7828\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 1999.8295 - mse: 1999.8295 - mae: 30.4398 - val_loss: 17800.8819 - val_mse: 17800.8848 - val_mae: 36.5134\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 682us/step - loss: 1948.7155 - mse: 1948.7157 - mae: 31.3255 - val_loss: 17591.0952 - val_mse: 17591.0938 - val_mae: 35.9963\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 2152.8315 - mse: 2152.8318 - mae: 31.8480 - val_loss: 17947.2334 - val_mse: 17947.2344 - val_mae: 37.2532\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2061.1124 - mse: 2061.1121 - mae: 30.4569 - val_loss: 17762.6153 - val_mse: 17762.6133 - val_mae: 36.3680\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 1920.0718 - mse: 1920.0718 - mae: 30.3908 - val_loss: 17655.2132 - val_mse: 17655.2129 - val_mae: 36.1087\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 2128.1309 - mse: 2128.1309 - mae: 32.0852 - val_loss: 18056.5942 - val_mse: 18056.5918 - val_mae: 37.8998\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 637us/step - loss: 2020.8047 - mse: 2020.8048 - mae: 31.5527 - val_loss: 17784.7208 - val_mse: 17784.7207 - val_mae: 36.4416\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 1943.8180 - mse: 1943.8180 - mae: 30.7957 - val_loss: 17733.8311 - val_mse: 17733.8301 - val_mae: 36.2999\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 485us/step - loss: 2001.6945 - mse: 2001.6945 - mae: 30.7472 - val_loss: 17884.0144 - val_mse: 17884.0137 - val_mae: 36.8580\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 2035.8294 - mse: 2035.8293 - mae: 30.8778 - val_loss: 17721.7929 - val_mse: 17721.7930 - val_mae: 36.2758\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 2037.5089 - mse: 2037.5090 - mae: 30.9478 - val_loss: 17905.8632 - val_mse: 17905.8652 - val_mae: 36.9692\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 543us/step - loss: 1911.6235 - mse: 1911.6235 - mae: 29.4616 - val_loss: 17589.0144 - val_mse: 17589.0137 - val_mae: 36.1135\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 461us/step - loss: 1915.7579 - mse: 1915.7579 - mae: 29.3154 - val_loss: 17781.9276 - val_mse: 17781.9277 - val_mae: 36.4580\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 499us/step - loss: 2024.4169 - mse: 2024.4169 - mae: 30.1036 - val_loss: 17743.8678 - val_mse: 17743.8652 - val_mae: 36.3517\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2032.2050 - mse: 2032.2050 - mae: 30.4504 - val_loss: 17792.5878 - val_mse: 17792.5879 - val_mae: 36.4992\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 652us/step - loss: 1796.3414 - mse: 1796.3414 - mae: 28.9437 - val_loss: 17774.2921 - val_mse: 17774.2930 - val_mae: 36.4474\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 1909.9142 - mse: 1909.9144 - mae: 28.6660 - val_loss: 17872.8296 - val_mse: 17872.8301 - val_mae: 36.8289\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 661us/step - loss: 2062.0349 - mse: 2062.0352 - mae: 31.3959 - val_loss: 17825.2209 - val_mse: 17825.2227 - val_mae: 36.6211\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 1842.8307 - mse: 1842.8308 - mae: 29.1945 - val_loss: 17699.8472 - val_mse: 17699.8477 - val_mae: 36.3076\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4275.3786 - mse: 4275.3784 - mae: 34.1673 - val_loss: 2192.4506 - val_mse: 2192.4504 - val_mae: 31.4461\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4307.0253 - mse: 4307.0249 - mae: 35.1540 - val_loss: 2291.6813 - val_mse: 2291.6812 - val_mae: 31.7262\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4207.7405 - mse: 4207.7402 - mae: 35.1415 - val_loss: 2364.1286 - val_mse: 2364.1284 - val_mae: 31.9434\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4402.2795 - mse: 4402.2788 - mae: 34.7664 - val_loss: 2314.8279 - val_mse: 2314.8276 - val_mae: 31.7953\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 653us/step - loss: 4156.0940 - mse: 4156.0938 - mae: 34.6513 - val_loss: 2346.1639 - val_mse: 2346.1638 - val_mae: 31.8905\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4272.0607 - mse: 4272.0610 - mae: 36.0060 - val_loss: 2376.3581 - val_mse: 2376.3579 - val_mae: 31.9840\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 714us/step - loss: 4320.2161 - mse: 4320.2163 - mae: 34.6990 - val_loss: 2315.1427 - val_mse: 2315.1426 - val_mae: 31.7965\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4403.0632 - mse: 4403.0630 - mae: 36.5384 - val_loss: 2359.9159 - val_mse: 2359.9160 - val_mae: 31.9323\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4337.6274 - mse: 4337.6279 - mae: 34.9943 - val_loss: 2367.2294 - val_mse: 2367.2295 - val_mae: 31.9533\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4134.4823 - mse: 4134.4819 - mae: 33.4974 - val_loss: 2389.9295 - val_mse: 2389.9294 - val_mae: 32.0291\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4176.7671 - mse: 4176.7671 - mae: 34.3900 - val_loss: 2329.2332 - val_mse: 2329.2329 - val_mae: 31.8364\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4389.2036 - mse: 4389.2031 - mae: 34.9711 - val_loss: 2376.7492 - val_mse: 2376.7493 - val_mae: 31.9859\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 3993.3034 - mse: 3993.3030 - mae: 32.7929 - val_loss: 2277.0190 - val_mse: 2277.0190 - val_mae: 31.6929\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 622us/step - loss: 4267.3843 - mse: 4267.3848 - mae: 34.6923 - val_loss: 2384.0878 - val_mse: 2384.0879 - val_mae: 32.0124\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4013.0243 - mse: 4013.0242 - mae: 34.4113 - val_loss: 2362.3995 - val_mse: 2362.3994 - val_mae: 31.9411\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4369.8699 - mse: 4369.8701 - mae: 34.7866 - val_loss: 2310.3771 - val_mse: 2310.3770 - val_mae: 31.7873\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 3985.0633 - mse: 3985.0632 - mae: 34.7403 - val_loss: 2320.4045 - val_mse: 2320.4045 - val_mae: 31.8153\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4237.9257 - mse: 4237.9253 - mae: 34.8500 - val_loss: 2377.8695 - val_mse: 2377.8694 - val_mae: 31.9933\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4152.1921 - mse: 4152.1919 - mae: 34.0489 - val_loss: 2383.4164 - val_mse: 2383.4163 - val_mae: 32.0139\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 671us/step - loss: 4125.2468 - mse: 4125.2471 - mae: 34.1656 - val_loss: 2397.9864 - val_mse: 2397.9866 - val_mae: 32.0621\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4181.5401 - mse: 4181.5400 - mae: 35.0864 - val_loss: 2357.0486 - val_mse: 2357.0488 - val_mae: 31.9256\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 534us/step - loss: 4030.8291 - mse: 4030.8296 - mae: 33.9644 - val_loss: 2323.8745 - val_mse: 2323.8745 - val_mae: 31.8312\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 544us/step - loss: 4173.3154 - mse: 4173.3154 - mae: 34.0741 - val_loss: 2356.8995 - val_mse: 2356.8994 - val_mae: 31.9289\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 3967.4784 - mse: 3967.4780 - mae: 33.4013 - val_loss: 2352.4161 - val_mse: 2352.4165 - val_mae: 31.9154\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4156.5324 - mse: 4156.5322 - mae: 34.4543 - val_loss: 2342.8799 - val_mse: 2342.8799 - val_mae: 31.8858\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 516us/step - loss: 4140.9299 - mse: 4140.9297 - mae: 33.8520 - val_loss: 2334.0410 - val_mse: 2334.0410 - val_mae: 31.8606\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4146.5463 - mse: 4146.5459 - mae: 34.8793 - val_loss: 2399.0407 - val_mse: 2399.0408 - val_mae: 32.0724\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4113.9394 - mse: 4113.9395 - mae: 34.1145 - val_loss: 2383.8274 - val_mse: 2383.8271 - val_mae: 32.0230\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 494us/step - loss: 4136.0341 - mse: 4136.0342 - mae: 33.9108 - val_loss: 2376.2847 - val_mse: 2376.2844 - val_mae: 31.9978\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4143.7170 - mse: 4143.7163 - mae: 34.0070 - val_loss: 2365.1837 - val_mse: 2365.1836 - val_mae: 31.9596\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4069.0673 - mse: 4069.0671 - mae: 34.4172 - val_loss: 2323.5464 - val_mse: 2323.5461 - val_mae: 31.8373\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4111.3996 - mse: 4111.3999 - mae: 34.3659 - val_loss: 2262.9054 - val_mse: 2262.9055 - val_mae: 31.6774\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 4044.7370 - mse: 4044.7363 - mae: 33.4948 - val_loss: 2371.8232 - val_mse: 2371.8235 - val_mae: 31.9901\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4321.4222 - mse: 4321.4224 - mae: 35.1227 - val_loss: 2391.7657 - val_mse: 2391.7659 - val_mae: 32.0577\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 614us/step - loss: 4116.8025 - mse: 4116.8022 - mae: 34.0307 - val_loss: 2323.1395 - val_mse: 2323.1394 - val_mae: 31.8429\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4090.4974 - mse: 4090.4976 - mae: 33.4244 - val_loss: 2373.4107 - val_mse: 2373.4106 - val_mae: 32.0001\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 3971.9897 - mse: 3971.9895 - mae: 33.1890 - val_loss: 2327.4016 - val_mse: 2327.4016 - val_mae: 31.8572\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4095.8870 - mse: 4095.8870 - mae: 33.9109 - val_loss: 2329.1201 - val_mse: 2329.1201 - val_mae: 31.8613\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 662us/step - loss: 4110.3652 - mse: 4110.3652 - mae: 34.1187 - val_loss: 2370.1263 - val_mse: 2370.1260 - val_mae: 31.9860\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 697us/step - loss: 4217.7592 - mse: 4217.7598 - mae: 34.1579 - val_loss: 2335.8134 - val_mse: 2335.8132 - val_mae: 31.8794\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 669us/step - loss: 4214.9808 - mse: 4214.9810 - mae: 34.4792 - val_loss: 2370.8701 - val_mse: 2370.8701 - val_mae: 31.9891\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4016.6775 - mse: 4016.6780 - mae: 33.7289 - val_loss: 2366.0597 - val_mse: 2366.0596 - val_mae: 31.9716\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 700us/step - loss: 3960.0999 - mse: 3960.0999 - mae: 33.4857 - val_loss: 2330.8553 - val_mse: 2330.8552 - val_mae: 31.8665\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4139.9213 - mse: 4139.9209 - mae: 33.5107 - val_loss: 2364.7967 - val_mse: 2364.7966 - val_mae: 31.9686\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4044.9536 - mse: 4044.9536 - mae: 33.5000 - val_loss: 2313.3436 - val_mse: 2313.3435 - val_mae: 31.8203\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4012.6305 - mse: 4012.6306 - mae: 33.4736 - val_loss: 2290.8496 - val_mse: 2290.8496 - val_mae: 31.7618\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 3827.7767 - mse: 3827.7776 - mae: 32.1430 - val_loss: 2260.2195 - val_mse: 2260.2195 - val_mae: 31.6777\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 705us/step - loss: 4090.1748 - mse: 4090.1750 - mae: 33.2092 - val_loss: 2346.5069 - val_mse: 2346.5071 - val_mae: 31.9159\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 699us/step - loss: 4125.3387 - mse: 4125.3384 - mae: 34.1509 - val_loss: 2436.4535 - val_mse: 2436.4536 - val_mae: 32.2082\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4162.8090 - mse: 4162.8081 - mae: 33.6190 - val_loss: 2436.4940 - val_mse: 2436.4937 - val_mae: 32.2100\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4209.3418 - mse: 4209.3418 - mae: 33.6399 - val_loss: 2349.8709 - val_mse: 2349.8711 - val_mae: 31.9294\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4096.8579 - mse: 4096.8579 - mae: 33.6123 - val_loss: 2363.8651 - val_mse: 2363.8650 - val_mae: 31.9722\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 4132.3973 - mse: 4132.3970 - mae: 33.3829 - val_loss: 2320.0935 - val_mse: 2320.0938 - val_mae: 31.8453\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 4049.5428 - mse: 4049.5437 - mae: 33.8857 - val_loss: 2308.7921 - val_mse: 2308.7920 - val_mae: 31.8157\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4035.1775 - mse: 4035.1775 - mae: 33.1397 - val_loss: 2292.9777 - val_mse: 2292.9778 - val_mae: 31.7728\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4128.6985 - mse: 4128.6987 - mae: 33.5777 - val_loss: 2362.9733 - val_mse: 2362.9731 - val_mae: 31.9661\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4053.7364 - mse: 4053.7358 - mae: 34.0620 - val_loss: 2332.6468 - val_mse: 2332.6467 - val_mae: 31.8782\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 686us/step - loss: 4235.7906 - mse: 4235.7905 - mae: 34.3123 - val_loss: 2337.8804 - val_mse: 2337.8806 - val_mae: 31.8944\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 670us/step - loss: 4075.1670 - mse: 4075.1670 - mae: 33.7857 - val_loss: 2357.4688 - val_mse: 2357.4688 - val_mae: 31.9504\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 4051.9256 - mse: 4051.9250 - mae: 33.7879 - val_loss: 2368.4719 - val_mse: 2368.4719 - val_mae: 31.9850\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 686us/step - loss: 4090.0758 - mse: 4090.0759 - mae: 33.3868 - val_loss: 2378.7168 - val_mse: 2378.7168 - val_mae: 32.0181\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4089.6146 - mse: 4089.6147 - mae: 33.1905 - val_loss: 2363.6738 - val_mse: 2363.6738 - val_mae: 31.9682\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 3967.1610 - mse: 3967.1611 - mae: 33.1699 - val_loss: 2361.3439 - val_mse: 2361.3440 - val_mae: 31.9649\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 535us/step - loss: 3835.9188 - mse: 3835.9189 - mae: 32.5439 - val_loss: 2343.4754 - val_mse: 2343.4753 - val_mae: 31.9115\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4096.1786 - mse: 4096.1792 - mae: 33.8676 - val_loss: 2307.5295 - val_mse: 2307.5295 - val_mae: 31.8125\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4033.8204 - mse: 4033.8208 - mae: 33.2840 - val_loss: 2331.5036 - val_mse: 2331.5034 - val_mae: 31.8797\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 4135.2334 - mse: 4135.2339 - mae: 33.6146 - val_loss: 2363.8326 - val_mse: 2363.8325 - val_mae: 31.9763\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4014.0613 - mse: 4014.0615 - mae: 33.1470 - val_loss: 2372.0130 - val_mse: 2372.0129 - val_mae: 32.0050\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 3900.4743 - mse: 3900.4741 - mae: 32.5159 - val_loss: 2341.0737 - val_mse: 2341.0737 - val_mae: 31.9150\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4185.3856 - mse: 4185.3857 - mae: 33.9819 - val_loss: 2356.5905 - val_mse: 2356.5906 - val_mae: 31.9613\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4128.4898 - mse: 4128.4897 - mae: 33.5987 - val_loss: 2392.6596 - val_mse: 2392.6597 - val_mae: 32.0774\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4153.8393 - mse: 4153.8394 - mae: 34.1120 - val_loss: 2349.1420 - val_mse: 2349.1421 - val_mae: 31.9398\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 701us/step - loss: 4203.6183 - mse: 4203.6187 - mae: 34.0728 - val_loss: 2340.0866 - val_mse: 2340.0867 - val_mae: 31.9139\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 728us/step - loss: 4076.4999 - mse: 4076.4998 - mae: 33.1097 - val_loss: 2349.3643 - val_mse: 2349.3643 - val_mae: 31.9434\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 715us/step - loss: 3967.3358 - mse: 3967.3357 - mae: 33.3694 - val_loss: 2344.4413 - val_mse: 2344.4409 - val_mae: 31.9311\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 643us/step - loss: 4032.6094 - mse: 4032.6091 - mae: 32.9441 - val_loss: 2312.0024 - val_mse: 2312.0024 - val_mae: 31.8414\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 681us/step - loss: 4043.9856 - mse: 4043.9856 - mae: 33.2607 - val_loss: 2361.9251 - val_mse: 2361.9250 - val_mae: 31.9845\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 3941.2077 - mse: 3941.2080 - mae: 32.4189 - val_loss: 2332.6387 - val_mse: 2332.6389 - val_mae: 31.9007\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 3955.8501 - mse: 3955.8499 - mae: 33.1915 - val_loss: 2294.2657 - val_mse: 2294.2654 - val_mae: 31.7943\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 711us/step - loss: 4075.8492 - mse: 4075.8491 - mae: 34.2151 - val_loss: 2394.4302 - val_mse: 2394.4302 - val_mae: 32.0877\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3495.0382 - mse: 3495.0371 - mae: 33.1389 - val_loss: 1470.1678 - val_mse: 1470.1677 - val_mae: 25.8153\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3434.9229 - mse: 3434.9231 - mae: 32.8113 - val_loss: 1474.3467 - val_mse: 1474.3468 - val_mae: 26.5395\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 687us/step - loss: 3453.5732 - mse: 3453.5735 - mae: 33.1513 - val_loss: 1471.9179 - val_mse: 1471.9180 - val_mae: 26.0566\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3504.1903 - mse: 3504.1902 - mae: 33.8080 - val_loss: 1471.8362 - val_mse: 1471.8361 - val_mae: 25.6883\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3401.7592 - mse: 3401.7585 - mae: 32.3031 - val_loss: 1474.5562 - val_mse: 1474.5562 - val_mae: 26.3362\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3335.3969 - mse: 3335.3972 - mae: 32.5201 - val_loss: 1472.8174 - val_mse: 1472.8175 - val_mae: 25.7342\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3424.0748 - mse: 3424.0752 - mae: 32.7576 - val_loss: 1473.5856 - val_mse: 1473.5857 - val_mae: 25.8914\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3395.3896 - mse: 3395.3896 - mae: 32.7703 - val_loss: 1475.5995 - val_mse: 1475.5996 - val_mae: 26.2728\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3330.0407 - mse: 3330.0408 - mae: 33.1309 - val_loss: 1474.6777 - val_mse: 1474.6776 - val_mae: 25.5166\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3343.8391 - mse: 3343.8394 - mae: 32.4399 - val_loss: 1475.8374 - val_mse: 1475.8374 - val_mae: 26.1308\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 654us/step - loss: 3393.4986 - mse: 3393.4990 - mae: 32.7966 - val_loss: 1476.0483 - val_mse: 1476.0483 - val_mae: 25.9621\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 699us/step - loss: 3449.6840 - mse: 3449.6846 - mae: 33.4989 - val_loss: 1477.5829 - val_mse: 1477.5830 - val_mae: 26.1061\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3494.9652 - mse: 3494.9663 - mae: 33.0512 - val_loss: 1478.5458 - val_mse: 1478.5459 - val_mae: 26.0833\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3440.7600 - mse: 3440.7605 - mae: 32.8754 - val_loss: 1478.9771 - val_mse: 1478.9773 - val_mae: 25.6013\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3306.8468 - mse: 3306.8469 - mae: 31.9106 - val_loss: 1480.2510 - val_mse: 1480.2510 - val_mae: 26.1451\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3391.7009 - mse: 3391.7007 - mae: 32.4814 - val_loss: 1480.5026 - val_mse: 1480.5026 - val_mae: 25.9660\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3385.0956 - mse: 3385.0947 - mae: 33.0122 - val_loss: 1480.9943 - val_mse: 1480.9941 - val_mae: 25.8943\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3400.8306 - mse: 3400.8308 - mae: 32.9606 - val_loss: 1481.5153 - val_mse: 1481.5155 - val_mae: 25.7834\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3372.7153 - mse: 3372.7161 - mae: 32.2310 - val_loss: 1482.5584 - val_mse: 1482.5585 - val_mae: 26.1636\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3346.4202 - mse: 3346.4202 - mae: 32.5308 - val_loss: 1483.5898 - val_mse: 1483.5897 - val_mae: 26.2502\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3375.9523 - mse: 3375.9521 - mae: 33.0897 - val_loss: 1482.4819 - val_mse: 1482.4819 - val_mae: 25.7987\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3387.5038 - mse: 3387.5044 - mae: 32.9325 - val_loss: 1482.0827 - val_mse: 1482.0828 - val_mae: 25.6962\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3370.0159 - mse: 3370.0164 - mae: 32.8868 - val_loss: 1484.3015 - val_mse: 1484.3016 - val_mae: 26.4133\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 703us/step - loss: 3344.6408 - mse: 3344.6404 - mae: 32.2153 - val_loss: 1484.2130 - val_mse: 1484.2129 - val_mae: 25.2824\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3332.6010 - mse: 3332.6011 - mae: 32.1972 - val_loss: 1482.7188 - val_mse: 1482.7189 - val_mae: 25.8063\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3371.8407 - mse: 3371.8411 - mae: 33.1458 - val_loss: 1485.0868 - val_mse: 1485.0869 - val_mae: 26.4323\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3414.2922 - mse: 3414.2922 - mae: 32.8754 - val_loss: 1485.3422 - val_mse: 1485.3423 - val_mae: 26.4289\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3450.1327 - mse: 3450.1328 - mae: 33.0913 - val_loss: 1483.2698 - val_mse: 1483.2699 - val_mae: 25.7204\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3391.1830 - mse: 3391.1833 - mae: 32.5885 - val_loss: 1485.8668 - val_mse: 1485.8668 - val_mae: 26.4540\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3395.1378 - mse: 3395.1375 - mae: 32.4596 - val_loss: 1484.7601 - val_mse: 1484.7603 - val_mae: 26.1008\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 470us/step - loss: 3521.8975 - mse: 3521.8970 - mae: 33.4248 - val_loss: 1485.9142 - val_mse: 1485.9141 - val_mae: 25.3814\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 526us/step - loss: 3418.3494 - mse: 3418.3496 - mae: 32.7131 - val_loss: 1485.3652 - val_mse: 1485.3651 - val_mae: 25.8929\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3312.1312 - mse: 3312.1318 - mae: 32.3125 - val_loss: 1486.0156 - val_mse: 1486.0155 - val_mae: 26.0243\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3341.1724 - mse: 3341.1726 - mae: 32.1293 - val_loss: 1485.9362 - val_mse: 1485.9363 - val_mae: 25.6798\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 540us/step - loss: 3346.5896 - mse: 3346.5901 - mae: 32.1687 - val_loss: 1487.1591 - val_mse: 1487.1593 - val_mae: 26.2939\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3272.1423 - mse: 3272.1426 - mae: 31.8773 - val_loss: 1487.0408 - val_mse: 1487.0408 - val_mae: 26.2137\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 675us/step - loss: 3379.4222 - mse: 3379.4216 - mae: 32.3154 - val_loss: 1487.8791 - val_mse: 1487.8790 - val_mae: 26.3351\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3412.8114 - mse: 3412.8108 - mae: 32.4505 - val_loss: 1486.9975 - val_mse: 1486.9973 - val_mae: 26.0744\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3403.4437 - mse: 3403.4431 - mae: 32.3707 - val_loss: 1486.9113 - val_mse: 1486.9114 - val_mae: 25.9178\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3341.8751 - mse: 3341.8762 - mae: 32.5189 - val_loss: 1486.8890 - val_mse: 1486.8892 - val_mae: 25.7855\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3492.8285 - mse: 3492.8281 - mae: 32.8137 - val_loss: 1487.2410 - val_mse: 1487.2410 - val_mae: 25.8976\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3232.1702 - mse: 3232.1689 - mae: 32.0718 - val_loss: 1487.1757 - val_mse: 1487.1758 - val_mae: 25.8652\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3396.0616 - mse: 3396.0615 - mae: 32.6517 - val_loss: 1488.1416 - val_mse: 1488.1416 - val_mae: 26.2069\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3442.5829 - mse: 3442.5820 - mae: 32.6964 - val_loss: 1487.9477 - val_mse: 1487.9478 - val_mae: 26.0768\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3352.7442 - mse: 3352.7456 - mae: 32.7267 - val_loss: 1488.3357 - val_mse: 1488.3356 - val_mae: 26.2098\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3349.0298 - mse: 3349.0298 - mae: 33.1312 - val_loss: 1488.3913 - val_mse: 1488.3914 - val_mae: 26.0153\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3284.4468 - mse: 3284.4468 - mae: 32.7809 - val_loss: 1488.6847 - val_mse: 1488.6847 - val_mae: 25.5912\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3281.2797 - mse: 3281.2788 - mae: 31.8438 - val_loss: 1489.7765 - val_mse: 1489.7764 - val_mae: 26.2861\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3287.4028 - mse: 3287.4036 - mae: 31.6688 - val_loss: 1490.8039 - val_mse: 1490.8038 - val_mae: 26.4269\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3200.3517 - mse: 3200.3511 - mae: 31.8954 - val_loss: 1488.9916 - val_mse: 1488.9917 - val_mae: 26.0123\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3295.6463 - mse: 3295.6467 - mae: 32.7165 - val_loss: 1489.5978 - val_mse: 1489.5978 - val_mae: 26.1976\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3297.4037 - mse: 3297.4041 - mae: 32.0207 - val_loss: 1488.7607 - val_mse: 1488.7607 - val_mae: 25.9164\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 696us/step - loss: 3388.1075 - mse: 3388.1077 - mae: 32.5708 - val_loss: 1489.1214 - val_mse: 1489.1213 - val_mae: 26.0117\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3357.5506 - mse: 3357.5500 - mae: 32.0490 - val_loss: 1490.2416 - val_mse: 1490.2417 - val_mae: 26.3426\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3399.9664 - mse: 3399.9666 - mae: 31.8375 - val_loss: 1488.7204 - val_mse: 1488.7206 - val_mae: 25.6204\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3289.0646 - mse: 3289.0649 - mae: 31.9072 - val_loss: 1488.4365 - val_mse: 1488.4365 - val_mae: 26.0057\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3170.1208 - mse: 3170.1211 - mae: 31.2224 - val_loss: 1493.7321 - val_mse: 1493.7322 - val_mae: 26.7839\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3354.1598 - mse: 3354.1594 - mae: 32.3407 - val_loss: 1489.7072 - val_mse: 1489.7073 - val_mae: 26.1393\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3305.9111 - mse: 3305.9114 - mae: 31.7109 - val_loss: 1493.5448 - val_mse: 1493.5447 - val_mae: 26.7358\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3382.5799 - mse: 3382.5796 - mae: 32.5116 - val_loss: 1489.3064 - val_mse: 1489.3062 - val_mae: 26.1901\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 659us/step - loss: 3253.6423 - mse: 3253.6414 - mae: 31.9033 - val_loss: 1488.3867 - val_mse: 1488.3867 - val_mae: 25.7740\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3298.3312 - mse: 3298.3320 - mae: 31.7363 - val_loss: 1489.3167 - val_mse: 1489.3167 - val_mae: 26.2064\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3233.7762 - mse: 3233.7766 - mae: 31.6441 - val_loss: 1489.6133 - val_mse: 1489.6135 - val_mae: 26.2504\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 741us/step - loss: 3126.6045 - mse: 3126.6050 - mae: 30.8336 - val_loss: 1492.4460 - val_mse: 1492.4462 - val_mae: 26.6935\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3350.7639 - mse: 3350.7642 - mae: 32.1964 - val_loss: 1489.9103 - val_mse: 1489.9103 - val_mae: 26.3191\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3268.7732 - mse: 3268.7722 - mae: 32.1156 - val_loss: 1488.9266 - val_mse: 1488.9265 - val_mae: 25.5245\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3376.6131 - mse: 3376.6133 - mae: 32.9067 - val_loss: 1488.5998 - val_mse: 1488.6000 - val_mae: 25.8025\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3251.5465 - mse: 3251.5469 - mae: 30.9439 - val_loss: 1489.2867 - val_mse: 1489.2866 - val_mae: 25.9775\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3302.8671 - mse: 3302.8682 - mae: 31.7345 - val_loss: 1488.7494 - val_mse: 1488.7491 - val_mae: 25.7696\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3428.7208 - mse: 3428.7212 - mae: 32.9783 - val_loss: 1488.5401 - val_mse: 1488.5402 - val_mae: 25.7055\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3281.7002 - mse: 3281.6992 - mae: 32.0289 - val_loss: 1489.0592 - val_mse: 1489.0592 - val_mae: 26.1976\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3288.5622 - mse: 3288.5615 - mae: 32.0085 - val_loss: 1488.5505 - val_mse: 1488.5504 - val_mae: 25.6856\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3251.9512 - mse: 3251.9512 - mae: 31.5368 - val_loss: 1488.4890 - val_mse: 1488.4890 - val_mae: 25.9456\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3249.8239 - mse: 3249.8230 - mae: 31.3170 - val_loss: 1488.2590 - val_mse: 1488.2590 - val_mae: 25.7989\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 741us/step - loss: 3389.6761 - mse: 3389.6765 - mae: 32.3778 - val_loss: 1488.4409 - val_mse: 1488.4410 - val_mae: 25.9916\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3303.3622 - mse: 3303.3625 - mae: 32.4242 - val_loss: 1489.5273 - val_mse: 1489.5272 - val_mae: 26.2561\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3347.5129 - mse: 3347.5120 - mae: 31.8456 - val_loss: 1489.5425 - val_mse: 1489.5426 - val_mae: 26.3344\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3327.4136 - mse: 3327.4133 - mae: 32.0615 - val_loss: 1491.2216 - val_mse: 1491.2216 - val_mae: 26.6342\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 660us/step - loss: 3294.8540 - mse: 3294.8533 - mae: 32.0688 - val_loss: 1489.4621 - val_mse: 1489.4620 - val_mae: 26.3947\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 656us/step - loss: 3397.3070 - mse: 3397.3064 - mae: 32.1706 - val_loss: 1487.3856 - val_mse: 1487.3857 - val_mae: 26.0958\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2892.9058 - mse: 2892.9072 - mae: 31.5935 - val_loss: 1094.1277 - val_mse: 1094.1277 - val_mae: 24.3542\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2892.4129 - mse: 2892.4126 - mae: 30.7935 - val_loss: 1092.9122 - val_mse: 1092.9124 - val_mae: 24.4131\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2918.1221 - mse: 2918.1230 - mae: 30.9288 - val_loss: 1092.3181 - val_mse: 1092.3180 - val_mae: 24.2806\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 3002.2049 - mse: 3002.2046 - mae: 31.3444 - val_loss: 1094.5594 - val_mse: 1094.5593 - val_mae: 24.1333\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2951.5374 - mse: 2951.5369 - mae: 31.1348 - val_loss: 1093.4400 - val_mse: 1093.4398 - val_mae: 24.1903\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 3011.2601 - mse: 3011.2595 - mae: 31.4392 - val_loss: 1095.1049 - val_mse: 1095.1049 - val_mae: 24.0483\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2883.5186 - mse: 2883.5183 - mae: 31.1853 - val_loss: 1091.6583 - val_mse: 1091.6582 - val_mae: 24.2726\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2917.6768 - mse: 2917.6765 - mae: 30.8771 - val_loss: 1090.1582 - val_mse: 1090.1582 - val_mae: 24.3620\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2986.7704 - mse: 2986.7712 - mae: 31.2577 - val_loss: 1089.1770 - val_mse: 1089.1770 - val_mae: 24.4919\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2964.9732 - mse: 2964.9731 - mae: 31.0326 - val_loss: 1090.1308 - val_mse: 1090.1309 - val_mae: 24.2020\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 2909.5848 - mse: 2909.5850 - mae: 31.4519 - val_loss: 1090.3570 - val_mse: 1090.3569 - val_mae: 24.1102\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2925.4012 - mse: 2925.4019 - mae: 31.5958 - val_loss: 1087.2234 - val_mse: 1087.2234 - val_mae: 24.6347\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2921.0252 - mse: 2921.0251 - mae: 31.0416 - val_loss: 1087.0585 - val_mse: 1087.0585 - val_mae: 24.8384\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2860.1318 - mse: 2860.1313 - mae: 30.7688 - val_loss: 1087.1733 - val_mse: 1087.1733 - val_mae: 24.2070\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2927.9054 - mse: 2927.9060 - mae: 30.8959 - val_loss: 1090.9888 - val_mse: 1090.9889 - val_mae: 23.9305\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 673us/step - loss: 2906.6612 - mse: 2906.6609 - mae: 31.0459 - val_loss: 1086.5055 - val_mse: 1086.5054 - val_mae: 24.2810\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2992.7599 - mse: 2992.7607 - mae: 31.4493 - val_loss: 1087.0463 - val_mse: 1087.0463 - val_mae: 24.1480\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2987.3481 - mse: 2987.3481 - mae: 31.4294 - val_loss: 1086.2117 - val_mse: 1086.2118 - val_mae: 24.3734\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2884.0645 - mse: 2884.0645 - mae: 31.2305 - val_loss: 1086.0333 - val_mse: 1086.0332 - val_mae: 24.3846\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 663us/step - loss: 2913.4498 - mse: 2913.4500 - mae: 31.0188 - val_loss: 1085.6805 - val_mse: 1085.6805 - val_mae: 24.4430\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2897.8357 - mse: 2897.8354 - mae: 31.1829 - val_loss: 1085.7029 - val_mse: 1085.7030 - val_mae: 24.3994\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2910.3740 - mse: 2910.3743 - mae: 31.3958 - val_loss: 1087.7737 - val_mse: 1087.7738 - val_mae: 24.0662\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2921.6310 - mse: 2921.6311 - mae: 30.8394 - val_loss: 1085.4467 - val_mse: 1085.4467 - val_mae: 24.3607\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2911.0516 - mse: 2911.0518 - mae: 30.9412 - val_loss: 1085.3990 - val_mse: 1085.3989 - val_mae: 24.7077\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2904.7603 - mse: 2904.7603 - mae: 31.6823 - val_loss: 1088.1979 - val_mse: 1088.1980 - val_mae: 23.9902\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 553us/step - loss: 3027.8988 - mse: 3027.8984 - mae: 31.7850 - val_loss: 1087.4331 - val_mse: 1087.4331 - val_mae: 23.9885\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 687us/step - loss: 2882.4388 - mse: 2882.4385 - mae: 31.2631 - val_loss: 1087.3862 - val_mse: 1087.3861 - val_mae: 23.9860\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2910.7283 - mse: 2910.7280 - mae: 30.7057 - val_loss: 1084.7999 - val_mse: 1084.7998 - val_mae: 24.8504\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2801.6728 - mse: 2801.6729 - mae: 30.9342 - val_loss: 1084.2214 - val_mse: 1084.2214 - val_mae: 24.4096\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2946.1634 - mse: 2946.1628 - mae: 31.7067 - val_loss: 1084.4048 - val_mse: 1084.4049 - val_mae: 24.2723\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2909.2343 - mse: 2909.2341 - mae: 30.8062 - val_loss: 1084.0232 - val_mse: 1084.0233 - val_mae: 24.5249\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2930.7194 - mse: 2930.7202 - mae: 31.0689 - val_loss: 1084.3007 - val_mse: 1084.3008 - val_mae: 24.3876\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2953.0851 - mse: 2953.0842 - mae: 31.0760 - val_loss: 1085.9475 - val_mse: 1085.9475 - val_mae: 24.0545\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2889.4138 - mse: 2889.4136 - mae: 31.0553 - val_loss: 1083.8029 - val_mse: 1083.8030 - val_mae: 24.3768\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2947.3373 - mse: 2947.3367 - mae: 31.3730 - val_loss: 1083.4691 - val_mse: 1083.4692 - val_mae: 24.2942\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2846.3176 - mse: 2846.3179 - mae: 30.9788 - val_loss: 1083.0625 - val_mse: 1083.0624 - val_mae: 24.4301\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2849.1558 - mse: 2849.1558 - mae: 30.8344 - val_loss: 1082.9586 - val_mse: 1082.9585 - val_mae: 24.3056\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2910.8954 - mse: 2910.8953 - mae: 31.0875 - val_loss: 1087.3846 - val_mse: 1087.3848 - val_mae: 23.8656\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 668us/step - loss: 2826.3325 - mse: 2826.3325 - mae: 30.3760 - val_loss: 1082.6851 - val_mse: 1082.6852 - val_mae: 24.4251\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 508us/step - loss: 2820.3298 - mse: 2820.3296 - mae: 30.5757 - val_loss: 1083.3589 - val_mse: 1083.3588 - val_mae: 24.1778\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2868.7811 - mse: 2868.7817 - mae: 30.8000 - val_loss: 1083.2311 - val_mse: 1083.2311 - val_mae: 24.9087\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2866.1072 - mse: 2866.1069 - mae: 30.9271 - val_loss: 1082.0030 - val_mse: 1082.0031 - val_mae: 24.4817\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2846.8836 - mse: 2846.8833 - mae: 30.9778 - val_loss: 1081.7940 - val_mse: 1081.7941 - val_mae: 24.3158\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2970.4662 - mse: 2970.4663 - mae: 31.1225 - val_loss: 1081.3034 - val_mse: 1081.3033 - val_mae: 24.3954\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2900.3834 - mse: 2900.3835 - mae: 30.7424 - val_loss: 1081.3862 - val_mse: 1081.3862 - val_mae: 24.2624\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 686us/step - loss: 2887.1337 - mse: 2887.1338 - mae: 30.6033 - val_loss: 1081.2669 - val_mse: 1081.2667 - val_mae: 24.1667\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2809.5169 - mse: 2809.5183 - mae: 30.4855 - val_loss: 1080.6682 - val_mse: 1080.6682 - val_mae: 24.2236\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2911.6096 - mse: 2911.6101 - mae: 30.6594 - val_loss: 1080.5049 - val_mse: 1080.5049 - val_mae: 24.7769\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 689us/step - loss: 2809.9199 - mse: 2809.9189 - mae: 30.6890 - val_loss: 1079.9301 - val_mse: 1079.9301 - val_mae: 24.3486\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2925.3479 - mse: 2925.3477 - mae: 30.8223 - val_loss: 1080.0166 - val_mse: 1080.0165 - val_mae: 24.2392\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 3013.7770 - mse: 3013.7764 - mae: 32.0247 - val_loss: 1083.9740 - val_mse: 1083.9741 - val_mae: 23.8421\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2908.6540 - mse: 2908.6536 - mae: 30.8003 - val_loss: 1080.2265 - val_mse: 1080.2266 - val_mae: 24.2624\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2913.9037 - mse: 2913.9041 - mae: 31.0685 - val_loss: 1080.7185 - val_mse: 1080.7184 - val_mae: 24.1544\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2937.7938 - mse: 2937.7942 - mae: 30.3527 - val_loss: 1079.6523 - val_mse: 1079.6523 - val_mae: 24.6562\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2903.8153 - mse: 2903.8147 - mae: 31.4144 - val_loss: 1080.2955 - val_mse: 1080.2955 - val_mae: 24.2070\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 523us/step - loss: 2861.6368 - mse: 2861.6360 - mae: 30.6655 - val_loss: 1079.4747 - val_mse: 1079.4747 - val_mae: 24.5027\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2891.6656 - mse: 2891.6665 - mae: 30.7311 - val_loss: 1081.7453 - val_mse: 1081.7452 - val_mae: 23.9806\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2868.6977 - mse: 2868.6982 - mae: 30.3080 - val_loss: 1079.0880 - val_mse: 1079.0879 - val_mae: 24.4609\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2889.2967 - mse: 2889.2974 - mae: 31.0374 - val_loss: 1078.7741 - val_mse: 1078.7742 - val_mae: 24.4238\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 678us/step - loss: 2866.7236 - mse: 2866.7244 - mae: 30.7296 - val_loss: 1079.1500 - val_mse: 1079.1501 - val_mae: 24.2250\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 656us/step - loss: 2889.3901 - mse: 2889.3906 - mae: 31.0903 - val_loss: 1078.7759 - val_mse: 1078.7758 - val_mae: 24.2911\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2913.2824 - mse: 2913.2825 - mae: 31.1197 - val_loss: 1078.4201 - val_mse: 1078.4200 - val_mae: 24.4992\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2897.8179 - mse: 2897.8188 - mae: 30.9790 - val_loss: 1078.9946 - val_mse: 1078.9948 - val_mae: 24.1719\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2926.2782 - mse: 2926.2778 - mae: 31.2653 - val_loss: 1079.1984 - val_mse: 1079.1984 - val_mae: 24.1278\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 672us/step - loss: 2861.3507 - mse: 2861.3506 - mae: 30.7019 - val_loss: 1077.8202 - val_mse: 1077.8203 - val_mae: 24.5045\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2906.4567 - mse: 2906.4570 - mae: 30.7228 - val_loss: 1079.0032 - val_mse: 1079.0032 - val_mae: 24.1700\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 679us/step - loss: 2878.0283 - mse: 2878.0283 - mae: 30.4169 - val_loss: 1077.9478 - val_mse: 1077.9478 - val_mae: 24.4998\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2848.2338 - mse: 2848.2334 - mae: 30.1461 - val_loss: 1077.7271 - val_mse: 1077.7271 - val_mae: 24.3774\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - ETA: 0s - loss: 2946.0665 - mse: 2946.0662 - mae: 30.98 - 1s 608us/step - loss: 2929.4373 - mse: 2929.4370 - mae: 31.0364 - val_loss: 1077.4487 - val_mse: 1077.4487 - val_mae: 24.3115\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 641us/step - loss: 2903.3388 - mse: 2903.3389 - mae: 30.7686 - val_loss: 1077.2987 - val_mse: 1077.2987 - val_mae: 24.3281\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2876.7587 - mse: 2876.7590 - mae: 30.5359 - val_loss: 1077.0508 - val_mse: 1077.0509 - val_mae: 24.7695\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2819.4102 - mse: 2819.4102 - mae: 30.2673 - val_loss: 1076.4902 - val_mse: 1076.4904 - val_mae: 24.4287\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2857.8177 - mse: 2857.8176 - mae: 30.6714 - val_loss: 1076.4249 - val_mse: 1076.4248 - val_mae: 24.2807\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2933.9901 - mse: 2933.9902 - mae: 31.0717 - val_loss: 1076.7767 - val_mse: 1076.7769 - val_mae: 24.2434\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 2735.9220 - mse: 2735.9221 - mae: 30.2703 - val_loss: 1076.6465 - val_mse: 1076.6465 - val_mae: 24.2496\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2873.7130 - mse: 2873.7129 - mae: 30.9491 - val_loss: 1075.6897 - val_mse: 1075.6896 - val_mae: 24.4684\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2824.5664 - mse: 2824.5667 - mae: 30.3871 - val_loss: 1075.4910 - val_mse: 1075.4910 - val_mae: 24.5066\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 553us/step - loss: 2855.7702 - mse: 2855.7717 - mae: 30.5997 - val_loss: 1075.1446 - val_mse: 1075.1447 - val_mae: 24.5933\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2803.2245 - mse: 2803.2241 - mae: 30.0712 - val_loss: 1075.0476 - val_mse: 1075.0476 - val_mae: 24.5812\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2916.3999 - mse: 2916.3994 - mae: 31.0083 - val_loss: 1075.3818 - val_mse: 1075.3818 - val_mae: 24.7971\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2597.8340 - mse: 2597.8335 - mae: 30.1242 - val_loss: 1564.5614 - val_mse: 1564.5616 - val_mae: 28.0047\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 665us/step - loss: 2596.0267 - mse: 2596.0266 - mae: 30.4186 - val_loss: 1569.3867 - val_mse: 1569.3866 - val_mae: 27.8356\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 679us/step - loss: 2575.1408 - mse: 2575.1406 - mae: 29.5617 - val_loss: 1560.3649 - val_mse: 1560.3650 - val_mae: 28.1236\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 679us/step - loss: 2606.5498 - mse: 2606.5496 - mae: 29.9809 - val_loss: 1563.9293 - val_mse: 1563.9292 - val_mae: 27.9844\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2596.4077 - mse: 2596.4077 - mae: 30.4523 - val_loss: 1565.6894 - val_mse: 1565.6895 - val_mae: 27.9143\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 680us/step - loss: 2529.1795 - mse: 2529.1790 - mae: 29.9452 - val_loss: 1562.0577 - val_mse: 1562.0577 - val_mae: 28.0299\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2595.0535 - mse: 2595.0540 - mae: 30.1465 - val_loss: 1566.9536 - val_mse: 1566.9535 - val_mae: 27.8614\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2632.6565 - mse: 2632.6567 - mae: 30.5025 - val_loss: 1566.6539 - val_mse: 1566.6541 - val_mae: 27.8588\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2521.0267 - mse: 2521.0264 - mae: 29.8078 - val_loss: 1557.6893 - val_mse: 1557.6892 - val_mae: 28.1656\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2568.7248 - mse: 2568.7244 - mae: 29.9723 - val_loss: 1553.1499 - val_mse: 1553.1500 - val_mae: 28.3919\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2623.4676 - mse: 2623.4678 - mae: 30.1694 - val_loss: 1559.5400 - val_mse: 1559.5399 - val_mae: 28.0730\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2592.6246 - mse: 2592.6243 - mae: 30.2554 - val_loss: 1564.3898 - val_mse: 1564.3898 - val_mae: 27.8733\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2582.3903 - mse: 2582.3906 - mae: 30.0115 - val_loss: 1553.8093 - val_mse: 1553.8094 - val_mae: 28.2684\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2587.2191 - mse: 2587.2190 - mae: 30.0603 - val_loss: 1553.0061 - val_mse: 1553.0059 - val_mae: 28.2960\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2648.6384 - mse: 2648.6389 - mae: 30.3542 - val_loss: 1557.0020 - val_mse: 1557.0020 - val_mae: 28.1159\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 663us/step - loss: 2634.7033 - mse: 2634.7034 - mae: 30.1810 - val_loss: 1558.5824 - val_mse: 1558.5823 - val_mae: 28.0424\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2573.3675 - mse: 2573.3677 - mae: 29.9225 - val_loss: 1559.2796 - val_mse: 1559.2797 - val_mae: 28.0143\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2537.6094 - mse: 2537.6089 - mae: 29.6765 - val_loss: 1556.8667 - val_mse: 1556.8666 - val_mae: 28.0737\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 534us/step - loss: 2634.4203 - mse: 2634.4197 - mae: 30.2322 - val_loss: 1562.7476 - val_mse: 1562.7476 - val_mae: 27.8439\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2551.0577 - mse: 2551.0583 - mae: 30.0922 - val_loss: 1564.8395 - val_mse: 1564.8395 - val_mae: 27.7566\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2545.2653 - mse: 2545.2651 - mae: 30.0269 - val_loss: 1555.2760 - val_mse: 1555.2761 - val_mae: 28.0649\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 508us/step - loss: 2528.1402 - mse: 2528.1396 - mae: 29.7106 - val_loss: 1549.6991 - val_mse: 1549.6991 - val_mae: 28.2970\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 711us/step - loss: 2616.4731 - mse: 2616.4729 - mae: 30.3861 - val_loss: 1552.1430 - val_mse: 1552.1431 - val_mae: 28.1505\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2515.9125 - mse: 2515.9128 - mae: 29.7443 - val_loss: 1541.4997 - val_mse: 1541.4996 - val_mae: 28.7958\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2569.5431 - mse: 2569.5432 - mae: 30.4657 - val_loss: 1550.3956 - val_mse: 1550.3955 - val_mae: 28.1488\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2597.1064 - mse: 2597.1067 - mae: 29.9024 - val_loss: 1554.9161 - val_mse: 1554.9161 - val_mae: 27.9810\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2618.3844 - mse: 2618.3840 - mae: 29.9924 - val_loss: 1558.5439 - val_mse: 1558.5439 - val_mae: 27.8475\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2597.4709 - mse: 2597.4709 - mae: 30.1600 - val_loss: 1551.4655 - val_mse: 1551.4653 - val_mae: 28.0891\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2517.1302 - mse: 2517.1301 - mae: 29.7703 - val_loss: 1541.8591 - val_mse: 1541.8590 - val_mae: 28.5860\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2516.0822 - mse: 2516.0828 - mae: 29.6630 - val_loss: 1543.6193 - val_mse: 1543.6191 - val_mae: 28.4086\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2608.1763 - mse: 2608.1768 - mae: 29.8008 - val_loss: 1546.6687 - val_mse: 1546.6687 - val_mae: 28.2074\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 666us/step - loss: 2519.9485 - mse: 2519.9485 - mae: 29.9416 - val_loss: 1545.1596 - val_mse: 1545.1597 - val_mae: 28.2753\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2593.3830 - mse: 2593.3826 - mae: 30.2312 - val_loss: 1544.6038 - val_mse: 1544.6039 - val_mae: 28.2840\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2559.4617 - mse: 2559.4609 - mae: 29.6524 - val_loss: 1538.2403 - val_mse: 1538.2402 - val_mae: 28.7233\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2635.5722 - mse: 2635.5723 - mae: 29.7629 - val_loss: 1543.3228 - val_mse: 1543.3228 - val_mae: 28.3098\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2518.0555 - mse: 2518.0559 - mae: 29.5865 - val_loss: 1542.5687 - val_mse: 1542.5686 - val_mae: 28.3423\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2494.0252 - mse: 2494.0251 - mae: 29.2577 - val_loss: 1540.9055 - val_mse: 1540.9054 - val_mae: 28.4507\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2516.3205 - mse: 2516.3203 - mae: 29.5716 - val_loss: 1538.7401 - val_mse: 1538.7399 - val_mae: 28.5957\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 715us/step - loss: 2558.3420 - mse: 2558.3413 - mae: 29.9871 - val_loss: 1549.2293 - val_mse: 1549.2292 - val_mae: 28.0063\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 691us/step - loss: 2606.9505 - mse: 2606.9509 - mae: 30.1262 - val_loss: 1544.6283 - val_mse: 1544.6282 - val_mae: 28.2885\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2519.6703 - mse: 2519.6702 - mae: 29.2773 - val_loss: 1541.1282 - val_mse: 1541.1283 - val_mae: 28.4848\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2578.3209 - mse: 2578.3208 - mae: 29.5250 - val_loss: 1543.6070 - val_mse: 1543.6072 - val_mae: 28.3045\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2511.8908 - mse: 2511.8911 - mae: 29.7857 - val_loss: 1540.3647 - val_mse: 1540.3646 - val_mae: 28.4484\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2570.4290 - mse: 2570.4282 - mae: 29.9159 - val_loss: 1540.9341 - val_mse: 1540.9340 - val_mae: 28.3588\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2528.6674 - mse: 2528.6677 - mae: 30.0172 - val_loss: 1538.9742 - val_mse: 1538.9744 - val_mae: 28.4323\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2524.0059 - mse: 2524.0066 - mae: 29.5051 - val_loss: 1539.6576 - val_mse: 1539.6577 - val_mae: 28.3582\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2546.1160 - mse: 2546.1162 - mae: 29.4131 - val_loss: 1534.9015 - val_mse: 1534.9014 - val_mae: 28.5902\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2577.2380 - mse: 2577.2378 - mae: 30.0406 - val_loss: 1534.7271 - val_mse: 1534.7273 - val_mae: 28.5311\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2524.2306 - mse: 2524.2312 - mae: 29.6564 - val_loss: 1532.0654 - val_mse: 1532.0654 - val_mae: 28.7575\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2583.4250 - mse: 2583.4250 - mae: 30.0574 - val_loss: 1535.0670 - val_mse: 1535.0669 - val_mae: 28.4113\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2554.7085 - mse: 2554.7080 - mae: 29.4198 - val_loss: 1531.2991 - val_mse: 1531.2992 - val_mae: 28.7159\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2537.4107 - mse: 2537.4111 - mae: 29.4836 - val_loss: 1531.7104 - val_mse: 1531.7103 - val_mae: 28.6440\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 545us/step - loss: 2502.4398 - mse: 2502.4397 - mae: 29.3563 - val_loss: 1536.6772 - val_mse: 1536.6772 - val_mae: 28.2831\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2561.2285 - mse: 2561.2280 - mae: 29.5896 - val_loss: 1537.8779 - val_mse: 1537.8778 - val_mae: 28.2219\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2497.0531 - mse: 2497.0532 - mae: 29.6919 - val_loss: 1537.7427 - val_mse: 1537.7424 - val_mae: 28.1435\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2587.1220 - mse: 2587.1211 - mae: 30.0759 - val_loss: 1530.5947 - val_mse: 1530.5948 - val_mae: 28.6148\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2523.5540 - mse: 2523.5544 - mae: 29.3740 - val_loss: 1528.8010 - val_mse: 1528.8011 - val_mae: 28.7682\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2563.8072 - mse: 2563.8071 - mae: 30.0542 - val_loss: 1529.4292 - val_mse: 1529.4293 - val_mae: 28.5893\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2433.4096 - mse: 2433.4099 - mae: 29.3223 - val_loss: 1529.7073 - val_mse: 1529.7073 - val_mae: 28.5452\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2502.0020 - mse: 2502.0012 - mae: 29.0960 - val_loss: 1527.5242 - val_mse: 1527.5239 - val_mae: 29.0046\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2491.3522 - mse: 2491.3513 - mae: 29.4454 - val_loss: 1527.0019 - val_mse: 1527.0020 - val_mae: 29.2472\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2588.7918 - mse: 2588.7910 - mae: 29.5696 - val_loss: 1531.5790 - val_mse: 1531.5791 - val_mae: 28.3940\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2510.4489 - mse: 2510.4487 - mae: 29.4401 - val_loss: 1526.8230 - val_mse: 1526.8232 - val_mae: 28.9366\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 668us/step - loss: 2533.4945 - mse: 2533.4939 - mae: 29.6384 - val_loss: 1526.4473 - val_mse: 1526.4474 - val_mae: 28.9824\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2467.4817 - mse: 2467.4810 - mae: 29.3600 - val_loss: 1527.0559 - val_mse: 1527.0558 - val_mae: 28.6871\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2491.8377 - mse: 2491.8376 - mae: 29.4665 - val_loss: 1526.3227 - val_mse: 1526.3226 - val_mae: 28.6634\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2519.6104 - mse: 2519.6099 - mae: 29.1363 - val_loss: 1525.7905 - val_mse: 1525.7905 - val_mae: 28.6823\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2528.3270 - mse: 2528.3271 - mae: 29.5485 - val_loss: 1525.2248 - val_mse: 1525.2249 - val_mae: 28.7981\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2504.9979 - mse: 2504.9983 - mae: 29.3240 - val_loss: 1526.8671 - val_mse: 1526.8669 - val_mae: 28.6403\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2481.0936 - mse: 2481.0942 - mae: 29.4365 - val_loss: 1525.2664 - val_mse: 1525.2664 - val_mae: 28.8659\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2516.2468 - mse: 2516.2468 - mae: 29.3702 - val_loss: 1527.6243 - val_mse: 1527.6244 - val_mae: 28.4934\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2453.8850 - mse: 2453.8848 - mae: 28.9769 - val_loss: 1524.8937 - val_mse: 1524.8936 - val_mae: 29.1166\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2513.5011 - mse: 2513.5002 - mae: 29.0939 - val_loss: 1527.5686 - val_mse: 1527.5687 - val_mae: 28.5720\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2463.6314 - mse: 2463.6318 - mae: 29.1122 - val_loss: 1524.9350 - val_mse: 1524.9349 - val_mae: 28.8405\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2511.5413 - mse: 2511.5413 - mae: 29.5005 - val_loss: 1523.9229 - val_mse: 1523.9229 - val_mae: 29.0045\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2492.4626 - mse: 2492.4634 - mae: 29.2395 - val_loss: 1524.3733 - val_mse: 1524.3732 - val_mae: 28.7223\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2488.6096 - mse: 2488.6094 - mae: 29.2951 - val_loss: 1523.9602 - val_mse: 1523.9603 - val_mae: 28.7975\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2486.8990 - mse: 2486.8992 - mae: 29.1168 - val_loss: 1522.9366 - val_mse: 1522.9364 - val_mae: 29.1973\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2514.5293 - mse: 2514.5291 - mae: 29.9524 - val_loss: 1524.9447 - val_mse: 1524.9447 - val_mae: 28.5718\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2492.8870 - mse: 2492.8875 - mae: 28.9344 - val_loss: 1523.1515 - val_mse: 1523.1514 - val_mae: 29.0429\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2452.6771 - mse: 2452.6775 - mae: 30.2248 - val_loss: 3699.5716 - val_mse: 3699.5713 - val_mae: 25.3583\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 708us/step - loss: 2405.2156 - mse: 2405.2161 - mae: 29.8903 - val_loss: 3697.9399 - val_mse: 3697.9397 - val_mae: 25.2277\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 685us/step - loss: 2381.3959 - mse: 2381.3958 - mae: 29.6985 - val_loss: 3701.9442 - val_mse: 3701.9434 - val_mae: 25.6551\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 688us/step - loss: 2392.5960 - mse: 2392.5964 - mae: 29.9616 - val_loss: 3696.7192 - val_mse: 3696.7185 - val_mae: 25.2175\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2414.3516 - mse: 2414.3511 - mae: 29.5139 - val_loss: 3694.3557 - val_mse: 3694.3555 - val_mae: 24.9991\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2384.5962 - mse: 2384.5967 - mae: 29.4094 - val_loss: 3698.3782 - val_mse: 3698.3782 - val_mae: 25.3997\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2433.1116 - mse: 2433.1113 - mae: 29.8761 - val_loss: 3702.3248 - val_mse: 3702.3247 - val_mae: 25.7497\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2437.3204 - mse: 2437.3203 - mae: 29.7075 - val_loss: 3700.2617 - val_mse: 3700.2610 - val_mae: 25.6205\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2431.9543 - mse: 2431.9539 - mae: 29.8907 - val_loss: 3692.7016 - val_mse: 3692.7007 - val_mae: 24.9320\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2410.7892 - mse: 2410.7896 - mae: 29.7891 - val_loss: 3694.4378 - val_mse: 3694.4375 - val_mae: 25.1709\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2366.9804 - mse: 2366.9810 - mae: 29.4708 - val_loss: 3695.7806 - val_mse: 3695.7800 - val_mae: 25.3435\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2446.1866 - mse: 2446.1868 - mae: 29.9963 - val_loss: 3696.4685 - val_mse: 3696.4683 - val_mae: 25.4308\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2451.0210 - mse: 2451.0215 - mae: 30.1615 - val_loss: 3688.3361 - val_mse: 3688.3359 - val_mae: 24.5863\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2431.9283 - mse: 2431.9287 - mae: 29.7722 - val_loss: 3695.0519 - val_mse: 3695.0518 - val_mae: 25.4085\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2427.6581 - mse: 2427.6577 - mae: 29.6533 - val_loss: 3692.0189 - val_mse: 3692.0190 - val_mae: 25.1443\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2343.5115 - mse: 2343.5110 - mae: 29.5459 - val_loss: 3691.7498 - val_mse: 3691.7507 - val_mae: 25.1567\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2381.9315 - mse: 2381.9319 - mae: 29.6295 - val_loss: 3693.0883 - val_mse: 3693.0894 - val_mae: 25.2916\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2391.1825 - mse: 2391.1819 - mae: 29.7952 - val_loss: 3689.7744 - val_mse: 3689.7744 - val_mae: 24.9817\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2417.1993 - mse: 2417.2000 - mae: 29.9928 - val_loss: 3687.8620 - val_mse: 3687.8613 - val_mae: 24.7430\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2398.3085 - mse: 2398.3086 - mae: 29.8812 - val_loss: 3687.5148 - val_mse: 3687.5144 - val_mae: 24.7047\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 634us/step - loss: 2376.2153 - mse: 2376.2151 - mae: 29.2689 - val_loss: 3703.6817 - val_mse: 3703.6814 - val_mae: 26.1354\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 537us/step - loss: 2408.4544 - mse: 2408.4546 - mae: 30.0589 - val_loss: 3686.7167 - val_mse: 3686.7166 - val_mae: 24.5983\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2374.7898 - mse: 2374.7908 - mae: 29.5006 - val_loss: 3689.3705 - val_mse: 3689.3699 - val_mae: 25.0118\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2385.6551 - mse: 2385.6558 - mae: 29.5515 - val_loss: 3690.7990 - val_mse: 3690.7986 - val_mae: 25.2031\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2366.2189 - mse: 2366.2190 - mae: 29.2987 - val_loss: 3695.5987 - val_mse: 3695.5986 - val_mae: 25.6528\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2354.4886 - mse: 2354.4880 - mae: 29.2987 - val_loss: 3696.1016 - val_mse: 3696.1013 - val_mae: 25.6839\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 531us/step - loss: 2393.2064 - mse: 2393.2068 - mae: 29.9055 - val_loss: 3686.0981 - val_mse: 3686.0981 - val_mae: 24.7252\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 673us/step - loss: 2412.7999 - mse: 2412.8000 - mae: 29.6307 - val_loss: 3690.4915 - val_mse: 3690.4910 - val_mae: 25.2371\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 678us/step - loss: 2431.5703 - mse: 2431.5703 - mae: 29.9164 - val_loss: 3686.3789 - val_mse: 3686.3789 - val_mae: 24.8410\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2357.8247 - mse: 2357.8264 - mae: 29.1239 - val_loss: 3688.2932 - val_mse: 3688.2927 - val_mae: 25.0876\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 634us/step - loss: 2359.1770 - mse: 2359.1777 - mae: 29.5104 - val_loss: 3691.6067 - val_mse: 3691.6064 - val_mae: 25.4206\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2402.7815 - mse: 2402.7810 - mae: 29.6103 - val_loss: 3688.1606 - val_mse: 3688.1609 - val_mae: 25.1420\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 645us/step - loss: 2431.8537 - mse: 2431.8540 - mae: 30.0093 - val_loss: 3686.8486 - val_mse: 3686.8477 - val_mae: 24.9946\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2364.8415 - mse: 2364.8418 - mae: 29.2466 - val_loss: 3693.8207 - val_mse: 3693.8201 - val_mae: 25.6806\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2408.8038 - mse: 2408.8037 - mae: 29.7849 - val_loss: 3691.5074 - val_mse: 3691.5071 - val_mae: 25.5187\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2454.3171 - mse: 2454.3171 - mae: 29.9885 - val_loss: 3686.9770 - val_mse: 3686.9773 - val_mae: 25.1466\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2335.5949 - mse: 2335.5940 - mae: 28.8828 - val_loss: 3691.3019 - val_mse: 3691.3015 - val_mae: 25.5486\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 671us/step - loss: 2390.2353 - mse: 2390.2354 - mae: 29.3527 - val_loss: 3687.1566 - val_mse: 3687.1560 - val_mae: 25.1853\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2384.9014 - mse: 2384.9009 - mae: 29.5482 - val_loss: 3696.8943 - val_mse: 3696.8936 - val_mae: 25.9591\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2399.0936 - mse: 2399.0933 - mae: 29.1696 - val_loss: 3692.3536 - val_mse: 3692.3530 - val_mae: 25.6074\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 665us/step - loss: 2401.7450 - mse: 2401.7444 - mae: 29.3438 - val_loss: 3688.7018 - val_mse: 3688.7007 - val_mae: 25.3131\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2420.5841 - mse: 2420.5835 - mae: 29.7768 - val_loss: 3688.0938 - val_mse: 3688.0940 - val_mae: 25.2511\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2423.5124 - mse: 2423.5117 - mae: 29.6857 - val_loss: 3684.8694 - val_mse: 3684.8691 - val_mae: 24.9772\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2336.6695 - mse: 2336.6689 - mae: 29.3064 - val_loss: 3688.0525 - val_mse: 3688.0532 - val_mae: 25.3110\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 564us/step - loss: 2356.9763 - mse: 2356.9768 - mae: 29.0685 - val_loss: 3690.1414 - val_mse: 3690.1411 - val_mae: 25.5053\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2398.9538 - mse: 2398.9529 - mae: 29.9699 - val_loss: 3684.9994 - val_mse: 3684.9993 - val_mae: 24.9999\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2404.8927 - mse: 2404.8923 - mae: 29.3658 - val_loss: 3687.9061 - val_mse: 3687.9062 - val_mae: 25.3120\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2405.2335 - mse: 2405.2339 - mae: 29.6905 - val_loss: 3693.4777 - val_mse: 3693.4780 - val_mae: 25.7752\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 704us/step - loss: 2377.2610 - mse: 2377.2610 - mae: 29.4995 - val_loss: 3687.2232 - val_mse: 3687.2227 - val_mae: 25.3005\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2401.6556 - mse: 2401.6555 - mae: 29.6754 - val_loss: 3681.3541 - val_mse: 3681.3545 - val_mae: 24.6446\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2366.2260 - mse: 2366.2256 - mae: 29.3199 - val_loss: 3689.9758 - val_mse: 3689.9756 - val_mae: 25.5815\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2334.9543 - mse: 2334.9546 - mae: 29.2148 - val_loss: 3681.7146 - val_mse: 3681.7146 - val_mae: 24.8687\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 677us/step - loss: 2397.7958 - mse: 2397.7957 - mae: 29.4885 - val_loss: 3678.2757 - val_mse: 3678.2769 - val_mae: 24.3615\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 692us/step - loss: 2366.1057 - mse: 2366.1047 - mae: 29.5277 - val_loss: 3682.9207 - val_mse: 3682.9189 - val_mae: 25.0563\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 697us/step - loss: 2349.9682 - mse: 2349.9683 - mae: 29.6284 - val_loss: 3686.5967 - val_mse: 3686.5972 - val_mae: 25.3722\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2314.3918 - mse: 2314.3911 - mae: 29.1744 - val_loss: 3692.4328 - val_mse: 3692.4324 - val_mae: 25.8515\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2370.7979 - mse: 2370.7981 - mae: 29.5175 - val_loss: 3686.2942 - val_mse: 3686.2942 - val_mae: 25.3822\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 706us/step - loss: 2403.6704 - mse: 2403.6707 - mae: 29.5761 - val_loss: 3681.1529 - val_mse: 3681.1536 - val_mae: 24.8903\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 662us/step - loss: 2361.8943 - mse: 2361.8945 - mae: 29.5540 - val_loss: 3683.4727 - val_mse: 3683.4729 - val_mae: 25.1358\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2389.9288 - mse: 2389.9290 - mae: 29.4655 - val_loss: 3682.7969 - val_mse: 3682.7966 - val_mae: 25.0882\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2376.6746 - mse: 2376.6746 - mae: 29.3951 - val_loss: 3678.6724 - val_mse: 3678.6724 - val_mae: 24.7396\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2375.0096 - mse: 2375.0098 - mae: 29.2908 - val_loss: 3681.4941 - val_mse: 3681.4939 - val_mae: 25.1157\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 669us/step - loss: 2353.8429 - mse: 2353.8430 - mae: 29.7269 - val_loss: 3676.8082 - val_mse: 3676.8079 - val_mae: 24.4986\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 656us/step - loss: 2368.2678 - mse: 2368.2693 - mae: 29.3260 - val_loss: 3682.5009 - val_mse: 3682.5010 - val_mae: 25.1895\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2374.7190 - mse: 2374.7192 - mae: 29.2751 - val_loss: 3682.2365 - val_mse: 3682.2363 - val_mae: 25.2061\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2414.5529 - mse: 2414.5527 - mae: 29.7847 - val_loss: 3681.2953 - val_mse: 3681.2961 - val_mae: 25.1570\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2415.7634 - mse: 2415.7637 - mae: 29.7103 - val_loss: 3682.2692 - val_mse: 3682.2700 - val_mae: 25.1972\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2387.3500 - mse: 2387.3499 - mae: 29.6300 - val_loss: 3679.6601 - val_mse: 3679.6599 - val_mae: 24.8828\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2316.5294 - mse: 2316.5291 - mae: 29.2325 - val_loss: 3683.0990 - val_mse: 3683.0991 - val_mae: 25.2732\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2405.5856 - mse: 2405.5864 - mae: 29.7889 - val_loss: 3677.9741 - val_mse: 3677.9739 - val_mae: 24.7701\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2347.3782 - mse: 2347.3784 - mae: 29.3209 - val_loss: 3685.7607 - val_mse: 3685.7607 - val_mae: 25.5487\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2334.8986 - mse: 2334.8987 - mae: 29.1650 - val_loss: 3688.0374 - val_mse: 3688.0378 - val_mae: 25.7140\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2365.8883 - mse: 2365.8877 - mae: 29.4005 - val_loss: 3685.0858 - val_mse: 3685.0857 - val_mae: 25.4448\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2310.7621 - mse: 2310.7625 - mae: 28.9497 - val_loss: 3685.9440 - val_mse: 3685.9438 - val_mae: 25.5455\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 688us/step - loss: 2389.7592 - mse: 2389.7583 - mae: 29.3671 - val_loss: 3679.8941 - val_mse: 3679.8938 - val_mae: 24.9987\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2333.6825 - mse: 2333.6819 - mae: 29.1053 - val_loss: 3679.5027 - val_mse: 3679.5027 - val_mae: 24.9242\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2400.8326 - mse: 2400.8328 - mae: 29.4157 - val_loss: 3683.4731 - val_mse: 3683.4739 - val_mae: 25.3435\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2384.3014 - mse: 2384.3018 - mae: 29.5402 - val_loss: 3683.4983 - val_mse: 3683.4983 - val_mae: 25.3348\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2364.5805 - mse: 2364.5806 - mae: 29.6268 - val_loss: 3689.1748 - val_mse: 3689.1743 - val_mae: 25.8355\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2356.3874 - mse: 2356.3877 - mae: 29.5425 - val_loss: 3681.3281 - val_mse: 3681.3276 - val_mae: 25.2191\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 675us/step - loss: 2829.9364 - mse: 2829.9373 - mae: 29.1824 - val_loss: 2423.7475 - val_mse: 2423.7478 - val_mae: 27.5839\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2748.0010 - mse: 2748.0010 - mae: 28.6114 - val_loss: 2421.5770 - val_mse: 2421.5771 - val_mae: 28.1844\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2708.6266 - mse: 2708.6265 - mae: 28.9243 - val_loss: 2425.8040 - val_mse: 2425.8040 - val_mae: 28.4169\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2769.8297 - mse: 2769.8296 - mae: 29.1159 - val_loss: 2438.2641 - val_mse: 2438.2642 - val_mae: 27.7959\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2775.7217 - mse: 2775.7207 - mae: 29.4936 - val_loss: 2442.7205 - val_mse: 2442.7200 - val_mae: 27.8211\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 655us/step - loss: 2776.0392 - mse: 2776.0376 - mae: 29.1083 - val_loss: 2444.0324 - val_mse: 2444.0325 - val_mae: 27.8281\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 676us/step - loss: 2734.9694 - mse: 2734.9692 - mae: 29.1345 - val_loss: 2440.2607 - val_mse: 2440.2605 - val_mae: 27.9033\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2710.0227 - mse: 2710.0225 - mae: 28.8063 - val_loss: 2434.1392 - val_mse: 2434.1392 - val_mae: 28.2320\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2762.0077 - mse: 2762.0076 - mae: 29.0010 - val_loss: 2442.9355 - val_mse: 2442.9358 - val_mae: 27.9332\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2770.1571 - mse: 2770.1572 - mae: 29.0634 - val_loss: 2441.1600 - val_mse: 2441.1597 - val_mae: 28.2752\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2749.9413 - mse: 2749.9424 - mae: 28.8827 - val_loss: 2453.2955 - val_mse: 2453.2957 - val_mae: 27.5262\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2720.6430 - mse: 2720.6431 - mae: 28.6987 - val_loss: 2440.9522 - val_mse: 2440.9524 - val_mae: 28.3152\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2737.2062 - mse: 2737.2061 - mae: 28.9083 - val_loss: 2451.6269 - val_mse: 2451.6270 - val_mae: 27.5826\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 677us/step - loss: 2692.8199 - mse: 2692.8198 - mae: 28.6712 - val_loss: 2444.8294 - val_mse: 2444.8293 - val_mae: 27.6882\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2785.4719 - mse: 2785.4727 - mae: 29.1645 - val_loss: 2448.2052 - val_mse: 2448.2051 - val_mae: 27.5825\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2770.7476 - mse: 2770.7476 - mae: 28.9010 - val_loss: 2450.0539 - val_mse: 2450.0537 - val_mae: 27.7959\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2751.9930 - mse: 2751.9932 - mae: 28.5358 - val_loss: 2447.0151 - val_mse: 2447.0154 - val_mae: 28.1422\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2740.1548 - mse: 2740.1553 - mae: 28.7172 - val_loss: 2442.8614 - val_mse: 2442.8618 - val_mae: 28.3332\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2752.0439 - mse: 2752.0447 - mae: 29.2141 - val_loss: 2446.7015 - val_mse: 2446.7012 - val_mae: 27.7763\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2734.0181 - mse: 2734.0183 - mae: 28.7500 - val_loss: 2449.8186 - val_mse: 2449.8186 - val_mae: 27.9665\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - ETA: 0s - loss: 2759.1403 - mse: 2759.1414 - mae: 28.79 - 2s 565us/step - loss: 2755.9313 - mse: 2755.9324 - mae: 28.7781 - val_loss: 2445.7727 - val_mse: 2445.7725 - val_mae: 27.9596\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2723.0896 - mse: 2723.0901 - mae: 28.8015 - val_loss: 2439.8586 - val_mse: 2439.8584 - val_mae: 28.0296\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2727.9854 - mse: 2727.9854 - mae: 29.0683 - val_loss: 2440.0665 - val_mse: 2440.0664 - val_mae: 27.9693\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2735.5489 - mse: 2735.5496 - mae: 28.9676 - val_loss: 2449.5921 - val_mse: 2449.5923 - val_mae: 27.8344\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2809.0859 - mse: 2809.0857 - mae: 29.2891 - val_loss: 2450.7373 - val_mse: 2450.7373 - val_mae: 27.6717\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2724.5928 - mse: 2724.5935 - mae: 28.6032 - val_loss: 2450.3759 - val_mse: 2450.3760 - val_mae: 28.0580\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2771.7725 - mse: 2771.7727 - mae: 28.9654 - val_loss: 2454.4198 - val_mse: 2454.4204 - val_mae: 27.7883\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2774.0759 - mse: 2774.0771 - mae: 28.8436 - val_loss: 2458.7034 - val_mse: 2458.7034 - val_mae: 27.5856\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2745.2716 - mse: 2745.2720 - mae: 29.0160 - val_loss: 2451.2102 - val_mse: 2451.2102 - val_mae: 27.8605\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2696.3699 - mse: 2696.3701 - mae: 29.0497 - val_loss: 2452.3691 - val_mse: 2452.3689 - val_mae: 27.8916\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 3s 718us/step - loss: 2676.9075 - mse: 2676.9087 - mae: 28.9694 - val_loss: 2453.0879 - val_mse: 2453.0876 - val_mae: 28.2951\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 675us/step - loss: 2697.0355 - mse: 2697.0349 - mae: 28.6559 - val_loss: 2453.9153 - val_mse: 2453.9155 - val_mae: 27.9353\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 550us/step - loss: 2692.6355 - mse: 2692.6350 - mae: 28.7303 - val_loss: 2439.0428 - val_mse: 2439.0425 - val_mae: 28.4302\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2725.3349 - mse: 2725.3345 - mae: 28.7343 - val_loss: 2434.1916 - val_mse: 2434.1912 - val_mae: 28.2228\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2749.5247 - mse: 2749.5242 - mae: 29.0606 - val_loss: 2444.1495 - val_mse: 2444.1497 - val_mae: 27.7179\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2765.1577 - mse: 2765.1582 - mae: 28.9536 - val_loss: 2451.4124 - val_mse: 2451.4124 - val_mae: 28.0121\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 677us/step - loss: 2814.8698 - mse: 2814.8694 - mae: 29.0756 - val_loss: 2448.9295 - val_mse: 2448.9299 - val_mae: 27.7926\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2697.5669 - mse: 2697.5667 - mae: 28.7817 - val_loss: 2452.0301 - val_mse: 2452.0303 - val_mae: 27.8521\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2748.8107 - mse: 2748.8113 - mae: 29.0316 - val_loss: 2447.0296 - val_mse: 2447.0295 - val_mae: 27.7117\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2700.5974 - mse: 2700.5974 - mae: 28.4899 - val_loss: 2449.2232 - val_mse: 2449.2234 - val_mae: 27.3494\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2696.1130 - mse: 2696.1128 - mae: 28.6052 - val_loss: 2447.3621 - val_mse: 2447.3616 - val_mae: 27.5331\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2730.7650 - mse: 2730.7646 - mae: 28.8851 - val_loss: 2456.5465 - val_mse: 2456.5464 - val_mae: 27.4764\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2759.2072 - mse: 2759.2068 - mae: 29.0015 - val_loss: 2456.7115 - val_mse: 2456.7117 - val_mae: 27.6890\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 672us/step - loss: 2771.2069 - mse: 2771.2068 - mae: 29.1272 - val_loss: 2457.6387 - val_mse: 2457.6387 - val_mae: 27.5197\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2740.7409 - mse: 2740.7407 - mae: 29.0132 - val_loss: 2459.2331 - val_mse: 2459.2332 - val_mae: 27.5042\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2695.6710 - mse: 2695.6709 - mae: 28.5127 - val_loss: 2445.7217 - val_mse: 2445.7217 - val_mae: 28.2537\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 670us/step - loss: 2734.9667 - mse: 2734.9663 - mae: 28.9507 - val_loss: 2451.5473 - val_mse: 2451.5474 - val_mae: 27.9722\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2755.2790 - mse: 2755.2786 - mae: 28.6811 - val_loss: 2459.2452 - val_mse: 2459.2451 - val_mae: 27.8096\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2787.7096 - mse: 2787.7092 - mae: 28.8308 - val_loss: 2457.0044 - val_mse: 2457.0046 - val_mae: 27.8003\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2687.1742 - mse: 2687.1748 - mae: 28.4992 - val_loss: 2457.7605 - val_mse: 2457.7603 - val_mae: 27.6851\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2760.1234 - mse: 2760.1243 - mae: 28.5700 - val_loss: 2457.5842 - val_mse: 2457.5840 - val_mae: 27.9027\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2748.4547 - mse: 2748.4548 - mae: 28.9712 - val_loss: 2470.4878 - val_mse: 2470.4878 - val_mae: 27.2046\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2694.0677 - mse: 2694.0671 - mae: 28.7263 - val_loss: 2462.7505 - val_mse: 2462.7502 - val_mae: 27.7139\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2722.8370 - mse: 2722.8369 - mae: 28.5305 - val_loss: 2460.4816 - val_mse: 2460.4817 - val_mae: 27.8815\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2735.6863 - mse: 2735.6863 - mae: 29.0447 - val_loss: 2461.3041 - val_mse: 2461.3042 - val_mae: 28.0435\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2737.2976 - mse: 2737.2969 - mae: 28.7192 - val_loss: 2463.4645 - val_mse: 2463.4644 - val_mae: 27.8104\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 540us/step - loss: 2725.1094 - mse: 2725.1091 - mae: 28.8909 - val_loss: 2462.1961 - val_mse: 2462.1958 - val_mae: 27.8631\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2744.0787 - mse: 2744.0784 - mae: 28.8432 - val_loss: 2459.7630 - val_mse: 2459.7627 - val_mae: 28.0110\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2769.2869 - mse: 2769.2866 - mae: 28.9851 - val_loss: 2471.2379 - val_mse: 2471.2380 - val_mae: 27.5380\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 678us/step - loss: 2687.3500 - mse: 2687.3496 - mae: 28.4261 - val_loss: 2468.7738 - val_mse: 2468.7737 - val_mae: 27.6454\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2740.6188 - mse: 2740.6199 - mae: 28.9685 - val_loss: 2464.8833 - val_mse: 2464.8835 - val_mae: 28.1066\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2707.8486 - mse: 2707.8484 - mae: 28.5887 - val_loss: 2466.9833 - val_mse: 2466.9832 - val_mae: 27.8028\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2728.8629 - mse: 2728.8630 - mae: 28.7371 - val_loss: 2465.5186 - val_mse: 2465.5183 - val_mae: 27.8567\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2769.8002 - mse: 2769.7993 - mae: 28.8692 - val_loss: 2464.1003 - val_mse: 2464.1008 - val_mae: 27.7944\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2751.5933 - mse: 2751.5942 - mae: 28.8804 - val_loss: 2458.7025 - val_mse: 2458.7024 - val_mae: 28.3453\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2782.7523 - mse: 2782.7522 - mae: 29.0199 - val_loss: 2474.7866 - val_mse: 2474.7866 - val_mae: 27.5758\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2740.4449 - mse: 2740.4441 - mae: 29.0498 - val_loss: 2478.3476 - val_mse: 2478.3474 - val_mae: 27.8406\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2734.9610 - mse: 2734.9604 - mae: 28.6845 - val_loss: 2474.6462 - val_mse: 2474.6465 - val_mae: 27.8765\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 516us/step - loss: 2749.3313 - mse: 2749.3311 - mae: 28.9422 - val_loss: 2473.5653 - val_mse: 2473.5652 - val_mae: 27.9281\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 448us/step - loss: 2731.1845 - mse: 2731.1846 - mae: 28.4428 - val_loss: 2471.0785 - val_mse: 2471.0784 - val_mae: 27.7932\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 484us/step - loss: 2642.2421 - mse: 2642.2419 - mae: 28.5358 - val_loss: 2461.1658 - val_mse: 2461.1660 - val_mae: 28.1164\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 475us/step - loss: 2725.0900 - mse: 2725.0906 - mae: 28.6802 - val_loss: 2473.0465 - val_mse: 2473.0466 - val_mae: 27.8215\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 502us/step - loss: 2722.8008 - mse: 2722.8003 - mae: 28.5232 - val_loss: 2470.6614 - val_mse: 2470.6614 - val_mae: 27.9841\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2735.7798 - mse: 2735.7803 - mae: 28.9979 - val_loss: 2463.0652 - val_mse: 2463.0657 - val_mae: 28.1716\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2732.5029 - mse: 2732.5034 - mae: 28.7860 - val_loss: 2461.9477 - val_mse: 2461.9478 - val_mae: 27.7830\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2750.2568 - mse: 2750.2559 - mae: 28.9015 - val_loss: 2458.4108 - val_mse: 2458.4106 - val_mae: 28.2256\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2718.4910 - mse: 2718.4910 - mae: 28.7482 - val_loss: 2457.1423 - val_mse: 2457.1426 - val_mae: 28.2100\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2719.5820 - mse: 2719.5808 - mae: 28.8545 - val_loss: 2464.5781 - val_mse: 2464.5779 - val_mae: 28.0794\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2676.9139 - mse: 2676.9143 - mae: 28.6623 - val_loss: 2463.1991 - val_mse: 2463.1992 - val_mae: 27.9925\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2700.3747 - mse: 2700.3743 - mae: 28.9357 - val_loss: 2465.1592 - val_mse: 2465.1592 - val_mae: 28.0601\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 13336.4061 - mse: 13336.4062 - mae: 109.9371 - val_loss: 34628.4481 - val_mse: 34628.4492 - val_mae: 132.7664\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 13211.4515 - mse: 13211.4502 - mae: 109.3645 - val_loss: 34371.0813 - val_mse: 34371.0781 - val_mae: 131.7950\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 12867.1960 - mse: 12867.1963 - mae: 107.7931 - val_loss: 33660.8281 - val_mse: 33660.8281 - val_mae: 129.0765\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 627us/step - loss: 11971.1161 - mse: 11971.1162 - mae: 103.5566 - val_loss: 31884.8189 - val_mse: 31884.8184 - val_mae: 122.0105\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 10012.8018 - mse: 10012.8018 - mae: 93.3589 - val_loss: 27921.6280 - val_mse: 27921.6270 - val_mae: 104.5078\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 6249.1962 - mse: 6249.1963 - mae: 68.1105 - val_loss: 21180.6429 - val_mse: 21180.6445 - val_mae: 64.3425\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 651us/step - loss: 3410.0997 - mse: 3410.0996 - mae: 43.7207 - val_loss: 17488.8815 - val_mse: 17488.8828 - val_mae: 40.3398\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2885.8879 - mse: 2885.8879 - mae: 38.5429 - val_loss: 17681.6144 - val_mse: 17681.6152 - val_mae: 40.0472\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 2684.9574 - mse: 2684.9570 - mae: 38.4873 - val_loss: 17943.0017 - val_mse: 17943.0020 - val_mae: 40.1138\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2850.3743 - mse: 2850.3743 - mae: 38.6653 - val_loss: 17791.0903 - val_mse: 17791.0898 - val_mae: 39.8018\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 2784.6314 - mse: 2784.6318 - mae: 38.4216 - val_loss: 17758.5743 - val_mse: 17758.5762 - val_mae: 39.6828\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 689us/step - loss: 2616.5079 - mse: 2616.5078 - mae: 37.1413 - val_loss: 17572.0715 - val_mse: 17572.0703 - val_mae: 39.5876\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 605us/step - loss: 2700.2605 - mse: 2700.2605 - mae: 36.7615 - val_loss: 17690.6300 - val_mse: 17690.6309 - val_mae: 39.4599\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2858.7877 - mse: 2858.7878 - mae: 38.4549 - val_loss: 17928.1388 - val_mse: 17928.1387 - val_mae: 39.4851\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2837.0520 - mse: 2837.0520 - mae: 36.7618 - val_loss: 17918.8738 - val_mse: 17918.8730 - val_mae: 39.3972\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 673us/step - loss: 2581.3794 - mse: 2581.3794 - mae: 37.1272 - val_loss: 17710.9268 - val_mse: 17710.9258 - val_mae: 39.1985\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 722us/step - loss: 2580.6485 - mse: 2580.6487 - mae: 37.3630 - val_loss: 17654.4776 - val_mse: 17654.4766 - val_mae: 39.1181\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 2603.4882 - mse: 2603.4880 - mae: 35.9790 - val_loss: 17621.2668 - val_mse: 17621.2676 - val_mae: 39.0460\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 2567.3058 - mse: 2567.3057 - mae: 37.1537 - val_loss: 17733.0183 - val_mse: 17733.0176 - val_mae: 38.9960\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 2468.5834 - mse: 2468.5835 - mae: 35.7029 - val_loss: 17752.1577 - val_mse: 17752.1562 - val_mae: 38.9289\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2383.0938 - mse: 2383.0938 - mae: 35.1924 - val_loss: 17644.6195 - val_mse: 17644.6191 - val_mae: 38.8297\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 505us/step - loss: 2417.2172 - mse: 2417.2170 - mae: 34.9461 - val_loss: 17625.3277 - val_mse: 17625.3281 - val_mae: 38.7650\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2387.7995 - mse: 2387.7998 - mae: 34.3614 - val_loss: 17759.5506 - val_mse: 17759.5508 - val_mae: 38.7637\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2400.7512 - mse: 2400.7512 - mae: 35.3455 - val_loss: 17731.3044 - val_mse: 17731.3047 - val_mae: 38.6967\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2346.1920 - mse: 2346.1919 - mae: 34.2166 - val_loss: 17677.1149 - val_mse: 17677.1152 - val_mae: 38.6005\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 594us/step - loss: 2418.4880 - mse: 2418.4883 - mae: 34.7065 - val_loss: 17729.8328 - val_mse: 17729.8320 - val_mae: 38.5851\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 652us/step - loss: 2538.2931 - mse: 2538.2930 - mae: 35.0007 - val_loss: 17844.9689 - val_mse: 17844.9707 - val_mae: 38.6093\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 524us/step - loss: 2401.6073 - mse: 2401.6072 - mae: 33.8824 - val_loss: 17669.5876 - val_mse: 17669.5859 - val_mae: 38.4280\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 508us/step - loss: 2382.0095 - mse: 2382.0095 - mae: 35.2736 - val_loss: 17799.6177 - val_mse: 17799.6152 - val_mae: 38.4505\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 654us/step - loss: 2421.3548 - mse: 2421.3547 - mae: 35.6164 - val_loss: 17735.6362 - val_mse: 17735.6387 - val_mae: 38.3379\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 2290.2693 - mse: 2290.2693 - mae: 33.0108 - val_loss: 17633.6863 - val_mse: 17633.6875 - val_mae: 38.2171\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 2367.7442 - mse: 2367.7441 - mae: 34.3170 - val_loss: 17721.9963 - val_mse: 17721.9961 - val_mae: 38.2279\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2462.4563 - mse: 2462.4565 - mae: 34.7717 - val_loss: 17676.4510 - val_mse: 17676.4512 - val_mae: 38.1602\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2232.9181 - mse: 2232.9180 - mae: 33.1547 - val_loss: 17693.1691 - val_mse: 17693.1699 - val_mae: 38.1237\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2203.6258 - mse: 2203.6257 - mae: 32.8751 - val_loss: 17605.9901 - val_mse: 17605.9902 - val_mae: 38.0313\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2406.1397 - mse: 2406.1394 - mae: 35.1813 - val_loss: 17596.4119 - val_mse: 17596.4121 - val_mae: 37.9888\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 2161.2260 - mse: 2161.2261 - mae: 32.4670 - val_loss: 17758.1336 - val_mse: 17758.1348 - val_mae: 38.0126\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 2238.9941 - mse: 2238.9941 - mae: 33.8014 - val_loss: 17680.3577 - val_mse: 17680.3555 - val_mae: 37.9165\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 675us/step - loss: 2037.7995 - mse: 2037.7997 - mae: 32.4663 - val_loss: 17642.3907 - val_mse: 17642.3906 - val_mae: 37.8586\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 706us/step - loss: 2298.6560 - mse: 2298.6560 - mae: 32.9734 - val_loss: 17676.3921 - val_mse: 17676.3926 - val_mae: 37.8262\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2050.9683 - mse: 2050.9683 - mae: 31.8614 - val_loss: 17741.0544 - val_mse: 17741.0547 - val_mae: 37.7946\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 2365.1592 - mse: 2365.1594 - mae: 34.2472 - val_loss: 17807.4807 - val_mse: 17807.4805 - val_mae: 37.7987\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 1947.3834 - mse: 1947.3834 - mae: 31.6436 - val_loss: 17592.4823 - val_mse: 17592.4824 - val_mae: 37.6737\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2401.9411 - mse: 2401.9409 - mae: 34.4573 - val_loss: 17921.1820 - val_mse: 17921.1816 - val_mae: 37.8658\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 472us/step - loss: 2021.5807 - mse: 2021.5806 - mae: 31.5225 - val_loss: 17947.5658 - val_mse: 17947.5645 - val_mae: 37.9085\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 460us/step - loss: 2205.7952 - mse: 2205.7952 - mae: 32.6591 - val_loss: 17702.9549 - val_mse: 17702.9531 - val_mae: 37.5733\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 494us/step - loss: 2039.7751 - mse: 2039.7750 - mae: 31.0086 - val_loss: 17752.0481 - val_mse: 17752.0488 - val_mae: 37.5615\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 508us/step - loss: 2081.0466 - mse: 2081.0466 - mae: 31.8323 - val_loss: 17772.3804 - val_mse: 17772.3809 - val_mae: 37.5252\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2077.1333 - mse: 2077.1333 - mae: 31.5242 - val_loss: 17645.2814 - val_mse: 17645.2812 - val_mae: 37.4178\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 559us/step - loss: 2038.9312 - mse: 2038.9312 - mae: 31.4421 - val_loss: 17690.5976 - val_mse: 17690.5977 - val_mae: 37.4053\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 508us/step - loss: 2061.1159 - mse: 2061.1160 - mae: 30.8586 - val_loss: 17702.8134 - val_mse: 17702.8145 - val_mae: 37.3721\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 734us/step - loss: 2008.4927 - mse: 2008.4928 - mae: 30.2942 - val_loss: 17672.6980 - val_mse: 17672.6973 - val_mae: 37.3343\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 652us/step - loss: 2135.5612 - mse: 2135.5613 - mae: 31.9846 - val_loss: 17905.6523 - val_mse: 17905.6543 - val_mae: 37.6404\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2067.0463 - mse: 2067.0464 - mae: 31.4379 - val_loss: 17675.7748 - val_mse: 17675.7754 - val_mae: 37.2761\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 1973.3064 - mse: 1973.3065 - mae: 30.5776 - val_loss: 17583.7022 - val_mse: 17583.7012 - val_mae: 37.3538\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 1878.3237 - mse: 1878.3236 - mae: 30.3772 - val_loss: 17677.4570 - val_mse: 17677.4551 - val_mae: 37.2336\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 781us/step - loss: 1871.2946 - mse: 1871.2946 - mae: 29.5692 - val_loss: 17579.4881 - val_mse: 17579.4883 - val_mae: 37.3447\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 637us/step - loss: 1973.4681 - mse: 1973.4684 - mae: 30.6435 - val_loss: 17672.2955 - val_mse: 17672.2969 - val_mae: 37.1938\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2140.5547 - mse: 2140.5547 - mae: 31.9266 - val_loss: 17990.6085 - val_mse: 17990.6074 - val_mae: 37.9168\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2090.9583 - mse: 2090.9580 - mae: 31.8064 - val_loss: 17691.2068 - val_mse: 17691.2070 - val_mae: 37.1403\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2019.0563 - mse: 2019.0563 - mae: 31.4474 - val_loss: 17838.7556 - val_mse: 17838.7559 - val_mae: 37.3470\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 1932.2259 - mse: 1932.2258 - mae: 30.3281 - val_loss: 17771.7309 - val_mse: 17771.7305 - val_mae: 37.1765\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 1996.6942 - mse: 1996.6942 - mae: 30.9744 - val_loss: 17754.7666 - val_mse: 17754.7676 - val_mae: 37.1368\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2028.9730 - mse: 2028.9728 - mae: 30.8933 - val_loss: 17659.0014 - val_mse: 17659.0000 - val_mae: 37.1577\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2039.0209 - mse: 2039.0209 - mae: 31.0060 - val_loss: 17837.3187 - val_mse: 17837.3203 - val_mae: 37.3168\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 705us/step - loss: 2083.1041 - mse: 2083.1040 - mae: 31.5777 - val_loss: 17845.1788 - val_mse: 17845.1797 - val_mae: 37.3281\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 609us/step - loss: 2149.2895 - mse: 2149.2896 - mae: 33.2560 - val_loss: 17909.7595 - val_mse: 17909.7598 - val_mae: 37.5532\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2118.3115 - mse: 2118.3115 - mae: 30.9980 - val_loss: 17831.7293 - val_mse: 17831.7305 - val_mae: 37.2603\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 1986.8045 - mse: 1986.8046 - mae: 30.3754 - val_loss: 17731.9017 - val_mse: 17731.9023 - val_mae: 37.0329\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 1979.3421 - mse: 1979.3423 - mae: 30.7421 - val_loss: 17679.2482 - val_mse: 17679.2480 - val_mae: 37.0940\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2075.1617 - mse: 2075.1619 - mae: 30.4854 - val_loss: 17721.5976 - val_mse: 17721.5977 - val_mae: 37.0220\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 1941.7528 - mse: 1941.7532 - mae: 30.7821 - val_loss: 17779.4127 - val_mse: 17779.4141 - val_mae: 37.0805\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 666us/step - loss: 1786.0489 - mse: 1786.0487 - mae: 28.8009 - val_loss: 17762.2362 - val_mse: 17762.2363 - val_mae: 37.0358\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 661us/step - loss: 2023.2045 - mse: 2023.2045 - mae: 30.8877 - val_loss: 17819.6735 - val_mse: 17819.6719 - val_mae: 37.1745\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 1853.7963 - mse: 1853.7965 - mae: 29.4179 - val_loss: 17711.3563 - val_mse: 17711.3555 - val_mae: 37.0294\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 1946.3771 - mse: 1946.3772 - mae: 30.8356 - val_loss: 17884.5720 - val_mse: 17884.5703 - val_mae: 37.4178\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 563us/step - loss: 1903.0242 - mse: 1903.0243 - mae: 29.5791 - val_loss: 17643.3531 - val_mse: 17643.3516 - val_mae: 37.1713\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 1844.1116 - mse: 1844.1116 - mae: 28.4482 - val_loss: 17756.8505 - val_mse: 17756.8496 - val_mae: 37.0486\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 1866.8502 - mse: 1866.8502 - mae: 29.8065 - val_loss: 17660.7256 - val_mse: 17660.7266 - val_mae: 37.1376\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 1820.7916 - mse: 1820.7916 - mae: 29.6495 - val_loss: 17766.1125 - val_mse: 17766.1133 - val_mae: 37.0444\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4215.3266 - mse: 4215.3271 - mae: 33.9552 - val_loss: 2217.0268 - val_mse: 2217.0269 - val_mae: 31.4505\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4282.3891 - mse: 4282.3892 - mae: 35.3556 - val_loss: 2320.4043 - val_mse: 2320.4041 - val_mae: 31.7654\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4272.5689 - mse: 4272.5688 - mae: 35.2613 - val_loss: 2354.7912 - val_mse: 2354.7913 - val_mae: 31.8728\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4276.0065 - mse: 4276.0068 - mae: 35.2076 - val_loss: 2379.3127 - val_mse: 2379.3127 - val_mae: 31.9598\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 522us/step - loss: 4191.9024 - mse: 4191.9023 - mae: 34.0950 - val_loss: 2320.1393 - val_mse: 2320.1392 - val_mae: 31.7674\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4221.5903 - mse: 4221.5903 - mae: 34.3891 - val_loss: 2333.2131 - val_mse: 2333.2134 - val_mae: 31.8134\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4284.2081 - mse: 4284.2075 - mae: 35.1549 - val_loss: 2401.6362 - val_mse: 2401.6360 - val_mae: 32.0508\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4178.0740 - mse: 4178.0742 - mae: 34.3539 - val_loss: 2364.3799 - val_mse: 2364.3796 - val_mae: 31.9176\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 756us/step - loss: 4275.1987 - mse: 4275.1992 - mae: 34.5142 - val_loss: 2383.3017 - val_mse: 2383.3018 - val_mae: 31.9899\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4155.8344 - mse: 4155.8345 - mae: 34.5919 - val_loss: 2404.4977 - val_mse: 2404.4976 - val_mae: 32.0642\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 703us/step - loss: 4122.2574 - mse: 4122.2578 - mae: 33.9165 - val_loss: 2408.8886 - val_mse: 2408.8882 - val_mae: 32.0802\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4093.7609 - mse: 4093.7607 - mae: 33.9367 - val_loss: 2363.6780 - val_mse: 2363.6780 - val_mae: 31.9253\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 787us/step - loss: 4320.7981 - mse: 4320.7983 - mae: 35.1777 - val_loss: 2394.1735 - val_mse: 2394.1736 - val_mae: 32.0310\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 690us/step - loss: 3906.2517 - mse: 3906.2520 - mae: 32.9993 - val_loss: 2332.0559 - val_mse: 2332.0559 - val_mae: 31.8197\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4229.8576 - mse: 4229.8579 - mae: 34.1942 - val_loss: 2373.6602 - val_mse: 2373.6602 - val_mae: 31.9581\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 4187.8078 - mse: 4187.8076 - mae: 34.5095 - val_loss: 2357.3197 - val_mse: 2357.3196 - val_mae: 31.9016\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4091.9779 - mse: 4091.9780 - mae: 32.8466 - val_loss: 2324.0057 - val_mse: 2324.0059 - val_mae: 31.7941\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 4123.1255 - mse: 4123.1255 - mae: 34.6318 - val_loss: 2399.3519 - val_mse: 2399.3521 - val_mae: 32.0483\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 649us/step - loss: 4201.4807 - mse: 4201.4810 - mae: 34.6497 - val_loss: 2357.5913 - val_mse: 2357.5913 - val_mae: 31.9004\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4188.3216 - mse: 4188.3213 - mae: 34.8778 - val_loss: 2394.9784 - val_mse: 2394.9785 - val_mae: 32.0346\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4236.3767 - mse: 4236.3765 - mae: 33.4554 - val_loss: 2340.6910 - val_mse: 2340.6909 - val_mae: 31.8548\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 729us/step - loss: 4010.0136 - mse: 4010.0132 - mae: 33.7347 - val_loss: 2335.8229 - val_mse: 2335.8228 - val_mae: 31.8371\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 4262.2365 - mse: 4262.2358 - mae: 35.4579 - val_loss: 2383.4108 - val_mse: 2383.4106 - val_mae: 32.0014\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4074.6202 - mse: 4074.6199 - mae: 33.8456 - val_loss: 2356.8349 - val_mse: 2356.8350 - val_mae: 31.9070\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4223.5292 - mse: 4223.5293 - mae: 34.5422 - val_loss: 2358.6014 - val_mse: 2358.6016 - val_mae: 31.9150\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 706us/step - loss: 4221.2263 - mse: 4221.2261 - mae: 34.2037 - val_loss: 2402.1094 - val_mse: 2402.1091 - val_mae: 32.0633\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4189.1940 - mse: 4189.1938 - mae: 33.3539 - val_loss: 2367.3617 - val_mse: 2367.3616 - val_mae: 31.9429\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 670us/step - loss: 4183.1545 - mse: 4183.1543 - mae: 33.5437 - val_loss: 2386.0647 - val_mse: 2386.0645 - val_mae: 32.0053\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 660us/step - loss: 4139.5290 - mse: 4139.5298 - mae: 33.3788 - val_loss: 2360.4998 - val_mse: 2360.5000 - val_mae: 31.9122\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 4121.4674 - mse: 4121.4668 - mae: 34.1668 - val_loss: 2312.7762 - val_mse: 2312.7764 - val_mae: 31.7587\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 779us/step - loss: 4169.5521 - mse: 4169.5518 - mae: 34.2419 - val_loss: 2299.9835 - val_mse: 2299.9834 - val_mae: 31.7226\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 718us/step - loss: 4316.6530 - mse: 4316.6528 - mae: 35.0225 - val_loss: 2366.9413 - val_mse: 2366.9412 - val_mae: 31.9395\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 711us/step - loss: 4034.5736 - mse: 4034.5737 - mae: 32.8597 - val_loss: 2383.5473 - val_mse: 2383.5474 - val_mae: 32.0001\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 4025.2377 - mse: 4025.2378 - mae: 34.0800 - val_loss: 2363.0550 - val_mse: 2363.0549 - val_mae: 31.9375\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4160.6536 - mse: 4160.6533 - mae: 33.7280 - val_loss: 2411.5740 - val_mse: 2411.5737 - val_mae: 32.1028\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4067.6748 - mse: 4067.6748 - mae: 33.4526 - val_loss: 2311.4668 - val_mse: 2311.4668 - val_mae: 31.7572\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 3987.2687 - mse: 3987.2690 - mae: 33.7678 - val_loss: 2326.9393 - val_mse: 2326.9395 - val_mae: 31.8071\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 717us/step - loss: 4108.1906 - mse: 4108.1899 - mae: 33.7794 - val_loss: 2400.9200 - val_mse: 2400.9197 - val_mae: 32.0616\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 691us/step - loss: 4006.1651 - mse: 4006.1643 - mae: 32.9919 - val_loss: 2350.7388 - val_mse: 2350.7388 - val_mae: 31.8881\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4016.4383 - mse: 4016.4380 - mae: 33.1192 - val_loss: 2367.0759 - val_mse: 2367.0757 - val_mae: 31.9515\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 3993.4973 - mse: 3993.4973 - mae: 34.4803 - val_loss: 2337.6321 - val_mse: 2337.6321 - val_mae: 31.8464\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 3985.8611 - mse: 3985.8611 - mae: 34.0398 - val_loss: 2326.0537 - val_mse: 2326.0537 - val_mae: 31.8105\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 4109.1814 - mse: 4109.1816 - mae: 33.2045 - val_loss: 2348.5195 - val_mse: 2348.5193 - val_mae: 31.8883\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4052.2195 - mse: 4052.2195 - mae: 33.1900 - val_loss: 2324.5776 - val_mse: 2324.5774 - val_mae: 31.8084\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 552us/step - loss: 3950.5409 - mse: 3950.5408 - mae: 32.3642 - val_loss: 2346.8031 - val_mse: 2346.8032 - val_mae: 31.8836\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4117.2550 - mse: 4117.2544 - mae: 33.3572 - val_loss: 2413.0720 - val_mse: 2413.0720 - val_mae: 32.1205\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4040.0520 - mse: 4040.0530 - mae: 33.6485 - val_loss: 2367.1417 - val_mse: 2367.1418 - val_mae: 31.9593\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 4137.9353 - mse: 4137.9351 - mae: 32.9759 - val_loss: 2363.0046 - val_mse: 2363.0044 - val_mae: 31.9462\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4023.5242 - mse: 4023.5247 - mae: 32.9596 - val_loss: 2363.1507 - val_mse: 2363.1506 - val_mae: 31.9475\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4004.4353 - mse: 4004.4358 - mae: 32.9716 - val_loss: 2361.4817 - val_mse: 2361.4817 - val_mae: 31.9397\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 453us/step - loss: 4221.2730 - mse: 4221.2729 - mae: 34.2864 - val_loss: 2405.2887 - val_mse: 2405.2888 - val_mae: 32.0922\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 484us/step - loss: 4157.8064 - mse: 4157.8062 - mae: 33.4407 - val_loss: 2382.6907 - val_mse: 2382.6907 - val_mae: 32.0140\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 692us/step - loss: 3716.1655 - mse: 3716.1655 - mae: 31.6032 - val_loss: 2330.0285 - val_mse: 2330.0283 - val_mae: 31.8262\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 519us/step - loss: 4033.7316 - mse: 4033.7314 - mae: 32.9867 - val_loss: 2410.2653 - val_mse: 2410.2654 - val_mae: 32.1032\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 527us/step - loss: 3727.4143 - mse: 3727.4143 - mae: 32.2644 - val_loss: 2309.3494 - val_mse: 2309.3494 - val_mae: 31.7539\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 3955.9637 - mse: 3955.9634 - mae: 33.4926 - val_loss: 2399.5500 - val_mse: 2399.5498 - val_mae: 32.0651\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 3897.5256 - mse: 3897.5261 - mae: 33.0966 - val_loss: 2317.2018 - val_mse: 2317.2017 - val_mae: 31.7833\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 3881.0412 - mse: 3881.0405 - mae: 31.8207 - val_loss: 2310.0768 - val_mse: 2310.0769 - val_mae: 31.7575\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 516us/step - loss: 4143.3404 - mse: 4143.3398 - mae: 33.3921 - val_loss: 2444.2664 - val_mse: 2444.2668 - val_mae: 32.2169\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4073.4369 - mse: 4073.4373 - mae: 33.7438 - val_loss: 2396.9622 - val_mse: 2396.9619 - val_mae: 32.0650\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 701us/step - loss: 4219.3588 - mse: 4219.3584 - mae: 34.3666 - val_loss: 2393.7822 - val_mse: 2393.7822 - val_mae: 32.0558\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 814us/step - loss: 3907.8091 - mse: 3907.8091 - mae: 32.3710 - val_loss: 2333.6472 - val_mse: 2333.6472 - val_mae: 31.8433\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 3958.7012 - mse: 3958.7014 - mae: 33.5542 - val_loss: 2343.9389 - val_mse: 2343.9390 - val_mae: 31.8780\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 697us/step - loss: 4079.4619 - mse: 4079.4619 - mae: 32.4745 - val_loss: 2344.0580 - val_mse: 2344.0581 - val_mae: 31.8821\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 3957.3457 - mse: 3957.3459 - mae: 33.4552 - val_loss: 2373.9783 - val_mse: 2373.9783 - val_mae: 31.9929\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4076.3843 - mse: 4076.3828 - mae: 32.4481 - val_loss: 2350.0125 - val_mse: 2350.0127 - val_mae: 31.9072\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4010.7248 - mse: 4010.7253 - mae: 33.2803 - val_loss: 2415.6006 - val_mse: 2415.6008 - val_mae: 32.1388\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 4077.5006 - mse: 4077.5005 - mae: 32.8231 - val_loss: 2388.0493 - val_mse: 2388.0493 - val_mae: 32.0435\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4105.1098 - mse: 4105.1099 - mae: 33.4925 - val_loss: 2360.0083 - val_mse: 2360.0085 - val_mae: 31.9445\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 3904.4242 - mse: 3904.4241 - mae: 31.6518 - val_loss: 2330.9315 - val_mse: 2330.9314 - val_mae: 31.8393\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 3923.1816 - mse: 3923.1809 - mae: 32.5421 - val_loss: 2349.4101 - val_mse: 2349.4099 - val_mae: 31.9052\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 3972.9518 - mse: 3972.9521 - mae: 33.0334 - val_loss: 2382.7144 - val_mse: 2382.7144 - val_mae: 32.0261\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 3911.0805 - mse: 3911.0803 - mae: 31.9202 - val_loss: 2310.1018 - val_mse: 2310.1021 - val_mae: 31.7671\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 725us/step - loss: 4019.7796 - mse: 4019.7786 - mae: 32.8716 - val_loss: 2363.5218 - val_mse: 2363.5217 - val_mae: 31.9594\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4135.6432 - mse: 4135.6436 - mae: 33.6178 - val_loss: 2358.9554 - val_mse: 2358.9553 - val_mae: 31.9429\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4077.3754 - mse: 4077.3755 - mae: 32.3292 - val_loss: 2371.4761 - val_mse: 2371.4761 - val_mae: 31.9866\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4095.6727 - mse: 4095.6729 - mae: 33.1841 - val_loss: 2364.9653 - val_mse: 2364.9653 - val_mae: 31.9677\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 4095.3669 - mse: 4095.3667 - mae: 32.7027 - val_loss: 2332.3293 - val_mse: 2332.3296 - val_mae: 31.8519\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4103.7917 - mse: 4103.7925 - mae: 34.3804 - val_loss: 2342.3927 - val_mse: 2342.3928 - val_mae: 31.8898\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 3889.3326 - mse: 3889.3328 - mae: 32.4613 - val_loss: 2353.7410 - val_mse: 2353.7410 - val_mae: 31.9338\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3393.9791 - mse: 3393.9795 - mae: 32.5288 - val_loss: 1473.1359 - val_mse: 1473.1360 - val_mae: 25.7826\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3350.6211 - mse: 3350.6218 - mae: 32.4642 - val_loss: 1473.7203 - val_mse: 1473.7202 - val_mae: 26.1873\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3320.1028 - mse: 3320.1038 - mae: 33.0873 - val_loss: 1472.8549 - val_mse: 1472.8547 - val_mae: 26.0965\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 687us/step - loss: 3352.3586 - mse: 3352.3579 - mae: 32.1331 - val_loss: 1471.3853 - val_mse: 1471.3854 - val_mae: 25.5631\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3428.9914 - mse: 3428.9927 - mae: 33.1449 - val_loss: 1471.7752 - val_mse: 1471.7753 - val_mae: 25.3246\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3405.4006 - mse: 3405.3999 - mae: 32.2666 - val_loss: 1476.9459 - val_mse: 1476.9458 - val_mae: 26.6687\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3386.5742 - mse: 3386.5742 - mae: 32.3220 - val_loss: 1476.6532 - val_mse: 1476.6532 - val_mae: 26.6461\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3282.8721 - mse: 3282.8708 - mae: 32.6419 - val_loss: 1471.8696 - val_mse: 1471.8695 - val_mae: 25.2988\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3381.5915 - mse: 3381.5920 - mae: 32.4871 - val_loss: 1473.7052 - val_mse: 1473.7052 - val_mae: 26.2540\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3494.0945 - mse: 3494.0942 - mae: 33.8137 - val_loss: 1471.5987 - val_mse: 1471.5986 - val_mae: 25.4798\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3267.1077 - mse: 3267.1074 - mae: 31.6957 - val_loss: 1477.5973 - val_mse: 1477.5973 - val_mae: 26.6806\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3318.1450 - mse: 3318.1455 - mae: 32.7562 - val_loss: 1471.7606 - val_mse: 1471.7607 - val_mae: 25.8701\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3442.1234 - mse: 3442.1228 - mae: 32.8086 - val_loss: 1471.7600 - val_mse: 1471.7600 - val_mae: 25.4761\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 543us/step - loss: 3343.2877 - mse: 3343.2886 - mae: 32.0925 - val_loss: 1472.8531 - val_mse: 1472.8531 - val_mae: 26.0883\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3430.8529 - mse: 3430.8533 - mae: 33.0822 - val_loss: 1471.5375 - val_mse: 1471.5376 - val_mae: 25.7723\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3270.6540 - mse: 3270.6543 - mae: 32.8125 - val_loss: 1471.8820 - val_mse: 1471.8822 - val_mae: 25.8202\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3314.0402 - mse: 3314.0400 - mae: 32.5224 - val_loss: 1471.9597 - val_mse: 1471.9596 - val_mae: 25.5339\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3410.6347 - mse: 3410.6348 - mae: 32.2649 - val_loss: 1472.7132 - val_mse: 1472.7133 - val_mae: 25.9716\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3425.5905 - mse: 3425.5906 - mae: 33.1936 - val_loss: 1472.4393 - val_mse: 1472.4393 - val_mae: 25.6692\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3336.0973 - mse: 3336.0984 - mae: 32.3040 - val_loss: 1472.9140 - val_mse: 1472.9139 - val_mae: 25.8611\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3403.0868 - mse: 3403.0862 - mae: 33.0119 - val_loss: 1474.0812 - val_mse: 1474.0812 - val_mae: 25.2821\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 527us/step - loss: 3381.8664 - mse: 3381.8662 - mae: 32.7273 - val_loss: 1473.2800 - val_mse: 1473.2800 - val_mae: 25.7398\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 698us/step - loss: 3376.9035 - mse: 3376.9041 - mae: 32.1233 - val_loss: 1475.2060 - val_mse: 1475.2059 - val_mae: 26.1806\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3241.4020 - mse: 3241.4023 - mae: 31.9205 - val_loss: 1473.7504 - val_mse: 1473.7501 - val_mae: 25.6457\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3328.6588 - mse: 3328.6594 - mae: 32.2920 - val_loss: 1474.4877 - val_mse: 1474.4877 - val_mae: 26.0295\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 543us/step - loss: 3369.2982 - mse: 3369.2983 - mae: 32.6803 - val_loss: 1474.2066 - val_mse: 1474.2067 - val_mae: 25.5316\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3349.3431 - mse: 3349.3433 - mae: 33.2866 - val_loss: 1474.9744 - val_mse: 1474.9742 - val_mae: 26.0449\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 657us/step - loss: 3362.2662 - mse: 3362.2666 - mae: 32.4669 - val_loss: 1475.9692 - val_mse: 1475.9692 - val_mae: 26.2558\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 524us/step - loss: 3382.9791 - mse: 3382.9783 - mae: 32.7786 - val_loss: 1475.6476 - val_mse: 1475.6476 - val_mae: 26.2067\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 656us/step - loss: 3371.3109 - mse: 3371.3113 - mae: 32.3160 - val_loss: 1475.3700 - val_mse: 1475.3700 - val_mae: 25.2677\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3206.7608 - mse: 3206.7610 - mae: 31.1974 - val_loss: 1474.4564 - val_mse: 1474.4564 - val_mae: 25.9550\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3376.9960 - mse: 3376.9963 - mae: 32.4175 - val_loss: 1476.1989 - val_mse: 1476.1989 - val_mae: 26.2154\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 733us/step - loss: 3308.5843 - mse: 3308.5835 - mae: 31.9384 - val_loss: 1474.4981 - val_mse: 1474.4980 - val_mae: 25.9461\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 662us/step - loss: 3296.5194 - mse: 3296.5183 - mae: 32.1143 - val_loss: 1474.5037 - val_mse: 1474.5038 - val_mae: 25.9511\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3484.8176 - mse: 3484.8176 - mae: 33.6455 - val_loss: 1473.8058 - val_mse: 1473.8058 - val_mae: 25.7785\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3223.9411 - mse: 3223.9412 - mae: 31.8205 - val_loss: 1477.0857 - val_mse: 1477.0857 - val_mae: 26.3809\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3295.5635 - mse: 3295.5632 - mae: 32.2673 - val_loss: 1473.6663 - val_mse: 1473.6661 - val_mae: 25.8070\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3386.0543 - mse: 3386.0542 - mae: 32.2088 - val_loss: 1474.1711 - val_mse: 1474.1711 - val_mae: 25.9170\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 651us/step - loss: 3297.7643 - mse: 3297.7649 - mae: 31.7807 - val_loss: 1476.0765 - val_mse: 1476.0765 - val_mae: 26.2345\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3313.7312 - mse: 3313.7314 - mae: 31.7890 - val_loss: 1475.5501 - val_mse: 1475.5502 - val_mae: 26.1702\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3271.5085 - mse: 3271.5083 - mae: 31.6447 - val_loss: 1474.0316 - val_mse: 1474.0317 - val_mae: 25.9165\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 533us/step - loss: 3347.7012 - mse: 3347.7014 - mae: 32.0055 - val_loss: 1474.0908 - val_mse: 1474.0906 - val_mae: 25.9622\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3338.8667 - mse: 3338.8672 - mae: 32.8417 - val_loss: 1474.6784 - val_mse: 1474.6781 - val_mae: 26.0232\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3191.7916 - mse: 3191.7913 - mae: 31.5267 - val_loss: 1478.0335 - val_mse: 1478.0336 - val_mae: 26.4515\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 667us/step - loss: 3215.4408 - mse: 3215.4404 - mae: 31.8176 - val_loss: 1475.0219 - val_mse: 1475.0219 - val_mae: 26.0186\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 710us/step - loss: 3220.4918 - mse: 3220.4917 - mae: 31.3508 - val_loss: 1476.0185 - val_mse: 1476.0186 - val_mae: 26.1612\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3302.9278 - mse: 3302.9277 - mae: 31.7704 - val_loss: 1475.0441 - val_mse: 1475.0443 - val_mae: 25.9076\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 700us/step - loss: 3328.8103 - mse: 3328.8101 - mae: 32.2995 - val_loss: 1474.8651 - val_mse: 1474.8651 - val_mae: 25.8811\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3234.3286 - mse: 3234.3289 - mae: 31.8229 - val_loss: 1476.4608 - val_mse: 1476.4607 - val_mae: 26.1546\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3383.3224 - mse: 3383.3228 - mae: 32.6541 - val_loss: 1476.1159 - val_mse: 1476.1160 - val_mae: 26.1524\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 660us/step - loss: 3240.5425 - mse: 3240.5427 - mae: 31.6584 - val_loss: 1474.8543 - val_mse: 1474.8542 - val_mae: 25.6676\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3227.2803 - mse: 3227.2812 - mae: 31.9600 - val_loss: 1477.8642 - val_mse: 1477.8645 - val_mae: 26.2978\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3348.5086 - mse: 3348.5083 - mae: 31.5057 - val_loss: 1475.5407 - val_mse: 1475.5406 - val_mae: 25.7598\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3285.5681 - mse: 3285.5676 - mae: 31.3456 - val_loss: 1476.1882 - val_mse: 1476.1881 - val_mae: 25.9924\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 654us/step - loss: 3312.6406 - mse: 3312.6409 - mae: 31.4470 - val_loss: 1476.9022 - val_mse: 1476.9022 - val_mae: 26.1095\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3274.0454 - mse: 3274.0454 - mae: 31.5983 - val_loss: 1481.5623 - val_mse: 1481.5624 - val_mae: 26.6695\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3259.2186 - mse: 3259.2190 - mae: 31.5232 - val_loss: 1479.7760 - val_mse: 1479.7761 - val_mae: 26.4961\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 671us/step - loss: 3298.3151 - mse: 3298.3149 - mae: 32.1420 - val_loss: 1476.0730 - val_mse: 1476.0732 - val_mae: 25.4369\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3329.8347 - mse: 3329.8337 - mae: 32.0005 - val_loss: 1476.6848 - val_mse: 1476.6848 - val_mae: 26.0335\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3331.9445 - mse: 3331.9448 - mae: 32.1057 - val_loss: 1476.4218 - val_mse: 1476.4218 - val_mae: 25.6539\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3364.5813 - mse: 3364.5811 - mae: 31.4952 - val_loss: 1476.2453 - val_mse: 1476.2451 - val_mae: 25.6690\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3427.3822 - mse: 3427.3823 - mae: 32.1378 - val_loss: 1477.9891 - val_mse: 1477.9891 - val_mae: 26.2603\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3290.9714 - mse: 3290.9717 - mae: 31.6775 - val_loss: 1479.6584 - val_mse: 1479.6584 - val_mae: 26.4963\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3343.6753 - mse: 3343.6755 - mae: 32.4387 - val_loss: 1477.0120 - val_mse: 1477.0121 - val_mae: 26.1240\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3232.8068 - mse: 3232.8071 - mae: 31.4166 - val_loss: 1475.8902 - val_mse: 1475.8903 - val_mae: 25.6193\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3201.2589 - mse: 3201.2603 - mae: 31.0737 - val_loss: 1478.3339 - val_mse: 1478.3339 - val_mae: 26.2280\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3203.3528 - mse: 3203.3523 - mae: 31.6544 - val_loss: 1479.0482 - val_mse: 1479.0481 - val_mae: 26.2940\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3093.6490 - mse: 3093.6499 - mae: 30.8048 - val_loss: 1480.8551 - val_mse: 1480.8550 - val_mae: 26.4791\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3236.9635 - mse: 3236.9636 - mae: 31.4923 - val_loss: 1478.5986 - val_mse: 1478.5986 - val_mae: 26.2518\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3301.9470 - mse: 3301.9465 - mae: 31.7053 - val_loss: 1480.4736 - val_mse: 1480.4738 - val_mae: 26.5084\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3236.7821 - mse: 3236.7822 - mae: 31.1830 - val_loss: 1476.2894 - val_mse: 1476.2894 - val_mae: 25.9654\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3258.7260 - mse: 3258.7244 - mae: 31.0685 - val_loss: 1477.6253 - val_mse: 1477.6254 - val_mae: 26.2137\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3211.4675 - mse: 3211.4678 - mae: 31.4955 - val_loss: 1477.5921 - val_mse: 1477.5923 - val_mae: 26.2094\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3153.4827 - mse: 3153.4827 - mae: 31.3944 - val_loss: 1480.4190 - val_mse: 1480.4191 - val_mae: 26.5694\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3249.9309 - mse: 3249.9312 - mae: 30.6837 - val_loss: 1476.0094 - val_mse: 1476.0094 - val_mae: 26.0252\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3245.5542 - mse: 3245.5542 - mae: 31.6292 - val_loss: 1478.2188 - val_mse: 1478.2188 - val_mae: 26.3731\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3250.3571 - mse: 3250.3582 - mae: 31.2920 - val_loss: 1478.1083 - val_mse: 1478.1083 - val_mae: 26.3446\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 511us/step - loss: 3294.9437 - mse: 3294.9438 - mae: 31.8349 - val_loss: 1475.8634 - val_mse: 1475.8635 - val_mae: 25.9760\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3209.5419 - mse: 3209.5425 - mae: 31.1564 - val_loss: 1476.9101 - val_mse: 1476.9100 - val_mae: 26.1348\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3278.4936 - mse: 3278.4934 - mae: 31.9221 - val_loss: 1479.9922 - val_mse: 1479.9923 - val_mae: 26.5086\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2888.2577 - mse: 2888.2585 - mae: 31.6276 - val_loss: 1074.0317 - val_mse: 1074.0317 - val_mae: 24.6693\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2918.0227 - mse: 2918.0229 - mae: 31.7069 - val_loss: 1073.6680 - val_mse: 1073.6681 - val_mae: 24.4717\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2993.6334 - mse: 2993.6343 - mae: 31.2005 - val_loss: 1073.4195 - val_mse: 1073.4196 - val_mae: 24.3843\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2894.4494 - mse: 2894.4492 - mae: 31.0850 - val_loss: 1073.3682 - val_mse: 1073.3684 - val_mae: 24.5910\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2980.7882 - mse: 2980.7883 - mae: 31.2411 - val_loss: 1073.4367 - val_mse: 1073.4368 - val_mae: 24.4326\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2917.6906 - mse: 2917.6907 - mae: 30.9667 - val_loss: 1074.1154 - val_mse: 1074.1155 - val_mae: 24.2251\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2816.6506 - mse: 2816.6492 - mae: 30.6785 - val_loss: 1073.9838 - val_mse: 1073.9839 - val_mae: 24.7306\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 651us/step - loss: 2839.9341 - mse: 2839.9338 - mae: 30.6844 - val_loss: 1075.2507 - val_mse: 1075.2507 - val_mae: 25.0037\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 667us/step - loss: 2868.9534 - mse: 2868.9531 - mae: 30.8947 - val_loss: 1074.5113 - val_mse: 1074.5114 - val_mae: 24.8556\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 696us/step - loss: 2883.4820 - mse: 2883.4805 - mae: 31.0473 - val_loss: 1073.4297 - val_mse: 1073.4296 - val_mae: 24.5727\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2957.2464 - mse: 2957.2468 - mae: 31.0269 - val_loss: 1073.4914 - val_mse: 1073.4916 - val_mae: 24.6400\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2992.2614 - mse: 2992.2617 - mae: 31.5176 - val_loss: 1073.6000 - val_mse: 1073.6001 - val_mae: 24.3700\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2817.8016 - mse: 2817.8018 - mae: 30.4610 - val_loss: 1073.6814 - val_mse: 1073.6815 - val_mae: 24.7279\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2866.6322 - mse: 2866.6328 - mae: 30.8157 - val_loss: 1075.6311 - val_mse: 1075.6312 - val_mae: 25.1182\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2837.8204 - mse: 2837.8203 - mae: 30.6880 - val_loss: 1075.2412 - val_mse: 1075.2412 - val_mae: 25.1071\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2920.8925 - mse: 2920.8926 - mae: 31.3821 - val_loss: 1072.6897 - val_mse: 1072.6896 - val_mae: 24.4473\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2872.9174 - mse: 2872.9180 - mae: 30.8662 - val_loss: 1074.0886 - val_mse: 1074.0886 - val_mae: 24.9224\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2915.8705 - mse: 2915.8708 - mae: 31.2427 - val_loss: 1072.8196 - val_mse: 1072.8196 - val_mae: 24.2967\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2819.5937 - mse: 2819.5947 - mae: 30.3517 - val_loss: 1074.4454 - val_mse: 1074.4454 - val_mae: 25.0257\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2917.2552 - mse: 2917.2549 - mae: 31.2725 - val_loss: 1073.2579 - val_mse: 1073.2579 - val_mae: 24.2144\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2855.3884 - mse: 2855.3882 - mae: 30.5583 - val_loss: 1071.9905 - val_mse: 1071.9906 - val_mae: 24.5505\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2912.2530 - mse: 2912.2527 - mae: 31.2512 - val_loss: 1071.8175 - val_mse: 1071.8174 - val_mae: 24.5948\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 672us/step - loss: 2889.3713 - mse: 2889.3708 - mae: 31.1230 - val_loss: 1071.8931 - val_mse: 1071.8932 - val_mae: 24.7028\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2921.2171 - mse: 2921.2178 - mae: 30.5782 - val_loss: 1071.5462 - val_mse: 1071.5461 - val_mae: 24.6254\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 687us/step - loss: 2834.1613 - mse: 2834.1606 - mae: 30.7228 - val_loss: 1071.9717 - val_mse: 1071.9714 - val_mae: 24.1746\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2862.2483 - mse: 2862.2476 - mae: 30.9162 - val_loss: 1071.2439 - val_mse: 1071.2439 - val_mae: 24.5448\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2949.8888 - mse: 2949.8879 - mae: 31.3081 - val_loss: 1071.5283 - val_mse: 1071.5283 - val_mae: 24.2623\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2934.1366 - mse: 2934.1355 - mae: 31.4039 - val_loss: 1071.1027 - val_mse: 1071.1028 - val_mae: 24.5008\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2954.4684 - mse: 2954.4685 - mae: 31.4918 - val_loss: 1070.9734 - val_mse: 1070.9733 - val_mae: 24.4757\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2843.4827 - mse: 2843.4819 - mae: 30.5566 - val_loss: 1072.6176 - val_mse: 1072.6176 - val_mae: 24.9679\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2949.9510 - mse: 2949.9504 - mae: 31.3519 - val_loss: 1070.6578 - val_mse: 1070.6578 - val_mae: 24.5673\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 536us/step - loss: 2969.5095 - mse: 2969.5093 - mae: 30.9296 - val_loss: 1071.5271 - val_mse: 1071.5273 - val_mae: 24.2390\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2848.4169 - mse: 2848.4170 - mae: 30.3382 - val_loss: 1070.5557 - val_mse: 1070.5558 - val_mae: 24.5028\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2879.8870 - mse: 2879.8867 - mae: 30.4968 - val_loss: 1071.2467 - val_mse: 1071.2467 - val_mae: 24.7890\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2898.6121 - mse: 2898.6123 - mae: 31.0346 - val_loss: 1071.8176 - val_mse: 1071.8174 - val_mae: 24.8306\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 663us/step - loss: 2923.3763 - mse: 2923.3762 - mae: 30.8389 - val_loss: 1071.4782 - val_mse: 1071.4781 - val_mae: 24.6742\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 663us/step - loss: 2916.3334 - mse: 2916.3335 - mae: 30.8552 - val_loss: 1071.1405 - val_mse: 1071.1406 - val_mae: 24.3503\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2802.9239 - mse: 2802.9248 - mae: 30.0862 - val_loss: 1072.0090 - val_mse: 1072.0090 - val_mae: 24.8145\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2876.6606 - mse: 2876.6609 - mae: 30.5724 - val_loss: 1071.1528 - val_mse: 1071.1527 - val_mae: 24.5856\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2904.4200 - mse: 2904.4207 - mae: 31.0159 - val_loss: 1072.1461 - val_mse: 1072.1461 - val_mae: 24.9323\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2953.8095 - mse: 2953.8091 - mae: 31.8323 - val_loss: 1070.3338 - val_mse: 1070.3339 - val_mae: 24.4998\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2959.2013 - mse: 2959.2017 - mae: 31.2702 - val_loss: 1070.3019 - val_mse: 1070.3018 - val_mae: 24.6075\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2808.4488 - mse: 2808.4492 - mae: 30.7044 - val_loss: 1070.5690 - val_mse: 1070.5690 - val_mae: 24.5934\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2872.8766 - mse: 2872.8767 - mae: 30.3571 - val_loss: 1070.3114 - val_mse: 1070.3114 - val_mae: 24.5691\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2896.0600 - mse: 2896.0603 - mae: 30.8443 - val_loss: 1072.3876 - val_mse: 1072.3878 - val_mae: 25.0156\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2868.0335 - mse: 2868.0337 - mae: 30.7029 - val_loss: 1072.1622 - val_mse: 1072.1624 - val_mae: 24.9846\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 683us/step - loss: 2847.3578 - mse: 2847.3582 - mae: 30.5261 - val_loss: 1070.5794 - val_mse: 1070.5795 - val_mae: 24.7005\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 673us/step - loss: 2825.6049 - mse: 2825.6050 - mae: 30.5023 - val_loss: 1070.0532 - val_mse: 1070.0532 - val_mae: 24.5387\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2865.3104 - mse: 2865.3101 - mae: 30.1151 - val_loss: 1070.0679 - val_mse: 1070.0680 - val_mae: 24.5008\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2893.3855 - mse: 2893.3853 - mae: 30.9589 - val_loss: 1070.3622 - val_mse: 1070.3622 - val_mae: 24.7911\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2907.8169 - mse: 2907.8167 - mae: 30.4265 - val_loss: 1069.6835 - val_mse: 1069.6833 - val_mae: 24.5058\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2861.2973 - mse: 2861.2971 - mae: 30.0708 - val_loss: 1069.3651 - val_mse: 1069.3651 - val_mae: 24.6175\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2828.1418 - mse: 2828.1416 - mae: 30.4274 - val_loss: 1069.3874 - val_mse: 1069.3876 - val_mae: 24.6474\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2907.8989 - mse: 2907.8982 - mae: 31.0554 - val_loss: 1069.3665 - val_mse: 1069.3665 - val_mae: 24.6093\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2828.8043 - mse: 2828.8057 - mae: 30.3046 - val_loss: 1069.5512 - val_mse: 1069.5511 - val_mae: 24.3746\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2861.4378 - mse: 2861.4380 - mae: 30.5767 - val_loss: 1071.6039 - val_mse: 1071.6040 - val_mae: 24.9952\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2777.7284 - mse: 2777.7292 - mae: 30.8036 - val_loss: 1069.6235 - val_mse: 1069.6237 - val_mae: 24.6040\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2810.1972 - mse: 2810.1982 - mae: 30.7998 - val_loss: 1069.3598 - val_mse: 1069.3599 - val_mae: 24.6496\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2859.0827 - mse: 2859.0830 - mae: 30.5702 - val_loss: 1069.4972 - val_mse: 1069.4971 - val_mae: 24.6086\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2879.4135 - mse: 2879.4131 - mae: 30.4177 - val_loss: 1069.0259 - val_mse: 1069.0259 - val_mae: 24.5965\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2875.7115 - mse: 2875.7117 - mae: 31.0837 - val_loss: 1069.1514 - val_mse: 1069.1517 - val_mae: 24.8325\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2868.1921 - mse: 2868.1912 - mae: 30.8659 - val_loss: 1068.7640 - val_mse: 1068.7640 - val_mae: 24.2541\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2880.1437 - mse: 2880.1436 - mae: 30.7666 - val_loss: 1067.8873 - val_mse: 1067.8873 - val_mae: 24.5515\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2849.6937 - mse: 2849.6938 - mae: 30.0237 - val_loss: 1070.3768 - val_mse: 1070.3767 - val_mae: 25.1198\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2972.7304 - mse: 2972.7307 - mae: 31.2854 - val_loss: 1067.7138 - val_mse: 1067.7137 - val_mae: 24.3748\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 676us/step - loss: 2885.1966 - mse: 2885.1970 - mae: 30.5807 - val_loss: 1067.9221 - val_mse: 1067.9221 - val_mae: 24.6553\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2851.4789 - mse: 2851.4790 - mae: 30.0603 - val_loss: 1067.9130 - val_mse: 1067.9130 - val_mae: 24.6500\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2906.5892 - mse: 2906.5891 - mae: 30.7882 - val_loss: 1068.4327 - val_mse: 1068.4325 - val_mae: 24.7584\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 682us/step - loss: 2837.3489 - mse: 2837.3486 - mae: 30.1109 - val_loss: 1070.6994 - val_mse: 1070.6993 - val_mae: 25.1145\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 666us/step - loss: 2845.6086 - mse: 2845.6084 - mae: 30.6688 - val_loss: 1068.3262 - val_mse: 1068.3262 - val_mae: 24.8056\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2828.4965 - mse: 2828.4963 - mae: 30.2866 - val_loss: 1067.8010 - val_mse: 1067.8009 - val_mae: 24.7754\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2868.4557 - mse: 2868.4551 - mae: 30.8608 - val_loss: 1067.0225 - val_mse: 1067.0225 - val_mae: 24.6762\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2893.1058 - mse: 2893.1062 - mae: 30.7588 - val_loss: 1066.9197 - val_mse: 1066.9196 - val_mae: 24.6489\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 659us/step - loss: 2890.5116 - mse: 2890.5125 - mae: 30.4344 - val_loss: 1068.0654 - val_mse: 1068.0654 - val_mae: 24.8856\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2898.4658 - mse: 2898.4658 - mae: 30.7898 - val_loss: 1067.3703 - val_mse: 1067.3704 - val_mae: 24.6912\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2910.4886 - mse: 2910.4888 - mae: 31.0767 - val_loss: 1067.0347 - val_mse: 1067.0348 - val_mae: 24.4811\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2787.5839 - mse: 2787.5842 - mae: 30.6824 - val_loss: 1067.4873 - val_mse: 1067.4872 - val_mae: 24.7655\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2806.3117 - mse: 2806.3110 - mae: 30.5308 - val_loss: 1068.2645 - val_mse: 1068.2644 - val_mae: 25.0143\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 554us/step - loss: 2846.9942 - mse: 2846.9934 - mae: 30.6278 - val_loss: 1066.0982 - val_mse: 1066.0981 - val_mae: 24.5468\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2833.6639 - mse: 2833.6633 - mae: 30.4186 - val_loss: 1065.3749 - val_mse: 1065.3749 - val_mae: 24.4296\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2560.7986 - mse: 2560.7993 - mae: 29.9303 - val_loss: 1568.1997 - val_mse: 1568.1996 - val_mae: 27.6771\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2531.2408 - mse: 2531.2417 - mae: 30.0660 - val_loss: 1567.2611 - val_mse: 1567.2611 - val_mae: 27.6618\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2602.1233 - mse: 2602.1238 - mae: 30.1751 - val_loss: 1563.0291 - val_mse: 1563.0289 - val_mae: 27.7749\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2534.5034 - mse: 2534.5032 - mae: 29.5036 - val_loss: 1562.8826 - val_mse: 1562.8826 - val_mae: 27.7658\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 536us/step - loss: 2558.0319 - mse: 2558.0325 - mae: 29.5351 - val_loss: 1557.5085 - val_mse: 1557.5087 - val_mae: 27.9288\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2571.5676 - mse: 2571.5679 - mae: 30.1642 - val_loss: 1555.4280 - val_mse: 1555.4283 - val_mae: 27.9877\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2523.4577 - mse: 2523.4578 - mae: 29.8470 - val_loss: 1552.6606 - val_mse: 1552.6605 - val_mae: 28.0891\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2573.8577 - mse: 2573.8577 - mae: 30.1141 - val_loss: 1576.1189 - val_mse: 1576.1190 - val_mae: 27.4039\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2566.1379 - mse: 2566.1375 - mae: 29.5558 - val_loss: 1571.1165 - val_mse: 1571.1165 - val_mae: 27.5128\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 504us/step - loss: 2524.3245 - mse: 2524.3245 - mae: 29.7460 - val_loss: 1556.8156 - val_mse: 1556.8156 - val_mae: 27.9061\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2585.5763 - mse: 2585.5764 - mae: 29.8987 - val_loss: 1564.1139 - val_mse: 1564.1138 - val_mae: 27.6703\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2604.9922 - mse: 2604.9917 - mae: 29.9363 - val_loss: 1555.7385 - val_mse: 1555.7384 - val_mae: 27.9175\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2528.7654 - mse: 2528.7654 - mae: 29.7240 - val_loss: 1552.2623 - val_mse: 1552.2625 - val_mae: 28.0390\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2497.3730 - mse: 2497.3723 - mae: 29.4583 - val_loss: 1556.2820 - val_mse: 1556.2819 - val_mae: 27.8892\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2567.8869 - mse: 2567.8865 - mae: 30.0660 - val_loss: 1566.2997 - val_mse: 1566.2998 - val_mae: 27.6107\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2514.7560 - mse: 2514.7561 - mae: 29.5509 - val_loss: 1553.6304 - val_mse: 1553.6304 - val_mae: 27.9913\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2569.4917 - mse: 2569.4912 - mae: 30.1832 - val_loss: 1564.1078 - val_mse: 1564.1078 - val_mae: 27.6858\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2563.6024 - mse: 2563.6028 - mae: 29.6464 - val_loss: 1556.1029 - val_mse: 1556.1028 - val_mae: 27.9095\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2577.7177 - mse: 2577.7178 - mae: 29.8148 - val_loss: 1555.7604 - val_mse: 1555.7604 - val_mae: 27.9243\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2565.5075 - mse: 2565.5071 - mae: 30.0846 - val_loss: 1563.5168 - val_mse: 1563.5168 - val_mae: 27.6947\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 647us/step - loss: 2556.6152 - mse: 2556.6162 - mae: 29.8509 - val_loss: 1557.9393 - val_mse: 1557.9395 - val_mae: 27.8454\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2504.3751 - mse: 2504.3755 - mae: 29.5270 - val_loss: 1553.2716 - val_mse: 1553.2715 - val_mae: 28.0143\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2504.0754 - mse: 2504.0750 - mae: 29.7146 - val_loss: 1567.3103 - val_mse: 1567.3102 - val_mae: 27.5809\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2541.3237 - mse: 2541.3240 - mae: 29.6612 - val_loss: 1559.2928 - val_mse: 1559.2925 - val_mae: 27.7646\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 698us/step - loss: 2560.6147 - mse: 2560.6143 - mae: 30.0091 - val_loss: 1555.6013 - val_mse: 1555.6014 - val_mae: 27.8789\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 668us/step - loss: 2543.7550 - mse: 2543.7549 - mae: 29.7101 - val_loss: 1551.9187 - val_mse: 1551.9186 - val_mae: 27.9927\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2503.1264 - mse: 2503.1262 - mae: 29.4978 - val_loss: 1563.1523 - val_mse: 1563.1522 - val_mae: 27.6534\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2584.6696 - mse: 2584.6699 - mae: 29.5467 - val_loss: 1553.2126 - val_mse: 1553.2125 - val_mae: 27.9518\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2488.9178 - mse: 2488.9192 - mae: 29.2405 - val_loss: 1556.3412 - val_mse: 1556.3412 - val_mae: 27.8511\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2551.2411 - mse: 2551.2412 - mae: 29.7024 - val_loss: 1562.1143 - val_mse: 1562.1140 - val_mae: 27.6727\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2568.8091 - mse: 2568.8086 - mae: 29.9371 - val_loss: 1562.1001 - val_mse: 1562.1000 - val_mae: 27.6732\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 690us/step - loss: 2533.3287 - mse: 2533.3286 - mae: 29.7011 - val_loss: 1550.4214 - val_mse: 1550.4215 - val_mae: 28.0096\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2557.6924 - mse: 2557.6921 - mae: 29.9987 - val_loss: 1557.2672 - val_mse: 1557.2672 - val_mae: 27.7741\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2439.4142 - mse: 2439.4141 - mae: 28.6400 - val_loss: 1543.9555 - val_mse: 1543.9552 - val_mae: 28.2323\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2547.5000 - mse: 2547.5015 - mae: 29.6997 - val_loss: 1544.6179 - val_mse: 1544.6180 - val_mae: 28.1926\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2520.9509 - mse: 2520.9512 - mae: 29.6196 - val_loss: 1552.9923 - val_mse: 1552.9921 - val_mae: 27.8670\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2574.9780 - mse: 2574.9778 - mae: 30.0031 - val_loss: 1562.8290 - val_mse: 1562.8291 - val_mae: 27.5732\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2531.0750 - mse: 2531.0750 - mae: 29.9419 - val_loss: 1552.9302 - val_mse: 1552.9301 - val_mae: 27.8100\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2605.3082 - mse: 2605.3083 - mae: 30.3251 - val_loss: 1548.7594 - val_mse: 1548.7594 - val_mae: 27.9515\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 685us/step - loss: 2569.8169 - mse: 2569.8167 - mae: 29.8095 - val_loss: 1549.6113 - val_mse: 1549.6112 - val_mae: 27.9112\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 686us/step - loss: 2494.9425 - mse: 2494.9426 - mae: 29.2134 - val_loss: 1546.4966 - val_mse: 1546.4967 - val_mae: 28.0476\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2603.6310 - mse: 2603.6313 - mae: 30.0760 - val_loss: 1570.1375 - val_mse: 1570.1376 - val_mae: 27.3622\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2570.9349 - mse: 2570.9346 - mae: 29.8855 - val_loss: 1554.8195 - val_mse: 1554.8193 - val_mae: 27.7430\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 665us/step - loss: 2504.1217 - mse: 2504.1223 - mae: 29.3955 - val_loss: 1546.6514 - val_mse: 1546.6514 - val_mae: 27.9995\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2519.0876 - mse: 2519.0869 - mae: 29.6134 - val_loss: 1551.2362 - val_mse: 1551.2362 - val_mae: 27.8155\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2493.4304 - mse: 2493.4312 - mae: 29.2782 - val_loss: 1551.4217 - val_mse: 1551.4216 - val_mae: 27.7958\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2492.7067 - mse: 2492.7058 - mae: 29.0931 - val_loss: 1547.7801 - val_mse: 1547.7802 - val_mae: 27.8999\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2549.8396 - mse: 2549.8394 - mae: 29.7077 - val_loss: 1556.8200 - val_mse: 1556.8201 - val_mae: 27.6326\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2570.0122 - mse: 2570.0125 - mae: 29.7615 - val_loss: 1554.8573 - val_mse: 1554.8573 - val_mae: 27.7104\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2500.3945 - mse: 2500.3943 - mae: 29.2273 - val_loss: 1546.7891 - val_mse: 1546.7891 - val_mae: 27.9961\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2537.0568 - mse: 2537.0571 - mae: 29.8848 - val_loss: 1544.5275 - val_mse: 1544.5276 - val_mae: 28.0616\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2522.0434 - mse: 2522.0435 - mae: 29.6219 - val_loss: 1559.5323 - val_mse: 1559.5323 - val_mae: 27.5746\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2532.2767 - mse: 2532.2769 - mae: 29.3113 - val_loss: 1552.4034 - val_mse: 1552.4034 - val_mae: 27.7876\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2509.2338 - mse: 2509.2336 - mae: 29.6713 - val_loss: 1552.3322 - val_mse: 1552.3323 - val_mae: 27.7873\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 659us/step - loss: 2506.1960 - mse: 2506.1960 - mae: 29.2877 - val_loss: 1542.4952 - val_mse: 1542.4952 - val_mae: 28.1424\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2510.0226 - mse: 2510.0225 - mae: 29.3920 - val_loss: 1555.9129 - val_mse: 1555.9130 - val_mae: 27.6683\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2539.5201 - mse: 2539.5195 - mae: 29.6813 - val_loss: 1544.5856 - val_mse: 1544.5857 - val_mae: 28.0778\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2502.4603 - mse: 2502.4602 - mae: 29.8234 - val_loss: 1541.7184 - val_mse: 1541.7183 - val_mae: 28.1622\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2507.0772 - mse: 2507.0781 - mae: 29.1968 - val_loss: 1548.4141 - val_mse: 1548.4142 - val_mae: 27.8700\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2484.3175 - mse: 2484.3169 - mae: 29.3219 - val_loss: 1545.7785 - val_mse: 1545.7784 - val_mae: 27.9833\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2535.4159 - mse: 2535.4158 - mae: 29.8922 - val_loss: 1544.8799 - val_mse: 1544.8800 - val_mae: 27.9566\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2476.8406 - mse: 2476.8394 - mae: 28.8671 - val_loss: 1546.3511 - val_mse: 1546.3510 - val_mae: 27.9244\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2526.5699 - mse: 2526.5701 - mae: 29.5745 - val_loss: 1545.3606 - val_mse: 1545.3606 - val_mae: 27.9523\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 694us/step - loss: 2555.1757 - mse: 2555.1760 - mae: 29.5582 - val_loss: 1543.5629 - val_mse: 1543.5630 - val_mae: 28.0758\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 757us/step - loss: 2503.5682 - mse: 2503.5693 - mae: 29.5236 - val_loss: 1549.3587 - val_mse: 1549.3589 - val_mae: 27.8446\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 682us/step - loss: 2475.3834 - mse: 2475.3833 - mae: 29.5300 - val_loss: 1546.5111 - val_mse: 1546.5111 - val_mae: 27.8887\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2480.0268 - mse: 2480.0271 - mae: 29.1483 - val_loss: 1545.0236 - val_mse: 1545.0238 - val_mae: 27.9438\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2442.1371 - mse: 2442.1379 - mae: 29.3026 - val_loss: 1549.6032 - val_mse: 1549.6033 - val_mae: 27.7633\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2498.5658 - mse: 2498.5659 - mae: 29.5568 - val_loss: 1549.6435 - val_mse: 1549.6433 - val_mae: 27.7370\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2535.3257 - mse: 2535.3252 - mae: 29.4736 - val_loss: 1537.4081 - val_mse: 1537.4080 - val_mae: 28.1905\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2485.9257 - mse: 2485.9263 - mae: 29.1856 - val_loss: 1544.1026 - val_mse: 1544.1025 - val_mae: 27.9346\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2494.3076 - mse: 2494.3071 - mae: 29.1844 - val_loss: 1549.6804 - val_mse: 1549.6804 - val_mae: 27.7348\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2527.2805 - mse: 2527.2800 - mae: 29.4186 - val_loss: 1552.9078 - val_mse: 1552.9078 - val_mae: 27.6602\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2531.7510 - mse: 2531.7510 - mae: 29.6561 - val_loss: 1547.3538 - val_mse: 1547.3539 - val_mae: 27.8621\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 670us/step - loss: 2566.8612 - mse: 2566.8616 - mae: 29.4488 - val_loss: 1542.4876 - val_mse: 1542.4878 - val_mae: 28.0177\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2439.8269 - mse: 2439.8269 - mae: 28.9642 - val_loss: 1541.3338 - val_mse: 1541.3337 - val_mae: 28.0776\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2487.1612 - mse: 2487.1611 - mae: 29.0211 - val_loss: 1542.1409 - val_mse: 1542.1410 - val_mae: 27.9937\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2472.2079 - mse: 2472.2078 - mae: 29.2290 - val_loss: 1543.3528 - val_mse: 1543.3529 - val_mae: 27.9330\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2460.8313 - mse: 2460.8303 - mae: 29.3014 - val_loss: 1543.6120 - val_mse: 1543.6119 - val_mae: 27.9113\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2566.3898 - mse: 2566.3892 - mae: 29.8989 - val_loss: 1551.7005 - val_mse: 1551.7007 - val_mae: 27.6751\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2468.5230 - mse: 2468.5232 - mae: 30.1190 - val_loss: 3702.2343 - val_mse: 3702.2346 - val_mae: 24.7521\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2406.0746 - mse: 2406.0747 - mae: 30.0798 - val_loss: 3701.7439 - val_mse: 3701.7437 - val_mae: 24.7262\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2392.0373 - mse: 2392.0381 - mae: 29.7171 - val_loss: 3702.1830 - val_mse: 3702.1826 - val_mae: 24.7738\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2388.1573 - mse: 2388.1565 - mae: 29.7534 - val_loss: 3700.7198 - val_mse: 3700.7192 - val_mae: 24.4411\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2400.5200 - mse: 2400.5205 - mae: 29.8840 - val_loss: 3702.6510 - val_mse: 3702.6514 - val_mae: 24.9078\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2404.9811 - mse: 2404.9814 - mae: 29.7648 - val_loss: 3697.8716 - val_mse: 3697.8726 - val_mae: 24.2050\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 659us/step - loss: 2451.2368 - mse: 2451.2368 - mae: 29.9736 - val_loss: 3699.9626 - val_mse: 3699.9639 - val_mae: 24.6856\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2387.7469 - mse: 2387.7473 - mae: 29.6155 - val_loss: 3700.4803 - val_mse: 3700.4805 - val_mae: 24.8558\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2404.7378 - mse: 2404.7383 - mae: 29.8154 - val_loss: 3697.4520 - val_mse: 3697.4529 - val_mae: 24.4108\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2427.9743 - mse: 2427.9749 - mae: 30.0162 - val_loss: 3697.1394 - val_mse: 3697.1389 - val_mae: 24.5450\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2378.7623 - mse: 2378.7620 - mae: 29.8270 - val_loss: 3698.2179 - val_mse: 3698.2175 - val_mae: 24.7734\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2405.7976 - mse: 2405.7974 - mae: 29.8293 - val_loss: 3700.2226 - val_mse: 3700.2227 - val_mae: 24.9345\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2385.2851 - mse: 2385.2854 - mae: 29.6056 - val_loss: 3698.5081 - val_mse: 3698.5081 - val_mae: 24.6647\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2379.5579 - mse: 2379.5579 - mae: 29.3798 - val_loss: 3699.9536 - val_mse: 3699.9548 - val_mae: 24.9116\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2442.9225 - mse: 2442.9216 - mae: 30.2155 - val_loss: 3696.9714 - val_mse: 3696.9719 - val_mae: 24.5179\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 659us/step - loss: 2471.7727 - mse: 2471.7720 - mae: 30.1011 - val_loss: 3695.3067 - val_mse: 3695.3076 - val_mae: 24.2047\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2400.4224 - mse: 2400.4221 - mae: 29.4196 - val_loss: 3701.2898 - val_mse: 3701.2896 - val_mae: 25.1084\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2430.6127 - mse: 2430.6121 - mae: 29.9166 - val_loss: 3697.6793 - val_mse: 3697.6794 - val_mae: 24.7310\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2420.7316 - mse: 2420.7319 - mae: 29.7905 - val_loss: 3693.9985 - val_mse: 3693.9980 - val_mae: 23.7523\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 673us/step - loss: 2404.6316 - mse: 2404.6311 - mae: 29.6763 - val_loss: 3697.2188 - val_mse: 3697.2185 - val_mae: 24.5820\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 669us/step - loss: 2367.0254 - mse: 2367.0254 - mae: 29.6938 - val_loss: 3696.2046 - val_mse: 3696.2039 - val_mae: 24.5173\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2401.8117 - mse: 2401.8125 - mae: 29.8249 - val_loss: 3696.9590 - val_mse: 3696.9595 - val_mae: 24.6220\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2389.6876 - mse: 2389.6877 - mae: 29.7441 - val_loss: 3701.9247 - val_mse: 3701.9250 - val_mae: 25.2212\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 670us/step - loss: 2440.0279 - mse: 2440.0276 - mae: 29.7958 - val_loss: 3697.3649 - val_mse: 3697.3645 - val_mae: 24.6752\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2382.4146 - mse: 2382.4143 - mae: 29.6233 - val_loss: 3697.9855 - val_mse: 3697.9858 - val_mae: 24.7440\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2445.7754 - mse: 2445.7756 - mae: 29.8778 - val_loss: 3697.1989 - val_mse: 3697.1987 - val_mae: 24.7164\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2382.9476 - mse: 2382.9475 - mae: 29.6406 - val_loss: 3697.8127 - val_mse: 3697.8132 - val_mae: 24.7753\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2368.2355 - mse: 2368.2344 - mae: 29.8904 - val_loss: 3696.6299 - val_mse: 3696.6296 - val_mae: 24.6552\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2397.5212 - mse: 2397.5210 - mae: 29.7157 - val_loss: 3698.5129 - val_mse: 3698.5134 - val_mae: 24.8544\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2438.5616 - mse: 2438.5613 - mae: 29.9540 - val_loss: 3697.3052 - val_mse: 3697.3059 - val_mae: 24.6493\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2450.7385 - mse: 2450.7390 - mae: 30.0178 - val_loss: 3701.8376 - val_mse: 3701.8374 - val_mae: 25.1970\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2425.0125 - mse: 2425.0127 - mae: 29.9980 - val_loss: 3698.0485 - val_mse: 3698.0479 - val_mae: 24.8016\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2350.2222 - mse: 2350.2231 - mae: 29.8217 - val_loss: 3697.1807 - val_mse: 3697.1812 - val_mae: 24.7381\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2417.7263 - mse: 2417.7268 - mae: 30.1150 - val_loss: 3697.8865 - val_mse: 3697.8860 - val_mae: 24.8136\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2430.1832 - mse: 2430.1833 - mae: 30.0197 - val_loss: 3696.2944 - val_mse: 3696.2935 - val_mae: 24.5470\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2434.7280 - mse: 2434.7280 - mae: 29.7026 - val_loss: 3700.3930 - val_mse: 3700.3933 - val_mae: 25.1037\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2389.2716 - mse: 2389.2727 - mae: 29.6889 - val_loss: 3699.6214 - val_mse: 3699.6228 - val_mae: 25.0151\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2424.5990 - mse: 2424.5989 - mae: 29.7212 - val_loss: 3695.7046 - val_mse: 3695.7041 - val_mae: 24.4715\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2359.3028 - mse: 2359.3032 - mae: 29.2043 - val_loss: 3699.2373 - val_mse: 3699.2373 - val_mae: 24.9916\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2418.0601 - mse: 2418.0605 - mae: 30.0733 - val_loss: 3695.8032 - val_mse: 3695.8032 - val_mae: 24.3689\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2368.5607 - mse: 2368.5603 - mae: 29.8782 - val_loss: 3698.6384 - val_mse: 3698.6375 - val_mae: 24.7966\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2405.5120 - mse: 2405.5120 - mae: 29.7690 - val_loss: 3698.2872 - val_mse: 3698.2871 - val_mae: 24.6752\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2348.1603 - mse: 2348.1604 - mae: 29.7493 - val_loss: 3697.3128 - val_mse: 3697.3125 - val_mae: 24.5020\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2401.5601 - mse: 2401.5603 - mae: 29.8651 - val_loss: 3696.5874 - val_mse: 3696.5874 - val_mae: 24.4966\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2378.6327 - mse: 2378.6323 - mae: 29.8463 - val_loss: 3701.6289 - val_mse: 3701.6292 - val_mae: 25.1606\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2412.6483 - mse: 2412.6477 - mae: 29.6700 - val_loss: 3699.5445 - val_mse: 3699.5444 - val_mae: 24.9833\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2412.6585 - mse: 2412.6589 - mae: 29.5640 - val_loss: 3699.9300 - val_mse: 3699.9294 - val_mae: 25.0853\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 671us/step - loss: 2367.3267 - mse: 2367.3271 - mae: 29.6272 - val_loss: 3698.5667 - val_mse: 3698.5662 - val_mae: 24.8726\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 673us/step - loss: 2373.4661 - mse: 2373.4663 - mae: 29.5091 - val_loss: 3696.8116 - val_mse: 3696.8123 - val_mae: 24.6536\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 654us/step - loss: 2366.7907 - mse: 2366.7908 - mae: 29.4780 - val_loss: 3700.9224 - val_mse: 3700.9221 - val_mae: 25.1261\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 665us/step - loss: 2400.4117 - mse: 2400.4104 - mae: 29.9415 - val_loss: 3697.0528 - val_mse: 3697.0532 - val_mae: 24.6753\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2354.2602 - mse: 2354.2603 - mae: 29.4751 - val_loss: 3700.0863 - val_mse: 3700.0864 - val_mae: 25.0014\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2431.1207 - mse: 2431.1211 - mae: 30.0030 - val_loss: 3700.7049 - val_mse: 3700.7048 - val_mae: 25.0620\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2377.0302 - mse: 2377.0305 - mae: 29.6122 - val_loss: 3700.9780 - val_mse: 3700.9780 - val_mae: 25.1521\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2394.5838 - mse: 2394.5837 - mae: 29.9181 - val_loss: 3695.2879 - val_mse: 3695.2878 - val_mae: 24.5515\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2409.1256 - mse: 2409.1252 - mae: 29.7442 - val_loss: 3697.0271 - val_mse: 3697.0271 - val_mae: 24.8011\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - ETA: 0s - loss: 2396.0667 - mse: 2396.0667 - mae: 29.29 - 2s 594us/step - loss: 2412.7932 - mse: 2412.7930 - mae: 29.5075 - val_loss: 3702.1765 - val_mse: 3702.1760 - val_mae: 25.3525\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2404.6147 - mse: 2404.6150 - mae: 29.4221 - val_loss: 3694.8098 - val_mse: 3694.8098 - val_mae: 24.6024\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2345.2501 - mse: 2345.2498 - mae: 29.5205 - val_loss: 3700.8861 - val_mse: 3700.8862 - val_mae: 25.1763\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2391.1430 - mse: 2391.1426 - mae: 29.4293 - val_loss: 3698.6442 - val_mse: 3698.6445 - val_mae: 24.9685\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2361.9797 - mse: 2361.9802 - mae: 29.7186 - val_loss: 3697.5438 - val_mse: 3697.5442 - val_mae: 24.7874\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2411.7695 - mse: 2411.7695 - mae: 29.6777 - val_loss: 3695.0495 - val_mse: 3695.0496 - val_mae: 24.5652\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2415.3644 - mse: 2415.3647 - mae: 29.7011 - val_loss: 3696.0576 - val_mse: 3696.0583 - val_mae: 24.8254\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2394.7736 - mse: 2394.7732 - mae: 29.5223 - val_loss: 3692.5121 - val_mse: 3692.5120 - val_mae: 24.4445\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2387.4511 - mse: 2387.4514 - mae: 29.4812 - val_loss: 3693.5101 - val_mse: 3693.5098 - val_mae: 24.5608\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2390.7077 - mse: 2390.7078 - mae: 29.6901 - val_loss: 3695.1715 - val_mse: 3695.1714 - val_mae: 24.8136\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2336.7676 - mse: 2336.7673 - mae: 29.0527 - val_loss: 3703.2523 - val_mse: 3703.2529 - val_mae: 25.5149\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2370.8101 - mse: 2370.8108 - mae: 29.3589 - val_loss: 3700.6548 - val_mse: 3700.6543 - val_mae: 25.2324\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2394.2158 - mse: 2394.2156 - mae: 29.6152 - val_loss: 3695.5620 - val_mse: 3695.5623 - val_mae: 24.7324\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2352.8709 - mse: 2352.8716 - mae: 29.2283 - val_loss: 3695.3500 - val_mse: 3695.3508 - val_mae: 24.6905\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2326.2145 - mse: 2326.2151 - mae: 29.0723 - val_loss: 3696.9233 - val_mse: 3696.9226 - val_mae: 24.8910\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2368.2747 - mse: 2368.2744 - mae: 29.3768 - val_loss: 3696.4881 - val_mse: 3696.4893 - val_mae: 24.8906\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2368.6484 - mse: 2368.6482 - mae: 29.5994 - val_loss: 3694.3177 - val_mse: 3694.3176 - val_mae: 24.6544\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2375.5918 - mse: 2375.5920 - mae: 29.9300 - val_loss: 3693.4391 - val_mse: 3693.4395 - val_mae: 24.5425\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2386.4211 - mse: 2386.4204 - mae: 29.7517 - val_loss: 3697.8820 - val_mse: 3697.8826 - val_mae: 25.0524\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2385.9441 - mse: 2385.9438 - mae: 29.7179 - val_loss: 3696.0745 - val_mse: 3696.0747 - val_mae: 24.8833\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2358.1952 - mse: 2358.1941 - mae: 29.2305 - val_loss: 3702.0987 - val_mse: 3702.0991 - val_mae: 25.4237\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2405.2491 - mse: 2405.2488 - mae: 29.7493 - val_loss: 3695.3992 - val_mse: 3695.3982 - val_mae: 24.7445\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2358.1457 - mse: 2358.1460 - mae: 29.8214 - val_loss: 3697.4108 - val_mse: 3697.4114 - val_mae: 24.9935\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2307.1829 - mse: 2307.1829 - mae: 29.2457 - val_loss: 3698.7623 - val_mse: 3698.7620 - val_mae: 25.1342\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2749.8636 - mse: 2749.8647 - mae: 28.9247 - val_loss: 2444.2110 - val_mse: 2444.2112 - val_mae: 27.1699\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2766.2829 - mse: 2766.2827 - mae: 29.1397 - val_loss: 2453.2490 - val_mse: 2453.2488 - val_mae: 27.0037\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2764.5044 - mse: 2764.5042 - mae: 28.7557 - val_loss: 2454.5883 - val_mse: 2454.5884 - val_mae: 27.4111\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2760.0187 - mse: 2760.0190 - mae: 29.1780 - val_loss: 2456.4658 - val_mse: 2456.4658 - val_mae: 27.4085\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 645us/step - loss: 2798.7748 - mse: 2798.7744 - mae: 29.1439 - val_loss: 2463.9878 - val_mse: 2463.9873 - val_mae: 27.2025\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 686us/step - loss: 2773.9861 - mse: 2773.9868 - mae: 28.8612 - val_loss: 2467.4476 - val_mse: 2467.4475 - val_mae: 27.0235\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2765.0288 - mse: 2765.0281 - mae: 28.7826 - val_loss: 2478.9735 - val_mse: 2478.9736 - val_mae: 26.9840\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2722.0426 - mse: 2722.0432 - mae: 28.7913 - val_loss: 2459.4141 - val_mse: 2459.4141 - val_mae: 27.7297\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2777.5332 - mse: 2777.5325 - mae: 29.0136 - val_loss: 2467.9955 - val_mse: 2467.9961 - val_mae: 27.1617\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2696.7300 - mse: 2696.7297 - mae: 28.8417 - val_loss: 2460.2267 - val_mse: 2460.2268 - val_mae: 27.5195\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 676us/step - loss: 2778.9999 - mse: 2779.0000 - mae: 29.1613 - val_loss: 2471.5594 - val_mse: 2471.5593 - val_mae: 27.3533\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2772.3096 - mse: 2772.3105 - mae: 29.2421 - val_loss: 2483.4629 - val_mse: 2483.4626 - val_mae: 27.2274\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2792.2657 - mse: 2792.2661 - mae: 29.1551 - val_loss: 2467.9539 - val_mse: 2467.9536 - val_mae: 27.8850\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2787.9841 - mse: 2787.9841 - mae: 28.8551 - val_loss: 2478.4721 - val_mse: 2478.4722 - val_mae: 27.2925\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2727.2567 - mse: 2727.2563 - mae: 28.9032 - val_loss: 2472.6551 - val_mse: 2472.6548 - val_mae: 27.3402\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2751.9963 - mse: 2751.9963 - mae: 28.9238 - val_loss: 2485.0560 - val_mse: 2485.0562 - val_mae: 27.1218\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2724.3936 - mse: 2724.3945 - mae: 28.5712 - val_loss: 2479.5543 - val_mse: 2479.5547 - val_mae: 27.1991\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2786.6172 - mse: 2786.6182 - mae: 29.0730 - val_loss: 2467.7419 - val_mse: 2467.7417 - val_mae: 27.4681\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2728.1578 - mse: 2728.1584 - mae: 29.1458 - val_loss: 2464.2421 - val_mse: 2464.2422 - val_mae: 27.5905\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2727.7669 - mse: 2727.7666 - mae: 28.8776 - val_loss: 2457.0205 - val_mse: 2457.0203 - val_mae: 27.7248\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2770.1320 - mse: 2770.1316 - mae: 28.9569 - val_loss: 2476.9662 - val_mse: 2476.9668 - val_mae: 27.0039\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2718.9641 - mse: 2718.9629 - mae: 28.8817 - val_loss: 2458.2329 - val_mse: 2458.2332 - val_mae: 27.7488\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2741.3910 - mse: 2741.3914 - mae: 28.9591 - val_loss: 2467.9329 - val_mse: 2467.9321 - val_mae: 27.2147\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2710.5833 - mse: 2710.5837 - mae: 28.7123 - val_loss: 2470.4580 - val_mse: 2470.4578 - val_mae: 27.1722\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2742.6965 - mse: 2742.6968 - mae: 28.8891 - val_loss: 2470.7381 - val_mse: 2470.7378 - val_mae: 27.5469\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2737.3365 - mse: 2737.3362 - mae: 28.7073 - val_loss: 2464.9498 - val_mse: 2464.9497 - val_mae: 27.6738\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2757.4703 - mse: 2757.4700 - mae: 29.1777 - val_loss: 2471.0352 - val_mse: 2471.0352 - val_mae: 27.3583\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2771.7217 - mse: 2771.7231 - mae: 28.9425 - val_loss: 2480.6427 - val_mse: 2480.6426 - val_mae: 27.0634\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2718.2808 - mse: 2718.2805 - mae: 28.9118 - val_loss: 2469.8430 - val_mse: 2469.8428 - val_mae: 27.4137\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2693.0014 - mse: 2693.0012 - mae: 28.8007 - val_loss: 2470.8564 - val_mse: 2470.8564 - val_mae: 27.3134\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2722.2053 - mse: 2722.2058 - mae: 28.7506 - val_loss: 2478.9716 - val_mse: 2478.9719 - val_mae: 27.1137\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 681us/step - loss: 2714.4938 - mse: 2714.4944 - mae: 28.6578 - val_loss: 2462.3471 - val_mse: 2462.3469 - val_mae: 27.6284\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2745.2518 - mse: 2745.2510 - mae: 28.9560 - val_loss: 2462.4450 - val_mse: 2462.4451 - val_mae: 27.1164\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2769.1549 - mse: 2769.1545 - mae: 29.0541 - val_loss: 2468.1527 - val_mse: 2468.1528 - val_mae: 27.3181\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2760.8608 - mse: 2760.8604 - mae: 29.0518 - val_loss: 2475.7532 - val_mse: 2475.7537 - val_mae: 27.2338\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2707.0835 - mse: 2707.0840 - mae: 29.1321 - val_loss: 2467.0890 - val_mse: 2467.0886 - val_mae: 27.5517\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2739.1506 - mse: 2739.1509 - mae: 28.9496 - val_loss: 2470.9910 - val_mse: 2470.9915 - val_mae: 27.2945\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2745.7036 - mse: 2745.7039 - mae: 28.7864 - val_loss: 2470.4832 - val_mse: 2470.4834 - val_mae: 27.4313\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2748.8769 - mse: 2748.8770 - mae: 28.6686 - val_loss: 2472.4422 - val_mse: 2472.4424 - val_mae: 27.3260\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2769.0819 - mse: 2769.0823 - mae: 28.9170 - val_loss: 2477.9308 - val_mse: 2477.9312 - val_mae: 27.3985\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2757.8730 - mse: 2757.8726 - mae: 28.8750 - val_loss: 2474.6597 - val_mse: 2474.6597 - val_mae: 27.2327\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2747.3121 - mse: 2747.3115 - mae: 28.7992 - val_loss: 2470.0560 - val_mse: 2470.0562 - val_mae: 27.4386\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2717.1116 - mse: 2717.1108 - mae: 28.7597 - val_loss: 2474.2265 - val_mse: 2474.2266 - val_mae: 27.4037\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 649us/step - loss: 2728.6467 - mse: 2728.6470 - mae: 28.7333 - val_loss: 2486.8615 - val_mse: 2486.8616 - val_mae: 26.6799\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2755.2262 - mse: 2755.2263 - mae: 28.9839 - val_loss: 2480.6771 - val_mse: 2480.6772 - val_mae: 27.1224\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 544us/step - loss: 2764.8717 - mse: 2764.8706 - mae: 28.7748 - val_loss: 2477.8046 - val_mse: 2477.8047 - val_mae: 27.4890\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2700.1858 - mse: 2700.1868 - mae: 28.9227 - val_loss: 2479.7306 - val_mse: 2479.7305 - val_mae: 27.5558\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2767.9959 - mse: 2767.9961 - mae: 29.0691 - val_loss: 2490.6778 - val_mse: 2490.6777 - val_mae: 27.1445\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2787.6176 - mse: 2787.6169 - mae: 29.1066 - val_loss: 2479.1741 - val_mse: 2479.1741 - val_mae: 27.5469\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2714.5708 - mse: 2714.5723 - mae: 28.7067 - val_loss: 2475.4546 - val_mse: 2475.4546 - val_mae: 27.4777\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2773.7403 - mse: 2773.7402 - mae: 29.0574 - val_loss: 2487.8951 - val_mse: 2487.8950 - val_mae: 27.0256\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2716.0823 - mse: 2716.0830 - mae: 28.8968 - val_loss: 2484.7803 - val_mse: 2484.7803 - val_mae: 27.3140\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2686.4851 - mse: 2686.4846 - mae: 28.6642 - val_loss: 2486.1252 - val_mse: 2486.1252 - val_mae: 27.2860\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 526us/step - loss: 2751.4147 - mse: 2751.4153 - mae: 28.8618 - val_loss: 2476.5901 - val_mse: 2476.5908 - val_mae: 27.5938\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2752.8474 - mse: 2752.8472 - mae: 28.9895 - val_loss: 2487.6230 - val_mse: 2487.6230 - val_mae: 26.9340\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 550us/step - loss: 2786.5097 - mse: 2786.5105 - mae: 29.0860 - val_loss: 2474.9710 - val_mse: 2474.9709 - val_mae: 27.3533\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2675.3829 - mse: 2675.3826 - mae: 28.5507 - val_loss: 2476.2097 - val_mse: 2476.2092 - val_mae: 27.5558\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2712.3100 - mse: 2712.3108 - mae: 28.6365 - val_loss: 2483.2130 - val_mse: 2483.2126 - val_mae: 26.9928\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2759.1616 - mse: 2759.1619 - mae: 28.5988 - val_loss: 2471.7396 - val_mse: 2471.7395 - val_mae: 27.4275\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 684us/step - loss: 2672.1783 - mse: 2672.1782 - mae: 28.5040 - val_loss: 2469.0401 - val_mse: 2469.0405 - val_mae: 27.5098\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 492us/step - loss: 2709.1990 - mse: 2709.1992 - mae: 28.7054 - val_loss: 2463.6011 - val_mse: 2463.6013 - val_mae: 27.3003\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2723.3791 - mse: 2723.3799 - mae: 28.7538 - val_loss: 2470.2097 - val_mse: 2470.2100 - val_mae: 26.9437\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2744.0932 - mse: 2744.0930 - mae: 28.9031 - val_loss: 2464.6444 - val_mse: 2464.6448 - val_mae: 27.2546\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2726.6895 - mse: 2726.6902 - mae: 28.7942 - val_loss: 2463.3679 - val_mse: 2463.3682 - val_mae: 27.0248\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2661.2887 - mse: 2661.2871 - mae: 28.3245 - val_loss: 2454.8962 - val_mse: 2454.8960 - val_mae: 27.6982\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 549us/step - loss: 2753.5482 - mse: 2753.5481 - mae: 29.0016 - val_loss: 2472.6820 - val_mse: 2472.6821 - val_mae: 26.9670\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2691.6975 - mse: 2691.6975 - mae: 28.1882 - val_loss: 2462.7608 - val_mse: 2462.7610 - val_mae: 27.4987\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 420us/step - loss: 2732.9458 - mse: 2732.9458 - mae: 28.5332 - val_loss: 2461.1484 - val_mse: 2461.1487 - val_mae: 27.3182\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 413us/step - loss: 2723.5283 - mse: 2723.5288 - mae: 28.7699 - val_loss: 2466.0029 - val_mse: 2466.0029 - val_mae: 27.4628\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 449us/step - loss: 2746.3582 - mse: 2746.3579 - mae: 29.0254 - val_loss: 2472.9572 - val_mse: 2472.9573 - val_mae: 27.1251\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 511us/step - loss: 2769.0187 - mse: 2769.0186 - mae: 28.6961 - val_loss: 2477.5079 - val_mse: 2477.5076 - val_mae: 26.8206\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2717.4766 - mse: 2717.4766 - mae: 28.6464 - val_loss: 2475.9558 - val_mse: 2475.9558 - val_mae: 27.0059\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2697.7514 - mse: 2697.7512 - mae: 28.4237 - val_loss: 2460.5570 - val_mse: 2460.5569 - val_mae: 27.5839\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2716.9351 - mse: 2716.9348 - mae: 28.8697 - val_loss: 2469.6810 - val_mse: 2469.6807 - val_mae: 27.2040\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2692.4098 - mse: 2692.4094 - mae: 28.5900 - val_loss: 2461.5886 - val_mse: 2461.5886 - val_mae: 27.3051\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2709.2870 - mse: 2709.2878 - mae: 28.7074 - val_loss: 2461.9121 - val_mse: 2461.9124 - val_mae: 26.9966\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2694.3743 - mse: 2694.3748 - mae: 28.2833 - val_loss: 2466.5099 - val_mse: 2466.5100 - val_mae: 27.3361\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2726.3571 - mse: 2726.3572 - mae: 28.6197 - val_loss: 2477.6724 - val_mse: 2477.6724 - val_mae: 26.9205\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2711.7419 - mse: 2711.7407 - mae: 28.5727 - val_loss: 2470.8053 - val_mse: 2470.8054 - val_mae: 27.4881\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2737.1860 - mse: 2737.1853 - mae: 28.9931 - val_loss: 2473.9780 - val_mse: 2473.9780 - val_mae: 27.1510\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 13334.6920 - mse: 13334.6924 - mae: 109.9298 - val_loss: 34623.0306 - val_mse: 34623.0312 - val_mae: 132.7478\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 13225.1259 - mse: 13225.1250 - mae: 109.4396 - val_loss: 34404.3730 - val_mse: 34404.3711 - val_mae: 131.9278\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 454us/step - loss: 12946.6021 - mse: 12946.6016 - mae: 108.1889 - val_loss: 33848.6235 - val_mse: 33848.6250 - val_mae: 129.8221\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 453us/step - loss: 12273.9298 - mse: 12273.9287 - mae: 105.0234 - val_loss: 32396.6732 - val_mse: 32396.6738 - val_mae: 124.1585\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 10610.3491 - mse: 10610.3496 - mae: 96.8188 - val_loss: 29027.6298 - val_mse: 29027.6289 - val_mae: 109.9074\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 7449.8943 - mse: 7449.8940 - mae: 77.9430 - val_loss: 22627.1758 - val_mse: 22627.1758 - val_mae: 75.8132\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 3446.4772 - mse: 3446.4775 - mae: 45.4856 - val_loss: 17195.9379 - val_mse: 17195.9355 - val_mae: 37.9867\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 2495.4379 - mse: 2495.4377 - mae: 36.4837 - val_loss: 17015.0660 - val_mse: 17015.0664 - val_mae: 38.2280\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 2435.9957 - mse: 2435.9956 - mae: 35.5636 - val_loss: 17134.4212 - val_mse: 17134.4199 - val_mae: 37.9637\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 2459.3505 - mse: 2459.3506 - mae: 36.1407 - val_loss: 17089.1672 - val_mse: 17089.1680 - val_mae: 38.0227\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 516us/step - loss: 2478.7957 - mse: 2478.7957 - mae: 36.2681 - val_loss: 17055.1357 - val_mse: 17055.1367 - val_mae: 38.0957\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 468us/step - loss: 2561.1614 - mse: 2561.1616 - mae: 36.2801 - val_loss: 17222.7617 - val_mse: 17222.7598 - val_mae: 37.8834\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 443us/step - loss: 2437.6598 - mse: 2437.6594 - mae: 34.8961 - val_loss: 17030.8362 - val_mse: 17030.8359 - val_mae: 38.1602\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 419us/step - loss: 2513.9359 - mse: 2513.9358 - mae: 37.2322 - val_loss: 17125.9662 - val_mse: 17125.9668 - val_mae: 37.9190\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 459us/step - loss: 2389.7716 - mse: 2389.7717 - mae: 36.0668 - val_loss: 17135.2592 - val_mse: 17135.2598 - val_mae: 37.8972\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2294.2915 - mse: 2294.2913 - mae: 34.9271 - val_loss: 16961.2299 - val_mse: 16961.2305 - val_mae: 38.4312\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 594us/step - loss: 2304.9964 - mse: 2304.9966 - mae: 33.8100 - val_loss: 16945.1578 - val_mse: 16945.1582 - val_mae: 38.5200\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2257.9915 - mse: 2257.9912 - mae: 35.2466 - val_loss: 17262.9714 - val_mse: 17262.9727 - val_mae: 37.8003\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 2274.0986 - mse: 2274.0984 - mae: 34.1406 - val_loss: 17101.5597 - val_mse: 17101.5586 - val_mae: 37.9353\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2433.3853 - mse: 2433.3853 - mae: 33.7471 - val_loss: 17078.9413 - val_mse: 17078.9395 - val_mae: 37.9949\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 629us/step - loss: 2346.6610 - mse: 2346.6609 - mae: 35.4263 - val_loss: 17299.7545 - val_mse: 17299.7559 - val_mae: 37.7548\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 689us/step - loss: 1957.2295 - mse: 1957.2296 - mae: 32.5341 - val_loss: 17200.0065 - val_mse: 17200.0059 - val_mae: 37.7618\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2441.3729 - mse: 2441.3730 - mae: 35.3918 - val_loss: 17182.7715 - val_mse: 17182.7715 - val_mae: 37.7758\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2365.8372 - mse: 2365.8372 - mae: 35.2285 - val_loss: 17207.2881 - val_mse: 17207.2871 - val_mae: 37.7385\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 2385.7601 - mse: 2385.7598 - mae: 35.1160 - val_loss: 17145.5970 - val_mse: 17145.5957 - val_mae: 37.8270\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2227.8393 - mse: 2227.8394 - mae: 33.3936 - val_loss: 17178.5186 - val_mse: 17178.5176 - val_mae: 37.7666\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2260.5892 - mse: 2260.5891 - mae: 34.1096 - val_loss: 17071.6806 - val_mse: 17071.6816 - val_mae: 38.0158\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2187.6731 - mse: 2187.6731 - mae: 33.1384 - val_loss: 17134.0233 - val_mse: 17134.0234 - val_mae: 37.8486\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 2130.8452 - mse: 2130.8452 - mae: 33.1510 - val_loss: 17161.9171 - val_mse: 17161.9180 - val_mae: 37.7836\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 513us/step - loss: 1963.4458 - mse: 1963.4458 - mae: 31.1785 - val_loss: 17131.3850 - val_mse: 17131.3867 - val_mae: 37.8546\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 512us/step - loss: 1938.0919 - mse: 1938.0922 - mae: 30.9448 - val_loss: 17140.8430 - val_mse: 17140.8418 - val_mae: 37.8317\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 691us/step - loss: 2191.7479 - mse: 2191.7480 - mae: 33.0855 - val_loss: 17183.6336 - val_mse: 17183.6348 - val_mae: 37.7366\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2225.1694 - mse: 2225.1692 - mae: 34.6153 - val_loss: 17231.4960 - val_mse: 17231.4980 - val_mae: 37.6567\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2161.3217 - mse: 2161.3218 - mae: 32.8730 - val_loss: 17229.3004 - val_mse: 17229.3008 - val_mae: 37.6578\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 556us/step - loss: 2072.7644 - mse: 2072.7644 - mae: 33.1186 - val_loss: 17097.4302 - val_mse: 17097.4297 - val_mae: 37.9489\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 494us/step - loss: 2073.3058 - mse: 2073.3057 - mae: 31.4763 - val_loss: 17200.7242 - val_mse: 17200.7246 - val_mae: 37.6962\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 445us/step - loss: 2105.1229 - mse: 2105.1230 - mae: 32.7876 - val_loss: 17143.0352 - val_mse: 17143.0332 - val_mae: 37.8287\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 509us/step - loss: 2176.7342 - mse: 2176.7341 - mae: 33.1155 - val_loss: 17188.7723 - val_mse: 17188.7715 - val_mae: 37.7200\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2161.0660 - mse: 2161.0657 - mae: 33.0131 - val_loss: 17209.8239 - val_mse: 17209.8262 - val_mae: 37.6717\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2115.2894 - mse: 2115.2896 - mae: 31.1024 - val_loss: 17064.9733 - val_mse: 17064.9746 - val_mae: 38.1102\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 2128.0363 - mse: 2128.0361 - mae: 33.4048 - val_loss: 17236.4991 - val_mse: 17236.5000 - val_mae: 37.6149\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 524us/step - loss: 2132.3627 - mse: 2132.3628 - mae: 32.7568 - val_loss: 17245.5097 - val_mse: 17245.5098 - val_mae: 37.5958\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 2037.0436 - mse: 2037.0435 - mae: 31.6599 - val_loss: 17165.4755 - val_mse: 17165.4766 - val_mae: 37.7722\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2114.2343 - mse: 2114.2341 - mae: 32.8927 - val_loss: 17312.8554 - val_mse: 17312.8555 - val_mae: 37.5107\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 1978.5162 - mse: 1978.5161 - mae: 30.1362 - val_loss: 17055.9655 - val_mse: 17055.9668 - val_mae: 38.2153\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 663us/step - loss: 2207.1243 - mse: 2207.1245 - mae: 33.4130 - val_loss: 17329.5572 - val_mse: 17329.5566 - val_mae: 37.4931\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 594us/step - loss: 2078.8361 - mse: 2078.8362 - mae: 30.5546 - val_loss: 17316.4727 - val_mse: 17316.4727 - val_mae: 37.4977\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2102.9418 - mse: 2102.9417 - mae: 31.6730 - val_loss: 17281.3874 - val_mse: 17281.3867 - val_mae: 37.5300\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 2114.7898 - mse: 2114.7898 - mae: 31.6772 - val_loss: 17182.6804 - val_mse: 17182.6797 - val_mae: 37.7374\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 694us/step - loss: 2282.5190 - mse: 2282.5193 - mae: 32.4817 - val_loss: 17208.0219 - val_mse: 17208.0234 - val_mae: 37.6757\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 667us/step - loss: 2014.4511 - mse: 2014.4509 - mae: 30.7717 - val_loss: 17124.9029 - val_mse: 17124.9023 - val_mae: 37.9653\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 1875.9965 - mse: 1875.9965 - mae: 30.2742 - val_loss: 17200.0761 - val_mse: 17200.0762 - val_mae: 37.6996\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2027.2073 - mse: 2027.2076 - mae: 30.9526 - val_loss: 17203.2487 - val_mse: 17203.2480 - val_mae: 37.6921\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 687us/step - loss: 1949.6190 - mse: 1949.6190 - mae: 30.2018 - val_loss: 17214.8627 - val_mse: 17214.8633 - val_mae: 37.6617\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 1919.1488 - mse: 1919.1488 - mae: 31.1792 - val_loss: 17214.3240 - val_mse: 17214.3262 - val_mae: 37.6649\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 1964.1622 - mse: 1964.1622 - mae: 31.3761 - val_loss: 17280.4923 - val_mse: 17280.4922 - val_mae: 37.5244\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 2037.5626 - mse: 2037.5626 - mae: 31.5307 - val_loss: 17293.7033 - val_mse: 17293.7031 - val_mae: 37.5021\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2021.5765 - mse: 2021.5765 - mae: 30.8022 - val_loss: 17229.1633 - val_mse: 17229.1621 - val_mae: 37.6304\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 1826.6908 - mse: 1826.6910 - mae: 28.6798 - val_loss: 17236.7038 - val_mse: 17236.7031 - val_mae: 37.6141\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 1941.2175 - mse: 1941.2174 - mae: 29.8348 - val_loss: 17272.0454 - val_mse: 17272.0469 - val_mae: 37.5410\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 1893.5379 - mse: 1893.5380 - mae: 30.3275 - val_loss: 17196.4095 - val_mse: 17196.4082 - val_mae: 37.7285\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 763us/step - loss: 1911.5409 - mse: 1911.5409 - mae: 30.4624 - val_loss: 17357.3473 - val_mse: 17357.3477 - val_mae: 37.4270\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 709us/step - loss: 1972.4349 - mse: 1972.4351 - mae: 30.9375 - val_loss: 17303.7183 - val_mse: 17303.7207 - val_mae: 37.4874\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 695us/step - loss: 2035.0168 - mse: 2035.0170 - mae: 30.8866 - val_loss: 17358.0042 - val_mse: 17358.0039 - val_mae: 37.4194\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 1886.5867 - mse: 1886.5868 - mae: 30.8752 - val_loss: 17246.1292 - val_mse: 17246.1309 - val_mae: 37.6024\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 1793.5110 - mse: 1793.5110 - mae: 29.0756 - val_loss: 17218.2729 - val_mse: 17218.2715 - val_mae: 37.6911\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2072.1682 - mse: 2072.1682 - mae: 31.3780 - val_loss: 17302.2579 - val_mse: 17302.2578 - val_mae: 37.4927\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 1861.0286 - mse: 1861.0288 - mae: 29.4986 - val_loss: 17435.0212 - val_mse: 17435.0195 - val_mae: 37.3612\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 1873.4047 - mse: 1873.4045 - mae: 30.4405 - val_loss: 17241.7175 - val_mse: 17241.7188 - val_mae: 37.6290\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 492us/step - loss: 1844.4966 - mse: 1844.4966 - mae: 29.5656 - val_loss: 17252.4694 - val_mse: 17252.4707 - val_mae: 37.5988\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 539us/step - loss: 1836.9811 - mse: 1836.9810 - mae: 29.7990 - val_loss: 17321.6565 - val_mse: 17321.6562 - val_mae: 37.4621\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 1956.4258 - mse: 1956.4258 - mae: 30.5295 - val_loss: 17232.8319 - val_mse: 17232.8320 - val_mae: 37.6643\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 1808.9442 - mse: 1808.9441 - mae: 28.6725 - val_loss: 17230.6850 - val_mse: 17230.6855 - val_mae: 37.6802\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 1791.1647 - mse: 1791.1647 - mae: 28.3218 - val_loss: 17204.7441 - val_mse: 17204.7441 - val_mae: 37.7960\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 1796.9569 - mse: 1796.9568 - mae: 29.0925 - val_loss: 17303.0012 - val_mse: 17303.0020 - val_mae: 37.4992\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 1931.0000 - mse: 1930.9999 - mae: 29.6055 - val_loss: 17346.1778 - val_mse: 17346.1777 - val_mae: 37.4244\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 1887.5580 - mse: 1887.5579 - mae: 29.3955 - val_loss: 17386.1783 - val_mse: 17386.1777 - val_mae: 37.3642\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 1997.3561 - mse: 1997.3562 - mae: 30.0678 - val_loss: 17340.7934 - val_mse: 17340.7930 - val_mae: 37.4334\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 1946.6785 - mse: 1946.6786 - mae: 29.7539 - val_loss: 17378.7400 - val_mse: 17378.7402 - val_mae: 37.3720\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 1725.1644 - mse: 1725.1644 - mae: 28.2869 - val_loss: 17308.4981 - val_mse: 17308.4961 - val_mae: 37.4977\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 3832.8645 - mse: 3832.8645 - mae: 33.4637 - val_loss: 2233.6341 - val_mse: 2233.6340 - val_mae: 29.9764\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4196.0760 - mse: 4196.0762 - mae: 34.7278 - val_loss: 2286.8269 - val_mse: 2286.8269 - val_mae: 30.1612\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4000.5236 - mse: 4000.5239 - mae: 34.2222 - val_loss: 2331.8554 - val_mse: 2331.8555 - val_mae: 30.4070\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4063.2964 - mse: 4063.2966 - mae: 33.2016 - val_loss: 2284.9227 - val_mse: 2284.9229 - val_mae: 30.1361\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4007.1667 - mse: 4007.1660 - mae: 34.7825 - val_loss: 2375.7004 - val_mse: 2375.7004 - val_mae: 30.6174\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 3869.1377 - mse: 3869.1377 - mae: 32.8716 - val_loss: 2269.0479 - val_mse: 2269.0479 - val_mae: 30.0509\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4204.4192 - mse: 4204.4194 - mae: 33.8668 - val_loss: 2321.9588 - val_mse: 2321.9590 - val_mae: 30.2902\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - ETA: 0s - loss: 4148.7306 - mse: 4148.7310 - mae: 33.11 - 1s 665us/step - loss: 4041.8158 - mse: 4041.8159 - mae: 32.9833 - val_loss: 2304.3194 - val_mse: 2304.3196 - val_mae: 30.1845\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4102.5348 - mse: 4102.5347 - mae: 33.3259 - val_loss: 2356.7576 - val_mse: 2356.7578 - val_mae: 30.4592\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4026.7923 - mse: 4026.7925 - mae: 34.1737 - val_loss: 2324.4761 - val_mse: 2324.4763 - val_mae: 30.2685\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 4163.4330 - mse: 4163.4321 - mae: 33.9221 - val_loss: 2304.7358 - val_mse: 2304.7361 - val_mae: 30.1724\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4156.2624 - mse: 4156.2627 - mae: 34.6376 - val_loss: 2377.2905 - val_mse: 2377.2905 - val_mae: 30.5773\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 494us/step - loss: 4086.8479 - mse: 4086.8484 - mae: 34.1346 - val_loss: 2289.0256 - val_mse: 2289.0256 - val_mae: 30.1091\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 474us/step - loss: 4035.2809 - mse: 4035.2805 - mae: 33.2230 - val_loss: 2298.9175 - val_mse: 2298.9177 - val_mae: 30.1523\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 504us/step - loss: 4001.9826 - mse: 4001.9829 - mae: 33.1126 - val_loss: 2312.8134 - val_mse: 2312.8137 - val_mae: 30.2152\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 534us/step - loss: 3858.8966 - mse: 3858.8967 - mae: 33.1573 - val_loss: 2314.9758 - val_mse: 2314.9761 - val_mae: 30.2267\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 717us/step - loss: 4018.5663 - mse: 4018.5667 - mae: 33.6904 - val_loss: 2298.0975 - val_mse: 2298.0969 - val_mae: 30.1464\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4002.5585 - mse: 4002.5581 - mae: 33.2444 - val_loss: 2291.5455 - val_mse: 2291.5457 - val_mae: 30.1117\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 3998.3207 - mse: 3998.3210 - mae: 33.5519 - val_loss: 2301.4837 - val_mse: 2301.4834 - val_mae: 30.1524\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 4026.6587 - mse: 4026.6587 - mae: 32.8645 - val_loss: 2299.8700 - val_mse: 2299.8696 - val_mae: 30.1408\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4018.9327 - mse: 4018.9329 - mae: 33.8171 - val_loss: 2309.0931 - val_mse: 2309.0933 - val_mae: 30.1779\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 4105.8023 - mse: 4105.8022 - mae: 34.2332 - val_loss: 2336.2926 - val_mse: 2336.2927 - val_mae: 30.3131\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 688us/step - loss: 3896.3015 - mse: 3896.3015 - mae: 33.0041 - val_loss: 2259.2205 - val_mse: 2259.2207 - val_mae: 29.9861\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 688us/step - loss: 4030.6554 - mse: 4030.6555 - mae: 33.4712 - val_loss: 2286.7517 - val_mse: 2286.7517 - val_mae: 30.0736\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4130.6190 - mse: 4130.6191 - mae: 34.2590 - val_loss: 2337.3543 - val_mse: 2337.3540 - val_mae: 30.2888\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4061.4485 - mse: 4061.4480 - mae: 34.0808 - val_loss: 2308.4282 - val_mse: 2308.4280 - val_mae: 30.1572\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 554us/step - loss: 3868.2137 - mse: 3868.2134 - mae: 33.2619 - val_loss: 2270.3802 - val_mse: 2270.3799 - val_mae: 30.0183\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 674us/step - loss: 3789.7814 - mse: 3789.7810 - mae: 32.4708 - val_loss: 2253.2188 - val_mse: 2253.2188 - val_mae: 29.9678\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 519us/step - loss: 3942.5500 - mse: 3942.5505 - mae: 33.0369 - val_loss: 2348.1457 - val_mse: 2348.1460 - val_mae: 30.3385\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4058.8053 - mse: 4058.8049 - mae: 33.5523 - val_loss: 2309.7707 - val_mse: 2309.7705 - val_mae: 30.1646\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4084.5477 - mse: 4084.5479 - mae: 33.0499 - val_loss: 2307.8316 - val_mse: 2307.8315 - val_mae: 30.1589\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4009.5673 - mse: 4009.5674 - mae: 33.3345 - val_loss: 2286.7348 - val_mse: 2286.7351 - val_mae: 30.0800\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 492us/step - loss: 4061.8619 - mse: 4061.8623 - mae: 32.9989 - val_loss: 2264.7650 - val_mse: 2264.7649 - val_mae: 30.0016\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 3937.4691 - mse: 3937.4692 - mae: 32.5748 - val_loss: 2334.2125 - val_mse: 2334.2124 - val_mae: 30.2795\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 547us/step - loss: 4106.4490 - mse: 4106.4492 - mae: 33.6416 - val_loss: 2293.9471 - val_mse: 2293.9473 - val_mae: 30.1107\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 554us/step - loss: 4060.8616 - mse: 4060.8613 - mae: 33.0171 - val_loss: 2275.2579 - val_mse: 2275.2578 - val_mae: 30.0409\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4084.4440 - mse: 4084.4438 - mae: 33.2623 - val_loss: 2347.6148 - val_mse: 2347.6145 - val_mae: 30.3449\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 460us/step - loss: 4104.2249 - mse: 4104.2251 - mae: 33.4929 - val_loss: 2344.2104 - val_mse: 2344.2104 - val_mae: 30.3301\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 4002.4387 - mse: 4002.4385 - mae: 32.7437 - val_loss: 2288.3012 - val_mse: 2288.3013 - val_mae: 30.0931\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 3944.2173 - mse: 3944.2170 - mae: 32.7293 - val_loss: 2331.0635 - val_mse: 2331.0635 - val_mae: 30.2632\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 664us/step - loss: 3975.0201 - mse: 3975.0203 - mae: 33.4812 - val_loss: 2303.7529 - val_mse: 2303.7529 - val_mae: 30.1511\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 3870.5919 - mse: 3870.5920 - mae: 32.3256 - val_loss: 2311.8124 - val_mse: 2311.8123 - val_mae: 30.1831\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 530us/step - loss: 3964.3639 - mse: 3964.3640 - mae: 32.7880 - val_loss: 2306.2076 - val_mse: 2306.2078 - val_mae: 30.1619\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 448us/step - loss: 3958.2648 - mse: 3958.2649 - mae: 32.3715 - val_loss: 2286.6839 - val_mse: 2286.6838 - val_mae: 30.0899\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4006.4674 - mse: 4006.4668 - mae: 32.0713 - val_loss: 2299.8238 - val_mse: 2299.8240 - val_mae: 30.1404\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 3813.6219 - mse: 3813.6223 - mae: 32.7365 - val_loss: 2292.4753 - val_mse: 2292.4751 - val_mae: 30.1122\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 463us/step - loss: 3953.9375 - mse: 3953.9377 - mae: 32.6632 - val_loss: 2317.5522 - val_mse: 2317.5522 - val_mae: 30.2078\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 532us/step - loss: 4061.3633 - mse: 4061.3635 - mae: 33.2652 - val_loss: 2270.7792 - val_mse: 2270.7793 - val_mae: 30.0333\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 553us/step - loss: 3956.8737 - mse: 3956.8735 - mae: 31.9693 - val_loss: 2279.0792 - val_mse: 2279.0791 - val_mae: 30.0621\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 3821.1708 - mse: 3821.1709 - mae: 31.8838 - val_loss: 2298.4225 - val_mse: 2298.4226 - val_mae: 30.1338\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 552us/step - loss: 3653.5862 - mse: 3653.5857 - mae: 31.7401 - val_loss: 2283.1343 - val_mse: 2283.1343 - val_mae: 30.0777\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4090.0159 - mse: 4090.0159 - mae: 32.8408 - val_loss: 2299.7956 - val_mse: 2299.7954 - val_mae: 30.1403\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 3993.3301 - mse: 3993.3298 - mae: 32.5619 - val_loss: 2293.4361 - val_mse: 2293.4363 - val_mae: 30.1163\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 3799.7725 - mse: 3799.7720 - mae: 32.6796 - val_loss: 2298.9789 - val_mse: 2298.9790 - val_mae: 30.1355\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 452us/step - loss: 3962.6052 - mse: 3962.6050 - mae: 32.4778 - val_loss: 2274.7520 - val_mse: 2274.7522 - val_mae: 30.0443\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4030.8122 - mse: 4030.8125 - mae: 32.7927 - val_loss: 2311.8639 - val_mse: 2311.8638 - val_mae: 30.1862\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4006.8526 - mse: 4006.8525 - mae: 32.6626 - val_loss: 2291.7599 - val_mse: 2291.7600 - val_mae: 30.1116\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 3984.3012 - mse: 3984.3013 - mae: 32.5158 - val_loss: 2328.1827 - val_mse: 2328.1829 - val_mae: 30.2608\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 633us/step - loss: 3936.8111 - mse: 3936.8110 - mae: 32.9285 - val_loss: 2288.0386 - val_mse: 2288.0386 - val_mae: 30.0971\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4116.8072 - mse: 4116.8076 - mae: 33.3387 - val_loss: 2338.1617 - val_mse: 2338.1616 - val_mae: 30.3062\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 3773.1730 - mse: 3773.1736 - mae: 31.6643 - val_loss: 2303.7422 - val_mse: 2303.7422 - val_mae: 30.1557\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 3905.2010 - mse: 3905.2014 - mae: 32.6780 - val_loss: 2297.2489 - val_mse: 2297.2490 - val_mae: 30.1301\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 568us/step - loss: 4047.6111 - mse: 4047.6111 - mae: 33.7224 - val_loss: 2282.8041 - val_mse: 2282.8040 - val_mae: 30.0763\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 3991.2098 - mse: 3991.2097 - mae: 31.6129 - val_loss: 2275.3965 - val_mse: 2275.3965 - val_mae: 30.0486\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 513us/step - loss: 3985.3145 - mse: 3985.3142 - mae: 32.4454 - val_loss: 2282.1983 - val_mse: 2282.1980 - val_mae: 30.0752\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 485us/step - loss: 3915.9331 - mse: 3915.9329 - mae: 31.8314 - val_loss: 2256.2692 - val_mse: 2256.2693 - val_mae: 29.9848\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4147.9501 - mse: 4147.9502 - mae: 33.4277 - val_loss: 2343.9663 - val_mse: 2343.9663 - val_mae: 30.3325\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 537us/step - loss: 4030.8695 - mse: 4030.8694 - mae: 33.1948 - val_loss: 2397.2099 - val_mse: 2397.2102 - val_mae: 30.6850\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 552us/step - loss: 4046.2378 - mse: 4046.2380 - mae: 34.1021 - val_loss: 2325.7614 - val_mse: 2325.7615 - val_mae: 30.2512\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4007.6008 - mse: 4007.6006 - mae: 32.6566 - val_loss: 2320.7710 - val_mse: 2320.7710 - val_mae: 30.2309\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 3874.0960 - mse: 3874.0967 - mae: 31.9399 - val_loss: 2287.5466 - val_mse: 2287.5466 - val_mae: 30.1011\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4008.7733 - mse: 4008.7742 - mae: 32.4809 - val_loss: 2331.1485 - val_mse: 2331.1484 - val_mae: 30.2854\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 3922.7205 - mse: 3922.7202 - mae: 31.9242 - val_loss: 2305.2867 - val_mse: 2305.2871 - val_mae: 30.1697\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 3954.0199 - mse: 3954.0203 - mae: 32.9862 - val_loss: 2323.0346 - val_mse: 2323.0347 - val_mae: 30.2565\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 3798.6217 - mse: 3798.6213 - mae: 31.8438 - val_loss: 2229.2478 - val_mse: 2229.2480 - val_mae: 29.9429\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 3715.6685 - mse: 3715.6685 - mae: 31.2743 - val_loss: 2237.7444 - val_mse: 2237.7439 - val_mae: 29.9569\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 664us/step - loss: 3818.4919 - mse: 3818.4919 - mae: 30.8532 - val_loss: 2299.2904 - val_mse: 2299.2905 - val_mae: 30.1422\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 3700.9175 - mse: 3700.9175 - mae: 31.0785 - val_loss: 2245.9369 - val_mse: 2245.9368 - val_mae: 29.9695\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4025.0509 - mse: 4025.0515 - mae: 33.7191 - val_loss: 2255.2380 - val_mse: 2255.2380 - val_mae: 29.9893\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 569us/step - loss: 3927.2007 - mse: 3927.2004 - mae: 31.5895 - val_loss: 2267.5675 - val_mse: 2267.5671 - val_mae: 30.0216\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3350.7325 - mse: 3350.7334 - mae: 32.9383 - val_loss: 1515.2327 - val_mse: 1515.2329 - val_mae: 26.8663\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3316.8463 - mse: 3316.8447 - mae: 31.5079 - val_loss: 1522.5061 - val_mse: 1522.5059 - val_mae: 27.6112\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3446.7004 - mse: 3446.6992 - mae: 33.0644 - val_loss: 1506.3848 - val_mse: 1506.3846 - val_mae: 27.1277\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3404.4365 - mse: 3404.4368 - mae: 32.8833 - val_loss: 1500.4389 - val_mse: 1500.4390 - val_mae: 27.0801\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3368.1374 - mse: 3368.1372 - mae: 31.9424 - val_loss: 1492.0069 - val_mse: 1492.0068 - val_mae: 26.8313\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3297.7391 - mse: 3297.7388 - mae: 32.1957 - val_loss: 1481.3724 - val_mse: 1481.3723 - val_mae: 26.3942\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3304.2792 - mse: 3304.2800 - mae: 31.3712 - val_loss: 1487.6085 - val_mse: 1487.6086 - val_mae: 27.0738\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 701us/step - loss: 3210.0171 - mse: 3210.0166 - mae: 31.2904 - val_loss: 1487.7393 - val_mse: 1487.7393 - val_mae: 27.1691\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 724us/step - loss: 3353.2284 - mse: 3353.2288 - mae: 31.9529 - val_loss: 1475.1148 - val_mse: 1475.1146 - val_mae: 26.5670\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3415.5821 - mse: 3415.5823 - mae: 31.6503 - val_loss: 1494.4324 - val_mse: 1494.4324 - val_mae: 27.6657\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3360.1388 - mse: 3360.1392 - mae: 32.0673 - val_loss: 1470.7240 - val_mse: 1470.7239 - val_mae: 26.5897\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3307.0979 - mse: 3307.0977 - mae: 31.7852 - val_loss: 1465.7787 - val_mse: 1465.7787 - val_mae: 26.3628\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3160.3830 - mse: 3160.3826 - mae: 31.1693 - val_loss: 1467.0724 - val_mse: 1467.0724 - val_mae: 26.5646\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3358.4398 - mse: 3358.4397 - mae: 32.0004 - val_loss: 1474.9955 - val_mse: 1474.9955 - val_mae: 27.0875\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3332.3213 - mse: 3332.3210 - mae: 31.9024 - val_loss: 1460.4963 - val_mse: 1460.4963 - val_mae: 26.1364\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3350.0194 - mse: 3350.0193 - mae: 32.2582 - val_loss: 1461.5359 - val_mse: 1461.5359 - val_mae: 26.4015\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 544us/step - loss: 3323.9707 - mse: 3323.9705 - mae: 31.8194 - val_loss: 1471.1186 - val_mse: 1471.1187 - val_mae: 27.1231\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3228.6875 - mse: 3228.6873 - mae: 31.8083 - val_loss: 1465.3152 - val_mse: 1465.3152 - val_mae: 26.8082\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3299.8182 - mse: 3299.8184 - mae: 31.8655 - val_loss: 1475.7473 - val_mse: 1475.7472 - val_mae: 27.3539\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3198.7766 - mse: 3198.7769 - mae: 31.8847 - val_loss: 1462.7651 - val_mse: 1462.7650 - val_mae: 26.5910\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 693us/step - loss: 3295.2976 - mse: 3295.2976 - mae: 31.9872 - val_loss: 1461.8147 - val_mse: 1461.8148 - val_mae: 26.6215\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 705us/step - loss: 3231.0365 - mse: 3231.0361 - mae: 31.3151 - val_loss: 1462.5665 - val_mse: 1462.5663 - val_mae: 26.7248\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3262.4137 - mse: 3262.4133 - mae: 31.8891 - val_loss: 1461.3263 - val_mse: 1461.3265 - val_mae: 26.6873\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 690us/step - loss: 3388.0616 - mse: 3388.0613 - mae: 32.5853 - val_loss: 1466.7304 - val_mse: 1466.7305 - val_mae: 27.0718\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 660us/step - loss: 3224.8016 - mse: 3224.8022 - mae: 31.9268 - val_loss: 1457.5545 - val_mse: 1457.5543 - val_mae: 26.4556\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3149.8411 - mse: 3149.8418 - mae: 31.2108 - val_loss: 1456.5322 - val_mse: 1456.5321 - val_mae: 26.4522\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3230.6992 - mse: 3230.6997 - mae: 31.0461 - val_loss: 1458.2017 - val_mse: 1458.2017 - val_mae: 26.6613\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3266.8316 - mse: 3266.8311 - mae: 31.5362 - val_loss: 1465.9771 - val_mse: 1465.9771 - val_mae: 27.1291\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3274.8054 - mse: 3274.8047 - mae: 31.5820 - val_loss: 1463.2994 - val_mse: 1463.2993 - val_mae: 26.9865\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3260.6560 - mse: 3260.6555 - mae: 31.4128 - val_loss: 1467.8900 - val_mse: 1467.8900 - val_mae: 27.2504\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 675us/step - loss: 3298.9224 - mse: 3298.9221 - mae: 31.9586 - val_loss: 1455.4040 - val_mse: 1455.4041 - val_mae: 26.4733\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 667us/step - loss: 3263.5944 - mse: 3263.5942 - mae: 31.7147 - val_loss: 1459.4738 - val_mse: 1459.4738 - val_mae: 26.8470\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 636us/step - loss: 3271.5193 - mse: 3271.5193 - mae: 31.7592 - val_loss: 1455.4840 - val_mse: 1455.4840 - val_mae: 26.6028\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3238.4392 - mse: 3238.4402 - mae: 31.4215 - val_loss: 1459.3416 - val_mse: 1459.3416 - val_mae: 26.8925\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3212.7285 - mse: 3212.7280 - mae: 31.2302 - val_loss: 1460.5206 - val_mse: 1460.5205 - val_mae: 26.9865\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3190.9016 - mse: 3190.9014 - mae: 31.9150 - val_loss: 1453.9112 - val_mse: 1453.9113 - val_mae: 26.5440\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3302.6284 - mse: 3302.6287 - mae: 31.12 - 1s 609us/step - loss: 3156.9781 - mse: 3156.9783 - mae: 30.9889 - val_loss: 1461.1000 - val_mse: 1461.1000 - val_mae: 27.0392\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3207.7646 - mse: 3207.7651 - mae: 31.4338 - val_loss: 1469.7894 - val_mse: 1469.7894 - val_mae: 27.4712\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3188.2615 - mse: 3188.2605 - mae: 31.5594 - val_loss: 1463.4585 - val_mse: 1463.4585 - val_mae: 27.1251\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 466us/step - loss: 3247.5085 - mse: 3247.5083 - mae: 31.3753 - val_loss: 1468.4758 - val_mse: 1468.4760 - val_mae: 27.3614\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3326.1966 - mse: 3326.1968 - mae: 31.7569 - val_loss: 1464.5944 - val_mse: 1464.5944 - val_mae: 27.1751\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3305.3355 - mse: 3305.3347 - mae: 31.8737 - val_loss: 1453.9628 - val_mse: 1453.9629 - val_mae: 26.4988\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3234.5824 - mse: 3234.5820 - mae: 31.5782 - val_loss: 1456.6531 - val_mse: 1456.6531 - val_mae: 26.7628\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3152.2514 - mse: 3152.2515 - mae: 31.0715 - val_loss: 1451.8147 - val_mse: 1451.8147 - val_mae: 26.3676\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3257.7014 - mse: 3257.7017 - mae: 31.1425 - val_loss: 1451.7506 - val_mse: 1451.7505 - val_mae: 26.4221\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3185.6632 - mse: 3185.6628 - mae: 31.0639 - val_loss: 1458.5555 - val_mse: 1458.5555 - val_mae: 26.9573\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3224.5240 - mse: 3224.5244 - mae: 31.6016 - val_loss: 1452.7147 - val_mse: 1452.7150 - val_mae: 26.5267\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3221.6122 - mse: 3221.6123 - mae: 31.6625 - val_loss: 1454.3563 - val_mse: 1454.3563 - val_mae: 26.6725\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3289.1825 - mse: 3289.1826 - mae: 31.4856 - val_loss: 1450.1797 - val_mse: 1450.1797 - val_mae: 26.3424\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 636us/step - loss: 3170.0384 - mse: 3170.0381 - mae: 31.0480 - val_loss: 1452.0060 - val_mse: 1452.0059 - val_mae: 26.5572\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 689us/step - loss: 3377.3273 - mse: 3377.3279 - mae: 32.4573 - val_loss: 1451.0260 - val_mse: 1451.0259 - val_mae: 26.5052\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 680us/step - loss: 3152.0007 - mse: 3152.0010 - mae: 31.8431 - val_loss: 1454.1368 - val_mse: 1454.1367 - val_mae: 26.7711\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 697us/step - loss: 3201.6710 - mse: 3201.6716 - mae: 30.9580 - val_loss: 1451.9106 - val_mse: 1451.9106 - val_mae: 26.6139\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 519us/step - loss: 3248.7317 - mse: 3248.7314 - mae: 31.5512 - val_loss: 1453.7843 - val_mse: 1453.7843 - val_mae: 26.7590\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3221.7570 - mse: 3221.7578 - mae: 30.9675 - val_loss: 1453.4314 - val_mse: 1453.4313 - val_mae: 26.7561\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3133.8467 - mse: 3133.8462 - mae: 31.4051 - val_loss: 1452.6614 - val_mse: 1452.6613 - val_mae: 26.7091\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3239.3177 - mse: 3239.3174 - mae: 30.8658 - val_loss: 1457.6949 - val_mse: 1457.6951 - val_mae: 27.0223\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 684us/step - loss: 3292.1870 - mse: 3292.1870 - mae: 31.2135 - val_loss: 1456.5466 - val_mse: 1456.5466 - val_mae: 26.9634\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3287.1612 - mse: 3287.1602 - mae: 31.39 - 1s 687us/step - loss: 3277.0172 - mse: 3277.0161 - mae: 31.4745 - val_loss: 1452.5669 - val_mse: 1452.5669 - val_mae: 26.6789\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3326.5171 - mse: 3326.5166 - mae: 31.4936 - val_loss: 1450.3982 - val_mse: 1450.3982 - val_mae: 26.5161\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 681us/step - loss: 3226.3669 - mse: 3226.3672 - mae: 31.0080 - val_loss: 1448.1768 - val_mse: 1448.1770 - val_mae: 26.2650\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3184.5347 - mse: 3184.5347 - mae: 31.3853 - val_loss: 1453.7917 - val_mse: 1453.7916 - val_mae: 26.8241\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3240.4065 - mse: 3240.4062 - mae: 31.1861 - val_loss: 1450.6588 - val_mse: 1450.6589 - val_mae: 26.6019\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3314.7700 - mse: 3314.7700 - mae: 31.8722 - val_loss: 1452.8209 - val_mse: 1452.8210 - val_mae: 26.7180\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 513us/step - loss: 3246.3699 - mse: 3246.3706 - mae: 31.3175 - val_loss: 1451.5808 - val_mse: 1451.5809 - val_mae: 26.6712\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3215.6506 - mse: 3215.6504 - mae: 31.1498 - val_loss: 1448.4381 - val_mse: 1448.4384 - val_mae: 26.3688\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3233.9359 - mse: 3233.9370 - mae: 31.3757 - val_loss: 1452.1747 - val_mse: 1452.1747 - val_mae: 26.7480\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3244.3640 - mse: 3244.3638 - mae: 31.2832 - val_loss: 1448.2689 - val_mse: 1448.2689 - val_mae: 26.4506\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3172.6805 - mse: 3172.6807 - mae: 31.1619 - val_loss: 1456.9285 - val_mse: 1456.9285 - val_mae: 27.0792\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3281.5758 - mse: 3281.5757 - mae: 31.3682 - val_loss: 1448.0272 - val_mse: 1448.0272 - val_mae: 26.3770\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3213.1252 - mse: 3213.1255 - mae: 31.8682 - val_loss: 1449.7433 - val_mse: 1449.7433 - val_mae: 26.5848\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3118.2056 - mse: 3118.2046 - mae: 31.0717 - val_loss: 1450.3763 - val_mse: 1450.3763 - val_mae: 26.6731\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 535us/step - loss: 3149.8567 - mse: 3149.8562 - mae: 30.8163 - val_loss: 1452.9076 - val_mse: 1452.9076 - val_mae: 26.8751\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3142.2239 - mse: 3142.2236 - mae: 30.6648 - val_loss: 1454.1253 - val_mse: 1454.1254 - val_mae: 26.9373\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3308.2332 - mse: 3308.2339 - mae: 31.8553 - val_loss: 1446.4243 - val_mse: 1446.4243 - val_mae: 26.2261\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3027.1027 - mse: 3027.1028 - mae: 30.0592 - val_loss: 1455.6300 - val_mse: 1455.6300 - val_mae: 27.0348\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3134.4753 - mse: 3134.4749 - mae: 31.2844 - val_loss: 1447.7135 - val_mse: 1447.7136 - val_mae: 26.3779\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3180.6673 - mse: 3180.6672 - mae: 30.7523 - val_loss: 1455.9325 - val_mse: 1455.9325 - val_mae: 27.0730\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3172.7200 - mse: 3172.7200 - mae: 31.0986 - val_loss: 1449.9575 - val_mse: 1449.9573 - val_mae: 26.6707\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3232.2342 - mse: 3232.2341 - mae: 31.3114 - val_loss: 1449.7839 - val_mse: 1449.7841 - val_mae: 26.6462\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2827.8373 - mse: 2827.8372 - mae: 30.8373 - val_loss: 1120.5023 - val_mse: 1120.5022 - val_mae: 23.4377\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 553us/step - loss: 2868.8327 - mse: 2868.8330 - mae: 30.9746 - val_loss: 1130.9213 - val_mse: 1130.9213 - val_mae: 23.2216\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2935.3934 - mse: 2935.3926 - mae: 30.4603 - val_loss: 1116.5495 - val_mse: 1116.5496 - val_mae: 23.3682\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2945.5666 - mse: 2945.5664 - mae: 30.6154 - val_loss: 1122.2163 - val_mse: 1122.2163 - val_mae: 23.2349\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2836.7441 - mse: 2836.7441 - mae: 30.6173 - val_loss: 1111.4595 - val_mse: 1111.4595 - val_mae: 23.4123\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2863.4418 - mse: 2863.4404 - mae: 30.8022 - val_loss: 1108.5932 - val_mse: 1108.5933 - val_mae: 23.4596\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2839.8558 - mse: 2839.8552 - mae: 30.5997 - val_loss: 1112.1268 - val_mse: 1112.1266 - val_mae: 23.3751\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2848.5556 - mse: 2848.5552 - mae: 30.9851 - val_loss: 1104.0986 - val_mse: 1104.0986 - val_mae: 23.5824\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2761.7297 - mse: 2761.7290 - mae: 30.4234 - val_loss: 1104.6151 - val_mse: 1104.6151 - val_mae: 23.5750\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2788.1652 - mse: 2788.1658 - mae: 30.8790 - val_loss: 1104.7729 - val_mse: 1104.7731 - val_mae: 23.5792\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2881.0911 - mse: 2881.0908 - mae: 30.6312 - val_loss: 1103.6625 - val_mse: 1103.6624 - val_mae: 23.6055\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2849.6921 - mse: 2849.6912 - mae: 30.7372 - val_loss: 1101.7427 - val_mse: 1101.7427 - val_mae: 23.6633\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2778.3378 - mse: 2778.3374 - mae: 30.1145 - val_loss: 1104.1527 - val_mse: 1104.1528 - val_mae: 23.5933\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2890.8621 - mse: 2890.8635 - mae: 30.7515 - val_loss: 1106.2350 - val_mse: 1106.2352 - val_mae: 23.5325\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 687us/step - loss: 2837.0728 - mse: 2837.0713 - mae: 29.9739 - val_loss: 1101.5188 - val_mse: 1101.5189 - val_mae: 23.6198\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 666us/step - loss: 2850.1453 - mse: 2850.1450 - mae: 30.5293 - val_loss: 1100.2106 - val_mse: 1100.2106 - val_mae: 23.6888\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2910.5841 - mse: 2910.5835 - mae: 30.1692 - val_loss: 1116.7080 - val_mse: 1116.7080 - val_mae: 23.3003\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2909.4990 - mse: 2909.4993 - mae: 30.7093 - val_loss: 1116.1016 - val_mse: 1116.1018 - val_mae: 23.3036\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2770.5387 - mse: 2770.5388 - mae: 30.2044 - val_loss: 1109.8937 - val_mse: 1109.8937 - val_mae: 23.4781\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 521us/step - loss: 2804.0938 - mse: 2804.0942 - mae: 30.1632 - val_loss: 1107.5870 - val_mse: 1107.5869 - val_mae: 23.5327\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 501us/step - loss: 2862.8186 - mse: 2862.8186 - mae: 30.4488 - val_loss: 1107.1294 - val_mse: 1107.1294 - val_mae: 23.5396\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2863.4905 - mse: 2863.4907 - mae: 30.5051 - val_loss: 1103.9251 - val_mse: 1103.9250 - val_mae: 23.6261\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2774.6440 - mse: 2774.6433 - mae: 29.5689 - val_loss: 1092.6447 - val_mse: 1092.6449 - val_mae: 24.1282\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 527us/step - loss: 2874.4370 - mse: 2874.4365 - mae: 30.9929 - val_loss: 1116.9311 - val_mse: 1116.9310 - val_mae: 23.2794\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2858.5064 - mse: 2858.5068 - mae: 30.6086 - val_loss: 1108.5859 - val_mse: 1108.5859 - val_mae: 23.4295\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 551us/step - loss: 2876.0169 - mse: 2876.0166 - mae: 30.9388 - val_loss: 1115.9334 - val_mse: 1115.9333 - val_mae: 23.2758\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 704us/step - loss: 2859.3420 - mse: 2859.3418 - mae: 30.6562 - val_loss: 1110.7928 - val_mse: 1110.7928 - val_mae: 23.3610\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 649us/step - loss: 2838.9656 - mse: 2838.9656 - mae: 30.3510 - val_loss: 1097.8205 - val_mse: 1097.8204 - val_mae: 23.7866\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 640us/step - loss: 2805.7019 - mse: 2805.7024 - mae: 30.0482 - val_loss: 1095.6792 - val_mse: 1095.6792 - val_mae: 23.8756\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2818.8983 - mse: 2818.8979 - mae: 30.0157 - val_loss: 1097.5341 - val_mse: 1097.5341 - val_mae: 23.8534\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 2905.9478 - mse: 2905.9485 - mae: 30.7538 - val_loss: 1111.5340 - val_mse: 1111.5341 - val_mae: 23.4016\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2828.0138 - mse: 2828.0137 - mae: 30.1608 - val_loss: 1106.8493 - val_mse: 1106.8492 - val_mae: 23.4768\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 677us/step - loss: 2789.2543 - mse: 2789.2546 - mae: 30.3176 - val_loss: 1100.8742 - val_mse: 1100.8741 - val_mae: 23.7089\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 687us/step - loss: 2833.3710 - mse: 2833.3718 - mae: 30.2254 - val_loss: 1101.9369 - val_mse: 1101.9369 - val_mae: 23.6882\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2850.2462 - mse: 2850.2458 - mae: 30.3608 - val_loss: 1100.4553 - val_mse: 1100.4553 - val_mae: 23.7583\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 666us/step - loss: 2799.2032 - mse: 2799.2034 - mae: 30.1532 - val_loss: 1108.3032 - val_mse: 1108.3032 - val_mae: 23.4922\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2813.5243 - mse: 2813.5247 - mae: 30.5511 - val_loss: 1105.9650 - val_mse: 1105.9650 - val_mae: 23.5552\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2831.2110 - mse: 2831.2112 - mae: 30.5436 - val_loss: 1097.6764 - val_mse: 1097.6763 - val_mae: 23.8839\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 543us/step - loss: 2832.5030 - mse: 2832.5029 - mae: 30.4965 - val_loss: 1102.1111 - val_mse: 1102.1108 - val_mae: 23.6936\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2854.2522 - mse: 2854.2524 - mae: 30.9965 - val_loss: 1104.2317 - val_mse: 1104.2318 - val_mae: 23.6974\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2770.0990 - mse: 2770.0984 - mae: 29.3411 - val_loss: 1103.5251 - val_mse: 1103.5253 - val_mae: 23.7636\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 497us/step - loss: 2790.7056 - mse: 2790.7056 - mae: 29.9927 - val_loss: 1108.6842 - val_mse: 1108.6842 - val_mae: 23.5568\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2862.1183 - mse: 2862.1187 - mae: 30.4500 - val_loss: 1106.7906 - val_mse: 1106.7905 - val_mae: 23.6213\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2704.1724 - mse: 2704.1724 - mae: 30.0954 - val_loss: 1105.5232 - val_mse: 1105.5233 - val_mae: 23.6362\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2760.8278 - mse: 2760.8276 - mae: 30.1021 - val_loss: 1110.3061 - val_mse: 1110.3062 - val_mae: 23.5657\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 2815.1690 - mse: 2815.1682 - mae: 30.6062 - val_loss: 1103.8429 - val_mse: 1103.8429 - val_mae: 23.7688\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 656us/step - loss: 2801.4753 - mse: 2801.4751 - mae: 30.0827 - val_loss: 1106.2176 - val_mse: 1106.2177 - val_mae: 23.8071\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2739.0192 - mse: 2739.0186 - mae: 30.1366 - val_loss: 1110.8422 - val_mse: 1110.8422 - val_mae: 23.7132\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2795.2055 - mse: 2795.2051 - mae: 29.8027 - val_loss: 1108.9214 - val_mse: 1108.9215 - val_mae: 23.7280\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 710us/step - loss: 2720.9015 - mse: 2720.9014 - mae: 30.2704 - val_loss: 1112.3470 - val_mse: 1112.3468 - val_mae: 23.6788\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2798.5431 - mse: 2798.5437 - mae: 30.1946 - val_loss: 1112.1958 - val_mse: 1112.1958 - val_mae: 23.7455\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2759.4537 - mse: 2759.4536 - mae: 29.8105 - val_loss: 1123.3734 - val_mse: 1123.3734 - val_mae: 23.4971\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2837.0562 - mse: 2837.0564 - mae: 30.1301 - val_loss: 1116.1925 - val_mse: 1116.1925 - val_mae: 23.6634\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 678us/step - loss: 2806.5626 - mse: 2806.5627 - mae: 30.2641 - val_loss: 1111.1977 - val_mse: 1111.1976 - val_mae: 23.7392\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 671us/step - loss: 2826.3039 - mse: 2826.3030 - mae: 29.7931 - val_loss: 1115.6392 - val_mse: 1115.6392 - val_mae: 23.6367\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2655.8710 - mse: 2655.8708 - mae: 29.5886 - val_loss: 1110.9242 - val_mse: 1110.9242 - val_mae: 23.7831\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2862.0384 - mse: 2862.0374 - mae: 30.5400 - val_loss: 1117.3982 - val_mse: 1117.3982 - val_mae: 23.6625\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2822.1142 - mse: 2822.1133 - mae: 30.2940 - val_loss: 1115.1521 - val_mse: 1115.1521 - val_mae: 23.7563\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2828.1994 - mse: 2828.1992 - mae: 30.2423 - val_loss: 1113.5989 - val_mse: 1113.5990 - val_mae: 23.7875\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2752.8574 - mse: 2752.8582 - mae: 29.9703 - val_loss: 1108.9433 - val_mse: 1108.9432 - val_mae: 23.7725\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2784.1254 - mse: 2784.1255 - mae: 30.3940 - val_loss: 1105.8574 - val_mse: 1105.8573 - val_mae: 23.8978\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2799.9508 - mse: 2799.9504 - mae: 30.3531 - val_loss: 1116.3939 - val_mse: 1116.3939 - val_mae: 23.5694\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2774.6279 - mse: 2774.6274 - mae: 30.1764 - val_loss: 1112.3070 - val_mse: 1112.3071 - val_mae: 23.6482\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2758.7069 - mse: 2758.7070 - mae: 29.2900 - val_loss: 1111.4088 - val_mse: 1111.4088 - val_mae: 23.7222\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2767.3008 - mse: 2767.3008 - mae: 30.0575 - val_loss: 1106.2586 - val_mse: 1106.2587 - val_mae: 23.8415\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2780.7026 - mse: 2780.7026 - mae: 30.1377 - val_loss: 1111.7928 - val_mse: 1111.7927 - val_mae: 23.7688\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2775.5939 - mse: 2775.5935 - mae: 30.1822 - val_loss: 1122.9446 - val_mse: 1122.9446 - val_mae: 23.6656\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2789.2149 - mse: 2789.2151 - mae: 30.2637 - val_loss: 1109.9577 - val_mse: 1109.9578 - val_mae: 24.0751\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2810.2196 - mse: 2810.2205 - mae: 30.4342 - val_loss: 1120.0390 - val_mse: 1120.0389 - val_mae: 23.8211\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2798.1583 - mse: 2798.1580 - mae: 30.4168 - val_loss: 1111.3420 - val_mse: 1111.3420 - val_mae: 24.0727\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2718.8246 - mse: 2718.8250 - mae: 29.9931 - val_loss: 1114.4619 - val_mse: 1114.4619 - val_mae: 23.9228\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 514us/step - loss: 2772.7566 - mse: 2772.7568 - mae: 29.9572 - val_loss: 1116.3868 - val_mse: 1116.3867 - val_mae: 23.9308\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2692.8158 - mse: 2692.8157 - mae: 29.7703 - val_loss: 1113.3265 - val_mse: 1113.3265 - val_mae: 24.0122\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2815.8021 - mse: 2815.8018 - mae: 30.1535 - val_loss: 1114.0387 - val_mse: 1114.0386 - val_mae: 23.9770\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2803.8506 - mse: 2803.8501 - mae: 30.0863 - val_loss: 1117.3587 - val_mse: 1117.3589 - val_mae: 23.6742\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 518us/step - loss: 2801.7073 - mse: 2801.7073 - mae: 29.9776 - val_loss: 1106.9058 - val_mse: 1106.9059 - val_mae: 23.9765\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 539us/step - loss: 2748.1051 - mse: 2748.1055 - mae: 29.7616 - val_loss: 1112.8295 - val_mse: 1112.8296 - val_mae: 23.9379\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2703.6279 - mse: 2703.6279 - mae: 30.1523 - val_loss: 1110.3837 - val_mse: 1110.3835 - val_mae: 23.7499\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2729.5125 - mse: 2729.5127 - mae: 29.5433 - val_loss: 1110.0890 - val_mse: 1110.0889 - val_mae: 23.7697\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2816.2574 - mse: 2816.2576 - mae: 29.8182 - val_loss: 1113.9186 - val_mse: 1113.9186 - val_mae: 23.7957\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2474.4337 - mse: 2474.4351 - mae: 29.7990 - val_loss: 1518.4495 - val_mse: 1518.4496 - val_mae: 27.4752\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2541.1838 - mse: 2541.1838 - mae: 29.7672 - val_loss: 1535.8624 - val_mse: 1535.8623 - val_mae: 27.0517\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2495.6531 - mse: 2495.6528 - mae: 29.8369 - val_loss: 1519.6419 - val_mse: 1519.6420 - val_mae: 27.4174\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2478.7063 - mse: 2478.7063 - mae: 29.2773 - val_loss: 1516.4727 - val_mse: 1516.4728 - val_mae: 27.5151\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2546.7041 - mse: 2546.7041 - mae: 29.7061 - val_loss: 1522.5010 - val_mse: 1522.5009 - val_mae: 27.3254\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2511.0731 - mse: 2511.0732 - mae: 29.5421 - val_loss: 1510.2787 - val_mse: 1510.2787 - val_mae: 27.7233\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2459.6658 - mse: 2459.6665 - mae: 29.4544 - val_loss: 1518.5529 - val_mse: 1518.5529 - val_mae: 27.4130\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2517.1736 - mse: 2517.1736 - mae: 29.8260 - val_loss: 1510.1029 - val_mse: 1510.1028 - val_mae: 27.7212\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2485.7327 - mse: 2485.7327 - mae: 29.5284 - val_loss: 1514.1391 - val_mse: 1514.1390 - val_mae: 27.5672\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2478.8791 - mse: 2478.8796 - mae: 29.2997 - val_loss: 1512.4583 - val_mse: 1512.4580 - val_mae: 27.6080\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2432.1286 - mse: 2432.1287 - mae: 29.3091 - val_loss: 1511.0136 - val_mse: 1511.0138 - val_mae: 27.6452\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2572.8850 - mse: 2572.8850 - mae: 29.7403 - val_loss: 1511.6242 - val_mse: 1511.6241 - val_mae: 27.6315\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2503.2084 - mse: 2503.2087 - mae: 29.5693 - val_loss: 1513.9496 - val_mse: 1513.9495 - val_mae: 27.5448\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2530.8189 - mse: 2530.8196 - mae: 29.4074 - val_loss: 1512.3223 - val_mse: 1512.3223 - val_mae: 27.5650\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2557.1724 - mse: 2557.1726 - mae: 29.7339 - val_loss: 1514.1910 - val_mse: 1514.1908 - val_mae: 27.5180\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2457.8395 - mse: 2457.8398 - mae: 29.3913 - val_loss: 1517.8159 - val_mse: 1517.8158 - val_mae: 27.4110\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2528.3137 - mse: 2528.3140 - mae: 29.3516 - val_loss: 1513.4087 - val_mse: 1513.4084 - val_mae: 27.5518\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 648us/step - loss: 2497.4074 - mse: 2497.4077 - mae: 29.4723 - val_loss: 1520.8314 - val_mse: 1520.8314 - val_mae: 27.2880\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2557.1705 - mse: 2557.1711 - mae: 29.7321 - val_loss: 1508.6714 - val_mse: 1508.6715 - val_mae: 27.7043\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2487.9357 - mse: 2487.9353 - mae: 29.4018 - val_loss: 1517.2265 - val_mse: 1517.2267 - val_mae: 27.3762\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2540.0318 - mse: 2540.0325 - mae: 29.5444 - val_loss: 1515.9344 - val_mse: 1515.9344 - val_mae: 27.4173\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 516us/step - loss: 2417.6983 - mse: 2417.6985 - mae: 29.3875 - val_loss: 1511.4945 - val_mse: 1511.4946 - val_mae: 27.5928\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 545us/step - loss: 2498.5261 - mse: 2498.5254 - mae: 29.7008 - val_loss: 1514.3799 - val_mse: 1514.3800 - val_mae: 27.4570\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2454.9536 - mse: 2454.9541 - mae: 28.6742 - val_loss: 1517.9723 - val_mse: 1517.9722 - val_mae: 27.3344\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2467.7387 - mse: 2467.7390 - mae: 29.4779 - val_loss: 1514.6595 - val_mse: 1514.6594 - val_mae: 27.4148\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2437.2293 - mse: 2437.2290 - mae: 29.0158 - val_loss: 1507.6762 - val_mse: 1507.6764 - val_mae: 27.6597\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2371.4261 - mse: 2371.4260 - mae: 28.7010 - val_loss: 1507.4801 - val_mse: 1507.4801 - val_mae: 27.6387\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2439.9509 - mse: 2439.9509 - mae: 29.2469 - val_loss: 1509.1749 - val_mse: 1509.1750 - val_mae: 27.5836\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2439.3723 - mse: 2439.3723 - mae: 29.2675 - val_loss: 1521.8803 - val_mse: 1521.8802 - val_mae: 27.1750\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2450.9943 - mse: 2450.9944 - mae: 29.2758 - val_loss: 1509.7924 - val_mse: 1509.7924 - val_mae: 27.5393\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2474.2260 - mse: 2474.2253 - mae: 29.3598 - val_loss: 1509.6487 - val_mse: 1509.6487 - val_mae: 27.5220\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2437.6727 - mse: 2437.6724 - mae: 28.7786 - val_loss: 1504.2296 - val_mse: 1504.2296 - val_mae: 27.7433\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2500.7240 - mse: 2500.7236 - mae: 29.4764 - val_loss: 1511.5517 - val_mse: 1511.5516 - val_mae: 27.4526\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2402.3744 - mse: 2402.3745 - mae: 28.4870 - val_loss: 1503.1302 - val_mse: 1503.1302 - val_mae: 27.7368\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2473.3820 - mse: 2473.3828 - mae: 28.9725 - val_loss: 1508.9505 - val_mse: 1508.9507 - val_mae: 27.4707\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2495.7322 - mse: 2495.7319 - mae: 29.3019 - val_loss: 1506.7625 - val_mse: 1506.7626 - val_mae: 27.5303\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2455.4455 - mse: 2455.4458 - mae: 29.3148 - val_loss: 1505.9866 - val_mse: 1505.9867 - val_mae: 27.5454\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2505.0874 - mse: 2505.0876 - mae: 29.4172 - val_loss: 1512.3159 - val_mse: 1512.3160 - val_mae: 27.3487\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 670us/step - loss: 2454.1705 - mse: 2454.1699 - mae: 29.3385 - val_loss: 1510.7777 - val_mse: 1510.7777 - val_mae: 27.3798\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2457.3147 - mse: 2457.3145 - mae: 29.1074 - val_loss: 1506.1928 - val_mse: 1506.1929 - val_mae: 27.5275\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2454.5624 - mse: 2454.5620 - mae: 29.2824 - val_loss: 1515.1824 - val_mse: 1515.1825 - val_mae: 27.2480\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2470.5390 - mse: 2470.5391 - mae: 29.1431 - val_loss: 1500.6647 - val_mse: 1500.6647 - val_mae: 27.7431\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2484.7838 - mse: 2484.7834 - mae: 29.5165 - val_loss: 1508.0497 - val_mse: 1508.0497 - val_mae: 27.4410\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2440.0503 - mse: 2440.0503 - mae: 29.1499 - val_loss: 1506.9117 - val_mse: 1506.9117 - val_mae: 27.4578\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2455.4358 - mse: 2455.4363 - mae: 28.8848 - val_loss: 1497.2584 - val_mse: 1497.2583 - val_mae: 27.8896\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2442.6540 - mse: 2442.6541 - mae: 29.4833 - val_loss: 1503.0080 - val_mse: 1503.0077 - val_mae: 27.5999\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2424.9984 - mse: 2424.9993 - mae: 28.7313 - val_loss: 1504.5025 - val_mse: 1504.5026 - val_mae: 27.5285\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2493.5418 - mse: 2493.5420 - mae: 29.0383 - val_loss: 1505.0692 - val_mse: 1505.0693 - val_mae: 27.5000\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2477.3538 - mse: 2477.3538 - mae: 28.9752 - val_loss: 1499.6768 - val_mse: 1499.6768 - val_mae: 27.7288\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2504.1751 - mse: 2504.1750 - mae: 29.3473 - val_loss: 1515.5218 - val_mse: 1515.5217 - val_mae: 27.1785\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2426.2880 - mse: 2426.2881 - mae: 29.2512 - val_loss: 1503.8971 - val_mse: 1503.8971 - val_mae: 27.5324\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2509.7595 - mse: 2509.7595 - mae: 29.5943 - val_loss: 1505.3796 - val_mse: 1505.3798 - val_mae: 27.4928\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2401.2073 - mse: 2401.2075 - mae: 28.8098 - val_loss: 1506.0876 - val_mse: 1506.0876 - val_mae: 27.4501\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2461.0014 - mse: 2461.0017 - mae: 28.8541 - val_loss: 1509.4838 - val_mse: 1509.4840 - val_mae: 27.3367\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2403.9027 - mse: 2403.9019 - mae: 29.0478 - val_loss: 1513.3806 - val_mse: 1513.3807 - val_mae: 27.2305\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2485.6556 - mse: 2485.6558 - mae: 29.1955 - val_loss: 1503.8375 - val_mse: 1503.8375 - val_mae: 27.5054\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2486.6699 - mse: 2486.6707 - mae: 29.1082 - val_loss: 1506.4334 - val_mse: 1506.4335 - val_mae: 27.4013\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2419.4898 - mse: 2419.4893 - mae: 28.7680 - val_loss: 1500.9052 - val_mse: 1500.9049 - val_mae: 27.5900\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 525us/step - loss: 2461.4347 - mse: 2461.4348 - mae: 29.1786 - val_loss: 1502.4143 - val_mse: 1502.4144 - val_mae: 27.5358\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 502us/step - loss: 2458.4025 - mse: 2458.4023 - mae: 28.9299 - val_loss: 1507.9012 - val_mse: 1507.9011 - val_mae: 27.3573\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2444.2722 - mse: 2444.2717 - mae: 29.3037 - val_loss: 1501.9510 - val_mse: 1501.9512 - val_mae: 27.5470\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2427.8808 - mse: 2427.8806 - mae: 28.9934 - val_loss: 1500.5162 - val_mse: 1500.5161 - val_mae: 27.5818\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 529us/step - loss: 2446.4489 - mse: 2446.4485 - mae: 28.9571 - val_loss: 1500.0435 - val_mse: 1500.0433 - val_mae: 27.6015\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2448.3168 - mse: 2448.3176 - mae: 28.7348 - val_loss: 1503.5490 - val_mse: 1503.5491 - val_mae: 27.4666\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2389.3851 - mse: 2389.3845 - mae: 29.1184 - val_loss: 1501.7048 - val_mse: 1501.7048 - val_mae: 27.5318\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2498.1330 - mse: 2498.1328 - mae: 29.0465 - val_loss: 1500.8246 - val_mse: 1500.8246 - val_mae: 27.5691\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2502.3479 - mse: 2502.3479 - mae: 29.1886 - val_loss: 1502.3687 - val_mse: 1502.3689 - val_mae: 27.5418\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2406.5348 - mse: 2406.5352 - mae: 28.6986 - val_loss: 1500.9620 - val_mse: 1500.9622 - val_mae: 27.5947\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2440.9413 - mse: 2440.9414 - mae: 29.0333 - val_loss: 1499.1785 - val_mse: 1499.1787 - val_mae: 27.7111\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2453.1292 - mse: 2453.1296 - mae: 29.3904 - val_loss: 1503.3005 - val_mse: 1503.3004 - val_mae: 27.5178\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2471.6275 - mse: 2471.6282 - mae: 28.8893 - val_loss: 1506.4220 - val_mse: 1506.4220 - val_mae: 27.3874\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 698us/step - loss: 2446.0223 - mse: 2446.0215 - mae: 28.8399 - val_loss: 1499.2302 - val_mse: 1499.2303 - val_mae: 27.6540\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 541us/step - loss: 2451.9126 - mse: 2451.9133 - mae: 29.2185 - val_loss: 1497.1072 - val_mse: 1497.1074 - val_mae: 27.7340\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 526us/step - loss: 2446.0099 - mse: 2446.0100 - mae: 29.2173 - val_loss: 1498.0433 - val_mse: 1498.0432 - val_mae: 27.6652\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2431.2950 - mse: 2431.2952 - mae: 28.6880 - val_loss: 1504.1469 - val_mse: 1504.1469 - val_mae: 27.4029\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2431.0931 - mse: 2431.0933 - mae: 28.8435 - val_loss: 1499.6294 - val_mse: 1499.6293 - val_mae: 27.5646\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2484.8442 - mse: 2484.8442 - mae: 29.0766 - val_loss: 1497.6846 - val_mse: 1497.6847 - val_mae: 27.6926\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2450.0549 - mse: 2450.0549 - mae: 29.1288 - val_loss: 1503.4853 - val_mse: 1503.4854 - val_mae: 27.4576\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2468.9226 - mse: 2468.9231 - mae: 29.4597 - val_loss: 1508.8715 - val_mse: 1508.8715 - val_mae: 27.3261\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2431.8534 - mse: 2431.8535 - mae: 28.5550 - val_loss: 1500.7034 - val_mse: 1500.7036 - val_mae: 27.6094\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2304.9145 - mse: 2304.9150 - mae: 29.1575 - val_loss: 3681.4122 - val_mse: 3681.4119 - val_mae: 24.8691\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2326.9571 - mse: 2326.9573 - mae: 29.7017 - val_loss: 3684.4188 - val_mse: 3684.4185 - val_mae: 25.1155\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 483us/step - loss: 2354.3702 - mse: 2354.3704 - mae: 29.2426 - val_loss: 3686.4337 - val_mse: 3686.4331 - val_mae: 25.5533\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2370.1970 - mse: 2370.1970 - mae: 29.5031 - val_loss: 3684.1143 - val_mse: 3684.1138 - val_mae: 25.3271\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2311.5541 - mse: 2311.5540 - mae: 29.4143 - val_loss: 3683.4557 - val_mse: 3683.4561 - val_mae: 25.2248\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2295.1371 - mse: 2295.1370 - mae: 28.9675 - val_loss: 3685.7338 - val_mse: 3685.7339 - val_mae: 25.6726\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 634us/step - loss: 2357.7273 - mse: 2357.7268 - mae: 29.2980 - val_loss: 3680.0774 - val_mse: 3680.0776 - val_mae: 25.2077\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2299.1300 - mse: 2299.1304 - mae: 29.2649 - val_loss: 3685.1552 - val_mse: 3685.1562 - val_mae: 25.6255\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2389.4424 - mse: 2389.4429 - mae: 30.1117 - val_loss: 3673.6903 - val_mse: 3673.6904 - val_mae: 24.4704\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2316.6327 - mse: 2316.6328 - mae: 28.9805 - val_loss: 3679.6907 - val_mse: 3679.6912 - val_mae: 25.4040\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 659us/step - loss: 2351.0814 - mse: 2351.0815 - mae: 28.9980 - val_loss: 3677.0976 - val_mse: 3677.0972 - val_mae: 25.2403\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2354.5196 - mse: 2354.5195 - mae: 29.3894 - val_loss: 3676.0178 - val_mse: 3676.0181 - val_mae: 25.0919\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2372.1934 - mse: 2372.1934 - mae: 29.3057 - val_loss: 3680.7337 - val_mse: 3680.7344 - val_mae: 25.5093\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 676us/step - loss: 2322.5455 - mse: 2322.5457 - mae: 29.1118 - val_loss: 3676.1207 - val_mse: 3676.1208 - val_mae: 25.1334\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2355.8097 - mse: 2355.8098 - mae: 29.6145 - val_loss: 3674.9568 - val_mse: 3674.9573 - val_mae: 25.0282\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2381.0181 - mse: 2381.0181 - mae: 29.6675 - val_loss: 3675.4733 - val_mse: 3675.4746 - val_mae: 25.0743\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2385.3432 - mse: 2385.3433 - mae: 29.7439 - val_loss: 3673.1231 - val_mse: 3673.1235 - val_mae: 24.9300\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2356.4406 - mse: 2356.4397 - mae: 29.6681 - val_loss: 3674.2036 - val_mse: 3674.2034 - val_mae: 25.0559\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2291.6711 - mse: 2291.6709 - mae: 29.1125 - val_loss: 3676.1173 - val_mse: 3676.1174 - val_mae: 25.0139\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2329.6530 - mse: 2329.6541 - mae: 29.4813 - val_loss: 3673.2327 - val_mse: 3673.2329 - val_mae: 24.9052\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2308.0763 - mse: 2308.0757 - mae: 29.0226 - val_loss: 3683.0313 - val_mse: 3683.0308 - val_mae: 25.7614\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2324.7321 - mse: 2324.7322 - mae: 29.6003 - val_loss: 3672.6597 - val_mse: 3672.6594 - val_mae: 24.6300\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2349.5760 - mse: 2349.5754 - mae: 29.2327 - val_loss: 3677.8304 - val_mse: 3677.8306 - val_mae: 25.3594\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2274.8974 - mse: 2274.8982 - mae: 29.1802 - val_loss: 3676.3285 - val_mse: 3676.3276 - val_mae: 25.0551\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2329.6317 - mse: 2329.6326 - mae: 29.1029 - val_loss: 3678.8771 - val_mse: 3678.8774 - val_mae: 25.4633\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2372.1374 - mse: 2372.1379 - mae: 29.5909 - val_loss: 3674.4644 - val_mse: 3674.4646 - val_mae: 25.0785\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 539us/step - loss: 2325.3515 - mse: 2325.3511 - mae: 29.3154 - val_loss: 3676.4356 - val_mse: 3676.4348 - val_mae: 25.2551\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 484us/step - loss: 2297.7236 - mse: 2297.7236 - mae: 29.1121 - val_loss: 3678.3511 - val_mse: 3678.3518 - val_mae: 25.4099\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2327.2605 - mse: 2327.2603 - mae: 29.8183 - val_loss: 3673.0758 - val_mse: 3673.0759 - val_mae: 24.9747\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2354.0374 - mse: 2354.0369 - mae: 29.2002 - val_loss: 3682.5912 - val_mse: 3682.5906 - val_mae: 25.7936\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2353.4830 - mse: 2353.4836 - mae: 29.5201 - val_loss: 3671.5034 - val_mse: 3671.5034 - val_mae: 24.7007\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 662us/step - loss: 2327.7966 - mse: 2327.7954 - mae: 29.1760 - val_loss: 3673.0321 - val_mse: 3673.0317 - val_mae: 24.9631\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2329.7048 - mse: 2329.7051 - mae: 29.5044 - val_loss: 3665.9718 - val_mse: 3665.9727 - val_mae: 24.5864\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2324.2600 - mse: 2324.2598 - mae: 29.4474 - val_loss: 3670.6024 - val_mse: 3670.6018 - val_mae: 25.1257\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2317.9059 - mse: 2317.9060 - mae: 29.4109 - val_loss: 3665.6634 - val_mse: 3665.6643 - val_mae: 24.4108\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2328.3441 - mse: 2328.3442 - mae: 29.1349 - val_loss: 3669.2724 - val_mse: 3669.2727 - val_mae: 24.8446\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2371.7402 - mse: 2371.7410 - mae: 29.8113 - val_loss: 3670.5757 - val_mse: 3670.5757 - val_mae: 24.8117\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2267.4431 - mse: 2267.4438 - mae: 29.1767 - val_loss: 3668.7891 - val_mse: 3668.7891 - val_mae: 24.6458\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2286.4833 - mse: 2286.4829 - mae: 28.8818 - val_loss: 3668.8608 - val_mse: 3668.8604 - val_mae: 24.7102\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2268.7886 - mse: 2268.7888 - mae: 29.0966 - val_loss: 3676.4607 - val_mse: 3676.4609 - val_mae: 25.5033\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2285.9718 - mse: 2285.9714 - mae: 28.9844 - val_loss: 3671.4855 - val_mse: 3671.4849 - val_mae: 25.1628\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 697us/step - loss: 2298.5067 - mse: 2298.5071 - mae: 29.0142 - val_loss: 3674.3175 - val_mse: 3674.3169 - val_mae: 25.2981\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2333.4611 - mse: 2333.4607 - mae: 29.4929 - val_loss: 3671.9818 - val_mse: 3671.9812 - val_mae: 24.7775\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 673us/step - loss: 2356.3025 - mse: 2356.3018 - mae: 29.0636 - val_loss: 3676.9313 - val_mse: 3676.9321 - val_mae: 25.4713\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2291.3039 - mse: 2291.3035 - mae: 29.2857 - val_loss: 3673.6767 - val_mse: 3673.6770 - val_mae: 25.1091\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 558us/step - loss: 2305.6591 - mse: 2305.6597 - mae: 28.9785 - val_loss: 3672.6437 - val_mse: 3672.6436 - val_mae: 25.0776\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2234.1104 - mse: 2234.1108 - mae: 28.8711 - val_loss: 3673.9107 - val_mse: 3673.9104 - val_mae: 24.9839\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 533us/step - loss: 2319.6952 - mse: 2319.6951 - mae: 28.8006 - val_loss: 3678.3794 - val_mse: 3678.3796 - val_mae: 25.6022\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2344.0547 - mse: 2344.0547 - mae: 29.4681 - val_loss: 3669.4823 - val_mse: 3669.4827 - val_mae: 24.5973\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2340.2400 - mse: 2340.2397 - mae: 29.1742 - val_loss: 3673.9416 - val_mse: 3673.9417 - val_mae: 25.2351\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2349.4145 - mse: 2349.4146 - mae: 29.5036 - val_loss: 3670.1050 - val_mse: 3670.1045 - val_mae: 24.9737\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 680us/step - loss: 2342.1098 - mse: 2342.1094 - mae: 29.0438 - val_loss: 3666.6621 - val_mse: 3666.6626 - val_mae: 24.5126\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2290.7860 - mse: 2290.7859 - mae: 29.0421 - val_loss: 3667.5131 - val_mse: 3667.5129 - val_mae: 24.7429\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2342.9059 - mse: 2342.9050 - mae: 29.3352 - val_loss: 3670.5355 - val_mse: 3670.5354 - val_mae: 25.1782\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2309.6207 - mse: 2309.6211 - mae: 29.3730 - val_loss: 3671.8280 - val_mse: 3671.8274 - val_mae: 25.1091\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 512us/step - loss: 2312.3132 - mse: 2312.3135 - mae: 29.0878 - val_loss: 3667.1573 - val_mse: 3667.1572 - val_mae: 24.5473\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 507us/step - loss: 2288.8846 - mse: 2288.8840 - mae: 29.1684 - val_loss: 3673.4643 - val_mse: 3673.4636 - val_mae: 25.3274\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2293.3572 - mse: 2293.3582 - mae: 28.9037 - val_loss: 3669.8173 - val_mse: 3669.8176 - val_mae: 25.0869\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2344.2436 - mse: 2344.2434 - mae: 29.1931 - val_loss: 3669.7352 - val_mse: 3669.7354 - val_mae: 24.9721\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2340.3386 - mse: 2340.3379 - mae: 29.6086 - val_loss: 3667.0293 - val_mse: 3667.0288 - val_mae: 24.8447\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2277.2459 - mse: 2277.2454 - mae: 28.8866 - val_loss: 3669.6106 - val_mse: 3669.6111 - val_mae: 24.9588\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2298.7199 - mse: 2298.7197 - mae: 28.8946 - val_loss: 3671.6276 - val_mse: 3671.6279 - val_mae: 25.3769\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2278.8505 - mse: 2278.8503 - mae: 29.0929 - val_loss: 3670.9598 - val_mse: 3670.9602 - val_mae: 25.2566\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2276.5803 - mse: 2276.5801 - mae: 29.0930 - val_loss: 3668.1872 - val_mse: 3668.1875 - val_mae: 25.0560\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2235.2752 - mse: 2235.2751 - mae: 29.0631 - val_loss: 3666.0359 - val_mse: 3666.0359 - val_mae: 24.8592\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2276.2154 - mse: 2276.2158 - mae: 28.6354 - val_loss: 3668.6538 - val_mse: 3668.6533 - val_mae: 25.1284\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2330.7675 - mse: 2330.7681 - mae: 29.3376 - val_loss: 3665.1924 - val_mse: 3665.1917 - val_mae: 24.9254\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2303.1516 - mse: 2303.1523 - mae: 29.0897 - val_loss: 3668.3673 - val_mse: 3668.3672 - val_mae: 25.0852\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2300.9255 - mse: 2300.9260 - mae: 29.0774 - val_loss: 3665.5026 - val_mse: 3665.5024 - val_mae: 24.8026\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2253.7653 - mse: 2253.7646 - mae: 28.5490 - val_loss: 3671.3092 - val_mse: 3671.3096 - val_mae: 25.5470\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 670us/step - loss: 2353.3815 - mse: 2353.3811 - mae: 29.3446 - val_loss: 3668.8070 - val_mse: 3668.8052 - val_mae: 25.1782\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 558us/step - loss: 2303.5357 - mse: 2303.5354 - mae: 28.9518 - val_loss: 3672.1080 - val_mse: 3672.1077 - val_mae: 25.5123\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2281.4380 - mse: 2281.4382 - mae: 29.3043 - val_loss: 3668.3603 - val_mse: 3668.3601 - val_mae: 25.2681\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2310.0056 - mse: 2310.0066 - mae: 29.0928 - val_loss: 3669.5855 - val_mse: 3669.5854 - val_mae: 25.3109\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2317.5667 - mse: 2317.5667 - mae: 28.8763 - val_loss: 3669.2931 - val_mse: 3669.2927 - val_mae: 25.2178\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 533us/step - loss: 2284.1695 - mse: 2284.1694 - mae: 29.0191 - val_loss: 3664.9785 - val_mse: 3664.9785 - val_mae: 24.9834\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 481us/step - loss: 2281.3099 - mse: 2281.3098 - mae: 28.6828 - val_loss: 3667.0031 - val_mse: 3667.0024 - val_mae: 25.2247\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2332.7224 - mse: 2332.7224 - mae: 29.1561 - val_loss: 3664.5396 - val_mse: 3664.5396 - val_mae: 24.8738\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2297.2442 - mse: 2297.2446 - mae: 29.3146 - val_loss: 3667.4608 - val_mse: 3667.4602 - val_mae: 25.1394\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2276.3527 - mse: 2276.3525 - mae: 28.8508 - val_loss: 3665.8686 - val_mse: 3665.8682 - val_mae: 24.9816\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2728.7792 - mse: 2728.7795 - mae: 29.1247 - val_loss: 2493.1403 - val_mse: 2493.1404 - val_mae: 26.6134\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2704.5790 - mse: 2704.5779 - mae: 28.9615 - val_loss: 2492.7594 - val_mse: 2492.7593 - val_mae: 26.5950\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2752.3866 - mse: 2752.3867 - mae: 28.9328 - val_loss: 2504.3382 - val_mse: 2504.3386 - val_mae: 26.5502\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2686.1442 - mse: 2686.1438 - mae: 28.5453 - val_loss: 2493.6665 - val_mse: 2493.6658 - val_mae: 26.9731\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2700.3028 - mse: 2700.3030 - mae: 28.6763 - val_loss: 2499.8016 - val_mse: 2499.8015 - val_mae: 26.7630\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2665.6071 - mse: 2665.6074 - mae: 28.4571 - val_loss: 2483.2440 - val_mse: 2483.2444 - val_mae: 27.0418\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 544us/step - loss: 2704.6994 - mse: 2704.7000 - mae: 28.5548 - val_loss: 2487.2411 - val_mse: 2487.2412 - val_mae: 26.8341\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 524us/step - loss: 2695.5106 - mse: 2695.5110 - mae: 28.7534 - val_loss: 2492.9173 - val_mse: 2492.9172 - val_mae: 26.6559\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 532us/step - loss: 2684.8357 - mse: 2684.8359 - mae: 28.5331 - val_loss: 2487.3227 - val_mse: 2487.3230 - val_mae: 26.9363\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2672.8508 - mse: 2672.8501 - mae: 28.5586 - val_loss: 2488.8880 - val_mse: 2488.8884 - val_mae: 26.9166\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2709.7653 - mse: 2709.7654 - mae: 28.6681 - val_loss: 2490.9350 - val_mse: 2490.9348 - val_mae: 26.8286\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2725.6936 - mse: 2725.6934 - mae: 28.7854 - val_loss: 2495.8900 - val_mse: 2495.8904 - val_mae: 26.7658\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2653.4924 - mse: 2653.4924 - mae: 28.6317 - val_loss: 2489.4250 - val_mse: 2489.4253 - val_mae: 27.0591\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2722.3356 - mse: 2722.3359 - mae: 28.7484 - val_loss: 2495.4741 - val_mse: 2495.4741 - val_mae: 26.8880\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2654.2360 - mse: 2654.2358 - mae: 28.7426 - val_loss: 2488.0213 - val_mse: 2488.0217 - val_mae: 26.9992\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2727.5198 - mse: 2727.5200 - mae: 28.9046 - val_loss: 2504.9868 - val_mse: 2504.9866 - val_mae: 26.4038\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2707.2940 - mse: 2707.2935 - mae: 28.4597 - val_loss: 2497.5975 - val_mse: 2497.5977 - val_mae: 26.6095\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 521us/step - loss: 2710.8949 - mse: 2710.8948 - mae: 28.8827 - val_loss: 2497.1684 - val_mse: 2497.1682 - val_mae: 26.6061\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2657.3762 - mse: 2657.3762 - mae: 28.3742 - val_loss: 2487.3107 - val_mse: 2487.3108 - val_mae: 26.9406\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2663.1794 - mse: 2663.1797 - mae: 28.5180 - val_loss: 2489.9027 - val_mse: 2489.9026 - val_mae: 26.8837\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2672.6792 - mse: 2672.6804 - mae: 28.4772 - val_loss: 2490.4274 - val_mse: 2490.4275 - val_mae: 26.8024\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2728.2878 - mse: 2728.2869 - mae: 28.8550 - val_loss: 2494.7771 - val_mse: 2494.7773 - val_mae: 26.7672\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2662.1764 - mse: 2662.1755 - mae: 28.6471 - val_loss: 2497.6815 - val_mse: 2497.6814 - val_mae: 26.6649\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2640.5737 - mse: 2640.5740 - mae: 28.5168 - val_loss: 2487.8248 - val_mse: 2487.8252 - val_mae: 27.0043\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 3s 728us/step - loss: 2736.0782 - mse: 2736.0781 - mae: 28.8784 - val_loss: 2501.9990 - val_mse: 2501.9988 - val_mae: 26.4670\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2707.1563 - mse: 2707.1553 - mae: 28.7573 - val_loss: 2494.6392 - val_mse: 2494.6396 - val_mae: 26.6790\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2712.4862 - mse: 2712.4871 - mae: 28.6757 - val_loss: 2497.8021 - val_mse: 2497.8020 - val_mae: 26.7006\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2697.7074 - mse: 2697.7065 - mae: 28.4404 - val_loss: 2502.5778 - val_mse: 2502.5779 - val_mae: 26.5452\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2743.5977 - mse: 2743.5972 - mae: 28.9165 - val_loss: 2507.0189 - val_mse: 2507.0193 - val_mae: 26.4862\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 518us/step - loss: 2732.6370 - mse: 2732.6377 - mae: 28.7279 - val_loss: 2501.0297 - val_mse: 2501.0298 - val_mae: 26.6404\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2666.4061 - mse: 2666.4065 - mae: 28.6096 - val_loss: 2498.6188 - val_mse: 2498.6187 - val_mae: 26.6560\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2650.3714 - mse: 2650.3713 - mae: 28.4449 - val_loss: 2495.1995 - val_mse: 2495.1992 - val_mae: 26.6958\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2684.5083 - mse: 2684.5085 - mae: 28.4094 - val_loss: 2493.9693 - val_mse: 2493.9690 - val_mae: 26.6489\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2666.3629 - mse: 2666.3623 - mae: 28.2303 - val_loss: 2488.6323 - val_mse: 2488.6326 - val_mae: 26.8685\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2751.1439 - mse: 2751.1440 - mae: 28.9751 - val_loss: 2500.7857 - val_mse: 2500.7859 - val_mae: 26.6285\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2705.8281 - mse: 2705.8271 - mae: 28.9292 - val_loss: 2502.1307 - val_mse: 2502.1306 - val_mae: 26.6515\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2699.5456 - mse: 2699.5452 - mae: 28.6821 - val_loss: 2501.3050 - val_mse: 2501.3054 - val_mae: 26.6089\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2694.8000 - mse: 2694.8008 - mae: 28.1727 - val_loss: 2496.7040 - val_mse: 2496.7039 - val_mae: 26.5743\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2634.2080 - mse: 2634.2083 - mae: 28.1956 - val_loss: 2493.2928 - val_mse: 2493.2930 - val_mae: 26.7611\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2691.9858 - mse: 2691.9849 - mae: 28.4227 - val_loss: 2494.7878 - val_mse: 2494.7876 - val_mae: 26.6288\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2705.8081 - mse: 2705.8074 - mae: 28.8585 - val_loss: 2492.5961 - val_mse: 2492.5957 - val_mae: 26.6753\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2724.4755 - mse: 2724.4761 - mae: 28.5670 - val_loss: 2495.9747 - val_mse: 2495.9749 - val_mae: 26.4608\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2682.4756 - mse: 2682.4756 - mae: 28.2600 - val_loss: 2495.6409 - val_mse: 2495.6409 - val_mae: 26.4053\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2605.3212 - mse: 2605.3220 - mae: 28.2540 - val_loss: 2484.8249 - val_mse: 2484.8245 - val_mae: 26.7946\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2656.9100 - mse: 2656.9094 - mae: 28.4130 - val_loss: 2488.3734 - val_mse: 2488.3735 - val_mae: 26.7815\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2668.9463 - mse: 2668.9460 - mae: 28.5580 - val_loss: 2485.6850 - val_mse: 2485.6853 - val_mae: 26.8166\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2645.1445 - mse: 2645.1453 - mae: 28.4973 - val_loss: 2489.7004 - val_mse: 2489.7009 - val_mae: 26.7823\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2611.3363 - mse: 2611.3359 - mae: 28.1794 - val_loss: 2495.0671 - val_mse: 2495.0669 - val_mae: 26.5560\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 511us/step - loss: 2719.6296 - mse: 2719.6296 - mae: 28.7138 - val_loss: 2494.0761 - val_mse: 2494.0762 - val_mae: 26.7891\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2645.8811 - mse: 2645.8816 - mae: 28.3127 - val_loss: 2497.2706 - val_mse: 2497.2703 - val_mae: 26.7199\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2652.8102 - mse: 2652.8113 - mae: 28.1796 - val_loss: 2498.6541 - val_mse: 2498.6541 - val_mae: 26.5380\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2708.6552 - mse: 2708.6550 - mae: 28.4382 - val_loss: 2507.2953 - val_mse: 2507.2954 - val_mae: 26.3447\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2681.5150 - mse: 2681.5151 - mae: 28.6819 - val_loss: 2511.3350 - val_mse: 2511.3354 - val_mae: 26.4866\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2680.9898 - mse: 2680.9895 - mae: 28.6394 - val_loss: 2505.4287 - val_mse: 2505.4287 - val_mae: 26.7823\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2716.6880 - mse: 2716.6873 - mae: 28.4582 - val_loss: 2504.2490 - val_mse: 2504.2495 - val_mae: 26.7864\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2699.5956 - mse: 2699.5959 - mae: 28.6385 - val_loss: 2501.7727 - val_mse: 2501.7727 - val_mae: 26.8025\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2692.5329 - mse: 2692.5330 - mae: 28.7626 - val_loss: 2501.0414 - val_mse: 2501.0417 - val_mae: 26.6859\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2699.2091 - mse: 2699.2085 - mae: 28.7006 - val_loss: 2504.3029 - val_mse: 2504.3030 - val_mae: 26.5308\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 671us/step - loss: 2681.0484 - mse: 2681.0471 - mae: 28.1789 - val_loss: 2497.1832 - val_mse: 2497.1838 - val_mae: 26.7585\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 653us/step - loss: 2709.7705 - mse: 2709.7705 - mae: 28.6005 - val_loss: 2506.1712 - val_mse: 2506.1711 - val_mae: 26.5894\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2651.8859 - mse: 2651.8860 - mae: 28.3986 - val_loss: 2492.1543 - val_mse: 2492.1545 - val_mae: 27.0365\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2656.7551 - mse: 2656.7544 - mae: 28.5752 - val_loss: 2497.7526 - val_mse: 2497.7529 - val_mae: 26.7708\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2654.9667 - mse: 2654.9675 - mae: 28.4206 - val_loss: 2499.0771 - val_mse: 2499.0771 - val_mae: 26.7106\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 530us/step - loss: 2687.3959 - mse: 2687.3950 - mae: 28.4463 - val_loss: 2505.1790 - val_mse: 2505.1794 - val_mae: 26.5066\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2675.3239 - mse: 2675.3247 - mae: 28.6096 - val_loss: 2494.5676 - val_mse: 2494.5679 - val_mae: 26.8883\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2670.8666 - mse: 2670.8662 - mae: 28.5970 - val_loss: 2497.0375 - val_mse: 2497.0376 - val_mae: 26.7232\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2646.6854 - mse: 2646.6860 - mae: 28.5444 - val_loss: 2492.6665 - val_mse: 2492.6665 - val_mae: 26.8016\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2720.7749 - mse: 2720.7744 - mae: 28.7297 - val_loss: 2499.6149 - val_mse: 2499.6150 - val_mae: 26.7008\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2715.2624 - mse: 2715.2625 - mae: 28.3323 - val_loss: 2508.1612 - val_mse: 2508.1611 - val_mae: 26.6047\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 518us/step - loss: 2687.9788 - mse: 2687.9790 - mae: 28.4336 - val_loss: 2502.8499 - val_mse: 2502.8501 - val_mae: 26.7864\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 454us/step - loss: 2711.9782 - mse: 2711.9773 - mae: 28.6165 - val_loss: 2491.9868 - val_mse: 2491.9866 - val_mae: 26.8944\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 412us/step - loss: 2683.4941 - mse: 2683.4939 - mae: 28.5969 - val_loss: 2498.4839 - val_mse: 2498.4841 - val_mae: 26.7066\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 490us/step - loss: 2672.0748 - mse: 2672.0740 - mae: 28.5012 - val_loss: 2499.6650 - val_mse: 2499.6650 - val_mae: 26.6699\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2660.1411 - mse: 2660.1418 - mae: 28.1683 - val_loss: 2498.5450 - val_mse: 2498.5447 - val_mae: 26.7106\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 649us/step - loss: 2662.0507 - mse: 2662.0510 - mae: 28.3883 - val_loss: 2490.0456 - val_mse: 2490.0457 - val_mae: 26.8536\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2700.4855 - mse: 2700.4856 - mae: 28.5712 - val_loss: 2501.8568 - val_mse: 2501.8567 - val_mae: 26.5416\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2687.0233 - mse: 2687.0234 - mae: 28.4694 - val_loss: 2499.6129 - val_mse: 2499.6130 - val_mae: 26.6307\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2678.8422 - mse: 2678.8423 - mae: 28.3188 - val_loss: 2498.4711 - val_mse: 2498.4707 - val_mae: 26.8050\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 506us/step - loss: 2678.2077 - mse: 2678.2073 - mae: 28.6568 - val_loss: 2508.2361 - val_mse: 2508.2358 - val_mae: 26.6337\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 551us/step - loss: 2680.9796 - mse: 2680.9792 - mae: 28.5472 - val_loss: 2501.4273 - val_mse: 2501.4275 - val_mae: 26.7229\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 13327.6262 - mse: 13327.6270 - mae: 109.8991 - val_loss: 34607.1290 - val_mse: 34607.1328 - val_mae: 132.6882\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 663us/step - loss: 13171.7896 - mse: 13171.7900 - mae: 109.1893 - val_loss: 34288.9779 - val_mse: 34288.9766 - val_mae: 131.4932\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 679us/step - loss: 12725.0379 - mse: 12725.0391 - mae: 107.1294 - val_loss: 33407.7815 - val_mse: 33407.7852 - val_mae: 128.1279\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 668us/step - loss: 11619.1534 - mse: 11619.1553 - mae: 101.9216 - val_loss: 31211.2660 - val_mse: 31211.2637 - val_mae: 119.3311\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 8932.1595 - mse: 8932.1592 - mae: 87.0805 - val_loss: 26291.5928 - val_mse: 26291.5938 - val_mae: 96.7907\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 4787.6076 - mse: 4787.6079 - mae: 56.7573 - val_loss: 19149.9798 - val_mse: 19149.9805 - val_mae: 48.5422\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 2735.0403 - mse: 2735.0403 - mae: 37.4576 - val_loss: 17521.0509 - val_mse: 17521.0527 - val_mae: 37.0902\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2751.6798 - mse: 2751.6797 - mae: 38.7549 - val_loss: 17930.7293 - val_mse: 17930.7305 - val_mae: 38.3127\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2514.2990 - mse: 2514.2996 - mae: 35.9710 - val_loss: 17786.3304 - val_mse: 17786.3301 - val_mae: 37.6556\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2426.0338 - mse: 2426.0337 - mae: 35.6513 - val_loss: 17606.8635 - val_mse: 17606.8633 - val_mae: 37.2282\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 464us/step - loss: 2484.5870 - mse: 2484.5869 - mae: 36.4037 - val_loss: 18134.6027 - val_mse: 18134.6016 - val_mae: 39.5542\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 436us/step - loss: 2349.8180 - mse: 2349.8181 - mae: 35.3194 - val_loss: 17665.1182 - val_mse: 17665.1191 - val_mae: 37.3252\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 510us/step - loss: 2610.0753 - mse: 2610.0752 - mae: 37.3825 - val_loss: 17787.4236 - val_mse: 17787.4219 - val_mae: 37.6306\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 517us/step - loss: 2462.9020 - mse: 2462.9021 - mae: 35.0004 - val_loss: 17778.9905 - val_mse: 17778.9883 - val_mae: 37.5946\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 627us/step - loss: 2544.5664 - mse: 2544.5664 - mae: 36.1610 - val_loss: 17870.7689 - val_mse: 17870.7695 - val_mae: 37.9629\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2509.1785 - mse: 2509.1785 - mae: 35.8982 - val_loss: 17770.0116 - val_mse: 17770.0117 - val_mae: 37.5515\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2470.5225 - mse: 2470.5225 - mae: 35.3026 - val_loss: 18025.4898 - val_mse: 18025.4922 - val_mae: 38.8072\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 556us/step - loss: 2304.1114 - mse: 2304.1116 - mae: 34.0114 - val_loss: 17602.1994 - val_mse: 17602.1992 - val_mae: 37.1711\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 2503.8359 - mse: 2503.8362 - mae: 35.3903 - val_loss: 17705.4463 - val_mse: 17705.4453 - val_mae: 37.3731\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 509us/step - loss: 2695.7463 - mse: 2695.7466 - mae: 37.0259 - val_loss: 18001.3126 - val_mse: 18001.3145 - val_mae: 38.6428\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 671us/step - loss: 2358.6562 - mse: 2358.6560 - mae: 34.7863 - val_loss: 17705.2470 - val_mse: 17705.2480 - val_mae: 37.3611\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 722us/step - loss: 2175.1829 - mse: 2175.1829 - mae: 34.1460 - val_loss: 17440.5859 - val_mse: 17440.5859 - val_mae: 36.9850\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 2258.2036 - mse: 2258.2039 - mae: 34.8100 - val_loss: 17668.7846 - val_mse: 17668.7832 - val_mae: 37.2694\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 691us/step - loss: 2104.3195 - mse: 2104.3196 - mae: 32.9758 - val_loss: 17513.1386 - val_mse: 17513.1406 - val_mae: 37.0082\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 700us/step - loss: 2204.0947 - mse: 2204.0947 - mae: 32.8393 - val_loss: 17585.4947 - val_mse: 17585.4961 - val_mae: 37.1034\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 620us/step - loss: 2216.9182 - mse: 2216.9182 - mae: 33.1627 - val_loss: 17474.2125 - val_mse: 17474.2129 - val_mae: 36.9885\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 627us/step - loss: 2101.4352 - mse: 2101.4355 - mae: 33.0736 - val_loss: 17902.9893 - val_mse: 17902.9902 - val_mae: 38.0575\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2164.4241 - mse: 2164.4241 - mae: 33.5190 - val_loss: 17591.9417 - val_mse: 17591.9414 - val_mae: 37.0971\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 645us/step - loss: 2476.0967 - mse: 2476.0967 - mae: 34.8737 - val_loss: 17920.4180 - val_mse: 17920.4180 - val_mae: 38.1387\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2132.5260 - mse: 2132.5256 - mae: 32.8622 - val_loss: 17551.6718 - val_mse: 17551.6699 - val_mae: 37.0230\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2255.3797 - mse: 2255.3796 - mae: 33.2158 - val_loss: 17672.3458 - val_mse: 17672.3457 - val_mae: 37.2305\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 2424.1421 - mse: 2424.1421 - mae: 35.3521 - val_loss: 17572.7996 - val_mse: 17572.8008 - val_mae: 37.0436\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2342.5143 - mse: 2342.5144 - mae: 33.7188 - val_loss: 17724.2251 - val_mse: 17724.2246 - val_mae: 37.3321\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 522us/step - loss: 2114.6351 - mse: 2114.6348 - mae: 32.4697 - val_loss: 17712.9055 - val_mse: 17712.9062 - val_mae: 37.2971\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2016.6755 - mse: 2016.6757 - mae: 30.9785 - val_loss: 17603.5671 - val_mse: 17603.5703 - val_mae: 37.0769\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 2280.0452 - mse: 2280.0449 - mae: 33.9710 - val_loss: 17691.2433 - val_mse: 17691.2441 - val_mae: 37.2338\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2200.6132 - mse: 2200.6135 - mae: 32.4114 - val_loss: 17536.5179 - val_mse: 17536.5176 - val_mae: 36.9844\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2102.2349 - mse: 2102.2351 - mae: 31.7166 - val_loss: 17725.0641 - val_mse: 17725.0625 - val_mae: 37.2909\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2165.7213 - mse: 2165.7212 - mae: 32.6956 - val_loss: 17492.9525 - val_mse: 17492.9531 - val_mae: 36.9624\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 2161.2401 - mse: 2161.2400 - mae: 32.6306 - val_loss: 17604.8735 - val_mse: 17604.8730 - val_mae: 37.0450\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 2097.2743 - mse: 2097.2742 - mae: 32.2220 - val_loss: 17578.7285 - val_mse: 17578.7285 - val_mae: 37.0115\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2122.2375 - mse: 2122.2373 - mae: 33.1417 - val_loss: 17597.7706 - val_mse: 17597.7715 - val_mae: 37.0299\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2014.7083 - mse: 2014.7080 - mae: 32.0880 - val_loss: 17571.0993 - val_mse: 17571.0977 - val_mae: 36.9959\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 500us/step - loss: 2188.5108 - mse: 2188.5107 - mae: 32.8646 - val_loss: 17618.2160 - val_mse: 17618.2148 - val_mae: 37.0486\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 500us/step - loss: 2134.8395 - mse: 2134.8398 - mae: 31.8772 - val_loss: 17696.4645 - val_mse: 17696.4668 - val_mae: 37.1803\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 487us/step - loss: 2233.8354 - mse: 2233.8352 - mae: 33.5006 - val_loss: 17643.2982 - val_mse: 17643.2988 - val_mae: 37.0744\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 461us/step - loss: 2117.2328 - mse: 2117.2329 - mae: 31.7581 - val_loss: 17672.1390 - val_mse: 17672.1387 - val_mae: 37.1144\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2214.1104 - mse: 2214.1104 - mae: 32.8601 - val_loss: 17706.9381 - val_mse: 17706.9395 - val_mae: 37.1794\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 637us/step - loss: 2178.8454 - mse: 2178.8455 - mae: 31.7806 - val_loss: 17410.9288 - val_mse: 17410.9297 - val_mae: 36.9834\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 637us/step - loss: 2003.3028 - mse: 2003.3027 - mae: 31.2056 - val_loss: 17490.7867 - val_mse: 17490.7871 - val_mae: 36.9548\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 490us/step - loss: 2046.2228 - mse: 2046.2230 - mae: 31.5711 - val_loss: 17583.0287 - val_mse: 17583.0293 - val_mae: 36.9815\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 501us/step - loss: 2214.2301 - mse: 2214.2300 - mae: 33.2458 - val_loss: 17719.6995 - val_mse: 17719.6992 - val_mae: 37.1771\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2108.3099 - mse: 2108.3101 - mae: 30.5721 - val_loss: 17521.1468 - val_mse: 17521.1484 - val_mae: 36.9470\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 464us/step - loss: 2066.8002 - mse: 2066.8000 - mae: 31.0739 - val_loss: 17639.2306 - val_mse: 17639.2324 - val_mae: 37.0355\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2028.1282 - mse: 2028.1284 - mae: 32.0141 - val_loss: 17534.4340 - val_mse: 17534.4336 - val_mae: 36.9436\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 378us/step - loss: 1968.5863 - mse: 1968.5864 - mae: 31.3204 - val_loss: 17554.5110 - val_mse: 17554.5098 - val_mae: 36.9387\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 531us/step - loss: 2050.6756 - mse: 2050.6758 - mae: 30.7419 - val_loss: 17706.8011 - val_mse: 17706.8027 - val_mae: 37.1165\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 501us/step - loss: 2004.8614 - mse: 2004.8612 - mae: 31.3193 - val_loss: 17765.3376 - val_mse: 17765.3379 - val_mae: 37.2559\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 720us/step - loss: 1960.5771 - mse: 1960.5770 - mae: 30.9507 - val_loss: 17474.5911 - val_mse: 17474.5898 - val_mae: 36.9520\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 1864.8114 - mse: 1864.8114 - mae: 30.7480 - val_loss: 17392.3336 - val_mse: 17392.3340 - val_mae: 37.0218\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 1927.8561 - mse: 1927.8561 - mae: 31.3525 - val_loss: 17680.4744 - val_mse: 17680.4746 - val_mae: 37.0583\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 438us/step - loss: 1853.9431 - mse: 1853.9432 - mae: 30.2011 - val_loss: 17450.9837 - val_mse: 17450.9844 - val_mae: 36.9639\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 430us/step - loss: 1963.5936 - mse: 1963.5938 - mae: 30.8423 - val_loss: 17680.2325 - val_mse: 17680.2324 - val_mae: 37.0518\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 440us/step - loss: 2080.4222 - mse: 2080.4224 - mae: 30.2173 - val_loss: 17454.7772 - val_mse: 17454.7773 - val_mae: 36.9617\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 428us/step - loss: 1812.4953 - mse: 1812.4954 - mae: 29.2652 - val_loss: 17610.1609 - val_mse: 17610.1621 - val_mae: 36.9616\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 735us/step - loss: 1894.4099 - mse: 1894.4100 - mae: 30.4352 - val_loss: 17521.3262 - val_mse: 17521.3262 - val_mae: 36.9386\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 1969.5149 - mse: 1969.5149 - mae: 30.9209 - val_loss: 17538.1369 - val_mse: 17538.1387 - val_mae: 36.9350\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 1960.1746 - mse: 1960.1748 - mae: 30.8059 - val_loss: 17731.2388 - val_mse: 17731.2383 - val_mae: 37.1103\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 2000.9530 - mse: 2000.9531 - mae: 30.6695 - val_loss: 17477.0901 - val_mse: 17477.0898 - val_mae: 36.9488\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 716us/step - loss: 1922.8238 - mse: 1922.8239 - mae: 30.5756 - val_loss: 17455.5868 - val_mse: 17455.5859 - val_mae: 36.9672\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 838us/step - loss: 1936.8521 - mse: 1936.8521 - mae: 29.5761 - val_loss: 17559.1252 - val_mse: 17559.1270 - val_mae: 36.9278\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 2022.9076 - mse: 2022.9076 - mae: 31.2307 - val_loss: 17686.8757 - val_mse: 17686.8770 - val_mae: 37.0280\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 1852.4592 - mse: 1852.4592 - mae: 29.9564 - val_loss: 17568.2299 - val_mse: 17568.2305 - val_mae: 36.9241\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 1971.3101 - mse: 1971.3098 - mae: 30.8614 - val_loss: 17585.9368 - val_mse: 17585.9355 - val_mae: 36.9201\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 664us/step - loss: 1953.8123 - mse: 1953.8124 - mae: 30.5590 - val_loss: 17518.1208 - val_mse: 17518.1191 - val_mae: 36.9328\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 1925.9905 - mse: 1925.9905 - mae: 29.8593 - val_loss: 17423.6527 - val_mse: 17423.6523 - val_mae: 37.0210\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2062.3402 - mse: 2062.3401 - mae: 31.0184 - val_loss: 17681.5708 - val_mse: 17681.5684 - val_mae: 37.0031\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 535us/step - loss: 1909.5470 - mse: 1909.5470 - mae: 29.3787 - val_loss: 17538.3795 - val_mse: 17538.3789 - val_mae: 36.9278\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 1821.7023 - mse: 1821.7023 - mae: 28.8021 - val_loss: 17534.1744 - val_mse: 17534.1719 - val_mae: 36.9278\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 1803.0070 - mse: 1803.0072 - mae: 29.4077 - val_loss: 17579.2023 - val_mse: 17579.2012 - val_mae: 36.9181\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 649us/step - loss: 4134.6938 - mse: 4134.6938 - mae: 33.5800 - val_loss: 2266.2134 - val_mse: 2266.2134 - val_mae: 31.5699\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 3959.2066 - mse: 3959.2068 - mae: 33.4872 - val_loss: 2333.0216 - val_mse: 2333.0215 - val_mae: 31.8013\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 4120.3035 - mse: 4120.3032 - mae: 34.3659 - val_loss: 2306.7694 - val_mse: 2306.7695 - val_mae: 31.7121\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4203.0144 - mse: 4203.0142 - mae: 34.4664 - val_loss: 2348.8818 - val_mse: 2348.8816 - val_mae: 31.8583\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4324.5331 - mse: 4324.5337 - mae: 35.8297 - val_loss: 2342.5262 - val_mse: 2342.5264 - val_mae: 31.8384\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 3975.6690 - mse: 3975.6687 - mae: 34.0543 - val_loss: 2386.1177 - val_mse: 2386.1177 - val_mae: 31.9804\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4192.7501 - mse: 4192.7505 - mae: 34.1095 - val_loss: 2338.3882 - val_mse: 2338.3884 - val_mae: 31.8271\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 4093.4391 - mse: 4093.4390 - mae: 33.8843 - val_loss: 2348.2673 - val_mse: 2348.2676 - val_mae: 31.8626\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4128.4623 - mse: 4128.4624 - mae: 34.2957 - val_loss: 2307.3245 - val_mse: 2307.3242 - val_mae: 31.7225\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 657us/step - loss: 4237.6738 - mse: 4237.6738 - mae: 35.1447 - val_loss: 2415.4381 - val_mse: 2415.4382 - val_mae: 32.0896\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 4113.1907 - mse: 4113.1914 - mae: 34.3817 - val_loss: 2306.2031 - val_mse: 2306.2029 - val_mae: 31.7222\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4211.7129 - mse: 4211.7134 - mae: 34.5058 - val_loss: 2377.8224 - val_mse: 2377.8225 - val_mae: 31.9650\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4158.9006 - mse: 4158.9009 - mae: 34.5739 - val_loss: 2411.1551 - val_mse: 2411.1550 - val_mae: 32.0746\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4115.5778 - mse: 4115.5776 - mae: 34.0750 - val_loss: 2353.9843 - val_mse: 2353.9841 - val_mae: 31.8908\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 4217.2153 - mse: 4217.2153 - mae: 33.3699 - val_loss: 2353.7762 - val_mse: 2353.7764 - val_mae: 31.8914\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4242.2328 - mse: 4242.2329 - mae: 34.8007 - val_loss: 2339.5159 - val_mse: 2339.5161 - val_mae: 31.8450\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4143.4147 - mse: 4143.4141 - mae: 33.7014 - val_loss: 2281.9967 - val_mse: 2281.9968 - val_mae: 31.6458\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 695us/step - loss: 4073.5166 - mse: 4073.5161 - mae: 34.4359 - val_loss: 2362.6703 - val_mse: 2362.6702 - val_mae: 31.9226\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4211.6766 - mse: 4211.6772 - mae: 34.6156 - val_loss: 2366.4167 - val_mse: 2366.4163 - val_mae: 31.9367\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 692us/step - loss: 3889.2758 - mse: 3889.2756 - mae: 32.2404 - val_loss: 2265.5623 - val_mse: 2265.5620 - val_mae: 31.5919\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4101.4061 - mse: 4101.4058 - mae: 34.3873 - val_loss: 2396.6035 - val_mse: 2396.6035 - val_mae: 32.0336\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4096.0089 - mse: 4096.0093 - mae: 34.6519 - val_loss: 2398.1664 - val_mse: 2398.1663 - val_mae: 32.0393\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 741us/step - loss: 4158.4709 - mse: 4158.4707 - mae: 34.5810 - val_loss: 2371.2827 - val_mse: 2371.2827 - val_mae: 31.9569\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 3898.6741 - mse: 3898.6741 - mae: 33.0172 - val_loss: 2287.8490 - val_mse: 2287.8491 - val_mae: 31.6731\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - ETA: 0s - loss: 4564.8531 - mse: 4564.8540 - mae: 34.52 - 1s 644us/step - loss: 4365.9819 - mse: 4365.9829 - mae: 34.7092 - val_loss: 2417.0091 - val_mse: 2417.0090 - val_mae: 32.0980\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 4144.5678 - mse: 4144.5679 - mae: 34.2237 - val_loss: 2360.0244 - val_mse: 2360.0244 - val_mae: 31.9211\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 643us/step - loss: 3938.0608 - mse: 3938.0605 - mae: 33.0800 - val_loss: 2324.4503 - val_mse: 2324.4502 - val_mae: 31.8017\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 4188.9189 - mse: 4188.9189 - mae: 33.0525 - val_loss: 2360.6344 - val_mse: 2360.6343 - val_mae: 31.9237\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 3900.5735 - mse: 3900.5735 - mae: 33.4918 - val_loss: 2353.0004 - val_mse: 2353.0005 - val_mae: 31.8990\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 3945.6500 - mse: 3945.6499 - mae: 32.6061 - val_loss: 2313.2772 - val_mse: 2313.2773 - val_mae: 31.7653\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 544us/step - loss: 4119.0079 - mse: 4119.0073 - mae: 33.6337 - val_loss: 2330.3726 - val_mse: 2330.3726 - val_mae: 31.8263\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4037.6838 - mse: 4037.6838 - mae: 33.5125 - val_loss: 2287.9047 - val_mse: 2287.9048 - val_mae: 31.6809\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 488us/step - loss: 4014.3035 - mse: 4014.3035 - mae: 32.5363 - val_loss: 2338.0654 - val_mse: 2338.0652 - val_mae: 31.8546\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 3953.9127 - mse: 3953.9124 - mae: 34.0373 - val_loss: 2311.4717 - val_mse: 2311.4717 - val_mae: 31.7655\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4057.8779 - mse: 4057.8782 - mae: 33.5734 - val_loss: 2304.1298 - val_mse: 2304.1296 - val_mae: 31.7416\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4084.9114 - mse: 4084.9109 - mae: 33.9065 - val_loss: 2328.4441 - val_mse: 2328.4441 - val_mae: 31.8262\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4096.0275 - mse: 4096.0283 - mae: 33.9357 - val_loss: 2353.3345 - val_mse: 2353.3347 - val_mae: 31.9110\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4221.3811 - mse: 4221.3809 - mae: 34.3154 - val_loss: 2429.2244 - val_mse: 2429.2244 - val_mae: 32.1478\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 3960.7793 - mse: 3960.7793 - mae: 32.6681 - val_loss: 2347.4543 - val_mse: 2347.4543 - val_mae: 31.8921\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4058.4299 - mse: 4058.4297 - mae: 34.2388 - val_loss: 2328.9853 - val_mse: 2328.9854 - val_mae: 31.8329\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4041.4087 - mse: 4041.4092 - mae: 34.2910 - val_loss: 2352.2111 - val_mse: 2352.2112 - val_mae: 31.9138\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 636us/step - loss: 3891.1354 - mse: 3891.1353 - mae: 32.6765 - val_loss: 2305.7246 - val_mse: 2305.7249 - val_mae: 31.7562\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 3955.8659 - mse: 3955.8652 - mae: 34.2802 - val_loss: 2322.3710 - val_mse: 2322.3711 - val_mae: 31.8126\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4008.8688 - mse: 4008.8691 - mae: 32.9309 - val_loss: 2333.5710 - val_mse: 2333.5710 - val_mae: 31.8512\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4055.2412 - mse: 4055.2417 - mae: 33.7141 - val_loss: 2360.9914 - val_mse: 2360.9915 - val_mae: 31.9439\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4133.7967 - mse: 4133.7959 - mae: 33.5915 - val_loss: 2379.2487 - val_mse: 2379.2488 - val_mae: 32.0034\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 3876.6367 - mse: 3876.6365 - mae: 32.7133 - val_loss: 2297.1122 - val_mse: 2297.1123 - val_mae: 31.7315\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 3843.2810 - mse: 3843.2808 - mae: 32.2343 - val_loss: 2246.4913 - val_mse: 2246.4912 - val_mae: 31.5705\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4119.4522 - mse: 4119.4521 - mae: 33.8926 - val_loss: 2364.8396 - val_mse: 2364.8396 - val_mae: 31.9588\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 3936.3974 - mse: 3936.3975 - mae: 32.7562 - val_loss: 2307.6713 - val_mse: 2307.6711 - val_mae: 31.7674\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4072.2445 - mse: 4072.2449 - mae: 33.4109 - val_loss: 2348.8934 - val_mse: 2348.8936 - val_mae: 31.9081\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 700us/step - loss: 3816.4409 - mse: 3816.4414 - mae: 32.1162 - val_loss: 2271.7954 - val_mse: 2271.7954 - val_mae: 31.6530\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 698us/step - loss: 4249.3074 - mse: 4249.3076 - mae: 34.8706 - val_loss: 2358.5618 - val_mse: 2358.5620 - val_mae: 31.9425\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4003.4924 - mse: 4003.4929 - mae: 33.0151 - val_loss: 2328.2526 - val_mse: 2328.2527 - val_mae: 31.8435\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 3942.6431 - mse: 3942.6428 - mae: 32.8621 - val_loss: 2253.0388 - val_mse: 2253.0391 - val_mae: 31.6033\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4246.5269 - mse: 4246.5269 - mae: 33.8953 - val_loss: 2401.1323 - val_mse: 2401.1323 - val_mae: 32.0830\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4009.9753 - mse: 4009.9753 - mae: 33.0538 - val_loss: 2325.6764 - val_mse: 2325.6763 - val_mae: 31.8394\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4125.7100 - mse: 4125.7104 - mae: 33.0777 - val_loss: 2354.0375 - val_mse: 2354.0376 - val_mae: 31.9361\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 3811.2388 - mse: 3811.2385 - mae: 32.0570 - val_loss: 2332.6627 - val_mse: 2332.6628 - val_mae: 31.8648\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4101.6106 - mse: 4101.6108 - mae: 33.5077 - val_loss: 2355.9613 - val_mse: 2355.9612 - val_mae: 31.9441\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 527us/step - loss: 3751.4138 - mse: 3751.4141 - mae: 31.9464 - val_loss: 2317.3605 - val_mse: 2317.3606 - val_mae: 31.8144\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 708us/step - loss: 4091.6667 - mse: 4091.6675 - mae: 33.6328 - val_loss: 2365.1889 - val_mse: 2365.1887 - val_mae: 31.9756\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 3990.7566 - mse: 3990.7568 - mae: 32.7924 - val_loss: 2358.5661 - val_mse: 2358.5662 - val_mae: 31.9562\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4001.6440 - mse: 4001.6438 - mae: 33.7527 - val_loss: 2349.4909 - val_mse: 2349.4907 - val_mae: 31.9260\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 705us/step - loss: 3955.5780 - mse: 3955.5776 - mae: 33.2163 - val_loss: 2322.1692 - val_mse: 2322.1692 - val_mae: 31.8350\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4035.1458 - mse: 4035.1460 - mae: 32.8186 - val_loss: 2317.0389 - val_mse: 2317.0388 - val_mae: 31.8190\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 3925.2560 - mse: 3925.2561 - mae: 33.2766 - val_loss: 2315.5191 - val_mse: 2315.5190 - val_mae: 31.8145\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4014.5722 - mse: 4014.5723 - mae: 33.0059 - val_loss: 2374.6139 - val_mse: 2374.6140 - val_mae: 32.0101\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4001.1062 - mse: 4001.1060 - mae: 32.6373 - val_loss: 2401.3645 - val_mse: 2401.3645 - val_mae: 32.0959\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 3969.1910 - mse: 3969.1914 - mae: 31.8649 - val_loss: 2323.5384 - val_mse: 2323.5381 - val_mae: 31.8448\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 3904.6797 - mse: 3904.6794 - mae: 32.4260 - val_loss: 2279.5423 - val_mse: 2279.5422 - val_mae: 31.7056\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4079.9619 - mse: 4079.9624 - mae: 33.0975 - val_loss: 2331.9433 - val_mse: 2331.9434 - val_mae: 31.8726\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 3974.5907 - mse: 3974.5908 - mae: 32.1668 - val_loss: 2327.9504 - val_mse: 2327.9502 - val_mae: 31.8599\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4095.3832 - mse: 4095.3831 - mae: 32.9414 - val_loss: 2320.0954 - val_mse: 2320.0952 - val_mae: 31.8354\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 3973.4759 - mse: 3973.4761 - mae: 33.5936 - val_loss: 2349.1256 - val_mse: 2349.1255 - val_mae: 31.9323\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 3879.5906 - mse: 3879.5906 - mae: 32.9252 - val_loss: 2330.9627 - val_mse: 2330.9626 - val_mae: 31.8716\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4028.2271 - mse: 4028.2271 - mae: 32.2402 - val_loss: 2327.1132 - val_mse: 2327.1133 - val_mae: 31.8586\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4056.0274 - mse: 4056.0276 - mae: 33.2128 - val_loss: 2306.5660 - val_mse: 2306.5659 - val_mae: 31.7907\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 3865.1146 - mse: 3865.1145 - mae: 31.5863 - val_loss: 2262.5035 - val_mse: 2262.5034 - val_mae: 31.6610\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 3977.6843 - mse: 3977.6841 - mae: 32.7164 - val_loss: 2291.7774 - val_mse: 2291.7771 - val_mae: 31.7479\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3372.0934 - mse: 3372.0938 - mae: 33.0215 - val_loss: 1444.2607 - val_mse: 1444.2607 - val_mae: 25.5710\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3386.7408 - mse: 3386.7412 - mae: 32.6048 - val_loss: 1445.8349 - val_mse: 1445.8348 - val_mae: 25.8622\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3339.4930 - mse: 3339.4934 - mae: 32.8087 - val_loss: 1446.8887 - val_mse: 1446.8887 - val_mae: 25.9797\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3428.1685 - mse: 3428.1685 - mae: 32.6474 - val_loss: 1445.0409 - val_mse: 1445.0409 - val_mae: 25.2312\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3369.1568 - mse: 3369.1567 - mae: 32.8584 - val_loss: 1448.9124 - val_mse: 1448.9122 - val_mae: 26.1545\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3327.1440 - mse: 3327.1445 - mae: 32.7474 - val_loss: 1445.9722 - val_mse: 1445.9723 - val_mae: 25.4428\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3422.1938 - mse: 3422.1926 - mae: 32.5513 - val_loss: 1446.8940 - val_mse: 1446.8940 - val_mae: 25.6448\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3437.4943 - mse: 3437.4946 - mae: 33.0640 - val_loss: 1449.4696 - val_mse: 1449.4696 - val_mae: 26.0310\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3324.5824 - mse: 3324.5823 - mae: 32.7504 - val_loss: 1447.3589 - val_mse: 1447.3590 - val_mae: 25.5367\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3313.6314 - mse: 3313.6318 - mae: 32.2620 - val_loss: 1448.0002 - val_mse: 1448.0001 - val_mae: 25.0956\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3400.1242 - mse: 3400.1240 - mae: 32.5638 - val_loss: 1449.0675 - val_mse: 1449.0674 - val_mae: 25.8265\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3399.1029 - mse: 3399.1030 - mae: 33.0253 - val_loss: 1448.9846 - val_mse: 1448.9846 - val_mae: 25.7197\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3406.0342 - mse: 3406.0332 - mae: 32.9598 - val_loss: 1450.1578 - val_mse: 1450.1577 - val_mae: 25.8813\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3431.3290 - mse: 3431.3293 - mae: 33.6221 - val_loss: 1464.8788 - val_mse: 1464.8788 - val_mae: 27.2067\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3369.6622 - mse: 3369.6628 - mae: 32.6029 - val_loss: 1453.5206 - val_mse: 1453.5204 - val_mae: 26.2483\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3251.3113 - mse: 3251.3127 - mae: 32.0661 - val_loss: 1453.8407 - val_mse: 1453.8406 - val_mae: 26.2420\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3304.8083 - mse: 3304.8081 - mae: 32.2359 - val_loss: 1451.5391 - val_mse: 1451.5391 - val_mae: 25.8089\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3521.0451 - mse: 3521.0452 - mae: 32.4239 - val_loss: 1452.8854 - val_mse: 1452.8853 - val_mae: 25.9935\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3334.9582 - mse: 3334.9578 - mae: 32.6181 - val_loss: 1457.0906 - val_mse: 1457.0905 - val_mae: 26.4738\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3259.5420 - mse: 3259.5425 - mae: 32.5769 - val_loss: 1453.9564 - val_mse: 1453.9564 - val_mae: 26.0974\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3350.1259 - mse: 3350.1265 - mae: 32.6536 - val_loss: 1452.8152 - val_mse: 1452.8153 - val_mae: 25.8592\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3253.8282 - mse: 3253.8284 - mae: 32.2762 - val_loss: 1452.4829 - val_mse: 1452.4829 - val_mae: 25.6864\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3240.2633 - mse: 3240.2634 - mae: 31.2534 - val_loss: 1453.5763 - val_mse: 1453.5764 - val_mae: 25.9127\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 493us/step - loss: 3314.9383 - mse: 3314.9382 - mae: 32.4893 - val_loss: 1455.6703 - val_mse: 1455.6704 - val_mae: 26.2127\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3421.1099 - mse: 3421.1101 - mae: 32.7392 - val_loss: 1455.4770 - val_mse: 1455.4771 - val_mae: 26.1568\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3377.8727 - mse: 3377.8723 - mae: 31.9664 - val_loss: 1458.3605 - val_mse: 1458.3605 - val_mae: 26.4637\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3294.3679 - mse: 3294.3689 - mae: 32.0792 - val_loss: 1453.8390 - val_mse: 1453.8389 - val_mae: 25.7131\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3346.3091 - mse: 3346.3086 - mae: 32.1210 - val_loss: 1454.9181 - val_mse: 1454.9183 - val_mae: 25.0545\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 518us/step - loss: 3343.6508 - mse: 3343.6509 - mae: 32.2840 - val_loss: 1455.6814 - val_mse: 1455.6815 - val_mae: 25.9883\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3374.3667 - mse: 3374.3672 - mae: 32.2233 - val_loss: 1454.8001 - val_mse: 1454.8002 - val_mae: 25.7725\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 657us/step - loss: 3354.3213 - mse: 3354.3208 - mae: 31.9599 - val_loss: 1456.3599 - val_mse: 1456.3600 - val_mae: 26.0260\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3365.8509 - mse: 3365.8511 - mae: 32.3357 - val_loss: 1458.2519 - val_mse: 1458.2520 - val_mae: 26.2553\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 526us/step - loss: 3230.7719 - mse: 3230.7722 - mae: 31.5263 - val_loss: 1460.5470 - val_mse: 1460.5471 - val_mae: 26.4926\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3369.6743 - mse: 3369.6743 - mae: 32.8072 - val_loss: 1456.9336 - val_mse: 1456.9337 - val_mae: 25.9616\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 653us/step - loss: 3351.2429 - mse: 3351.2432 - mae: 31.9214 - val_loss: 1461.0462 - val_mse: 1461.0463 - val_mae: 26.4914\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3354.4419 - mse: 3354.4421 - mae: 31.8376 - val_loss: 1456.2599 - val_mse: 1456.2600 - val_mae: 25.6358\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3189.8224 - mse: 3189.8225 - mae: 30.7757 - val_loss: 1460.7804 - val_mse: 1460.7804 - val_mae: 26.4126\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3182.4317 - mse: 3182.4314 - mae: 30.9088 - val_loss: 1464.7854 - val_mse: 1464.7853 - val_mae: 26.8007\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 672us/step - loss: 3300.4731 - mse: 3300.4731 - mae: 32.4026 - val_loss: 1457.4555 - val_mse: 1457.4553 - val_mae: 25.8358\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3329.5873 - mse: 3329.5876 - mae: 32.1437 - val_loss: 1460.0843 - val_mse: 1460.0844 - val_mae: 26.2581\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3264.3307 - mse: 3264.3308 - mae: 31.5168 - val_loss: 1457.9110 - val_mse: 1457.9111 - val_mae: 25.8538\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3302.8197 - mse: 3302.8203 - mae: 32.0029 - val_loss: 1458.7233 - val_mse: 1458.7234 - val_mae: 25.9675\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3344.4599 - mse: 3344.4595 - mae: 32.2795 - val_loss: 1458.3883 - val_mse: 1458.3882 - val_mae: 25.8488\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3346.8967 - mse: 3346.8967 - mae: 31.7231 - val_loss: 1460.5853 - val_mse: 1460.5852 - val_mae: 26.2038\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3401.1384 - mse: 3401.1389 - mae: 32.5417 - val_loss: 1459.3229 - val_mse: 1459.3229 - val_mae: 25.9403\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3273.7512 - mse: 3273.7505 - mae: 31.6690 - val_loss: 1461.4110 - val_mse: 1461.4111 - val_mae: 26.2453\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3264.7236 - mse: 3264.7249 - mae: 31.4351 - val_loss: 1469.6704 - val_mse: 1469.6704 - val_mae: 27.0189\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3417.5265 - mse: 3417.5269 - mae: 32.6836 - val_loss: 1459.7438 - val_mse: 1459.7440 - val_mae: 25.8431\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3302.8772 - mse: 3302.8767 - mae: 31.5741 - val_loss: 1460.1873 - val_mse: 1460.1874 - val_mae: 25.9082\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3294.2542 - mse: 3294.2534 - mae: 31.8484 - val_loss: 1459.8469 - val_mse: 1459.8470 - val_mae: 25.6966\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3215.8510 - mse: 3215.8518 - mae: 31.52 - 1s 665us/step - loss: 3200.2762 - mse: 3200.2769 - mae: 31.5519 - val_loss: 1468.4677 - val_mse: 1468.4678 - val_mae: 26.8546\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3262.6214 - mse: 3262.6213 - mae: 31.8214 - val_loss: 1469.3916 - val_mse: 1469.3917 - val_mae: 26.9122\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3270.6269 - mse: 3270.6265 - mae: 31.8806 - val_loss: 1461.5901 - val_mse: 1461.5900 - val_mae: 26.0166\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 718us/step - loss: 3262.8702 - mse: 3262.8713 - mae: 31.8581 - val_loss: 1463.7169 - val_mse: 1463.7169 - val_mae: 26.3505\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3237.5368 - mse: 3237.5369 - mae: 31.8750 - val_loss: 1467.7752 - val_mse: 1467.7753 - val_mae: 26.7702\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3309.3502 - mse: 3309.3508 - mae: 31.8835 - val_loss: 1461.5740 - val_mse: 1461.5740 - val_mae: 25.9496\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3256.8997 - mse: 3256.8997 - mae: 31.4950 - val_loss: 1464.5249 - val_mse: 1464.5249 - val_mae: 26.3625\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3264.6012 - mse: 3264.6011 - mae: 31.3795 - val_loss: 1467.6040 - val_mse: 1467.6041 - val_mae: 26.6573\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3222.1429 - mse: 3222.1431 - mae: 31.6081 - val_loss: 1465.4177 - val_mse: 1465.4177 - val_mae: 26.3973\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3266.6701 - mse: 3266.6709 - mae: 31.9191 - val_loss: 1463.4075 - val_mse: 1463.4076 - val_mae: 26.0674\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 525us/step - loss: 3174.0107 - mse: 3174.0107 - mae: 31.8412 - val_loss: 1464.8874 - val_mse: 1464.8876 - val_mae: 26.3109\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3341.6040 - mse: 3341.6040 - mae: 31.9041 - val_loss: 1463.3556 - val_mse: 1463.3558 - val_mae: 26.0437\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3302.4094 - mse: 3302.4094 - mae: 31.6227 - val_loss: 1464.6586 - val_mse: 1464.6586 - val_mae: 26.2470\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3216.7329 - mse: 3216.7332 - mae: 31.3577 - val_loss: 1463.2574 - val_mse: 1463.2576 - val_mae: 25.9786\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3325.2429 - mse: 3325.2434 - mae: 31.3069 - val_loss: 1464.7945 - val_mse: 1464.7944 - val_mae: 26.1947\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3354.9471 - mse: 3354.9468 - mae: 31.7721 - val_loss: 1463.0301 - val_mse: 1463.0299 - val_mae: 25.7594\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3247.2180 - mse: 3247.2180 - mae: 31.5934 - val_loss: 1463.8346 - val_mse: 1463.8344 - val_mae: 25.9900\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 683us/step - loss: 3307.2432 - mse: 3307.2434 - mae: 32.3062 - val_loss: 1463.4886 - val_mse: 1463.4885 - val_mae: 25.5247\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3204.1592 - mse: 3204.1592 - mae: 31.8493 - val_loss: 1468.1664 - val_mse: 1468.1665 - val_mae: 26.5234\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3251.2092 - mse: 3251.2095 - mae: 30.8783 - val_loss: 1465.9205 - val_mse: 1465.9207 - val_mae: 26.1873\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3355.8431 - mse: 3355.8435 - mae: 31.9921 - val_loss: 1467.1271 - val_mse: 1467.1270 - val_mae: 26.3610\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3248.7246 - mse: 3248.7241 - mae: 31.2201 - val_loss: 1465.2574 - val_mse: 1465.2576 - val_mae: 26.0035\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 698us/step - loss: 3284.7124 - mse: 3284.7126 - mae: 31.5892 - val_loss: 1470.1415 - val_mse: 1470.1414 - val_mae: 26.6577\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 687us/step - loss: 3271.4898 - mse: 3271.4902 - mae: 31.8505 - val_loss: 1466.1668 - val_mse: 1466.1667 - val_mae: 26.1933\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3243.0446 - mse: 3243.0444 - mae: 31.8555 - val_loss: 1464.8667 - val_mse: 1464.8667 - val_mae: 25.9506\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3268.5669 - mse: 3268.5669 - mae: 31.7025 - val_loss: 1466.0754 - val_mse: 1466.0753 - val_mae: 26.1976\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3155.7971 - mse: 3155.7966 - mae: 30.9773 - val_loss: 1466.0524 - val_mse: 1466.0522 - val_mae: 26.2148\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3218.3125 - mse: 3218.3137 - mae: 31.0708 - val_loss: 1472.0578 - val_mse: 1472.0580 - val_mae: 26.8806\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3180.2383 - mse: 3180.2385 - mae: 31.4666 - val_loss: 1474.5867 - val_mse: 1474.5868 - val_mae: 27.0897\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3234.9273 - mse: 3234.9270 - mae: 31.7808 - val_loss: 1464.4052 - val_mse: 1464.4054 - val_mae: 25.9342\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 724us/step - loss: 2962.9973 - mse: 2962.9985 - mae: 30.8765 - val_loss: 1073.5903 - val_mse: 1073.5905 - val_mae: 23.9628\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2880.6564 - mse: 2880.6555 - mae: 30.7892 - val_loss: 1072.5404 - val_mse: 1072.5405 - val_mae: 24.4119\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2965.4578 - mse: 2965.4583 - mae: 30.8945 - val_loss: 1073.9334 - val_mse: 1073.9333 - val_mae: 23.9205\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 453us/step - loss: 2810.4177 - mse: 2810.4180 - mae: 30.5142 - val_loss: 1072.5087 - val_mse: 1072.5085 - val_mae: 24.2103\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 498us/step - loss: 2923.5028 - mse: 2923.5024 - mae: 31.2289 - val_loss: 1072.4186 - val_mse: 1072.4187 - val_mae: 24.3727\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2945.5709 - mse: 2945.5710 - mae: 31.7868 - val_loss: 1073.7134 - val_mse: 1073.7133 - val_mae: 23.9754\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2919.7129 - mse: 2919.7134 - mae: 31.1546 - val_loss: 1072.6256 - val_mse: 1072.6256 - val_mae: 24.1853\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2862.4525 - mse: 2862.4531 - mae: 30.9134 - val_loss: 1073.5632 - val_mse: 1073.5631 - val_mae: 23.9556\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 716us/step - loss: 2937.7748 - mse: 2937.7744 - mae: 30.9523 - val_loss: 1072.1873 - val_mse: 1072.1873 - val_mae: 24.4676\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2830.0099 - mse: 2830.0095 - mae: 30.4932 - val_loss: 1072.2713 - val_mse: 1072.2711 - val_mae: 24.1022\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2948.3768 - mse: 2948.3757 - mae: 30.5808 - val_loss: 1072.8161 - val_mse: 1072.8163 - val_mae: 23.9655\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 551us/step - loss: 2835.9682 - mse: 2835.9678 - mae: 30.7011 - val_loss: 1074.0008 - val_mse: 1074.0009 - val_mae: 23.8242\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2862.6037 - mse: 2862.6035 - mae: 30.4306 - val_loss: 1071.6028 - val_mse: 1071.6028 - val_mae: 24.1063\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2803.4205 - mse: 2803.4189 - mae: 30.6970 - val_loss: 1071.3326 - val_mse: 1071.3325 - val_mae: 24.5372\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2903.9300 - mse: 2903.9302 - mae: 30.9017 - val_loss: 1070.6310 - val_mse: 1070.6310 - val_mae: 24.3166\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 517us/step - loss: 2921.8858 - mse: 2921.8855 - mae: 30.4087 - val_loss: 1070.4419 - val_mse: 1070.4419 - val_mae: 24.2696\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2955.9802 - mse: 2955.9805 - mae: 31.0378 - val_loss: 1070.5370 - val_mse: 1070.5370 - val_mae: 24.4289\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2953.7278 - mse: 2953.7288 - mae: 31.3464 - val_loss: 1071.9482 - val_mse: 1071.9482 - val_mae: 23.9336\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2847.0765 - mse: 2847.0762 - mae: 30.1905 - val_loss: 1072.2231 - val_mse: 1072.2229 - val_mae: 23.9051\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 3022.0855 - mse: 3022.0854 - mae: 31.5027 - val_loss: 1076.3207 - val_mse: 1076.3208 - val_mae: 23.6385\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2909.9796 - mse: 2909.9792 - mae: 30.7136 - val_loss: 1071.5200 - val_mse: 1071.5199 - val_mae: 24.0531\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 479us/step - loss: 2897.9624 - mse: 2897.9634 - mae: 30.5971 - val_loss: 1070.7255 - val_mse: 1070.7255 - val_mae: 24.2638\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 512us/step - loss: 2823.7388 - mse: 2823.7388 - mae: 30.2797 - val_loss: 1070.7252 - val_mse: 1070.7251 - val_mae: 24.5461\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2846.5106 - mse: 2846.5103 - mae: 30.6205 - val_loss: 1070.3083 - val_mse: 1070.3083 - val_mae: 24.1455\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2845.9605 - mse: 2845.9604 - mae: 30.7980 - val_loss: 1071.6627 - val_mse: 1071.6627 - val_mae: 23.9348\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2862.4728 - mse: 2862.4729 - mae: 30.7404 - val_loss: 1070.0776 - val_mse: 1070.0775 - val_mae: 24.2473\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2975.5747 - mse: 2975.5745 - mae: 31.1850 - val_loss: 1070.2409 - val_mse: 1070.2410 - val_mae: 24.2719\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2849.1943 - mse: 2849.1943 - mae: 30.2957 - val_loss: 1071.1652 - val_mse: 1071.1652 - val_mae: 23.9740\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2966.9335 - mse: 2966.9333 - mae: 31.6367 - val_loss: 1072.3689 - val_mse: 1072.3689 - val_mae: 23.8444\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2866.0271 - mse: 2866.0264 - mae: 30.7939 - val_loss: 1072.2709 - val_mse: 1072.2706 - val_mae: 23.8189\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2899.7772 - mse: 2899.7776 - mae: 30.7662 - val_loss: 1073.1468 - val_mse: 1073.1469 - val_mae: 23.7531\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 686us/step - loss: 2885.2771 - mse: 2885.2773 - mae: 30.9814 - val_loss: 1069.9550 - val_mse: 1069.9548 - val_mae: 24.0981\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2927.5620 - mse: 2927.5620 - mae: 31.0368 - val_loss: 1070.5770 - val_mse: 1070.5769 - val_mae: 23.9140\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2894.5398 - mse: 2894.5398 - mae: 30.3752 - val_loss: 1070.3740 - val_mse: 1070.3741 - val_mae: 23.9275\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2877.5658 - mse: 2877.5654 - mae: 30.6905 - val_loss: 1068.7856 - val_mse: 1068.7855 - val_mae: 24.2134\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2895.4633 - mse: 2895.4629 - mae: 30.9344 - val_loss: 1068.4707 - val_mse: 1068.4707 - val_mae: 24.3768\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2904.9531 - mse: 2904.9539 - mae: 30.4761 - val_loss: 1068.4838 - val_mse: 1068.4839 - val_mae: 24.1814\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 640us/step - loss: 2804.1269 - mse: 2804.1267 - mae: 30.2877 - val_loss: 1068.0097 - val_mse: 1068.0096 - val_mae: 24.2583\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2878.3466 - mse: 2878.3452 - mae: 30.7752 - val_loss: 1069.1163 - val_mse: 1069.1163 - val_mae: 23.9115\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2892.5711 - mse: 2892.5713 - mae: 30.8426 - val_loss: 1067.6450 - val_mse: 1067.6450 - val_mae: 24.3949\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 488us/step - loss: 2912.1835 - mse: 2912.1841 - mae: 30.4216 - val_loss: 1067.8317 - val_mse: 1067.8317 - val_mae: 24.4907\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2807.7994 - mse: 2807.7986 - mae: 30.3221 - val_loss: 1067.2848 - val_mse: 1067.2848 - val_mae: 24.1401\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2894.5658 - mse: 2894.5657 - mae: 30.5931 - val_loss: 1067.1611 - val_mse: 1067.1611 - val_mae: 24.5206\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2780.1822 - mse: 2780.1826 - mae: 30.1264 - val_loss: 1066.8411 - val_mse: 1066.8411 - val_mae: 24.2633\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2901.1575 - mse: 2901.1570 - mae: 30.6749 - val_loss: 1066.7354 - val_mse: 1066.7355 - val_mae: 24.4876\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2813.8728 - mse: 2813.8723 - mae: 29.8940 - val_loss: 1066.9130 - val_mse: 1066.9132 - val_mae: 24.0306\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2805.7682 - mse: 2805.7678 - mae: 30.1591 - val_loss: 1065.9757 - val_mse: 1065.9757 - val_mae: 24.3241\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 696us/step - loss: 2867.1463 - mse: 2867.1467 - mae: 30.4004 - val_loss: 1065.9185 - val_mse: 1065.9183 - val_mae: 24.3202\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 539us/step - loss: 2911.5513 - mse: 2911.5505 - mae: 31.3732 - val_loss: 1065.7750 - val_mse: 1065.7750 - val_mae: 24.1061\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 539us/step - loss: 2883.1711 - mse: 2883.1702 - mae: 30.2780 - val_loss: 1065.3433 - val_mse: 1065.3434 - val_mae: 24.1309\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2860.7376 - mse: 2860.7371 - mae: 30.4795 - val_loss: 1065.3710 - val_mse: 1065.3711 - val_mae: 24.1210\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 503us/step - loss: 2731.0579 - mse: 2731.0574 - mae: 30.1619 - val_loss: 1065.1752 - val_mse: 1065.1752 - val_mae: 24.1494\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2907.4008 - mse: 2907.4011 - mae: 30.6091 - val_loss: 1065.2967 - val_mse: 1065.2966 - val_mae: 23.9983\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2851.6879 - mse: 2851.6882 - mae: 30.9280 - val_loss: 1064.6196 - val_mse: 1064.6195 - val_mae: 24.0184\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2837.0171 - mse: 2837.0173 - mae: 30.2468 - val_loss: 1063.8421 - val_mse: 1063.8422 - val_mae: 24.1337\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 683us/step - loss: 2862.9877 - mse: 2862.9883 - mae: 30.8697 - val_loss: 1063.4205 - val_mse: 1063.4205 - val_mae: 24.2505\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2863.3782 - mse: 2863.3774 - mae: 30.5977 - val_loss: 1063.1592 - val_mse: 1063.1593 - val_mae: 24.1170\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 656us/step - loss: 2845.9876 - mse: 2845.9873 - mae: 30.5027 - val_loss: 1063.0643 - val_mse: 1063.0642 - val_mae: 24.1244\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2863.0527 - mse: 2863.0527 - mae: 30.5680 - val_loss: 1062.3545 - val_mse: 1062.3546 - val_mae: 24.1418\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2879.4553 - mse: 2879.4565 - mae: 30.6748 - val_loss: 1062.0771 - val_mse: 1062.0771 - val_mae: 24.1480\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2790.7904 - mse: 2790.7896 - mae: 29.6864 - val_loss: 1062.4661 - val_mse: 1062.4659 - val_mae: 24.7290\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2867.5328 - mse: 2867.5327 - mae: 31.0484 - val_loss: 1060.8292 - val_mse: 1060.8292 - val_mae: 24.4938\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 541us/step - loss: 2871.5655 - mse: 2871.5654 - mae: 30.7194 - val_loss: 1059.6215 - val_mse: 1059.6215 - val_mae: 24.2854\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2855.9608 - mse: 2855.9600 - mae: 30.6904 - val_loss: 1061.7366 - val_mse: 1061.7366 - val_mae: 23.7505\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 541us/step - loss: 2824.2395 - mse: 2824.2390 - mae: 30.1106 - val_loss: 1058.5802 - val_mse: 1058.5801 - val_mae: 24.4220\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 2882.0619 - mse: 2882.0615 - mae: 30.6166 - val_loss: 1058.4426 - val_mse: 1058.4427 - val_mae: 24.0257\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 543us/step - loss: 2851.2924 - mse: 2851.2925 - mae: 30.5797 - val_loss: 1058.3061 - val_mse: 1058.3062 - val_mae: 24.0144\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 530us/step - loss: 2875.5486 - mse: 2875.5479 - mae: 30.6224 - val_loss: 1058.5100 - val_mse: 1058.5100 - val_mae: 24.1288\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2812.4569 - mse: 2812.4568 - mae: 30.1689 - val_loss: 1058.0095 - val_mse: 1058.0095 - val_mae: 24.3462\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2839.9562 - mse: 2839.9565 - mae: 30.5178 - val_loss: 1057.3510 - val_mse: 1057.3510 - val_mae: 24.4015\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 521us/step - loss: 2763.7819 - mse: 2763.7820 - mae: 30.4415 - val_loss: 1057.1167 - val_mse: 1057.1166 - val_mae: 24.1722\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2755.3284 - mse: 2755.3289 - mae: 30.1813 - val_loss: 1056.6054 - val_mse: 1056.6055 - val_mae: 24.2211\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2857.8727 - mse: 2857.8723 - mae: 30.3997 - val_loss: 1056.5762 - val_mse: 1056.5762 - val_mae: 24.0928\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2824.2420 - mse: 2824.2422 - mae: 30.2008 - val_loss: 1057.0282 - val_mse: 1057.0282 - val_mae: 24.4751\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 692us/step - loss: 2813.5570 - mse: 2813.5562 - mae: 30.4798 - val_loss: 1056.4408 - val_mse: 1056.4408 - val_mae: 24.2036\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2889.0944 - mse: 2889.0945 - mae: 31.0908 - val_loss: 1057.1936 - val_mse: 1057.1935 - val_mae: 23.8720\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2880.3327 - mse: 2880.3330 - mae: 30.6865 - val_loss: 1059.4978 - val_mse: 1059.4979 - val_mae: 23.6621\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2835.5564 - mse: 2835.5566 - mae: 29.8266 - val_loss: 1056.2683 - val_mse: 1056.2682 - val_mae: 24.0797\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2877.8217 - mse: 2877.8220 - mae: 30.3772 - val_loss: 1055.8503 - val_mse: 1055.8503 - val_mae: 24.2228\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2828.9137 - mse: 2828.9126 - mae: 30.2862 - val_loss: 1056.6256 - val_mse: 1056.6256 - val_mae: 23.8499\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2552.8261 - mse: 2552.8259 - mae: 29.6606 - val_loss: 1541.4032 - val_mse: 1541.4033 - val_mae: 27.8255\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2578.3488 - mse: 2578.3491 - mae: 29.3996 - val_loss: 1538.9369 - val_mse: 1538.9370 - val_mae: 27.9127\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2517.6194 - mse: 2517.6191 - mae: 29.7295 - val_loss: 1539.2875 - val_mse: 1539.2875 - val_mae: 27.8554\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2518.0945 - mse: 2518.0940 - mae: 29.9392 - val_loss: 1534.8820 - val_mse: 1534.8821 - val_mae: 28.0134\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2540.5320 - mse: 2540.5322 - mae: 29.8048 - val_loss: 1543.4195 - val_mse: 1543.4194 - val_mae: 27.6575\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2531.3676 - mse: 2531.3677 - mae: 29.4851 - val_loss: 1545.2228 - val_mse: 1545.2228 - val_mae: 27.5923\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2509.2688 - mse: 2509.2686 - mae: 29.4248 - val_loss: 1535.1506 - val_mse: 1535.1506 - val_mae: 27.9590\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2490.3179 - mse: 2490.3176 - mae: 29.7075 - val_loss: 1530.0918 - val_mse: 1530.0917 - val_mae: 28.1331\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2509.1716 - mse: 2509.1714 - mae: 29.4530 - val_loss: 1540.1219 - val_mse: 1540.1219 - val_mae: 27.6642\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2470.0140 - mse: 2470.0137 - mae: 29.4771 - val_loss: 1534.8332 - val_mse: 1534.8333 - val_mae: 27.8345\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 652us/step - loss: 2538.7953 - mse: 2538.7959 - mae: 29.7204 - val_loss: 1529.7583 - val_mse: 1529.7582 - val_mae: 28.0469\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2533.6316 - mse: 2533.6309 - mae: 29.6058 - val_loss: 1532.5530 - val_mse: 1532.5532 - val_mae: 27.8931\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 2s 680us/step - loss: 2524.2122 - mse: 2524.2122 - mae: 29.8146 - val_loss: 1542.4950 - val_mse: 1542.4951 - val_mae: 27.5167\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2472.1610 - mse: 2472.1604 - mae: 29.4067 - val_loss: 1524.7960 - val_mse: 1524.7960 - val_mae: 28.2658\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2453.4471 - mse: 2453.4460 - mae: 29.2846 - val_loss: 1537.7904 - val_mse: 1537.7903 - val_mae: 27.6219\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2507.4834 - mse: 2507.4836 - mae: 29.5534 - val_loss: 1534.1534 - val_mse: 1534.1534 - val_mae: 27.7435\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 675us/step - loss: 2532.6598 - mse: 2532.6597 - mae: 29.5806 - val_loss: 1534.9393 - val_mse: 1534.9391 - val_mae: 27.7065\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 648us/step - loss: 2488.3134 - mse: 2488.3140 - mae: 29.3801 - val_loss: 1532.0652 - val_mse: 1532.0652 - val_mae: 27.8098\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2528.1512 - mse: 2528.1516 - mae: 29.6758 - val_loss: 1534.1997 - val_mse: 1534.1997 - val_mae: 27.7107\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2534.7024 - mse: 2534.7026 - mae: 29.4029 - val_loss: 1532.1278 - val_mse: 1532.1279 - val_mae: 27.7542\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2564.4930 - mse: 2564.4932 - mae: 30.0529 - val_loss: 1533.4105 - val_mse: 1533.4105 - val_mae: 27.7210\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2573.8458 - mse: 2573.8457 - mae: 29.6667 - val_loss: 1533.1681 - val_mse: 1533.1681 - val_mae: 27.7502\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2556.5356 - mse: 2556.5356 - mae: 29.1200 - val_loss: 1528.4103 - val_mse: 1528.4103 - val_mae: 27.9712\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2535.7466 - mse: 2535.7473 - mae: 29.8889 - val_loss: 1531.6163 - val_mse: 1531.6162 - val_mae: 27.8235\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2480.6360 - mse: 2480.6355 - mae: 29.2048 - val_loss: 1532.0056 - val_mse: 1532.0055 - val_mae: 27.7842\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - ETA: 0s - loss: 2629.1261 - mse: 2629.1260 - mae: 29.57 - 1s 560us/step - loss: 2554.7523 - mse: 2554.7522 - mae: 29.3115 - val_loss: 1527.7620 - val_mse: 1527.7620 - val_mae: 27.9822\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2537.6081 - mse: 2537.6079 - mae: 29.3859 - val_loss: 1526.2022 - val_mse: 1526.2024 - val_mae: 28.0350\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2536.2283 - mse: 2536.2280 - mae: 30.0412 - val_loss: 1525.2647 - val_mse: 1525.2648 - val_mae: 28.0392\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2474.7050 - mse: 2474.7043 - mae: 29.2353 - val_loss: 1526.2401 - val_mse: 1526.2401 - val_mae: 27.9853\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2500.8143 - mse: 2500.8142 - mae: 29.4301 - val_loss: 1530.3801 - val_mse: 1530.3799 - val_mae: 27.7876\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 682us/step - loss: 2529.6807 - mse: 2529.6812 - mae: 29.3126 - val_loss: 1531.4057 - val_mse: 1531.4056 - val_mae: 27.7665\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2517.8048 - mse: 2517.8047 - mae: 29.4719 - val_loss: 1528.2564 - val_mse: 1528.2563 - val_mae: 27.9379\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 688us/step - loss: 2506.2569 - mse: 2506.2571 - mae: 29.4190 - val_loss: 1525.5211 - val_mse: 1525.5211 - val_mae: 27.9889\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2510.9076 - mse: 2510.9082 - mae: 29.2578 - val_loss: 1523.4212 - val_mse: 1523.4214 - val_mae: 28.0963\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2507.4704 - mse: 2507.4705 - mae: 29.7305 - val_loss: 1525.6552 - val_mse: 1525.6554 - val_mae: 27.9667\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2542.0801 - mse: 2542.0798 - mae: 29.7271 - val_loss: 1530.3528 - val_mse: 1530.3529 - val_mae: 27.7448\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2530.3293 - mse: 2530.3291 - mae: 29.7882 - val_loss: 1531.6492 - val_mse: 1531.6492 - val_mae: 27.6760\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2468.9908 - mse: 2468.9910 - mae: 29.0950 - val_loss: 1527.6350 - val_mse: 1527.6349 - val_mae: 27.8511\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2478.0955 - mse: 2478.0964 - mae: 29.5223 - val_loss: 1534.2627 - val_mse: 1534.2627 - val_mae: 27.5972\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2489.0042 - mse: 2489.0032 - mae: 29.4303 - val_loss: 1525.6309 - val_mse: 1525.6309 - val_mae: 27.9063\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2544.4193 - mse: 2544.4182 - mae: 29.8072 - val_loss: 1529.8488 - val_mse: 1529.8489 - val_mae: 27.7337\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2492.0572 - mse: 2492.0564 - mae: 29.0610 - val_loss: 1535.6332 - val_mse: 1535.6329 - val_mae: 27.5159\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2477.6938 - mse: 2477.6931 - mae: 29.1424 - val_loss: 1526.2569 - val_mse: 1526.2571 - val_mae: 27.8659\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2545.7979 - mse: 2545.7979 - mae: 29.6754 - val_loss: 1530.9640 - val_mse: 1530.9641 - val_mae: 27.6362\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2482.0802 - mse: 2482.0798 - mae: 29.5472 - val_loss: 1522.1299 - val_mse: 1522.1298 - val_mae: 28.0224\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2474.8366 - mse: 2474.8364 - mae: 29.4347 - val_loss: 1526.5838 - val_mse: 1526.5836 - val_mae: 27.7929\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2550.4299 - mse: 2550.4294 - mae: 29.6086 - val_loss: 1519.2670 - val_mse: 1519.2668 - val_mae: 28.1920\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2510.6369 - mse: 2510.6362 - mae: 29.5240 - val_loss: 1524.5661 - val_mse: 1524.5662 - val_mae: 27.9112\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2491.1815 - mse: 2491.1819 - mae: 28.9379 - val_loss: 1521.4394 - val_mse: 1521.4390 - val_mae: 28.0887\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2521.1794 - mse: 2521.1792 - mae: 29.3946 - val_loss: 1525.8920 - val_mse: 1525.8921 - val_mae: 27.8557\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2531.3288 - mse: 2531.3281 - mae: 29.6441 - val_loss: 1529.3816 - val_mse: 1529.3815 - val_mae: 27.6277\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2521.1970 - mse: 2521.1973 - mae: 29.5127 - val_loss: 1540.2473 - val_mse: 1540.2472 - val_mae: 27.2321\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2468.6871 - mse: 2468.6868 - mae: 29.4020 - val_loss: 1524.6962 - val_mse: 1524.6962 - val_mae: 27.7619\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2550.3971 - mse: 2550.3967 - mae: 30.0937 - val_loss: 1528.9541 - val_mse: 1528.9541 - val_mae: 27.6237\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2489.6033 - mse: 2489.6035 - mae: 28.9289 - val_loss: 1522.0321 - val_mse: 1522.0319 - val_mae: 27.9656\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2461.8808 - mse: 2461.8816 - mae: 29.0483 - val_loss: 1523.1917 - val_mse: 1523.1917 - val_mae: 27.8373\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2514.9462 - mse: 2514.9463 - mae: 29.5041 - val_loss: 1527.0024 - val_mse: 1527.0026 - val_mae: 27.6693\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2451.7796 - mse: 2451.7791 - mae: 28.7376 - val_loss: 1517.9315 - val_mse: 1517.9315 - val_mae: 28.1301\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 661us/step - loss: 2417.9213 - mse: 2417.9214 - mae: 29.0533 - val_loss: 1522.7711 - val_mse: 1522.7711 - val_mae: 27.8462\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2476.0373 - mse: 2476.0374 - mae: 29.1770 - val_loss: 1523.1846 - val_mse: 1523.1846 - val_mae: 27.8343\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2463.8684 - mse: 2463.8672 - mae: 29.1232 - val_loss: 1525.3322 - val_mse: 1525.3323 - val_mae: 27.7330\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2463.1301 - mse: 2463.1296 - mae: 29.0003 - val_loss: 1526.6145 - val_mse: 1526.6146 - val_mae: 27.6876\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2464.2814 - mse: 2464.2817 - mae: 29.0014 - val_loss: 1523.9969 - val_mse: 1523.9969 - val_mae: 27.7848\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2492.2882 - mse: 2492.2878 - mae: 28.9745 - val_loss: 1521.9056 - val_mse: 1521.9054 - val_mae: 27.8814\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2541.1982 - mse: 2541.1987 - mae: 29.4423 - val_loss: 1521.0828 - val_mse: 1521.0828 - val_mae: 27.9014\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2452.2106 - mse: 2452.2100 - mae: 29.1446 - val_loss: 1524.5765 - val_mse: 1524.5765 - val_mae: 27.7233\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2542.0131 - mse: 2542.0129 - mae: 29.3739 - val_loss: 1526.7552 - val_mse: 1526.7554 - val_mae: 27.6378\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2516.0485 - mse: 2516.0491 - mae: 29.3359 - val_loss: 1526.8580 - val_mse: 1526.8582 - val_mae: 27.6319\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2474.8630 - mse: 2474.8621 - mae: 29.3213 - val_loss: 1523.9057 - val_mse: 1523.9056 - val_mae: 27.7717\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2482.6524 - mse: 2482.6519 - mae: 29.2210 - val_loss: 1522.4167 - val_mse: 1522.4164 - val_mae: 27.8530\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2463.0740 - mse: 2463.0742 - mae: 28.7953 - val_loss: 1522.6783 - val_mse: 1522.6782 - val_mae: 27.8850\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2507.3688 - mse: 2507.3694 - mae: 29.3740 - val_loss: 1526.1845 - val_mse: 1526.1846 - val_mae: 27.6743\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2468.9477 - mse: 2468.9485 - mae: 29.0623 - val_loss: 1523.0952 - val_mse: 1523.0951 - val_mae: 27.7959\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2549.3416 - mse: 2549.3416 - mae: 29.2430 - val_loss: 1520.7571 - val_mse: 1520.7572 - val_mae: 27.9448\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2420.4152 - mse: 2420.4146 - mae: 29.1996 - val_loss: 1517.1549 - val_mse: 1517.1549 - val_mae: 28.1425\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2464.5539 - mse: 2464.5540 - mae: 29.2024 - val_loss: 1516.3152 - val_mse: 1516.3151 - val_mae: 28.1477\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 504us/step - loss: 2455.5374 - mse: 2455.5374 - mae: 29.0469 - val_loss: 1515.9915 - val_mse: 1515.9916 - val_mae: 28.0821\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2507.0140 - mse: 2507.0142 - mae: 29.5746 - val_loss: 1520.3424 - val_mse: 1520.3422 - val_mae: 27.7557\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2512.3690 - mse: 2512.3691 - mae: 29.3954 - val_loss: 1527.8575 - val_mse: 1527.8573 - val_mae: 27.4623\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2505.6560 - mse: 2505.6558 - mae: 28.9813 - val_loss: 1518.6750 - val_mse: 1518.6753 - val_mae: 27.8440\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 519us/step - loss: 2406.2768 - mse: 2406.2769 - mae: 30.0029 - val_loss: 3692.5765 - val_mse: 3692.5769 - val_mae: 23.9750\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2360.2666 - mse: 2360.2656 - mae: 29.3131 - val_loss: 3693.2176 - val_mse: 3693.2173 - val_mae: 24.3296\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2400.8205 - mse: 2400.8206 - mae: 29.6996 - val_loss: 3691.5255 - val_mse: 3691.5251 - val_mae: 24.0560\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2404.4958 - mse: 2404.4961 - mae: 29.3489 - val_loss: 3697.6853 - val_mse: 3697.6853 - val_mae: 25.0653\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2371.1990 - mse: 2371.1992 - mae: 29.4768 - val_loss: 3693.2646 - val_mse: 3693.2646 - val_mae: 24.4424\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2360.9066 - mse: 2360.9072 - mae: 29.6124 - val_loss: 3691.5231 - val_mse: 3691.5237 - val_mae: 24.1229\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2332.0513 - mse: 2332.0515 - mae: 29.5770 - val_loss: 3693.4987 - val_mse: 3693.4988 - val_mae: 24.6022\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2440.4837 - mse: 2440.4841 - mae: 29.9881 - val_loss: 3691.2614 - val_mse: 3691.2617 - val_mae: 24.2503\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2405.6448 - mse: 2405.6450 - mae: 29.5752 - val_loss: 3695.9701 - val_mse: 3695.9705 - val_mae: 25.0658\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2411.4376 - mse: 2411.4385 - mae: 29.7695 - val_loss: 3691.1159 - val_mse: 3691.1152 - val_mae: 24.3946\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2386.9253 - mse: 2386.9258 - mae: 29.4192 - val_loss: 3690.8850 - val_mse: 3690.8853 - val_mae: 24.3589\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2453.9907 - mse: 2453.9907 - mae: 30.0670 - val_loss: 3688.9661 - val_mse: 3688.9651 - val_mae: 23.9899\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2401.8401 - mse: 2401.8401 - mae: 29.5534 - val_loss: 3690.3285 - val_mse: 3690.3284 - val_mae: 24.4340\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2371.4849 - mse: 2371.4849 - mae: 29.4234 - val_loss: 3689.9211 - val_mse: 3689.9209 - val_mae: 24.4382\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2328.1298 - mse: 2328.1304 - mae: 29.3392 - val_loss: 3689.4109 - val_mse: 3689.4106 - val_mae: 24.2498\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2379.1016 - mse: 2379.1016 - mae: 29.5038 - val_loss: 3689.5805 - val_mse: 3689.5803 - val_mae: 24.2538\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2369.0945 - mse: 2369.0945 - mae: 29.9634 - val_loss: 3688.4383 - val_mse: 3688.4380 - val_mae: 24.0695\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2375.9514 - mse: 2375.9509 - mae: 29.3402 - val_loss: 3689.6145 - val_mse: 3689.6147 - val_mae: 24.3370\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2368.5448 - mse: 2368.5439 - mae: 29.2502 - val_loss: 3692.7006 - val_mse: 3692.7004 - val_mae: 24.8735\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2377.8119 - mse: 2377.8115 - mae: 29.3026 - val_loss: 3688.3693 - val_mse: 3688.3691 - val_mae: 24.2977\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2331.6113 - mse: 2331.6108 - mae: 29.2505 - val_loss: 3688.1602 - val_mse: 3688.1599 - val_mae: 24.3605\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2376.3867 - mse: 2376.3875 - mae: 29.7704 - val_loss: 3686.8982 - val_mse: 3686.8987 - val_mae: 24.1935\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2387.7539 - mse: 2387.7534 - mae: 29.7700 - val_loss: 3687.4849 - val_mse: 3687.4846 - val_mae: 24.2603\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2332.5214 - mse: 2332.5210 - mae: 29.5331 - val_loss: 3688.4219 - val_mse: 3688.4221 - val_mae: 24.4684\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2400.0200 - mse: 2400.0200 - mae: 29.5933 - val_loss: 3689.6373 - val_mse: 3689.6379 - val_mae: 24.6750\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2331.7591 - mse: 2331.7583 - mae: 29.4120 - val_loss: 3688.5820 - val_mse: 3688.5820 - val_mae: 24.4952\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 513us/step - loss: 2417.7604 - mse: 2417.7598 - mae: 29.7404 - val_loss: 3685.6687 - val_mse: 3685.6687 - val_mae: 24.0811\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 656us/step - loss: 2345.9099 - mse: 2345.9109 - mae: 29.3163 - val_loss: 3685.6252 - val_mse: 3685.6248 - val_mae: 24.0748\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - ETA: 0s - loss: 2358.4129 - mse: 2358.4119 - mae: 29.20 - 2s 623us/step - loss: 2384.0320 - mse: 2384.0310 - mae: 29.4454 - val_loss: 3689.9289 - val_mse: 3689.9285 - val_mae: 24.7837\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2397.5025 - mse: 2397.5020 - mae: 29.7237 - val_loss: 3685.0069 - val_mse: 3685.0073 - val_mae: 23.9380\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2406.1572 - mse: 2406.1575 - mae: 29.7337 - val_loss: 3685.6661 - val_mse: 3685.6658 - val_mae: 24.1746\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2413.0363 - mse: 2413.0364 - mae: 29.7620 - val_loss: 3685.3153 - val_mse: 3685.3152 - val_mae: 24.1941\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2342.9079 - mse: 2342.9072 - mae: 29.1539 - val_loss: 3689.4553 - val_mse: 3689.4556 - val_mae: 24.8348\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2365.2325 - mse: 2365.2322 - mae: 29.2482 - val_loss: 3683.9334 - val_mse: 3683.9341 - val_mae: 24.0563\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2337.8812 - mse: 2337.8811 - mae: 29.4628 - val_loss: 3690.0800 - val_mse: 3690.0803 - val_mae: 24.9529\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2370.8679 - mse: 2370.8679 - mae: 29.7446 - val_loss: 3685.2536 - val_mse: 3685.2534 - val_mae: 24.2847\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2447.7000 - mse: 2447.7002 - mae: 29.9673 - val_loss: 3684.8747 - val_mse: 3684.8745 - val_mae: 24.1812\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2364.4718 - mse: 2364.4712 - mae: 29.7616 - val_loss: 3684.9612 - val_mse: 3684.9612 - val_mae: 24.2201\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2410.3351 - mse: 2410.3337 - mae: 29.4667 - val_loss: 3684.1176 - val_mse: 3684.1179 - val_mae: 23.9800\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2410.2719 - mse: 2410.2722 - mae: 29.4389 - val_loss: 3684.4385 - val_mse: 3684.4395 - val_mae: 24.2189\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2315.8530 - mse: 2315.8525 - mae: 29.1063 - val_loss: 3682.1931 - val_mse: 3682.1938 - val_mae: 23.7305\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2338.8835 - mse: 2338.8840 - mae: 29.0177 - val_loss: 3684.6980 - val_mse: 3684.6975 - val_mae: 24.3899\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2314.8629 - mse: 2314.8630 - mae: 29.2239 - val_loss: 3686.1333 - val_mse: 3686.1335 - val_mae: 24.6813\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2388.7040 - mse: 2388.7048 - mae: 29.6282 - val_loss: 3682.5948 - val_mse: 3682.5955 - val_mae: 24.2530\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2370.9915 - mse: 2370.9910 - mae: 29.5351 - val_loss: 3683.7630 - val_mse: 3683.7634 - val_mae: 24.4267\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 529us/step - loss: 2348.7844 - mse: 2348.7837 - mae: 29.2825 - val_loss: 3684.4890 - val_mse: 3684.4893 - val_mae: 24.6254\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2304.3279 - mse: 2304.3284 - mae: 29.1868 - val_loss: 3680.5000 - val_mse: 3680.4993 - val_mae: 24.0475\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2351.5860 - mse: 2351.5867 - mae: 29.0551 - val_loss: 3679.4855 - val_mse: 3679.4858 - val_mae: 23.8787\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 533us/step - loss: 2374.8704 - mse: 2374.8704 - mae: 29.3866 - val_loss: 3682.5205 - val_mse: 3682.5205 - val_mae: 24.5061\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2326.8518 - mse: 2326.8521 - mae: 29.1158 - val_loss: 3679.3596 - val_mse: 3679.3591 - val_mae: 23.9538\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2416.1567 - mse: 2416.1567 - mae: 29.5849 - val_loss: 3679.7163 - val_mse: 3679.7163 - val_mae: 23.9535\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2356.9323 - mse: 2356.9326 - mae: 29.4745 - val_loss: 3678.7295 - val_mse: 3678.7295 - val_mae: 23.7603\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2332.6721 - mse: 2332.6726 - mae: 29.0273 - val_loss: 3682.3509 - val_mse: 3682.3508 - val_mae: 24.4463\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2353.7825 - mse: 2353.7832 - mae: 29.6533 - val_loss: 3679.9451 - val_mse: 3679.9443 - val_mae: 24.0603\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2335.8608 - mse: 2335.8613 - mae: 29.0794 - val_loss: 3682.5321 - val_mse: 3682.5327 - val_mae: 24.4293\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2342.7138 - mse: 2342.7134 - mae: 29.3401 - val_loss: 3684.3516 - val_mse: 3684.3518 - val_mae: 24.6259\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2373.5404 - mse: 2373.5400 - mae: 29.3343 - val_loss: 3679.0615 - val_mse: 3679.0615 - val_mae: 23.9026\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2331.3868 - mse: 2331.3865 - mae: 29.3195 - val_loss: 3678.9998 - val_mse: 3679.0000 - val_mae: 24.0603\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2338.6989 - mse: 2338.6995 - mae: 29.3674 - val_loss: 3682.2215 - val_mse: 3682.2212 - val_mae: 24.5096\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2349.0802 - mse: 2349.0801 - mae: 29.3801 - val_loss: 3680.8827 - val_mse: 3680.8826 - val_mae: 24.2706\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2419.1090 - mse: 2419.1086 - mae: 29.6515 - val_loss: 3680.9781 - val_mse: 3680.9775 - val_mae: 24.1104\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2341.6099 - mse: 2341.6099 - mae: 29.3582 - val_loss: 3678.7199 - val_mse: 3678.7202 - val_mae: 23.6426\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2342.3103 - mse: 2342.3105 - mae: 29.1630 - val_loss: 3681.5533 - val_mse: 3681.5535 - val_mae: 24.4062\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2369.0681 - mse: 2369.0684 - mae: 29.4174 - val_loss: 3682.6408 - val_mse: 3682.6401 - val_mae: 24.6479\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2322.8286 - mse: 2322.8281 - mae: 28.9875 - val_loss: 3681.0895 - val_mse: 3681.0901 - val_mae: 24.3650\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2328.8242 - mse: 2328.8242 - mae: 29.2779 - val_loss: 3679.3755 - val_mse: 3679.3752 - val_mae: 24.0416\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2313.7779 - mse: 2313.7783 - mae: 28.8767 - val_loss: 3680.7628 - val_mse: 3680.7627 - val_mae: 24.3794\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2292.5479 - mse: 2292.5471 - mae: 28.7972 - val_loss: 3680.4511 - val_mse: 3680.4507 - val_mae: 24.3402\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 695us/step - loss: 2374.6726 - mse: 2374.6724 - mae: 28.9219 - val_loss: 3680.4781 - val_mse: 3680.4783 - val_mae: 24.4118\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2334.4679 - mse: 2334.4675 - mae: 29.2042 - val_loss: 3681.2133 - val_mse: 3681.2129 - val_mae: 24.5416\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2366.2045 - mse: 2366.2051 - mae: 29.3272 - val_loss: 3679.5095 - val_mse: 3679.5100 - val_mae: 24.3179\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2361.8171 - mse: 2361.8174 - mae: 29.3908 - val_loss: 3679.0432 - val_mse: 3679.0435 - val_mae: 24.3904\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 540us/step - loss: 2367.6404 - mse: 2367.6396 - mae: 29.1267 - val_loss: 3679.9292 - val_mse: 3679.9299 - val_mae: 24.4264\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2359.2214 - mse: 2359.2214 - mae: 29.3639 - val_loss: 3679.9548 - val_mse: 3679.9551 - val_mae: 24.4415\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2328.3121 - mse: 2328.3120 - mae: 29.2162 - val_loss: 3677.1268 - val_mse: 3677.1270 - val_mae: 24.0083\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2340.8100 - mse: 2340.8093 - mae: 29.0450 - val_loss: 3683.2359 - val_mse: 3683.2358 - val_mae: 24.9106\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 672us/step - loss: 2359.7604 - mse: 2359.7603 - mae: 29.2884 - val_loss: 3676.9407 - val_mse: 3676.9402 - val_mae: 24.0284\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2368.1106 - mse: 2368.1108 - mae: 29.8710 - val_loss: 3675.1827 - val_mse: 3675.1833 - val_mae: 23.7168\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 509us/step - loss: 2337.7026 - mse: 2337.7031 - mae: 29.1965 - val_loss: 3678.9329 - val_mse: 3678.9331 - val_mae: 24.4081\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2377.0037 - mse: 2377.0032 - mae: 29.1232 - val_loss: 3682.7336 - val_mse: 3682.7329 - val_mae: 24.9207\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 691us/step - loss: 2728.9321 - mse: 2728.9316 - mae: 28.8185 - val_loss: 2352.4655 - val_mse: 2352.4648 - val_mae: 26.4995\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2698.4402 - mse: 2698.4397 - mae: 28.6027 - val_loss: 2353.0562 - val_mse: 2353.0562 - val_mae: 27.2488\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2718.7344 - mse: 2718.7339 - mae: 28.8580 - val_loss: 2359.4648 - val_mse: 2359.4648 - val_mae: 26.9472\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2704.3313 - mse: 2704.3313 - mae: 28.5298 - val_loss: 2360.7421 - val_mse: 2360.7422 - val_mae: 26.9879\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2734.7464 - mse: 2734.7476 - mae: 28.4884 - val_loss: 2359.5077 - val_mse: 2359.5078 - val_mae: 27.5208\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2692.5327 - mse: 2692.5330 - mae: 28.6033 - val_loss: 2351.8895 - val_mse: 2351.8892 - val_mae: 27.2820\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2767.5033 - mse: 2767.5039 - mae: 28.8865 - val_loss: 2357.6169 - val_mse: 2357.6167 - val_mae: 27.3199\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2762.5219 - mse: 2762.5220 - mae: 28.8188 - val_loss: 2362.9288 - val_mse: 2362.9287 - val_mae: 26.5963\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2713.0159 - mse: 2713.0166 - mae: 28.5558 - val_loss: 2361.7273 - val_mse: 2361.7273 - val_mae: 27.2129\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 535us/step - loss: 2711.1121 - mse: 2711.1125 - mae: 28.5319 - val_loss: 2358.4208 - val_mse: 2358.4211 - val_mae: 27.4704\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2746.6687 - mse: 2746.6687 - mae: 28.9046 - val_loss: 2361.5322 - val_mse: 2361.5322 - val_mae: 27.1663\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2701.3499 - mse: 2701.3496 - mae: 28.7542 - val_loss: 2368.0648 - val_mse: 2368.0647 - val_mae: 27.0379\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 521us/step - loss: 2719.9390 - mse: 2719.9382 - mae: 28.6864 - val_loss: 2365.0350 - val_mse: 2365.0354 - val_mae: 27.2013\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2715.9521 - mse: 2715.9514 - mae: 28.6646 - val_loss: 2356.6903 - val_mse: 2356.6904 - val_mae: 27.2783\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2774.4144 - mse: 2774.4138 - mae: 28.8169 - val_loss: 2366.0650 - val_mse: 2366.0649 - val_mae: 26.8484\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2712.7445 - mse: 2712.7446 - mae: 28.2845 - val_loss: 2366.7913 - val_mse: 2366.7915 - val_mae: 27.5026\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2726.3308 - mse: 2726.3303 - mae: 28.7900 - val_loss: 2378.8928 - val_mse: 2378.8926 - val_mae: 26.6674\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 663us/step - loss: 2703.7392 - mse: 2703.7385 - mae: 28.3760 - val_loss: 2377.1641 - val_mse: 2377.1641 - val_mae: 27.1340\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2705.7957 - mse: 2705.7959 - mae: 28.6508 - val_loss: 2374.4221 - val_mse: 2374.4221 - val_mae: 27.2290\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2789.8073 - mse: 2789.8069 - mae: 29.2057 - val_loss: 2374.0833 - val_mse: 2374.0835 - val_mae: 27.4629\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 517us/step - loss: 2745.7351 - mse: 2745.7351 - mae: 28.8555 - val_loss: 2376.4071 - val_mse: 2376.4072 - val_mae: 26.8082\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 551us/step - loss: 2730.5651 - mse: 2730.5659 - mae: 28.4615 - val_loss: 2370.6104 - val_mse: 2370.6104 - val_mae: 27.3094\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2717.0475 - mse: 2717.0476 - mae: 28.6498 - val_loss: 2373.7237 - val_mse: 2373.7239 - val_mae: 27.0589\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2699.0823 - mse: 2699.0820 - mae: 28.6156 - val_loss: 2362.7383 - val_mse: 2362.7380 - val_mae: 26.8126\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2778.1009 - mse: 2778.1016 - mae: 28.9885 - val_loss: 2385.1853 - val_mse: 2385.1853 - val_mae: 26.9161\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2685.4903 - mse: 2685.4910 - mae: 28.5885 - val_loss: 2372.3992 - val_mse: 2372.3992 - val_mae: 27.3976\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2732.0767 - mse: 2732.0764 - mae: 29.0468 - val_loss: 2373.5416 - val_mse: 2373.5417 - val_mae: 27.4169\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2687.9031 - mse: 2687.9028 - mae: 28.4640 - val_loss: 2374.6902 - val_mse: 2374.6904 - val_mae: 27.3621\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2738.9436 - mse: 2738.9448 - mae: 28.8136 - val_loss: 2383.0477 - val_mse: 2383.0476 - val_mae: 27.1058\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2709.9584 - mse: 2709.9580 - mae: 28.8841 - val_loss: 2371.6077 - val_mse: 2371.6077 - val_mae: 26.7516\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2678.9360 - mse: 2678.9368 - mae: 28.7320 - val_loss: 2362.8031 - val_mse: 2362.8030 - val_mae: 27.3517\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2728.6034 - mse: 2728.6033 - mae: 28.5932 - val_loss: 2362.3846 - val_mse: 2362.3843 - val_mae: 27.2233\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2699.9331 - mse: 2699.9331 - mae: 28.4848 - val_loss: 2371.0502 - val_mse: 2371.0505 - val_mae: 27.2159\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2746.4688 - mse: 2746.4678 - mae: 28.9216 - val_loss: 2380.3627 - val_mse: 2380.3623 - val_mae: 26.6200\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2751.8461 - mse: 2751.8467 - mae: 28.6407 - val_loss: 2375.7533 - val_mse: 2375.7537 - val_mae: 26.8087\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2754.1276 - mse: 2754.1274 - mae: 28.6558 - val_loss: 2380.5983 - val_mse: 2380.5984 - val_mae: 27.2392\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2722.9248 - mse: 2722.9250 - mae: 28.7000 - val_loss: 2376.8321 - val_mse: 2376.8325 - val_mae: 26.7921\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2695.4391 - mse: 2695.4387 - mae: 28.8472 - val_loss: 2385.0807 - val_mse: 2385.0806 - val_mae: 26.9608\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2730.4670 - mse: 2730.4678 - mae: 28.4473 - val_loss: 2382.5288 - val_mse: 2382.5291 - val_mae: 27.0539\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2733.2442 - mse: 2733.2449 - mae: 28.8185 - val_loss: 2382.7252 - val_mse: 2382.7253 - val_mae: 27.0410\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2698.6600 - mse: 2698.6599 - mae: 28.7007 - val_loss: 2385.5825 - val_mse: 2385.5825 - val_mae: 26.7834\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2692.8884 - mse: 2692.8884 - mae: 28.1825 - val_loss: 2373.4654 - val_mse: 2373.4653 - val_mae: 27.1987\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2692.5905 - mse: 2692.5908 - mae: 28.7437 - val_loss: 2364.6735 - val_mse: 2364.6733 - val_mae: 26.9426\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2679.0453 - mse: 2679.0452 - mae: 28.5321 - val_loss: 2367.3887 - val_mse: 2367.3887 - val_mae: 27.0947\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2717.4172 - mse: 2717.4187 - mae: 28.6022 - val_loss: 2369.2354 - val_mse: 2369.2351 - val_mae: 26.6619\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2732.2275 - mse: 2732.2268 - mae: 28.8413 - val_loss: 2364.2923 - val_mse: 2364.2925 - val_mae: 26.5416\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2724.3117 - mse: 2724.3123 - mae: 28.5441 - val_loss: 2373.4804 - val_mse: 2373.4805 - val_mae: 27.0335\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2689.0640 - mse: 2689.0645 - mae: 28.3765 - val_loss: 2369.7156 - val_mse: 2369.7156 - val_mae: 27.3215\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2645.8375 - mse: 2645.8369 - mae: 27.6564 - val_loss: 2371.5536 - val_mse: 2371.5537 - val_mae: 27.1603\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2674.5825 - mse: 2674.5828 - mae: 28.4371 - val_loss: 2369.9603 - val_mse: 2369.9604 - val_mae: 27.1830\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2693.5289 - mse: 2693.5298 - mae: 28.2532 - val_loss: 2380.0170 - val_mse: 2380.0168 - val_mae: 26.9865\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 517us/step - loss: 2706.7280 - mse: 2706.7283 - mae: 28.4989 - val_loss: 2386.6616 - val_mse: 2386.6616 - val_mae: 26.7474\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2717.6894 - mse: 2717.6887 - mae: 28.3999 - val_loss: 2382.5621 - val_mse: 2382.5620 - val_mae: 27.0611\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2677.4507 - mse: 2677.4504 - mae: 28.2243 - val_loss: 2375.6298 - val_mse: 2375.6296 - val_mae: 27.1787\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 538us/step - loss: 2692.0495 - mse: 2692.0496 - mae: 28.6008 - val_loss: 2362.8265 - val_mse: 2362.8264 - val_mae: 27.0891\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2682.5742 - mse: 2682.5745 - mae: 28.5355 - val_loss: 2367.4391 - val_mse: 2367.4392 - val_mae: 26.8958\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 540us/step - loss: 2712.8486 - mse: 2712.8489 - mae: 28.3625 - val_loss: 2362.9968 - val_mse: 2362.9966 - val_mae: 27.4822\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2658.8605 - mse: 2658.8604 - mae: 28.4160 - val_loss: 2359.0058 - val_mse: 2359.0061 - val_mae: 26.8841\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2711.9320 - mse: 2711.9324 - mae: 28.5072 - val_loss: 2366.5057 - val_mse: 2366.5056 - val_mae: 26.6193\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 533us/step - loss: 2722.7888 - mse: 2722.7883 - mae: 28.4517 - val_loss: 2371.2649 - val_mse: 2371.2651 - val_mae: 26.6299\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2684.4477 - mse: 2684.4478 - mae: 28.4858 - val_loss: 2380.4552 - val_mse: 2380.4553 - val_mae: 27.0631\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2700.8030 - mse: 2700.8022 - mae: 28.5513 - val_loss: 2381.0070 - val_mse: 2381.0073 - val_mae: 26.6651\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2700.2406 - mse: 2700.2395 - mae: 28.5665 - val_loss: 2373.7476 - val_mse: 2373.7478 - val_mae: 26.8675\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 540us/step - loss: 2703.8878 - mse: 2703.8882 - mae: 28.4646 - val_loss: 2372.9824 - val_mse: 2372.9824 - val_mae: 26.9531\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2695.6089 - mse: 2695.6099 - mae: 28.5644 - val_loss: 2371.4909 - val_mse: 2371.4910 - val_mae: 26.4009\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2684.0537 - mse: 2684.0540 - mae: 28.4782 - val_loss: 2377.4700 - val_mse: 2377.4702 - val_mae: 26.9116\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2700.0655 - mse: 2700.0654 - mae: 28.5496 - val_loss: 2384.1367 - val_mse: 2384.1367 - val_mae: 26.9465\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2713.3916 - mse: 2713.3911 - mae: 28.6573 - val_loss: 2378.7657 - val_mse: 2378.7659 - val_mae: 27.3154\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2682.0753 - mse: 2682.0750 - mae: 28.7533 - val_loss: 2376.8054 - val_mse: 2376.8054 - val_mae: 26.9532\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2713.3142 - mse: 2713.3142 - mae: 28.1224 - val_loss: 2367.8731 - val_mse: 2367.8730 - val_mae: 26.9859\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2703.4856 - mse: 2703.4856 - mae: 28.3581 - val_loss: 2367.5139 - val_mse: 2367.5137 - val_mae: 27.0202\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2679.9921 - mse: 2679.9912 - mae: 28.4165 - val_loss: 2367.2127 - val_mse: 2367.2129 - val_mae: 27.2013\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 659us/step - loss: 2681.5105 - mse: 2681.5107 - mae: 28.3163 - val_loss: 2362.6275 - val_mse: 2362.6274 - val_mae: 26.9126\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 653us/step - loss: 2721.3689 - mse: 2721.3696 - mae: 28.8172 - val_loss: 2366.9074 - val_mse: 2366.9075 - val_mae: 27.2206\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2699.9734 - mse: 2699.9734 - mae: 28.5013 - val_loss: 2371.7934 - val_mse: 2371.7935 - val_mae: 26.7919\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 526us/step - loss: 2699.8991 - mse: 2699.8999 - mae: 28.3457 - val_loss: 2369.7129 - val_mse: 2369.7129 - val_mae: 27.0903\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 493us/step - loss: 2706.5196 - mse: 2706.5198 - mae: 28.3223 - val_loss: 2371.8427 - val_mse: 2371.8423 - val_mae: 27.2227\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 497us/step - loss: 2680.3985 - mse: 2680.3992 - mae: 28.4095 - val_loss: 2373.0353 - val_mse: 2373.0354 - val_mae: 27.0257\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2712.8067 - mse: 2712.8071 - mae: 28.6267 - val_loss: 2372.8674 - val_mse: 2372.8677 - val_mae: 26.9336\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2706.0745 - mse: 2706.0747 - mae: 28.6327 - val_loss: 2369.1105 - val_mse: 2369.1106 - val_mae: 26.8308\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 13324.9638 - mse: 13324.9629 - mae: 109.8830 - val_loss: 34594.1171 - val_mse: 34594.1172 - val_mae: 132.6397\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 13155.7520 - mse: 13155.7529 - mae: 109.1067 - val_loss: 34232.7487 - val_mse: 34232.7461 - val_mae: 131.2814\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 637us/step - loss: 12667.3080 - mse: 12667.3066 - mae: 106.8597 - val_loss: 33200.4617 - val_mse: 33200.4648 - val_mae: 127.3229\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 11342.6424 - mse: 11342.6416 - mae: 100.2894 - val_loss: 30375.3671 - val_mse: 30375.3672 - val_mae: 115.7970\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 517us/step - loss: 8353.9940 - mse: 8353.9941 - mae: 83.5011 - val_loss: 24114.5852 - val_mse: 24114.5840 - val_mae: 84.7652\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 515us/step - loss: 3943.7890 - mse: 3943.7893 - mae: 48.7561 - val_loss: 18042.9670 - val_mse: 18042.9688 - val_mae: 37.2575\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 3000.5368 - mse: 3000.5371 - mae: 39.9488 - val_loss: 17696.9233 - val_mse: 17696.9219 - val_mae: 35.8578\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 2903.7706 - mse: 2903.7710 - mae: 38.3312 - val_loss: 17911.7369 - val_mse: 17911.7383 - val_mae: 36.5386\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2709.1225 - mse: 2709.1226 - mae: 37.0316 - val_loss: 17803.0568 - val_mse: 17803.0566 - val_mae: 36.1356\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 492us/step - loss: 2882.4819 - mse: 2882.4819 - mae: 38.3801 - val_loss: 17860.0842 - val_mse: 17860.0840 - val_mae: 36.2813\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 3116.0893 - mse: 3116.0891 - mae: 40.4561 - val_loss: 18079.4015 - val_mse: 18079.4023 - val_mae: 37.0802\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 2950.2821 - mse: 2950.2820 - mae: 38.7800 - val_loss: 17805.2303 - val_mse: 17805.2305 - val_mae: 36.0238\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2879.2479 - mse: 2879.2476 - mae: 38.1920 - val_loss: 17835.8694 - val_mse: 17835.8691 - val_mae: 36.0842\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 2690.5722 - mse: 2690.5720 - mae: 36.2368 - val_loss: 17674.4640 - val_mse: 17674.4648 - val_mae: 35.6093\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2469.3115 - mse: 2469.3110 - mae: 35.6367 - val_loss: 17657.1164 - val_mse: 17657.1152 - val_mae: 35.5528\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2617.5978 - mse: 2617.5977 - mae: 37.3677 - val_loss: 17723.0856 - val_mse: 17723.0859 - val_mae: 35.7003\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2638.5788 - mse: 2638.5789 - mae: 36.7655 - val_loss: 17785.1131 - val_mse: 17785.1133 - val_mae: 35.8493\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2606.6824 - mse: 2606.6826 - mae: 35.7706 - val_loss: 17744.7118 - val_mse: 17744.7129 - val_mae: 35.7260\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 445us/step - loss: 2685.7225 - mse: 2685.7224 - mae: 36.8860 - val_loss: 17704.7480 - val_mse: 17704.7480 - val_mae: 35.6131\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2643.8425 - mse: 2643.8425 - mae: 37.0439 - val_loss: 17834.9874 - val_mse: 17834.9883 - val_mae: 35.9555\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2381.1991 - mse: 2381.1992 - mae: 34.0964 - val_loss: 17727.7865 - val_mse: 17727.7852 - val_mae: 35.6648\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 507us/step - loss: 2625.5030 - mse: 2625.5027 - mae: 36.4913 - val_loss: 17903.0544 - val_mse: 17903.0547 - val_mae: 36.1251\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 2596.9752 - mse: 2596.9753 - mae: 35.1915 - val_loss: 17864.7595 - val_mse: 17864.7598 - val_mae: 36.0138\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 782us/step - loss: 2596.0186 - mse: 2596.0188 - mae: 35.1380 - val_loss: 17790.5032 - val_mse: 17790.5039 - val_mae: 35.8627\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 2218.6428 - mse: 2218.6428 - mae: 33.6218 - val_loss: 17596.3506 - val_mse: 17596.3496 - val_mae: 35.5755\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 684us/step - loss: 2539.3271 - mse: 2539.3269 - mae: 35.6138 - val_loss: 17889.6582 - val_mse: 17889.6582 - val_mae: 36.1249\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 763us/step - loss: 2172.9647 - mse: 2172.9648 - mae: 33.1403 - val_loss: 17740.8029 - val_mse: 17740.8027 - val_mae: 35.8520\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2334.1521 - mse: 2334.1521 - mae: 33.8945 - val_loss: 17589.0109 - val_mse: 17589.0117 - val_mae: 35.6644\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2213.5095 - mse: 2213.5095 - mae: 33.5573 - val_loss: 17624.7971 - val_mse: 17624.7988 - val_mae: 35.7513\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2364.6465 - mse: 2364.6465 - mae: 34.9084 - val_loss: 17644.6916 - val_mse: 17644.6934 - val_mae: 35.8239\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2222.8869 - mse: 2222.8870 - mae: 32.6906 - val_loss: 17612.7851 - val_mse: 17612.7852 - val_mae: 35.8360\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 2328.7426 - mse: 2328.7427 - mae: 33.8822 - val_loss: 17618.4073 - val_mse: 17618.4062 - val_mae: 35.8792\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 2321.3994 - mse: 2321.3994 - mae: 33.6106 - val_loss: 17660.4014 - val_mse: 17660.4004 - val_mae: 35.9533\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2313.5086 - mse: 2313.5085 - mae: 33.4424 - val_loss: 17718.7834 - val_mse: 17718.7832 - val_mae: 36.0863\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 699us/step - loss: 2079.4203 - mse: 2079.4202 - mae: 31.8364 - val_loss: 17464.3225 - val_mse: 17464.3223 - val_mae: 35.9909\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 645us/step - loss: 2376.3757 - mse: 2376.3755 - mae: 34.6692 - val_loss: 17611.5404 - val_mse: 17611.5391 - val_mae: 36.0817\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2240.0824 - mse: 2240.0825 - mae: 33.3899 - val_loss: 17950.6519 - val_mse: 17950.6523 - val_mae: 36.8873\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2365.7530 - mse: 2365.7527 - mae: 34.1744 - val_loss: 17679.1545 - val_mse: 17679.1562 - val_mae: 36.2427\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 609us/step - loss: 2191.7646 - mse: 2191.7646 - mae: 32.6344 - val_loss: 17649.1101 - val_mse: 17649.1094 - val_mae: 36.2976\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 2469.9083 - mse: 2469.9084 - mae: 33.8281 - val_loss: 17811.3261 - val_mse: 17811.3281 - val_mae: 36.6496\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 668us/step - loss: 2258.9373 - mse: 2258.9370 - mae: 32.9527 - val_loss: 17685.0790 - val_mse: 17685.0801 - val_mae: 36.4524\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 620us/step - loss: 2246.0676 - mse: 2246.0681 - mae: 33.2154 - val_loss: 17792.4635 - val_mse: 17792.4629 - val_mae: 36.7488\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2147.3154 - mse: 2147.3152 - mae: 33.7751 - val_loss: 17692.5561 - val_mse: 17692.5566 - val_mae: 36.6205\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 2145.2438 - mse: 2145.2437 - mae: 31.8474 - val_loss: 17765.8792 - val_mse: 17765.8789 - val_mae: 36.8531\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 680us/step - loss: 2338.5907 - mse: 2338.5908 - mae: 33.9699 - val_loss: 17684.9062 - val_mse: 17684.9082 - val_mae: 36.7547\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2206.7285 - mse: 2206.7285 - mae: 33.0188 - val_loss: 17706.9925 - val_mse: 17706.9922 - val_mae: 36.8291\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 628us/step - loss: 2157.7928 - mse: 2157.7930 - mae: 33.1380 - val_loss: 17811.9811 - val_mse: 17811.9824 - val_mae: 37.0696\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 1979.9637 - mse: 1979.9634 - mae: 30.8942 - val_loss: 17426.8844 - val_mse: 17426.8848 - val_mae: 36.8799\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 2167.4360 - mse: 2167.4358 - mae: 32.7341 - val_loss: 17746.4823 - val_mse: 17746.4824 - val_mae: 37.0133\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 2003.7196 - mse: 2003.7195 - mae: 30.6057 - val_loss: 17488.2796 - val_mse: 17488.2773 - val_mae: 36.8966\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 696us/step - loss: 2311.4803 - mse: 2311.4805 - mae: 33.0120 - val_loss: 17912.8837 - val_mse: 17912.8848 - val_mae: 37.6648\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2014.2080 - mse: 2014.2079 - mae: 30.5628 - val_loss: 17431.7973 - val_mse: 17431.7969 - val_mae: 37.0642\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 2250.9137 - mse: 2250.9138 - mae: 33.1822 - val_loss: 17665.1008 - val_mse: 17665.0996 - val_mae: 37.0808\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 694us/step - loss: 2157.5713 - mse: 2157.5715 - mae: 31.1061 - val_loss: 17793.8531 - val_mse: 17793.8516 - val_mae: 37.3174\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 1952.5027 - mse: 1952.5028 - mae: 31.3824 - val_loss: 17808.8061 - val_mse: 17808.8066 - val_mae: 37.4504\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 2045.0278 - mse: 2045.0277 - mae: 31.6861 - val_loss: 17644.0828 - val_mse: 17644.0820 - val_mae: 37.2850\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 2278.8911 - mse: 2278.8911 - mae: 32.6231 - val_loss: 17813.8390 - val_mse: 17813.8379 - val_mae: 37.5432\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2174.1064 - mse: 2174.1064 - mae: 31.5921 - val_loss: 17646.1781 - val_mse: 17646.1797 - val_mae: 37.2832\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 529us/step - loss: 2020.2043 - mse: 2020.2043 - mae: 32.1819 - val_loss: 17806.4156 - val_mse: 17806.4141 - val_mae: 37.5690\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2033.8318 - mse: 2033.8319 - mae: 31.3962 - val_loss: 17718.1295 - val_mse: 17718.1309 - val_mae: 37.4457\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 1995.2701 - mse: 1995.2701 - mae: 30.7760 - val_loss: 17601.0645 - val_mse: 17601.0664 - val_mae: 37.3952\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 678us/step - loss: 2182.1497 - mse: 2182.1497 - mae: 32.9046 - val_loss: 17839.7725 - val_mse: 17839.7715 - val_mae: 37.9229\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 2034.8921 - mse: 2034.8920 - mae: 30.4816 - val_loss: 17755.9411 - val_mse: 17755.9414 - val_mae: 37.7522\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2080.9729 - mse: 2080.9727 - mae: 31.7469 - val_loss: 17658.5151 - val_mse: 17658.5156 - val_mae: 37.7107\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 1913.5744 - mse: 1913.5743 - mae: 30.3901 - val_loss: 17807.7228 - val_mse: 17807.7246 - val_mae: 38.0994\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 1934.9470 - mse: 1934.9471 - mae: 30.6131 - val_loss: 17730.0700 - val_mse: 17730.0703 - val_mae: 37.9384\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 2013.8071 - mse: 2013.8071 - mae: 31.1225 - val_loss: 17745.9277 - val_mse: 17745.9277 - val_mae: 38.0487\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 642us/step - loss: 1949.5222 - mse: 1949.5221 - mae: 30.9007 - val_loss: 17600.1185 - val_mse: 17600.1172 - val_mae: 37.9045\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 2161.7569 - mse: 2161.7568 - mae: 32.6632 - val_loss: 17952.2217 - val_mse: 17952.2227 - val_mae: 38.8710\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 627us/step - loss: 1876.0396 - mse: 1876.0396 - mae: 29.8788 - val_loss: 17677.7749 - val_mse: 17677.7734 - val_mae: 37.9343\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2017.5590 - mse: 2017.5590 - mae: 31.1487 - val_loss: 17808.3300 - val_mse: 17808.3281 - val_mae: 38.2820\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 1849.9728 - mse: 1849.9728 - mae: 29.8428 - val_loss: 17671.4466 - val_mse: 17671.4473 - val_mae: 37.9374\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 515us/step - loss: 1932.8725 - mse: 1932.8727 - mae: 30.2659 - val_loss: 17606.9334 - val_mse: 17606.9336 - val_mae: 37.9012\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 605us/step - loss: 1902.2939 - mse: 1902.2938 - mae: 30.2034 - val_loss: 17821.9448 - val_mse: 17821.9434 - val_mae: 38.3554\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 1855.6585 - mse: 1855.6586 - mae: 28.9040 - val_loss: 17676.4904 - val_mse: 17676.4922 - val_mae: 37.9778\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2040.8794 - mse: 2040.8795 - mae: 30.7839 - val_loss: 17756.8907 - val_mse: 17756.8926 - val_mae: 38.1660\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 1994.5539 - mse: 1994.5538 - mae: 30.2092 - val_loss: 17925.9197 - val_mse: 17925.9199 - val_mae: 38.8478\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 517us/step - loss: 1966.3618 - mse: 1966.3617 - mae: 30.1455 - val_loss: 17875.1901 - val_mse: 17875.1875 - val_mae: 38.6098\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 1791.3158 - mse: 1791.3157 - mae: 29.4830 - val_loss: 17671.5911 - val_mse: 17671.5918 - val_mae: 37.9563\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 1936.0857 - mse: 1936.0857 - mae: 30.9810 - val_loss: 17748.3374 - val_mse: 17748.3379 - val_mae: 38.1376\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4038.7943 - mse: 4038.7944 - mae: 33.7110 - val_loss: 2190.5024 - val_mse: 2190.5022 - val_mae: 31.0156\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4419.5456 - mse: 4419.5454 - mae: 35.7235 - val_loss: 2232.4233 - val_mse: 2232.4236 - val_mae: 31.1547\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4159.6740 - mse: 4159.6738 - mae: 34.8392 - val_loss: 2351.2664 - val_mse: 2351.2666 - val_mae: 31.5650\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4077.2043 - mse: 4077.2039 - mae: 34.6527 - val_loss: 2340.5962 - val_mse: 2340.5964 - val_mae: 31.5170\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4206.9452 - mse: 4206.9453 - mae: 35.0581 - val_loss: 2301.9285 - val_mse: 2301.9285 - val_mae: 31.3905\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 544us/step - loss: 4163.8686 - mse: 4163.8687 - mae: 34.5072 - val_loss: 2376.1755 - val_mse: 2376.1755 - val_mae: 31.6428\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 513us/step - loss: 4343.7435 - mse: 4343.7437 - mae: 35.0389 - val_loss: 2386.7467 - val_mse: 2386.7468 - val_mae: 31.6903\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 470us/step - loss: 4374.2974 - mse: 4374.2979 - mae: 35.7607 - val_loss: 2349.8452 - val_mse: 2349.8450 - val_mae: 31.5721\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4170.0698 - mse: 4170.0698 - mae: 34.0689 - val_loss: 2318.5014 - val_mse: 2318.5012 - val_mae: 31.4754\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4266.6251 - mse: 4266.6250 - mae: 35.7830 - val_loss: 2350.6807 - val_mse: 2350.6807 - val_mae: 31.5779\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 556us/step - loss: 4331.8991 - mse: 4331.8994 - mae: 35.7611 - val_loss: 2344.9776 - val_mse: 2344.9775 - val_mae: 31.5511\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 521us/step - loss: 4337.5962 - mse: 4337.5962 - mae: 35.1950 - val_loss: 2415.4171 - val_mse: 2415.4172 - val_mae: 31.8090\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4253.5830 - mse: 4253.5830 - mae: 35.6618 - val_loss: 2315.3469 - val_mse: 2315.3469 - val_mae: 31.4697\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4261.6625 - mse: 4261.6621 - mae: 34.5568 - val_loss: 2363.2080 - val_mse: 2363.2078 - val_mae: 31.6378\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4217.3778 - mse: 4217.3774 - mae: 33.8009 - val_loss: 2381.2242 - val_mse: 2381.2246 - val_mae: 31.6913\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 548us/step - loss: 4207.0294 - mse: 4207.0293 - mae: 34.2033 - val_loss: 2355.7993 - val_mse: 2355.7991 - val_mae: 31.6013\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4180.8467 - mse: 4180.8467 - mae: 33.8753 - val_loss: 2308.7897 - val_mse: 2308.7898 - val_mae: 31.4565\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4215.9264 - mse: 4215.9268 - mae: 34.8448 - val_loss: 2300.6472 - val_mse: 2300.6472 - val_mae: 31.4149\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4217.8322 - mse: 4217.8325 - mae: 34.6620 - val_loss: 2325.8983 - val_mse: 2325.8982 - val_mae: 31.4864\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 689us/step - loss: 4189.1338 - mse: 4189.1333 - mae: 34.0780 - val_loss: 2403.5666 - val_mse: 2403.5664 - val_mae: 31.7471\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 3928.2774 - mse: 3928.2771 - mae: 33.4672 - val_loss: 2304.4230 - val_mse: 2304.4231 - val_mae: 31.4073\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4323.5468 - mse: 4323.5454 - mae: 34.2334 - val_loss: 2436.7089 - val_mse: 2436.7087 - val_mae: 31.8791\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4195.8998 - mse: 4195.8999 - mae: 35.2966 - val_loss: 2354.2027 - val_mse: 2354.2024 - val_mae: 31.5708\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4080.0441 - mse: 4080.0437 - mae: 34.1916 - val_loss: 2321.7576 - val_mse: 2321.7581 - val_mae: 31.4693\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 513us/step - loss: 4178.9965 - mse: 4178.9966 - mae: 34.4194 - val_loss: 2352.0429 - val_mse: 2352.0430 - val_mae: 31.5716\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4099.0622 - mse: 4099.0625 - mae: 33.1261 - val_loss: 2347.5608 - val_mse: 2347.5608 - val_mae: 31.5622\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 4107.1029 - mse: 4107.1030 - mae: 33.4405 - val_loss: 2341.0524 - val_mse: 2341.0525 - val_mae: 31.5346\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4112.9093 - mse: 4112.9092 - mae: 33.5683 - val_loss: 2315.2890 - val_mse: 2315.2891 - val_mae: 31.4503\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 552us/step - loss: 4161.6880 - mse: 4161.6885 - mae: 34.8793 - val_loss: 2380.5521 - val_mse: 2380.5520 - val_mae: 31.6750\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 4058.0480 - mse: 4058.0483 - mae: 34.2482 - val_loss: 2342.3921 - val_mse: 2342.3923 - val_mae: 31.5697\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 681us/step - loss: 4075.5378 - mse: 4075.5376 - mae: 33.5545 - val_loss: 2388.9331 - val_mse: 2388.9333 - val_mae: 31.7137\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 3923.6010 - mse: 3923.6016 - mae: 33.0997 - val_loss: 2315.8052 - val_mse: 2315.8052 - val_mae: 31.4711\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4133.3427 - mse: 4133.3428 - mae: 34.2134 - val_loss: 2306.4350 - val_mse: 2306.4348 - val_mae: 31.4442\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4100.0427 - mse: 4100.0425 - mae: 33.6123 - val_loss: 2364.1203 - val_mse: 2364.1208 - val_mae: 31.6221\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 739us/step - loss: 4100.2077 - mse: 4100.2080 - mae: 33.9982 - val_loss: 2333.9132 - val_mse: 2333.9133 - val_mae: 31.5199\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4222.1381 - mse: 4222.1382 - mae: 34.2015 - val_loss: 2326.2197 - val_mse: 2326.2200 - val_mae: 31.4953\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4221.5809 - mse: 4221.5806 - mae: 34.8394 - val_loss: 2335.9860 - val_mse: 2335.9861 - val_mae: 31.5434\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4134.8726 - mse: 4134.8730 - mae: 34.3253 - val_loss: 2313.0796 - val_mse: 2313.0796 - val_mae: 31.4667\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 537us/step - loss: 3920.6800 - mse: 3920.6797 - mae: 33.8028 - val_loss: 2353.9475 - val_mse: 2353.9478 - val_mae: 31.6052\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 517us/step - loss: 4112.9354 - mse: 4112.9365 - mae: 34.0001 - val_loss: 2380.3344 - val_mse: 2380.3347 - val_mae: 31.6848\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 3946.4148 - mse: 3946.4148 - mae: 33.2925 - val_loss: 2381.5751 - val_mse: 2381.5752 - val_mae: 31.6814\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 3962.0265 - mse: 3962.0266 - mae: 33.7215 - val_loss: 2331.0427 - val_mse: 2331.0427 - val_mae: 31.5285\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 681us/step - loss: 4009.8696 - mse: 4009.8694 - mae: 32.7626 - val_loss: 2343.4286 - val_mse: 2343.4287 - val_mae: 31.5678\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 3912.9193 - mse: 3912.9189 - mae: 32.5745 - val_loss: 2347.2482 - val_mse: 2347.2483 - val_mae: 31.5859\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 3967.5456 - mse: 3967.5459 - mae: 33.2999 - val_loss: 2373.6547 - val_mse: 2373.6550 - val_mae: 31.6713\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 3977.2665 - mse: 3977.2676 - mae: 32.4645 - val_loss: 2331.7511 - val_mse: 2331.7510 - val_mae: 31.5072\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 490us/step - loss: 4047.2667 - mse: 4047.2666 - mae: 32.6306 - val_loss: 2337.0840 - val_mse: 2337.0840 - val_mae: 31.5123\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 4006.0351 - mse: 4006.0354 - mae: 33.6270 - val_loss: 2267.5559 - val_mse: 2267.5559 - val_mae: 31.2899\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 531us/step - loss: 4175.3316 - mse: 4175.3320 - mae: 34.7569 - val_loss: 2319.3321 - val_mse: 2319.3320 - val_mae: 31.4643\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 3955.5888 - mse: 3955.5886 - mae: 34.0121 - val_loss: 2330.1941 - val_mse: 2330.1941 - val_mae: 31.4867\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 4037.7988 - mse: 4037.7988 - mae: 32.8334 - val_loss: 2287.1754 - val_mse: 2287.1753 - val_mae: 31.3477\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 4079.9657 - mse: 4079.9653 - mae: 32.8737 - val_loss: 2294.3573 - val_mse: 2294.3569 - val_mae: 31.3728\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4108.3668 - mse: 4108.3672 - mae: 33.3003 - val_loss: 2344.4498 - val_mse: 2344.4497 - val_mae: 31.5482\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 675us/step - loss: 3947.9705 - mse: 3947.9705 - mae: 32.5520 - val_loss: 2311.1963 - val_mse: 2311.1960 - val_mae: 31.4326\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 3862.4869 - mse: 3862.4866 - mae: 32.8911 - val_loss: 2344.6467 - val_mse: 2344.6465 - val_mae: 31.5294\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 519us/step - loss: 3945.2099 - mse: 3945.2102 - mae: 32.5642 - val_loss: 2312.2323 - val_mse: 2312.2324 - val_mae: 31.4127\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 492us/step - loss: 4057.4864 - mse: 4057.4866 - mae: 33.8774 - val_loss: 2375.0247 - val_mse: 2375.0247 - val_mae: 31.6156\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 446us/step - loss: 4051.7294 - mse: 4051.7297 - mae: 33.2203 - val_loss: 2368.6590 - val_mse: 2368.6589 - val_mae: 31.5988\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 459us/step - loss: 4059.0475 - mse: 4059.0471 - mae: 33.0775 - val_loss: 2330.8393 - val_mse: 2330.8394 - val_mae: 31.4862\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 531us/step - loss: 4096.7240 - mse: 4096.7246 - mae: 33.4856 - val_loss: 2379.3520 - val_mse: 2379.3521 - val_mae: 31.6441\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4096.6407 - mse: 4096.6401 - mae: 32.9499 - val_loss: 2304.2196 - val_mse: 2304.2195 - val_mae: 31.3999\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 3877.4160 - mse: 3877.4158 - mae: 33.0031 - val_loss: 2258.3751 - val_mse: 2258.3750 - val_mae: 31.2327\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4023.0898 - mse: 4023.0903 - mae: 33.4318 - val_loss: 2326.3011 - val_mse: 2326.3010 - val_mae: 31.4670\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4028.4353 - mse: 4028.4358 - mae: 32.0850 - val_loss: 2310.0092 - val_mse: 2310.0090 - val_mae: 31.4213\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4037.2039 - mse: 4037.2036 - mae: 33.5028 - val_loss: 2361.5917 - val_mse: 2361.5916 - val_mae: 31.5897\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 3927.5755 - mse: 3927.5750 - mae: 32.8980 - val_loss: 2260.0669 - val_mse: 2260.0667 - val_mae: 31.2577\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 3986.8600 - mse: 3986.8601 - mae: 33.7116 - val_loss: 2337.0738 - val_mse: 2337.0737 - val_mae: 31.5275\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4117.5684 - mse: 4117.5688 - mae: 33.3631 - val_loss: 2288.7182 - val_mse: 2288.7183 - val_mae: 31.3647\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 3930.3927 - mse: 3930.3926 - mae: 32.6469 - val_loss: 2308.1013 - val_mse: 2308.1013 - val_mae: 31.4338\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 3911.4378 - mse: 3911.4377 - mae: 32.3107 - val_loss: 2298.6705 - val_mse: 2298.6704 - val_mae: 31.4071\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 3980.1892 - mse: 3980.1899 - mae: 33.2453 - val_loss: 2369.5272 - val_mse: 2369.5271 - val_mae: 31.6592\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 537us/step - loss: 3957.5536 - mse: 3957.5532 - mae: 32.0333 - val_loss: 2355.3965 - val_mse: 2355.3967 - val_mae: 31.6070\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 514us/step - loss: 4034.5529 - mse: 4034.5530 - mae: 33.3953 - val_loss: 2283.8478 - val_mse: 2283.8477 - val_mae: 31.3662\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4149.6708 - mse: 4149.6714 - mae: 33.4586 - val_loss: 2327.0817 - val_mse: 2327.0815 - val_mae: 31.5164\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 3907.0605 - mse: 3907.0608 - mae: 33.0718 - val_loss: 2302.3747 - val_mse: 2302.3748 - val_mae: 31.4387\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 510us/step - loss: 3960.8330 - mse: 3960.8330 - mae: 32.9897 - val_loss: 2311.3747 - val_mse: 2311.3748 - val_mae: 31.4548\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 503us/step - loss: 3987.2640 - mse: 3987.2644 - mae: 32.9267 - val_loss: 2312.5438 - val_mse: 2312.5439 - val_mae: 31.4517\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 521us/step - loss: 3987.4669 - mse: 3987.4666 - mae: 33.2867 - val_loss: 2322.3058 - val_mse: 2322.3062 - val_mae: 31.4979\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 568us/step - loss: 3994.3295 - mse: 3994.3298 - mae: 33.3629 - val_loss: 2317.4379 - val_mse: 2317.4380 - val_mae: 31.4936\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4015.8511 - mse: 4015.8513 - mae: 32.6388 - val_loss: 2304.2549 - val_mse: 2304.2549 - val_mae: 31.4365\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3294.3174 - mse: 3294.3184 - mae: 32.7311 - val_loss: 1487.1811 - val_mse: 1487.1810 - val_mae: 26.1664\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3465.8024 - mse: 3465.8027 - mae: 33.5523 - val_loss: 1486.6195 - val_mse: 1486.6193 - val_mae: 25.8899\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3354.5936 - mse: 3354.5930 - mae: 32.9609 - val_loss: 1496.5483 - val_mse: 1496.5483 - val_mae: 27.1556\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3358.3352 - mse: 3358.3354 - mae: 33.2096 - val_loss: 1487.5229 - val_mse: 1487.5231 - val_mae: 26.2672\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 499us/step - loss: 3379.4871 - mse: 3379.4863 - mae: 32.6212 - val_loss: 1486.6439 - val_mse: 1486.6440 - val_mae: 26.1466\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3388.6128 - mse: 3388.6133 - mae: 32.5175 - val_loss: 1486.0423 - val_mse: 1486.0422 - val_mae: 25.8702\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3460.2464 - mse: 3460.2461 - mae: 33.2245 - val_loss: 1485.4758 - val_mse: 1485.4757 - val_mae: 25.6627\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3408.9168 - mse: 3408.9170 - mae: 32.7981 - val_loss: 1485.2023 - val_mse: 1485.2024 - val_mae: 26.2833\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3488.3109 - mse: 3488.3110 - mae: 33.1913 - val_loss: 1485.6086 - val_mse: 1485.6086 - val_mae: 26.3474\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3388.3104 - mse: 3388.3101 - mae: 32.3243 - val_loss: 1486.8327 - val_mse: 1486.8328 - val_mae: 26.1912\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3402.4256 - mse: 3402.4258 - mae: 32.8227 - val_loss: 1487.1868 - val_mse: 1487.1866 - val_mae: 26.3100\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 654us/step - loss: 3338.3583 - mse: 3338.3579 - mae: 32.4387 - val_loss: 1488.6839 - val_mse: 1488.6838 - val_mae: 26.6383\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3323.2870 - mse: 3323.2866 - mae: 31.5841 - val_loss: 1483.8916 - val_mse: 1483.8916 - val_mae: 26.4143\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3386.6698 - mse: 3386.6699 - mae: 33.3186 - val_loss: 1481.2856 - val_mse: 1481.2854 - val_mae: 25.7932\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3324.2152 - mse: 3324.2141 - mae: 32.1974 - val_loss: 1481.4582 - val_mse: 1481.4583 - val_mae: 26.0698\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3243.1347 - mse: 3243.1340 - mae: 31.9814 - val_loss: 1482.1972 - val_mse: 1482.1970 - val_mae: 25.9868\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 657us/step - loss: 3387.6924 - mse: 3387.6929 - mae: 32.7627 - val_loss: 1482.0846 - val_mse: 1482.0846 - val_mae: 26.0987\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3436.0367 - mse: 3436.0364 - mae: 32.8941 - val_loss: 1482.0627 - val_mse: 1482.0625 - val_mae: 25.9069\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3545.2633 - mse: 3545.2625 - mae: 32.9202 - val_loss: 1482.9528 - val_mse: 1482.9529 - val_mae: 25.8550\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3333.3670 - mse: 3333.3669 - mae: 32.4079 - val_loss: 1484.3298 - val_mse: 1484.3297 - val_mae: 26.2267\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3209.7400 - mse: 3209.7397 - mae: 31.9915 - val_loss: 1484.8416 - val_mse: 1484.8416 - val_mae: 26.5655\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 534us/step - loss: 3334.0817 - mse: 3334.0823 - mae: 32.5319 - val_loss: 1482.7835 - val_mse: 1482.7836 - val_mae: 25.7217\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3206.2315 - mse: 3206.2322 - mae: 31.7315 - val_loss: 1482.0116 - val_mse: 1482.0115 - val_mae: 26.1783\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3327.9414 - mse: 3327.9419 - mae: 32.2515 - val_loss: 1484.3253 - val_mse: 1484.3253 - val_mae: 25.6135\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3334.0150 - mse: 3334.0156 - mae: 31.8647 - val_loss: 1486.5618 - val_mse: 1486.5619 - val_mae: 26.5748\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3321.9937 - mse: 3321.9946 - mae: 32.5586 - val_loss: 1485.4840 - val_mse: 1485.4840 - val_mae: 26.1702\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 508us/step - loss: 3364.7574 - mse: 3364.7578 - mae: 32.5580 - val_loss: 1487.5644 - val_mse: 1487.5643 - val_mae: 25.5128\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3319.4852 - mse: 3319.4856 - mae: 32.2659 - val_loss: 1487.0910 - val_mse: 1487.0911 - val_mae: 25.7518\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3359.1449 - mse: 3359.1455 - mae: 32.9296 - val_loss: 1489.4372 - val_mse: 1489.4374 - val_mae: 26.4306\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3303.8279 - mse: 3303.8279 - mae: 32.2118 - val_loss: 1488.0470 - val_mse: 1488.0468 - val_mae: 26.2751\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 528us/step - loss: 3319.0217 - mse: 3319.0220 - mae: 32.8272 - val_loss: 1489.7085 - val_mse: 1489.7084 - val_mae: 26.2211\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3449.7255 - mse: 3449.7251 - mae: 32.8136 - val_loss: 1489.5474 - val_mse: 1489.5475 - val_mae: 26.3528\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3331.2844 - mse: 3331.2847 - mae: 32.2833 - val_loss: 1489.2547 - val_mse: 1489.2549 - val_mae: 25.9452\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 686us/step - loss: 3223.1450 - mse: 3223.1455 - mae: 31.5176 - val_loss: 1490.9541 - val_mse: 1490.9542 - val_mae: 26.2494\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3298.4921 - mse: 3298.4927 - mae: 31.9028 - val_loss: 1491.2669 - val_mse: 1491.2668 - val_mae: 26.4223\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 688us/step - loss: 3269.7006 - mse: 3269.6997 - mae: 32.3928 - val_loss: 1491.9808 - val_mse: 1491.9808 - val_mae: 26.1807\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 705us/step - loss: 3188.5477 - mse: 3188.5476 - mae: 31.6469 - val_loss: 1492.7082 - val_mse: 1492.7083 - val_mae: 25.9796\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3275.3405 - mse: 3275.3408 - mae: 32.2740 - val_loss: 1493.2243 - val_mse: 1493.2242 - val_mae: 26.2535\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 742us/step - loss: 3206.8327 - mse: 3206.8330 - mae: 31.4032 - val_loss: 1492.5373 - val_mse: 1492.5372 - val_mae: 26.2032\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3280.2719 - mse: 3280.2722 - mae: 32.7619 - val_loss: 1494.6593 - val_mse: 1494.6594 - val_mae: 26.5686\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3380.3957 - mse: 3380.3960 - mae: 32.2946 - val_loss: 1495.8600 - val_mse: 1495.8600 - val_mae: 26.7644\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 651us/step - loss: 3388.0992 - mse: 3388.0994 - mae: 32.4741 - val_loss: 1490.0665 - val_mse: 1490.0667 - val_mae: 26.1032\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3232.7699 - mse: 3232.7703 - mae: 32.3083 - val_loss: 1492.5321 - val_mse: 1492.5321 - val_mae: 26.5004\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3292.4927 - mse: 3292.4927 - mae: 32.0585 - val_loss: 1491.4507 - val_mse: 1491.4507 - val_mae: 25.8610\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3359.0639 - mse: 3359.0642 - mae: 31.7336 - val_loss: 1493.8246 - val_mse: 1493.8246 - val_mae: 26.4408\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3193.4224 - mse: 3193.4221 - mae: 31.6630 - val_loss: 1493.6500 - val_mse: 1493.6500 - val_mae: 26.2099\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3350.4834 - mse: 3350.4834 - mae: 32.3633 - val_loss: 1493.8945 - val_mse: 1493.8944 - val_mae: 26.0441\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 657us/step - loss: 3240.4342 - mse: 3240.4338 - mae: 31.8023 - val_loss: 1499.9347 - val_mse: 1499.9348 - val_mae: 26.9307\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 520us/step - loss: 3269.4956 - mse: 3269.4963 - mae: 31.8575 - val_loss: 1494.5436 - val_mse: 1494.5435 - val_mae: 25.9078\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3198.0257 - mse: 3198.0261 - mae: 31.2284 - val_loss: 1496.6646 - val_mse: 1496.6646 - val_mae: 26.7456\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3259.2094 - mse: 3259.2090 - mae: 31.1720 - val_loss: 1494.2215 - val_mse: 1494.2216 - val_mae: 26.2308\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3201.2321 - mse: 3201.2322 - mae: 31.2866 - val_loss: 1494.3510 - val_mse: 1494.3510 - val_mae: 26.1675\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3269.8972 - mse: 3269.8972 - mae: 31.9478 - val_loss: 1494.4686 - val_mse: 1494.4684 - val_mae: 26.3690\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3219.5424 - mse: 3219.5422 - mae: 31.5594 - val_loss: 1494.2006 - val_mse: 1494.2007 - val_mae: 26.0801\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3290.1767 - mse: 3290.1770 - mae: 32.2084 - val_loss: 1494.1638 - val_mse: 1494.1639 - val_mae: 25.9060\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 674us/step - loss: 3348.5623 - mse: 3348.5623 - mae: 32.0739 - val_loss: 1494.3580 - val_mse: 1494.3579 - val_mae: 26.0190\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3208.7215 - mse: 3208.7217 - mae: 31.1398 - val_loss: 1497.5646 - val_mse: 1497.5647 - val_mae: 26.6023\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3245.8755 - mse: 3245.8752 - mae: 31.5254 - val_loss: 1495.9135 - val_mse: 1495.9135 - val_mae: 26.2230\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3272.5687 - mse: 3272.5681 - mae: 31.8639 - val_loss: 1494.7467 - val_mse: 1494.7468 - val_mae: 26.4476\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3210.7023 - mse: 3210.7026 - mae: 31.5529 - val_loss: 1493.0358 - val_mse: 1493.0356 - val_mae: 25.9234\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 544us/step - loss: 3223.2667 - mse: 3223.2668 - mae: 32.1772 - val_loss: 1495.9488 - val_mse: 1495.9489 - val_mae: 26.5308\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3333.8398 - mse: 3333.8401 - mae: 31.8421 - val_loss: 1492.7393 - val_mse: 1492.7393 - val_mae: 25.9349\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3212.9147 - mse: 3212.9150 - mae: 31.4378 - val_loss: 1493.2019 - val_mse: 1493.2019 - val_mae: 26.3141\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3235.5176 - mse: 3235.5178 - mae: 31.7273 - val_loss: 1493.2344 - val_mse: 1493.2345 - val_mae: 26.4686\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3289.9480 - mse: 3289.9485 - mae: 32.1519 - val_loss: 1491.3955 - val_mse: 1491.3954 - val_mae: 25.7160\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3304.9928 - mse: 3304.9929 - mae: 31.9466 - val_loss: 1493.6988 - val_mse: 1493.6989 - val_mae: 26.4790\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3314.4441 - mse: 3314.4438 - mae: 31.7026 - val_loss: 1492.9155 - val_mse: 1492.9154 - val_mae: 26.3493\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3210.8996 - mse: 3210.9004 - mae: 31.4948 - val_loss: 1493.6732 - val_mse: 1493.6731 - val_mae: 26.2631\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3236.5909 - mse: 3236.5901 - mae: 31.6894 - val_loss: 1494.8970 - val_mse: 1494.8969 - val_mae: 26.2542\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3261.3269 - mse: 3261.3267 - mae: 30.9951 - val_loss: 1495.6672 - val_mse: 1495.6672 - val_mae: 26.0702\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3393.2966 - mse: 3393.2966 - mae: 32.1820 - val_loss: 1495.7858 - val_mse: 1495.7859 - val_mae: 26.2981\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3142.5890 - mse: 3142.5889 - mae: 31.1271 - val_loss: 1497.3879 - val_mse: 1497.3878 - val_mae: 26.6625\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3257.5126 - mse: 3257.5125 - mae: 31.9028 - val_loss: 1495.2974 - val_mse: 1495.2975 - val_mae: 26.3726\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 537us/step - loss: 3240.9653 - mse: 3240.9648 - mae: 30.9023 - val_loss: 1495.7461 - val_mse: 1495.7462 - val_mae: 25.6733\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 508us/step - loss: 3151.2968 - mse: 3151.2969 - mae: 31.7272 - val_loss: 1495.7146 - val_mse: 1495.7146 - val_mae: 26.3452\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3279.7347 - mse: 3279.7341 - mae: 31.3792 - val_loss: 1496.0300 - val_mse: 1496.0300 - val_mae: 26.0843\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3314.7150 - mse: 3314.7153 - mae: 31.8960 - val_loss: 1496.0701 - val_mse: 1496.0701 - val_mae: 25.9046\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3215.2091 - mse: 3215.2090 - mae: 31.0695 - val_loss: 1497.0427 - val_mse: 1497.0427 - val_mae: 26.2075\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 511us/step - loss: 3183.7343 - mse: 3183.7349 - mae: 30.8496 - val_loss: 1497.2434 - val_mse: 1497.2433 - val_mae: 26.3859\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3152.1618 - mse: 3152.1611 - mae: 30.9170 - val_loss: 1497.5817 - val_mse: 1497.5817 - val_mae: 26.1713\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2908.2319 - mse: 2908.2317 - mae: 30.9729 - val_loss: 1086.2036 - val_mse: 1086.2035 - val_mae: 23.9138\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 471us/step - loss: 2929.9969 - mse: 2929.9968 - mae: 31.0537 - val_loss: 1076.3188 - val_mse: 1076.3188 - val_mae: 24.4083\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 548us/step - loss: 2997.3131 - mse: 2997.3120 - mae: 31.5501 - val_loss: 1085.1729 - val_mse: 1085.1727 - val_mae: 23.7750\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2884.5146 - mse: 2884.5146 - mae: 31.0618 - val_loss: 1080.0399 - val_mse: 1080.0398 - val_mae: 23.9490\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2963.7866 - mse: 2963.7866 - mae: 31.2054 - val_loss: 1077.9038 - val_mse: 1077.9037 - val_mae: 24.0282\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2935.5085 - mse: 2935.5078 - mae: 30.6341 - val_loss: 1076.6560 - val_mse: 1076.6560 - val_mae: 23.9849\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2875.5560 - mse: 2875.5552 - mae: 31.4808 - val_loss: 1073.7552 - val_mse: 1073.7550 - val_mae: 24.0821\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2878.3750 - mse: 2878.3755 - mae: 30.8250 - val_loss: 1071.6198 - val_mse: 1071.6199 - val_mae: 24.0145\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2881.6619 - mse: 2881.6619 - mae: 30.9175 - val_loss: 1069.7395 - val_mse: 1069.7396 - val_mae: 24.1837\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2957.3659 - mse: 2957.3657 - mae: 31.2411 - val_loss: 1073.4117 - val_mse: 1073.4117 - val_mae: 23.7745\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2871.3575 - mse: 2871.3574 - mae: 30.8226 - val_loss: 1072.2290 - val_mse: 1072.2290 - val_mae: 23.8869\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 550us/step - loss: 2863.7500 - mse: 2863.7505 - mae: 31.3452 - val_loss: 1071.4109 - val_mse: 1071.4109 - val_mae: 23.8771\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 502us/step - loss: 2912.9598 - mse: 2912.9587 - mae: 31.1422 - val_loss: 1068.5108 - val_mse: 1068.5107 - val_mae: 24.1602\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2840.9256 - mse: 2840.9250 - mae: 30.7832 - val_loss: 1071.0321 - val_mse: 1071.0321 - val_mae: 23.9230\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2899.6414 - mse: 2899.6416 - mae: 30.8512 - val_loss: 1069.9738 - val_mse: 1069.9739 - val_mae: 24.0492\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 513us/step - loss: 2881.6711 - mse: 2881.6711 - mae: 30.8564 - val_loss: 1069.3614 - val_mse: 1069.3612 - val_mae: 24.0840\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2940.6865 - mse: 2940.6863 - mae: 31.6817 - val_loss: 1069.8383 - val_mse: 1069.8381 - val_mae: 23.9731\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2915.2497 - mse: 2915.2490 - mae: 30.6783 - val_loss: 1071.3709 - val_mse: 1071.3710 - val_mae: 23.8429\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2820.2495 - mse: 2820.2493 - mae: 30.7546 - val_loss: 1069.6081 - val_mse: 1069.6080 - val_mae: 23.9392\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2916.9953 - mse: 2916.9951 - mae: 31.0885 - val_loss: 1069.9857 - val_mse: 1069.9856 - val_mae: 23.8061\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2879.3218 - mse: 2879.3220 - mae: 31.0637 - val_loss: 1069.8734 - val_mse: 1069.8734 - val_mae: 23.8504\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2939.2012 - mse: 2939.2012 - mae: 31.2156 - val_loss: 1067.2912 - val_mse: 1067.2913 - val_mae: 24.0570\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2880.8128 - mse: 2880.8125 - mae: 30.9329 - val_loss: 1067.5390 - val_mse: 1067.5389 - val_mae: 23.9376\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 685us/step - loss: 2821.1382 - mse: 2821.1377 - mae: 30.4266 - val_loss: 1067.3507 - val_mse: 1067.3506 - val_mae: 23.9809\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2904.5832 - mse: 2904.5833 - mae: 30.8876 - val_loss: 1066.7596 - val_mse: 1066.7596 - val_mae: 24.0174\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2967.5456 - mse: 2967.5459 - mae: 30.8578 - val_loss: 1065.5676 - val_mse: 1065.5675 - val_mae: 24.1572\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2859.6787 - mse: 2859.6792 - mae: 30.4880 - val_loss: 1068.8885 - val_mse: 1068.8885 - val_mae: 23.7699\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 532us/step - loss: 2997.7615 - mse: 2997.7622 - mae: 30.8274 - val_loss: 1067.7060 - val_mse: 1067.7061 - val_mae: 23.8109\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2979.8849 - mse: 2979.8845 - mae: 31.2822 - val_loss: 1065.0138 - val_mse: 1065.0138 - val_mae: 24.0731\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2847.3531 - mse: 2847.3538 - mae: 30.6638 - val_loss: 1065.7007 - val_mse: 1065.7007 - val_mae: 23.9256\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2831.7254 - mse: 2831.7258 - mae: 30.4782 - val_loss: 1063.2490 - val_mse: 1063.2491 - val_mae: 24.3288\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2870.1639 - mse: 2870.1648 - mae: 30.4331 - val_loss: 1063.3951 - val_mse: 1063.3950 - val_mae: 24.0368\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2867.2853 - mse: 2867.2852 - mae: 30.8238 - val_loss: 1062.3907 - val_mse: 1062.3907 - val_mae: 24.2056\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2892.9478 - mse: 2892.9475 - mae: 31.0029 - val_loss: 1062.3949 - val_mse: 1062.3950 - val_mae: 24.0256\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 685us/step - loss: 2934.3907 - mse: 2934.3904 - mae: 30.9592 - val_loss: 1065.8316 - val_mse: 1065.8317 - val_mae: 23.6458\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 672us/step - loss: 2852.9831 - mse: 2852.9829 - mae: 30.5542 - val_loss: 1063.7704 - val_mse: 1063.7704 - val_mae: 23.8424\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 700us/step - loss: 2809.9438 - mse: 2809.9443 - mae: 30.8465 - val_loss: 1061.9939 - val_mse: 1061.9940 - val_mae: 23.9307\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2819.8492 - mse: 2819.8491 - mae: 30.5481 - val_loss: 1061.3201 - val_mse: 1061.3202 - val_mae: 23.9146\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2865.9150 - mse: 2865.9143 - mae: 30.6588 - val_loss: 1060.8272 - val_mse: 1060.8271 - val_mae: 23.9833\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2965.2210 - mse: 2965.2214 - mae: 31.5613 - val_loss: 1061.3870 - val_mse: 1061.3872 - val_mae: 23.8723\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 519us/step - loss: 2851.6368 - mse: 2851.6370 - mae: 30.5823 - val_loss: 1059.8714 - val_mse: 1059.8713 - val_mae: 24.0973\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2805.4968 - mse: 2805.4976 - mae: 30.1923 - val_loss: 1059.6805 - val_mse: 1059.6805 - val_mae: 23.9905\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2783.8022 - mse: 2783.8020 - mae: 30.4733 - val_loss: 1058.6264 - val_mse: 1058.6263 - val_mae: 24.4035\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2912.2692 - mse: 2912.2688 - mae: 30.5926 - val_loss: 1060.2498 - val_mse: 1060.2496 - val_mae: 23.9515\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 543us/step - loss: 2860.8069 - mse: 2860.8066 - mae: 30.4892 - val_loss: 1058.6515 - val_mse: 1058.6514 - val_mae: 24.2950\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2824.5621 - mse: 2824.5627 - mae: 30.2521 - val_loss: 1058.4864 - val_mse: 1058.4863 - val_mae: 24.0676\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2887.3421 - mse: 2887.3416 - mae: 30.4083 - val_loss: 1058.1587 - val_mse: 1058.1587 - val_mae: 24.5180\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2924.7089 - mse: 2924.7092 - mae: 31.1090 - val_loss: 1057.1982 - val_mse: 1057.1981 - val_mae: 24.2090\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 554us/step - loss: 2833.0299 - mse: 2833.0303 - mae: 30.3259 - val_loss: 1058.4127 - val_mse: 1058.4126 - val_mae: 23.8167\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2840.6115 - mse: 2840.6113 - mae: 30.1761 - val_loss: 1056.2963 - val_mse: 1056.2963 - val_mae: 24.4124\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 506us/step - loss: 2818.6756 - mse: 2818.6765 - mae: 30.4447 - val_loss: 1056.4497 - val_mse: 1056.4498 - val_mae: 23.9329\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2777.9653 - mse: 2777.9648 - mae: 30.0130 - val_loss: 1054.8693 - val_mse: 1054.8693 - val_mae: 24.1290\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2788.1411 - mse: 2788.1411 - mae: 29.9681 - val_loss: 1056.1060 - val_mse: 1056.1062 - val_mae: 23.8575\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2827.7188 - mse: 2827.7190 - mae: 30.2578 - val_loss: 1054.1864 - val_mse: 1054.1864 - val_mae: 24.2010\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2818.2147 - mse: 2818.2158 - mae: 29.6695 - val_loss: 1055.8886 - val_mse: 1055.8887 - val_mae: 23.8654\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2826.4994 - mse: 2826.4998 - mae: 30.1744 - val_loss: 1054.1402 - val_mse: 1054.1404 - val_mae: 24.0272\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2895.7765 - mse: 2895.7764 - mae: 30.6332 - val_loss: 1054.2696 - val_mse: 1054.2697 - val_mae: 23.8220\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2833.8521 - mse: 2833.8516 - mae: 30.3744 - val_loss: 1053.2348 - val_mse: 1053.2350 - val_mae: 23.9491\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2837.6124 - mse: 2837.6121 - mae: 30.5955 - val_loss: 1052.1641 - val_mse: 1052.1641 - val_mae: 24.2235\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2862.5840 - mse: 2862.5842 - mae: 30.3152 - val_loss: 1051.4703 - val_mse: 1051.4702 - val_mae: 24.3249\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2782.9197 - mse: 2782.9197 - mae: 29.9714 - val_loss: 1050.6064 - val_mse: 1050.6063 - val_mae: 24.2150\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 691us/step - loss: 2796.7638 - mse: 2796.7622 - mae: 30.1142 - val_loss: 1050.9802 - val_mse: 1050.9801 - val_mae: 23.8887\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2965.5253 - mse: 2965.5251 - mae: 31.0048 - val_loss: 1049.9855 - val_mse: 1049.9855 - val_mae: 24.0737\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 659us/step - loss: 2800.5485 - mse: 2800.5479 - mae: 30.0959 - val_loss: 1050.0653 - val_mse: 1050.0654 - val_mae: 23.9757\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2842.4265 - mse: 2842.4263 - mae: 29.7322 - val_loss: 1048.3150 - val_mse: 1048.3149 - val_mae: 24.1871\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 482us/step - loss: 2845.0448 - mse: 2845.0444 - mae: 30.5177 - val_loss: 1048.5160 - val_mse: 1048.5160 - val_mae: 23.9951\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2926.6931 - mse: 2926.6931 - mae: 30.6379 - val_loss: 1047.5125 - val_mse: 1047.5126 - val_mae: 24.0857\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2856.9673 - mse: 2856.9675 - mae: 30.7362 - val_loss: 1047.9783 - val_mse: 1047.9783 - val_mae: 24.0263\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2779.2964 - mse: 2779.2964 - mae: 29.9940 - val_loss: 1048.4322 - val_mse: 1048.4321 - val_mae: 23.8039\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2782.2980 - mse: 2782.2974 - mae: 29.9516 - val_loss: 1048.5735 - val_mse: 1048.5734 - val_mae: 23.7948\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2878.6776 - mse: 2878.6775 - mae: 30.6268 - val_loss: 1046.5578 - val_mse: 1046.5579 - val_mae: 24.1360\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2813.5788 - mse: 2813.5796 - mae: 29.7272 - val_loss: 1046.0446 - val_mse: 1046.0447 - val_mae: 24.1230\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2896.2584 - mse: 2896.2583 - mae: 30.4919 - val_loss: 1045.4073 - val_mse: 1045.4073 - val_mae: 24.0923\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2794.4233 - mse: 2794.4233 - mae: 30.3931 - val_loss: 1044.4934 - val_mse: 1044.4935 - val_mae: 24.2433\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2797.2173 - mse: 2797.2166 - mae: 30.5814 - val_loss: 1043.5753 - val_mse: 1043.5754 - val_mae: 23.9898\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 662us/step - loss: 2826.1838 - mse: 2826.1833 - mae: 30.3918 - val_loss: 1042.3517 - val_mse: 1042.3516 - val_mae: 24.1297\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2849.4820 - mse: 2849.4817 - mae: 30.4629 - val_loss: 1042.3416 - val_mse: 1042.3416 - val_mae: 24.0272\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2852.5996 - mse: 2852.5994 - mae: 30.3855 - val_loss: 1043.0361 - val_mse: 1043.0361 - val_mae: 23.8123\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 551us/step - loss: 2782.7909 - mse: 2782.7910 - mae: 30.2730 - val_loss: 1041.7177 - val_mse: 1041.7178 - val_mae: 23.9725\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2772.0517 - mse: 2772.0520 - mae: 29.9941 - val_loss: 1042.4958 - val_mse: 1042.4958 - val_mae: 23.7996\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2541.8681 - mse: 2541.8687 - mae: 30.0578 - val_loss: 1511.7270 - val_mse: 1511.7268 - val_mae: 27.5924\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2564.2204 - mse: 2564.2209 - mae: 29.9633 - val_loss: 1505.5522 - val_mse: 1505.5524 - val_mae: 27.7103\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2557.0514 - mse: 2557.0515 - mae: 29.8786 - val_loss: 1502.3824 - val_mse: 1502.3826 - val_mae: 27.7833\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2521.1625 - mse: 2521.1624 - mae: 29.6217 - val_loss: 1501.5244 - val_mse: 1501.5244 - val_mae: 27.6784\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2585.7431 - mse: 2585.7427 - mae: 30.2371 - val_loss: 1499.4169 - val_mse: 1499.4171 - val_mae: 27.6924\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2519.9636 - mse: 2519.9639 - mae: 29.5430 - val_loss: 1502.5706 - val_mse: 1502.5704 - val_mae: 27.5005\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2513.8598 - mse: 2513.8604 - mae: 29.1402 - val_loss: 1491.2490 - val_mse: 1491.2489 - val_mae: 27.8354\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2549.0132 - mse: 2549.0129 - mae: 29.8908 - val_loss: 1490.1341 - val_mse: 1490.1342 - val_mae: 27.7857\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2568.1652 - mse: 2568.1648 - mae: 29.9045 - val_loss: 1490.2739 - val_mse: 1490.2739 - val_mae: 27.7055\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 530us/step - loss: 2553.9254 - mse: 2553.9255 - mae: 29.8127 - val_loss: 1493.2839 - val_mse: 1493.2841 - val_mae: 27.5520\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2585.1857 - mse: 2585.1853 - mae: 30.1386 - val_loss: 1483.0556 - val_mse: 1483.0558 - val_mae: 27.9189\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2536.6039 - mse: 2536.6042 - mae: 29.8041 - val_loss: 1479.3410 - val_mse: 1479.3412 - val_mae: 27.9719\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2521.7841 - mse: 2521.7837 - mae: 29.3496 - val_loss: 1474.5084 - val_mse: 1474.5084 - val_mae: 28.1288\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 648us/step - loss: 2556.0476 - mse: 2556.0476 - mae: 30.1332 - val_loss: 1483.1391 - val_mse: 1483.1390 - val_mae: 27.5174\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2487.8603 - mse: 2487.8606 - mae: 29.4663 - val_loss: 1473.5531 - val_mse: 1473.5530 - val_mae: 27.8420\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 657us/step - loss: 2535.3265 - mse: 2535.3267 - mae: 29.5710 - val_loss: 1477.3406 - val_mse: 1477.3407 - val_mae: 27.5676\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 679us/step - loss: 2519.7911 - mse: 2519.7913 - mae: 29.3869 - val_loss: 1471.1234 - val_mse: 1471.1233 - val_mae: 27.8200\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 701us/step - loss: 2499.6430 - mse: 2499.6423 - mae: 29.1802 - val_loss: 1468.1524 - val_mse: 1468.1525 - val_mae: 27.8295\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2469.9700 - mse: 2469.9695 - mae: 29.4450 - val_loss: 1479.5164 - val_mse: 1479.5164 - val_mae: 27.2172\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2503.6254 - mse: 2503.6262 - mae: 29.1497 - val_loss: 1469.6823 - val_mse: 1469.6824 - val_mae: 27.5302\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2521.0202 - mse: 2521.0210 - mae: 29.5246 - val_loss: 1469.0670 - val_mse: 1469.0669 - val_mae: 27.4669\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2535.7181 - mse: 2535.7170 - mae: 29.5955 - val_loss: 1462.5759 - val_mse: 1462.5757 - val_mae: 27.6657\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2510.5889 - mse: 2510.5891 - mae: 29.2304 - val_loss: 1462.8030 - val_mse: 1462.8029 - val_mae: 27.6449\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 533us/step - loss: 2413.0606 - mse: 2413.0601 - mae: 28.9107 - val_loss: 1455.2995 - val_mse: 1455.2997 - val_mae: 27.9636\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2477.9069 - mse: 2477.9072 - mae: 29.3922 - val_loss: 1456.5916 - val_mse: 1456.5916 - val_mae: 27.7002\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2492.6636 - mse: 2492.6638 - mae: 29.3716 - val_loss: 1456.0826 - val_mse: 1456.0825 - val_mae: 27.6450\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 706us/step - loss: 2498.5076 - mse: 2498.5078 - mae: 29.5697 - val_loss: 1458.3017 - val_mse: 1458.3016 - val_mae: 27.3953\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2458.6434 - mse: 2458.6436 - mae: 29.2089 - val_loss: 1455.8044 - val_mse: 1455.8043 - val_mae: 27.4062\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 656us/step - loss: 2514.8506 - mse: 2514.8508 - mae: 29.1593 - val_loss: 1450.2066 - val_mse: 1450.2068 - val_mae: 27.6298\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2448.9549 - mse: 2448.9556 - mae: 28.7333 - val_loss: 1446.3551 - val_mse: 1446.3551 - val_mae: 27.9367\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2472.9612 - mse: 2472.9617 - mae: 29.5138 - val_loss: 1447.1676 - val_mse: 1447.1674 - val_mae: 27.6876\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2508.2469 - mse: 2508.2473 - mae: 29.3184 - val_loss: 1449.7685 - val_mse: 1449.7684 - val_mae: 27.4806\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2536.1782 - mse: 2536.1782 - mae: 29.4299 - val_loss: 1447.9518 - val_mse: 1447.9518 - val_mae: 27.5562\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2471.3064 - mse: 2471.3059 - mae: 29.3477 - val_loss: 1442.0489 - val_mse: 1442.0487 - val_mae: 27.9053\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2460.8403 - mse: 2460.8411 - mae: 28.9587 - val_loss: 1442.9661 - val_mse: 1442.9663 - val_mae: 27.7237\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2510.7084 - mse: 2510.7083 - mae: 29.5565 - val_loss: 1441.8659 - val_mse: 1441.8657 - val_mae: 27.6641\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2497.9130 - mse: 2497.9133 - mae: 29.1951 - val_loss: 1440.9489 - val_mse: 1440.9490 - val_mae: 27.6445\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 691us/step - loss: 2472.2161 - mse: 2472.2166 - mae: 29.2602 - val_loss: 1439.9851 - val_mse: 1439.9852 - val_mae: 27.7309\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 671us/step - loss: 2434.9910 - mse: 2434.9915 - mae: 29.2080 - val_loss: 1437.5153 - val_mse: 1437.5155 - val_mae: 28.0625\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2501.1257 - mse: 2501.1250 - mae: 29.1341 - val_loss: 1436.6267 - val_mse: 1436.6266 - val_mae: 27.9149\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2459.3704 - mse: 2459.3691 - mae: 29.3041 - val_loss: 1438.6351 - val_mse: 1438.6349 - val_mae: 27.5463\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2532.8242 - mse: 2532.8240 - mae: 29.6890 - val_loss: 1441.3668 - val_mse: 1441.3668 - val_mae: 27.3928\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 548us/step - loss: 2457.5169 - mse: 2457.5164 - mae: 28.9862 - val_loss: 1435.6266 - val_mse: 1435.6266 - val_mae: 27.6478\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2530.4117 - mse: 2530.4124 - mae: 29.6343 - val_loss: 1434.4606 - val_mse: 1434.4606 - val_mae: 27.7713\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2446.4000 - mse: 2446.3999 - mae: 29.1487 - val_loss: 1430.9524 - val_mse: 1430.9524 - val_mae: 28.1050\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 549us/step - loss: 2517.6001 - mse: 2517.6001 - mae: 29.2116 - val_loss: 1432.0875 - val_mse: 1432.0878 - val_mae: 27.7607\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2473.5759 - mse: 2473.5762 - mae: 29.6401 - val_loss: 1434.0289 - val_mse: 1434.0289 - val_mae: 27.6079\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2450.1284 - mse: 2450.1284 - mae: 29.2246 - val_loss: 1430.0673 - val_mse: 1430.0674 - val_mae: 27.8978\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2526.7570 - mse: 2526.7563 - mae: 29.6287 - val_loss: 1429.0134 - val_mse: 1429.0133 - val_mae: 27.7120\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2500.8769 - mse: 2500.8777 - mae: 29.5251 - val_loss: 1429.7346 - val_mse: 1429.7346 - val_mae: 27.5715\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2537.0782 - mse: 2537.0776 - mae: 29.2803 - val_loss: 1429.4886 - val_mse: 1429.4885 - val_mae: 27.6539\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2442.0701 - mse: 2442.0698 - mae: 28.8675 - val_loss: 1430.5755 - val_mse: 1430.5756 - val_mae: 27.4897\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2445.0846 - mse: 2445.0837 - mae: 29.0353 - val_loss: 1430.5867 - val_mse: 1430.5868 - val_mae: 27.4629\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2472.1546 - mse: 2472.1543 - mae: 29.3962 - val_loss: 1428.8118 - val_mse: 1428.8118 - val_mae: 27.5633\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2524.3665 - mse: 2524.3677 - mae: 29.5224 - val_loss: 1423.9017 - val_mse: 1423.9015 - val_mae: 27.7390\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 666us/step - loss: 2455.3539 - mse: 2455.3538 - mae: 28.6386 - val_loss: 1424.0001 - val_mse: 1424.0001 - val_mae: 27.6900\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 667us/step - loss: 2489.1347 - mse: 2489.1348 - mae: 29.0990 - val_loss: 1421.4980 - val_mse: 1421.4979 - val_mae: 27.7868\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2445.7437 - mse: 2445.7441 - mae: 29.1481 - val_loss: 1420.5708 - val_mse: 1420.5708 - val_mae: 27.6430\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2475.6733 - mse: 2475.6731 - mae: 28.9720 - val_loss: 1420.3854 - val_mse: 1420.3853 - val_mae: 27.5764\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2459.7139 - mse: 2459.7131 - mae: 29.3024 - val_loss: 1421.7010 - val_mse: 1421.7013 - val_mae: 27.5112\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2490.8069 - mse: 2490.8081 - mae: 28.9249 - val_loss: 1421.5654 - val_mse: 1421.5654 - val_mae: 27.7315\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2458.1673 - mse: 2458.1672 - mae: 29.3910 - val_loss: 1420.0592 - val_mse: 1420.0591 - val_mae: 27.5181\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2444.7634 - mse: 2444.7627 - mae: 29.3206 - val_loss: 1418.4999 - val_mse: 1418.4999 - val_mae: 27.4404\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2456.3937 - mse: 2456.3943 - mae: 28.9694 - val_loss: 1416.5009 - val_mse: 1416.5009 - val_mae: 27.8794\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2456.0910 - mse: 2456.0913 - mae: 29.4783 - val_loss: 1416.2817 - val_mse: 1416.2816 - val_mae: 27.5983\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2466.0789 - mse: 2466.0786 - mae: 29.1891 - val_loss: 1414.0789 - val_mse: 1414.0789 - val_mae: 27.8085\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2510.5747 - mse: 2510.5750 - mae: 29.5522 - val_loss: 1414.7160 - val_mse: 1414.7161 - val_mae: 27.3268\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2433.3512 - mse: 2433.3513 - mae: 28.9096 - val_loss: 1411.6696 - val_mse: 1411.6696 - val_mae: 27.6948\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2512.2429 - mse: 2512.2429 - mae: 29.3235 - val_loss: 1412.7445 - val_mse: 1412.7443 - val_mae: 27.4680\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2444.3180 - mse: 2444.3174 - mae: 29.1469 - val_loss: 1410.6289 - val_mse: 1410.6290 - val_mae: 27.4629\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2495.4313 - mse: 2495.4314 - mae: 29.1889 - val_loss: 1414.0106 - val_mse: 1414.0107 - val_mae: 27.1476\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2456.7311 - mse: 2456.7314 - mae: 28.6093 - val_loss: 1412.5424 - val_mse: 1412.5422 - val_mae: 27.7233\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2443.1935 - mse: 2443.1936 - mae: 29.0774 - val_loss: 1409.7381 - val_mse: 1409.7382 - val_mae: 27.6107\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2422.1492 - mse: 2422.1492 - mae: 28.7304 - val_loss: 1411.7207 - val_mse: 1411.7208 - val_mae: 27.7201\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2534.4781 - mse: 2534.4790 - mae: 29.1592 - val_loss: 1413.9063 - val_mse: 1413.9061 - val_mae: 27.6600\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2483.0859 - mse: 2483.0864 - mae: 29.0382 - val_loss: 1411.6148 - val_mse: 1411.6147 - val_mae: 27.6582\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2463.4114 - mse: 2463.4124 - mae: 28.8798 - val_loss: 1412.4849 - val_mse: 1412.4851 - val_mae: 27.6031\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2389.4048 - mse: 2389.4050 - mae: 28.7630 - val_loss: 1415.2824 - val_mse: 1415.2823 - val_mae: 27.9479\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2375.8747 - mse: 2375.8740 - mae: 28.2643 - val_loss: 1415.0440 - val_mse: 1415.0441 - val_mae: 28.1277\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2451.1310 - mse: 2451.1311 - mae: 28.9025 - val_loss: 1413.3351 - val_mse: 1413.3353 - val_mae: 27.8926\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2306.5388 - mse: 2306.5378 - mae: 29.0903 - val_loss: 3770.3259 - val_mse: 3770.3257 - val_mae: 26.4588\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2358.3896 - mse: 2358.3894 - mae: 29.7057 - val_loss: 3759.1662 - val_mse: 3759.1663 - val_mae: 25.8052\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2332.1492 - mse: 2332.1497 - mae: 29.0310 - val_loss: 3764.5279 - val_mse: 3764.5273 - val_mae: 26.1882\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2374.4590 - mse: 2374.4592 - mae: 29.4056 - val_loss: 3757.8331 - val_mse: 3757.8330 - val_mae: 26.0472\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2359.7984 - mse: 2359.7986 - mae: 29.4303 - val_loss: 3748.8029 - val_mse: 3748.8025 - val_mae: 25.6082\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2351.1704 - mse: 2351.1694 - mae: 29.2641 - val_loss: 3763.0539 - val_mse: 3763.0532 - val_mae: 26.2899\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2327.9692 - mse: 2327.9692 - mae: 29.4848 - val_loss: 3764.0340 - val_mse: 3764.0342 - val_mae: 26.4059\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2345.0883 - mse: 2345.0879 - mae: 28.9169 - val_loss: 3756.6675 - val_mse: 3756.6670 - val_mae: 25.9721\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2358.3756 - mse: 2358.3752 - mae: 29.0300 - val_loss: 3756.6311 - val_mse: 3756.6311 - val_mae: 25.9949\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2318.4200 - mse: 2318.4199 - mae: 28.9304 - val_loss: 3758.5405 - val_mse: 3758.5400 - val_mae: 25.9833\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2295.1443 - mse: 2295.1445 - mae: 28.9256 - val_loss: 3765.4499 - val_mse: 3765.4504 - val_mae: 26.2764\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2361.0696 - mse: 2361.0696 - mae: 29.2301 - val_loss: 3760.9971 - val_mse: 3760.9973 - val_mae: 26.0609\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2309.0236 - mse: 2309.0232 - mae: 29.3618 - val_loss: 3752.0970 - val_mse: 3752.0977 - val_mae: 25.4169\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2328.3871 - mse: 2328.3870 - mae: 29.3359 - val_loss: 3762.9221 - val_mse: 3762.9226 - val_mae: 26.0011\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2334.2846 - mse: 2334.2847 - mae: 29.5152 - val_loss: 3755.3791 - val_mse: 3755.3789 - val_mae: 25.5143\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2371.5777 - mse: 2371.5769 - mae: 29.2882 - val_loss: 3757.6640 - val_mse: 3757.6646 - val_mae: 25.8319\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2292.6303 - mse: 2292.6301 - mae: 29.0455 - val_loss: 3754.8580 - val_mse: 3754.8577 - val_mae: 25.5082\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2378.5255 - mse: 2378.5244 - mae: 29.6440 - val_loss: 3760.3812 - val_mse: 3760.3809 - val_mae: 25.6100\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2362.8122 - mse: 2362.8118 - mae: 28.7356 - val_loss: 3767.6600 - val_mse: 3767.6604 - val_mae: 26.1332\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2382.4083 - mse: 2382.4084 - mae: 29.4115 - val_loss: 3762.6077 - val_mse: 3762.6082 - val_mae: 25.6951\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2382.4069 - mse: 2382.4067 - mae: 29.6045 - val_loss: 3753.9341 - val_mse: 3753.9338 - val_mae: 25.1541\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2292.8319 - mse: 2292.8323 - mae: 28.7717 - val_loss: 3771.5088 - val_mse: 3771.5090 - val_mae: 26.3721\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2318.0429 - mse: 2318.0425 - mae: 29.2289 - val_loss: 3779.8670 - val_mse: 3779.8667 - val_mae: 26.5133\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2245.6271 - mse: 2245.6274 - mae: 28.8779 - val_loss: 3771.8569 - val_mse: 3771.8567 - val_mae: 26.2204\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2358.9837 - mse: 2358.9844 - mae: 29.4912 - val_loss: 3755.1179 - val_mse: 3755.1182 - val_mae: 25.1965\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2335.9691 - mse: 2335.9685 - mae: 28.9137 - val_loss: 3766.1726 - val_mse: 3766.1724 - val_mae: 25.9555\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2254.6901 - mse: 2254.6902 - mae: 28.7842 - val_loss: 3777.7601 - val_mse: 3777.7598 - val_mae: 26.3921\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2384.4917 - mse: 2384.4922 - mae: 29.4555 - val_loss: 3767.6259 - val_mse: 3767.6265 - val_mae: 26.0247\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 711us/step - loss: 2304.0969 - mse: 2304.0964 - mae: 28.8559 - val_loss: 3760.5976 - val_mse: 3760.5991 - val_mae: 25.8861\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2353.8383 - mse: 2353.8381 - mae: 29.3100 - val_loss: 3763.7695 - val_mse: 3763.7698 - val_mae: 25.9664\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2283.6860 - mse: 2283.6855 - mae: 29.0633 - val_loss: 3758.1664 - val_mse: 3758.1667 - val_mae: 25.4103\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2316.8764 - mse: 2316.8762 - mae: 28.8160 - val_loss: 3764.8779 - val_mse: 3764.8774 - val_mae: 26.0374\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2342.2450 - mse: 2342.2446 - mae: 29.1604 - val_loss: 3766.1749 - val_mse: 3766.1748 - val_mae: 25.9597\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 662us/step - loss: 2332.8281 - mse: 2332.8281 - mae: 29.2244 - val_loss: 3761.9143 - val_mse: 3761.9141 - val_mae: 25.7020\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2350.7828 - mse: 2350.7827 - mae: 29.3240 - val_loss: 3752.4407 - val_mse: 3752.4402 - val_mae: 25.5290\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2311.8474 - mse: 2311.8477 - mae: 29.4459 - val_loss: 3758.1022 - val_mse: 3758.1018 - val_mae: 26.0146\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2334.9377 - mse: 2334.9370 - mae: 29.1442 - val_loss: 3763.2065 - val_mse: 3763.2061 - val_mae: 26.1369\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2353.6317 - mse: 2353.6326 - mae: 29.6171 - val_loss: 3770.9976 - val_mse: 3770.9983 - val_mae: 26.3762\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2320.6050 - mse: 2320.6057 - mae: 29.0023 - val_loss: 3760.6803 - val_mse: 3760.6799 - val_mae: 25.6725\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 528us/step - loss: 2334.0767 - mse: 2334.0759 - mae: 29.1375 - val_loss: 3754.6843 - val_mse: 3754.6851 - val_mae: 25.2423\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 509us/step - loss: 2337.0759 - mse: 2337.0764 - mae: 29.0159 - val_loss: 3757.9773 - val_mse: 3757.9775 - val_mae: 25.6634\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2348.5538 - mse: 2348.5535 - mae: 29.4920 - val_loss: 3759.3946 - val_mse: 3759.3943 - val_mae: 25.7207\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2399.1458 - mse: 2399.1460 - mae: 29.6695 - val_loss: 3757.5047 - val_mse: 3757.5051 - val_mae: 25.8995\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 674us/step - loss: 2358.2381 - mse: 2358.2380 - mae: 29.0848 - val_loss: 3763.1863 - val_mse: 3763.1865 - val_mae: 25.9879\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2339.0166 - mse: 2339.0168 - mae: 29.0769 - val_loss: 3765.9071 - val_mse: 3765.9070 - val_mae: 26.1282\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2372.6930 - mse: 2372.6934 - mae: 29.8282 - val_loss: 3756.9706 - val_mse: 3756.9705 - val_mae: 25.5615\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2317.9043 - mse: 2317.9036 - mae: 28.6603 - val_loss: 3765.4441 - val_mse: 3765.4443 - val_mae: 26.1056\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2352.6888 - mse: 2352.6887 - mae: 29.0977 - val_loss: 3758.8432 - val_mse: 3758.8433 - val_mae: 25.8019\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 672us/step - loss: 2304.0071 - mse: 2304.0073 - mae: 29.1355 - val_loss: 3766.6883 - val_mse: 3766.6892 - val_mae: 26.2309\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2392.2704 - mse: 2392.2695 - mae: 29.5159 - val_loss: 3764.2682 - val_mse: 3764.2671 - val_mae: 26.1654\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 540us/step - loss: 2345.9560 - mse: 2345.9556 - mae: 29.5018 - val_loss: 3767.1015 - val_mse: 3767.1008 - val_mae: 26.2471\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2305.4563 - mse: 2305.4558 - mae: 29.1371 - val_loss: 3760.0101 - val_mse: 3760.0103 - val_mae: 25.7633\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2317.7755 - mse: 2317.7761 - mae: 29.0821 - val_loss: 3766.7517 - val_mse: 3766.7520 - val_mae: 26.2565\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 669us/step - loss: 2315.0072 - mse: 2315.0068 - mae: 28.9055 - val_loss: 3759.0459 - val_mse: 3759.0459 - val_mae: 25.8152\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2343.6026 - mse: 2343.6021 - mae: 29.2619 - val_loss: 3756.9084 - val_mse: 3756.9080 - val_mae: 25.7812\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2327.6075 - mse: 2327.6077 - mae: 29.1693 - val_loss: 3751.1931 - val_mse: 3751.1929 - val_mae: 25.6052\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2324.7043 - mse: 2324.7041 - mae: 29.4053 - val_loss: 3750.1064 - val_mse: 3750.1057 - val_mae: 25.7053\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2344.7869 - mse: 2344.7871 - mae: 29.2961 - val_loss: 3756.2422 - val_mse: 3756.2417 - val_mae: 25.9197\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2298.0628 - mse: 2298.0642 - mae: 29.1814 - val_loss: 3754.0827 - val_mse: 3754.0833 - val_mae: 25.9517\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 670us/step - loss: 2320.4541 - mse: 2320.4541 - mae: 29.3956 - val_loss: 3770.7717 - val_mse: 3770.7710 - val_mae: 26.5356\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2257.0394 - mse: 2257.0398 - mae: 28.6853 - val_loss: 3765.7823 - val_mse: 3765.7827 - val_mae: 26.0915\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 525us/step - loss: 2302.3594 - mse: 2302.3596 - mae: 29.1451 - val_loss: 3760.2518 - val_mse: 3760.2520 - val_mae: 25.8162\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2357.4230 - mse: 2357.4238 - mae: 29.0481 - val_loss: 3764.8009 - val_mse: 3764.8008 - val_mae: 25.9779\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2305.7589 - mse: 2305.7588 - mae: 28.8229 - val_loss: 3771.0599 - val_mse: 3771.0596 - val_mae: 26.3058\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 510us/step - loss: 2311.4904 - mse: 2311.4902 - mae: 29.0840 - val_loss: 3752.2593 - val_mse: 3752.2598 - val_mae: 25.4316\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2300.1199 - mse: 2300.1204 - mae: 29.0603 - val_loss: 3748.6046 - val_mse: 3748.6040 - val_mae: 25.2524\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 523us/step - loss: 2324.8168 - mse: 2324.8167 - mae: 29.1691 - val_loss: 3761.4376 - val_mse: 3761.4385 - val_mae: 26.0493\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2296.7028 - mse: 2296.7029 - mae: 29.2053 - val_loss: 3770.4619 - val_mse: 3770.4619 - val_mae: 26.3057\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2315.7670 - mse: 2315.7673 - mae: 28.7475 - val_loss: 3763.2489 - val_mse: 3763.2483 - val_mae: 26.0237\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2328.1192 - mse: 2328.1194 - mae: 28.9380 - val_loss: 3763.1533 - val_mse: 3763.1533 - val_mae: 26.1070\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 505us/step - loss: 2358.1751 - mse: 2358.1748 - mae: 28.9936 - val_loss: 3780.1929 - val_mse: 3780.1929 - val_mae: 27.0181\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2258.6743 - mse: 2258.6738 - mae: 29.1818 - val_loss: 3753.2194 - val_mse: 3753.2183 - val_mae: 25.5969\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2296.4603 - mse: 2296.4604 - mae: 29.0375 - val_loss: 3755.0061 - val_mse: 3755.0063 - val_mae: 25.7480\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2289.1873 - mse: 2289.1870 - mae: 28.5027 - val_loss: 3756.2341 - val_mse: 3756.2339 - val_mae: 25.8880\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2344.2193 - mse: 2344.2192 - mae: 29.4066 - val_loss: 3750.6750 - val_mse: 3750.6750 - val_mae: 25.4454\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2340.7243 - mse: 2340.7249 - mae: 29.3657 - val_loss: 3757.8151 - val_mse: 3757.8167 - val_mae: 25.9750\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2321.2607 - mse: 2321.2612 - mae: 29.2587 - val_loss: 3764.5169 - val_mse: 3764.5161 - val_mae: 26.2568\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2357.9514 - mse: 2357.9517 - mae: 29.4991 - val_loss: 3757.3667 - val_mse: 3757.3662 - val_mae: 25.8601\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2288.1662 - mse: 2288.1663 - mae: 28.8050 - val_loss: 3756.3394 - val_mse: 3756.3394 - val_mae: 26.0387\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2274.7814 - mse: 2274.7810 - mae: 28.9889 - val_loss: 3752.6422 - val_mse: 3752.6416 - val_mae: 25.8725\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2709.4951 - mse: 2709.4944 - mae: 28.7621 - val_loss: 2317.3475 - val_mse: 2317.3474 - val_mae: 27.8411\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2733.2501 - mse: 2733.2500 - mae: 28.8828 - val_loss: 2326.3612 - val_mse: 2326.3616 - val_mae: 27.8000\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2672.2672 - mse: 2672.2673 - mae: 28.2886 - val_loss: 2326.0174 - val_mse: 2326.0176 - val_mae: 28.2659\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2701.8450 - mse: 2701.8447 - mae: 28.9227 - val_loss: 2329.3152 - val_mse: 2329.3154 - val_mae: 28.3552\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 670us/step - loss: 2661.7878 - mse: 2661.7878 - mae: 28.2550 - val_loss: 2325.3231 - val_mse: 2325.3230 - val_mae: 28.7097\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2706.6543 - mse: 2706.6543 - mae: 28.9074 - val_loss: 2327.2390 - val_mse: 2327.2388 - val_mae: 28.0133\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2693.4697 - mse: 2693.4697 - mae: 28.5345 - val_loss: 2333.9968 - val_mse: 2333.9971 - val_mae: 28.0610\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2730.8094 - mse: 2730.8088 - mae: 28.8628 - val_loss: 2335.7828 - val_mse: 2335.7830 - val_mae: 28.3299\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2720.3158 - mse: 2720.3162 - mae: 28.4170 - val_loss: 2337.5748 - val_mse: 2337.5750 - val_mae: 28.4678\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2689.2323 - mse: 2689.2322 - mae: 28.7990 - val_loss: 2333.7968 - val_mse: 2333.7964 - val_mae: 27.9977\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2710.0551 - mse: 2710.0557 - mae: 28.8076 - val_loss: 2339.4557 - val_mse: 2339.4556 - val_mae: 28.1359\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2681.0184 - mse: 2681.0183 - mae: 28.3904 - val_loss: 2338.8803 - val_mse: 2338.8801 - val_mae: 28.6713\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 498us/step - loss: 2750.6102 - mse: 2750.6101 - mae: 28.9771 - val_loss: 2341.4559 - val_mse: 2341.4558 - val_mae: 28.3349\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 555us/step - loss: 2674.0515 - mse: 2674.0518 - mae: 28.3407 - val_loss: 2341.1607 - val_mse: 2341.1606 - val_mae: 28.4623\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2715.7212 - mse: 2715.7202 - mae: 28.9616 - val_loss: 2343.5653 - val_mse: 2343.5652 - val_mae: 28.3179\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2689.4565 - mse: 2689.4568 - mae: 28.6056 - val_loss: 2341.1957 - val_mse: 2341.1956 - val_mae: 28.3292\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2646.3926 - mse: 2646.3926 - mae: 28.0457 - val_loss: 2332.3370 - val_mse: 2332.3374 - val_mae: 28.4158\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2653.8129 - mse: 2653.8130 - mae: 28.6483 - val_loss: 2339.6567 - val_mse: 2339.6570 - val_mae: 28.0428\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2699.3566 - mse: 2699.3569 - mae: 28.5434 - val_loss: 2352.1581 - val_mse: 2352.1582 - val_mae: 27.4819\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2695.9005 - mse: 2695.9004 - mae: 28.6477 - val_loss: 2352.8233 - val_mse: 2352.8230 - val_mae: 28.5533\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 535us/step - loss: 2706.0008 - mse: 2706.0000 - mae: 28.9357 - val_loss: 2358.9577 - val_mse: 2358.9578 - val_mae: 27.6797\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2715.7391 - mse: 2715.7388 - mae: 28.5019 - val_loss: 2366.4476 - val_mse: 2366.4478 - val_mae: 27.9944\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2671.5407 - mse: 2671.5408 - mae: 28.6138 - val_loss: 2355.5916 - val_mse: 2355.5918 - val_mae: 28.5602\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2689.9668 - mse: 2689.9670 - mae: 28.7066 - val_loss: 2359.8131 - val_mse: 2359.8130 - val_mae: 28.4842\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 547us/step - loss: 2730.1072 - mse: 2730.1077 - mae: 28.9461 - val_loss: 2359.7260 - val_mse: 2359.7258 - val_mae: 28.5828\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2747.7544 - mse: 2747.7546 - mae: 29.0108 - val_loss: 2365.3213 - val_mse: 2365.3213 - val_mae: 27.9098\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 539us/step - loss: 2677.8882 - mse: 2677.8882 - mae: 28.4294 - val_loss: 2361.0066 - val_mse: 2361.0068 - val_mae: 28.0728\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2664.4374 - mse: 2664.4358 - mae: 28.3871 - val_loss: 2352.4291 - val_mse: 2352.4292 - val_mae: 28.5748\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2679.2956 - mse: 2679.2947 - mae: 28.5170 - val_loss: 2342.8408 - val_mse: 2342.8406 - val_mae: 28.1608\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2714.0827 - mse: 2714.0833 - mae: 28.7976 - val_loss: 2346.3326 - val_mse: 2346.3328 - val_mae: 27.6748\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 535us/step - loss: 2727.8804 - mse: 2727.8799 - mae: 28.9056 - val_loss: 2345.2548 - val_mse: 2345.2551 - val_mae: 28.1206\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 546us/step - loss: 2679.1443 - mse: 2679.1440 - mae: 28.5547 - val_loss: 2346.5019 - val_mse: 2346.5015 - val_mae: 28.1334\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2680.1482 - mse: 2680.1484 - mae: 28.1858 - val_loss: 2348.1041 - val_mse: 2348.1040 - val_mae: 28.2180\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2688.2202 - mse: 2688.2207 - mae: 28.2585 - val_loss: 2345.9866 - val_mse: 2345.9866 - val_mae: 28.3608\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2724.1304 - mse: 2724.1304 - mae: 28.6079 - val_loss: 2344.0470 - val_mse: 2344.0471 - val_mae: 27.6643\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2680.5132 - mse: 2680.5127 - mae: 28.3019 - val_loss: 2345.8724 - val_mse: 2345.8728 - val_mae: 28.3537\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2661.5087 - mse: 2661.5088 - mae: 28.5696 - val_loss: 2348.1002 - val_mse: 2348.1003 - val_mae: 28.3454\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 524us/step - loss: 2739.9175 - mse: 2739.9175 - mae: 28.7460 - val_loss: 2345.7028 - val_mse: 2345.7029 - val_mae: 27.8217\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 512us/step - loss: 2725.5266 - mse: 2725.5266 - mae: 28.7392 - val_loss: 2349.8111 - val_mse: 2349.8110 - val_mae: 27.6847\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 665us/step - loss: 2652.3664 - mse: 2652.3677 - mae: 28.2332 - val_loss: 2345.3116 - val_mse: 2345.3115 - val_mae: 28.4264\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2683.6411 - mse: 2683.6404 - mae: 28.5119 - val_loss: 2351.1145 - val_mse: 2351.1143 - val_mae: 28.0306\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2684.0677 - mse: 2684.0669 - mae: 28.3963 - val_loss: 2340.5520 - val_mse: 2340.5522 - val_mae: 28.2263\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2683.9886 - mse: 2683.9880 - mae: 28.4565 - val_loss: 2347.9658 - val_mse: 2347.9661 - val_mae: 28.0299\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2677.3965 - mse: 2677.3965 - mae: 28.2261 - val_loss: 2353.1302 - val_mse: 2353.1304 - val_mae: 28.2950\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2669.4009 - mse: 2669.4009 - mae: 28.2168 - val_loss: 2355.6397 - val_mse: 2355.6399 - val_mae: 27.8845\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2700.5394 - mse: 2700.5388 - mae: 28.6976 - val_loss: 2349.5049 - val_mse: 2349.5046 - val_mae: 27.8104\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2638.4999 - mse: 2638.5007 - mae: 27.9964 - val_loss: 2346.4042 - val_mse: 2346.4043 - val_mae: 28.0439\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2692.8082 - mse: 2692.8083 - mae: 28.4629 - val_loss: 2343.9020 - val_mse: 2343.9023 - val_mae: 27.8117\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2671.5900 - mse: 2671.5908 - mae: 28.4499 - val_loss: 2348.7439 - val_mse: 2348.7439 - val_mae: 28.1860\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2723.9410 - mse: 2723.9404 - mae: 29.0577 - val_loss: 2353.5274 - val_mse: 2353.5273 - val_mae: 28.4629\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2656.6424 - mse: 2656.6428 - mae: 28.2667 - val_loss: 2350.1016 - val_mse: 2350.1016 - val_mae: 28.2441\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2688.6994 - mse: 2688.6990 - mae: 28.4760 - val_loss: 2346.0191 - val_mse: 2346.0186 - val_mae: 28.3125\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 664us/step - loss: 2669.8567 - mse: 2669.8550 - mae: 28.8190 - val_loss: 2339.5077 - val_mse: 2339.5081 - val_mae: 28.4696\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2658.2317 - mse: 2658.2307 - mae: 28.5265 - val_loss: 2335.6842 - val_mse: 2335.6841 - val_mae: 28.3147\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2660.4755 - mse: 2660.4758 - mae: 28.1218 - val_loss: 2329.9856 - val_mse: 2329.9861 - val_mae: 28.0618\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2660.3552 - mse: 2660.3550 - mae: 28.4135 - val_loss: 2331.9716 - val_mse: 2331.9717 - val_mae: 27.8462\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2680.2772 - mse: 2680.2769 - mae: 28.5693 - val_loss: 2344.0944 - val_mse: 2344.0942 - val_mae: 28.4918\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2687.0621 - mse: 2687.0623 - mae: 28.2738 - val_loss: 2347.2020 - val_mse: 2347.2019 - val_mae: 28.5520\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 493us/step - loss: 2699.8198 - mse: 2699.8188 - mae: 28.8874 - val_loss: 2346.2439 - val_mse: 2346.2439 - val_mae: 28.4668\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2691.3512 - mse: 2691.3506 - mae: 28.4288 - val_loss: 2341.9719 - val_mse: 2341.9722 - val_mae: 28.4419\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 663us/step - loss: 2721.3791 - mse: 2721.3787 - mae: 28.5430 - val_loss: 2343.3105 - val_mse: 2343.3105 - val_mae: 27.8919\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 704us/step - loss: 2686.2085 - mse: 2686.2085 - mae: 28.2838 - val_loss: 2344.4534 - val_mse: 2344.4536 - val_mae: 27.6234\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2682.4898 - mse: 2682.4893 - mae: 28.3523 - val_loss: 2340.6750 - val_mse: 2340.6748 - val_mae: 28.0798\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2706.4722 - mse: 2706.4727 - mae: 28.5733 - val_loss: 2347.1816 - val_mse: 2347.1816 - val_mae: 27.8609\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2656.6688 - mse: 2656.6682 - mae: 28.0014 - val_loss: 2339.0892 - val_mse: 2339.0891 - val_mae: 27.8518\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2709.3957 - mse: 2709.3948 - mae: 28.5964 - val_loss: 2341.1164 - val_mse: 2341.1165 - val_mae: 27.6576\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2678.9352 - mse: 2678.9346 - mae: 28.5501 - val_loss: 2336.6599 - val_mse: 2336.6597 - val_mae: 28.2632\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2696.0617 - mse: 2696.0615 - mae: 28.5339 - val_loss: 2339.5914 - val_mse: 2339.5916 - val_mae: 28.3143\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2718.0096 - mse: 2718.0095 - mae: 28.5671 - val_loss: 2350.5988 - val_mse: 2350.5986 - val_mae: 28.7709\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 659us/step - loss: 2663.8406 - mse: 2663.8413 - mae: 28.0849 - val_loss: 2349.3475 - val_mse: 2349.3477 - val_mae: 28.6215\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2719.9169 - mse: 2719.9167 - mae: 28.5417 - val_loss: 2351.1171 - val_mse: 2351.1172 - val_mae: 28.4804\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2702.7949 - mse: 2702.7952 - mae: 28.5561 - val_loss: 2354.7202 - val_mse: 2354.7202 - val_mae: 28.3177\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2615.6351 - mse: 2615.6355 - mae: 28.0339 - val_loss: 2351.8202 - val_mse: 2351.8196 - val_mae: 28.4245\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2651.9714 - mse: 2651.9717 - mae: 28.0261 - val_loss: 2350.9618 - val_mse: 2350.9619 - val_mae: 28.4288\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2752.5110 - mse: 2752.5117 - mae: 28.9305 - val_loss: 2351.4578 - val_mse: 2351.4578 - val_mae: 28.1061\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2688.1625 - mse: 2688.1619 - mae: 28.2336 - val_loss: 2356.2838 - val_mse: 2356.2839 - val_mae: 28.1008\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2712.0947 - mse: 2712.0938 - mae: 28.6622 - val_loss: 2354.6230 - val_mse: 2354.6228 - val_mae: 27.7609\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 512us/step - loss: 2704.0560 - mse: 2704.0554 - mae: 28.1964 - val_loss: 2351.0106 - val_mse: 2351.0107 - val_mae: 28.5591\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 383us/step - loss: 2626.1085 - mse: 2626.1096 - mae: 28.1618 - val_loss: 2344.0868 - val_mse: 2344.0862 - val_mae: 28.3772\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 488us/step - loss: 2672.4044 - mse: 2672.4048 - mae: 28.5276 - val_loss: 2335.4721 - val_mse: 2335.4722 - val_mae: 28.0969\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 13318.5241 - mse: 13318.5244 - mae: 109.8579 - val_loss: 34603.0871 - val_mse: 34603.0859 - val_mae: 132.6756\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 13161.8027 - mse: 13161.8037 - mae: 109.1514 - val_loss: 34327.9775 - val_mse: 34327.9766 - val_mae: 131.6493\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 12767.4952 - mse: 12767.4951 - mae: 107.3648 - val_loss: 33565.3401 - val_mse: 33565.3398 - val_mae: 128.7667\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 11727.4150 - mse: 11727.4160 - mae: 102.3914 - val_loss: 31610.2689 - val_mse: 31610.2695 - val_mae: 121.0551\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 559us/step - loss: 9344.9025 - mse: 9344.9023 - mae: 89.5251 - val_loss: 26928.1569 - val_mse: 26928.1562 - val_mae: 100.0875\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 4962.8477 - mse: 4962.8477 - mae: 58.3462 - val_loss: 19782.4659 - val_mse: 19782.4668 - val_mae: 55.2953\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 685us/step - loss: 2813.8939 - mse: 2813.8936 - mae: 38.4760 - val_loss: 17999.8682 - val_mse: 17999.8672 - val_mae: 40.4522\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 2919.5615 - mse: 2919.5613 - mae: 38.1312 - val_loss: 18461.1699 - val_mse: 18461.1699 - val_mae: 43.7658\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 638us/step - loss: 2546.8447 - mse: 2546.8447 - mae: 37.0724 - val_loss: 18119.2294 - val_mse: 18119.2305 - val_mae: 40.4237\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2771.1970 - mse: 2771.1970 - mae: 37.9791 - val_loss: 18241.4240 - val_mse: 18241.4238 - val_mae: 41.0447\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2565.9243 - mse: 2565.9243 - mae: 35.7451 - val_loss: 18056.4621 - val_mse: 18056.4629 - val_mae: 39.1887\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 2530.1770 - mse: 2530.1770 - mae: 35.4811 - val_loss: 17941.9023 - val_mse: 17941.9023 - val_mae: 38.1923\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2649.4660 - mse: 2649.4663 - mae: 36.6820 - val_loss: 18157.8120 - val_mse: 18157.8125 - val_mae: 39.4000\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 680us/step - loss: 2496.6427 - mse: 2496.6426 - mae: 36.7565 - val_loss: 17950.9719 - val_mse: 17950.9727 - val_mae: 37.9946\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 2594.2089 - mse: 2594.2085 - mae: 36.2788 - val_loss: 17909.6088 - val_mse: 17909.6094 - val_mae: 37.6992\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 684us/step - loss: 2557.4668 - mse: 2557.4666 - mae: 36.5474 - val_loss: 17829.7081 - val_mse: 17829.7070 - val_mae: 37.2852\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 681us/step - loss: 2502.3605 - mse: 2502.3606 - mae: 34.6814 - val_loss: 17794.9789 - val_mse: 17794.9785 - val_mae: 37.1168\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2362.0024 - mse: 2362.0022 - mae: 34.8135 - val_loss: 17755.4038 - val_mse: 17755.4043 - val_mae: 36.9682\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2444.0078 - mse: 2444.0078 - mae: 36.0000 - val_loss: 17878.9121 - val_mse: 17878.9121 - val_mae: 37.4427\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 484us/step - loss: 2380.1723 - mse: 2380.1726 - mae: 34.4143 - val_loss: 17792.2433 - val_mse: 17792.2441 - val_mae: 37.0859\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 2553.3332 - mse: 2553.3333 - mae: 35.3755 - val_loss: 18014.5711 - val_mse: 18014.5723 - val_mae: 38.0654\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 2237.8151 - mse: 2237.8147 - mae: 33.0313 - val_loss: 17887.0042 - val_mse: 17887.0039 - val_mae: 37.4440\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 539us/step - loss: 2373.5105 - mse: 2373.5105 - mae: 35.2320 - val_loss: 17754.5218 - val_mse: 17754.5234 - val_mae: 36.9271\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 638us/step - loss: 2181.9764 - mse: 2181.9763 - mae: 34.3461 - val_loss: 17859.7395 - val_mse: 17859.7402 - val_mae: 37.3175\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2231.0139 - mse: 2231.0137 - mae: 32.9891 - val_loss: 17782.1995 - val_mse: 17782.1992 - val_mae: 37.0135\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 2189.5784 - mse: 2189.5784 - mae: 31.0926 - val_loss: 17741.6478 - val_mse: 17741.6484 - val_mae: 36.8839\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 2205.4328 - mse: 2205.4329 - mae: 33.0494 - val_loss: 17691.0861 - val_mse: 17691.0859 - val_mae: 36.7410\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 722us/step - loss: 2284.8398 - mse: 2284.8403 - mae: 33.2066 - val_loss: 18030.6498 - val_mse: 18030.6504 - val_mae: 38.1039\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2203.7854 - mse: 2203.7854 - mae: 34.1107 - val_loss: 17679.1844 - val_mse: 17679.1836 - val_mae: 36.6973\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 638us/step - loss: 2356.8801 - mse: 2356.8804 - mae: 33.5332 - val_loss: 17850.4060 - val_mse: 17850.4082 - val_mae: 37.2492\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2093.9071 - mse: 2093.9070 - mae: 31.6338 - val_loss: 17502.1035 - val_mse: 17502.1035 - val_mae: 36.5696\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 2202.0192 - mse: 2202.0193 - mae: 32.9149 - val_loss: 17943.2550 - val_mse: 17943.2559 - val_mae: 37.6339\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 565us/step - loss: 2030.9733 - mse: 2030.9731 - mae: 31.5686 - val_loss: 17690.2200 - val_mse: 17690.2227 - val_mae: 36.7459\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2196.4437 - mse: 2196.4438 - mae: 32.6614 - val_loss: 17822.2986 - val_mse: 17822.3008 - val_mae: 37.1156\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2147.0838 - mse: 2147.0837 - mae: 32.0026 - val_loss: 17617.8021 - val_mse: 17617.8027 - val_mae: 36.6822\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2220.7360 - mse: 2220.7361 - mae: 33.2556 - val_loss: 17703.9578 - val_mse: 17703.9570 - val_mae: 36.7388\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 487us/step - loss: 2088.4275 - mse: 2088.4275 - mae: 31.4792 - val_loss: 17858.3218 - val_mse: 17858.3223 - val_mae: 37.2338\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 489us/step - loss: 2284.6060 - mse: 2284.6062 - mae: 34.0954 - val_loss: 17904.6098 - val_mse: 17904.6094 - val_mae: 37.4210\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2056.0293 - mse: 2056.0291 - mae: 32.5122 - val_loss: 17846.2529 - val_mse: 17846.2539 - val_mae: 37.1822\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 2032.6524 - mse: 2032.6523 - mae: 31.5505 - val_loss: 17918.2730 - val_mse: 17918.2715 - val_mae: 37.4822\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 471us/step - loss: 2205.9997 - mse: 2205.9995 - mae: 32.3983 - val_loss: 17816.2981 - val_mse: 17816.3008 - val_mae: 37.0575\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 641us/step - loss: 2279.4243 - mse: 2279.4243 - mae: 32.9307 - val_loss: 17772.5268 - val_mse: 17772.5273 - val_mae: 36.9063\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 651us/step - loss: 2249.8167 - mse: 2249.8167 - mae: 31.6880 - val_loss: 17916.3148 - val_mse: 17916.3164 - val_mae: 37.4656\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2052.3654 - mse: 2052.3652 - mae: 31.9824 - val_loss: 17894.0843 - val_mse: 17894.0840 - val_mae: 37.3834\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 484us/step - loss: 2100.0795 - mse: 2100.0798 - mae: 31.7893 - val_loss: 17598.8082 - val_mse: 17598.8086 - val_mae: 36.6685\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 464us/step - loss: 2248.0663 - mse: 2248.0662 - mae: 32.8147 - val_loss: 17714.4601 - val_mse: 17714.4570 - val_mae: 36.7358\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2162.4519 - mse: 2162.4519 - mae: 32.3979 - val_loss: 17766.0347 - val_mse: 17766.0332 - val_mae: 36.8212\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 482us/step - loss: 2287.7664 - mse: 2287.7666 - mae: 33.8358 - val_loss: 17815.8824 - val_mse: 17815.8828 - val_mae: 37.0001\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 1971.5695 - mse: 1971.5696 - mae: 31.4693 - val_loss: 17660.8665 - val_mse: 17660.8652 - val_mae: 36.6665\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 710us/step - loss: 1995.1125 - mse: 1995.1124 - mae: 31.3880 - val_loss: 17677.0299 - val_mse: 17677.0273 - val_mae: 36.6724\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2017.1153 - mse: 2017.1152 - mae: 30.6660 - val_loss: 17662.2017 - val_mse: 17662.2031 - val_mae: 36.6268\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 1761.8112 - mse: 1761.8113 - mae: 30.0997 - val_loss: 17793.2166 - val_mse: 17793.2188 - val_mae: 36.9025\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2019.7304 - mse: 2019.7305 - mae: 30.5768 - val_loss: 17486.3856 - val_mse: 17486.3867 - val_mae: 36.5060\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2016.9766 - mse: 2016.9767 - mae: 30.7875 - val_loss: 17671.8433 - val_mse: 17671.8418 - val_mae: 36.5549\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 760us/step - loss: 2013.3475 - mse: 2013.3475 - mae: 30.4428 - val_loss: 17850.4222 - val_mse: 17850.4238 - val_mae: 37.1013\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2022.5324 - mse: 2022.5326 - mae: 30.7843 - val_loss: 17677.6840 - val_mse: 17677.6836 - val_mae: 36.5729\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 629us/step - loss: 2050.6603 - mse: 2050.6604 - mae: 31.4054 - val_loss: 17641.7704 - val_mse: 17641.7695 - val_mae: 36.5238\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 2109.4602 - mse: 2109.4600 - mae: 31.3432 - val_loss: 17721.7293 - val_mse: 17721.7305 - val_mae: 36.6818\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2113.1344 - mse: 2113.1345 - mae: 31.2193 - val_loss: 17755.4564 - val_mse: 17755.4570 - val_mae: 36.7684\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 628us/step - loss: 1999.7134 - mse: 1999.7134 - mae: 31.0816 - val_loss: 17731.8424 - val_mse: 17731.8438 - val_mae: 36.7077\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2085.8371 - mse: 2085.8372 - mae: 30.8423 - val_loss: 17647.4480 - val_mse: 17647.4473 - val_mae: 36.6272\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 1853.6449 - mse: 1853.6449 - mae: 29.3811 - val_loss: 17698.8888 - val_mse: 17698.8887 - val_mae: 36.6553\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 2045.7818 - mse: 2045.7819 - mae: 31.0857 - val_loss: 17699.0692 - val_mse: 17699.0703 - val_mae: 36.6574\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2058.0724 - mse: 2058.0725 - mae: 30.8558 - val_loss: 17667.4177 - val_mse: 17667.4160 - val_mae: 36.6067\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2071.8768 - mse: 2071.8770 - mae: 31.1963 - val_loss: 17843.7568 - val_mse: 17843.7578 - val_mae: 37.0271\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 1915.7043 - mse: 1915.7043 - mae: 29.7514 - val_loss: 17767.5926 - val_mse: 17767.5918 - val_mae: 36.7755\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 1840.3012 - mse: 1840.3014 - mae: 29.7925 - val_loss: 17634.5801 - val_mse: 17634.5801 - val_mae: 36.6239\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2039.3398 - mse: 2039.3397 - mae: 31.1292 - val_loss: 17647.6485 - val_mse: 17647.6484 - val_mae: 36.6236\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 1969.0218 - mse: 1969.0217 - mae: 30.6520 - val_loss: 17699.8513 - val_mse: 17699.8516 - val_mae: 36.6569\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2096.7624 - mse: 2096.7625 - mae: 31.3467 - val_loss: 17805.5331 - val_mse: 17805.5332 - val_mae: 36.9105\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2122.0926 - mse: 2122.0925 - mae: 30.6455 - val_loss: 17820.7323 - val_mse: 17820.7305 - val_mae: 36.9460\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 1855.5227 - mse: 1855.5227 - mae: 29.3930 - val_loss: 17652.6691 - val_mse: 17652.6680 - val_mae: 36.5985\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 1968.0547 - mse: 1968.0549 - mae: 30.4467 - val_loss: 17713.6089 - val_mse: 17713.6074 - val_mae: 36.6427\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 1968.0011 - mse: 1968.0010 - mae: 29.6784 - val_loss: 17656.2666 - val_mse: 17656.2676 - val_mae: 36.6023\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 1769.0991 - mse: 1769.0991 - mae: 28.6868 - val_loss: 17581.6785 - val_mse: 17581.6777 - val_mae: 36.6144\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 1889.1202 - mse: 1889.1205 - mae: 29.0785 - val_loss: 17598.0574 - val_mse: 17598.0586 - val_mae: 36.5772\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 1955.9155 - mse: 1955.9155 - mae: 30.0472 - val_loss: 17828.9782 - val_mse: 17828.9785 - val_mae: 36.9595\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 1952.2170 - mse: 1952.2173 - mae: 29.2221 - val_loss: 17711.6019 - val_mse: 17711.6016 - val_mae: 36.6909\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 1857.4885 - mse: 1857.4884 - mae: 30.1766 - val_loss: 17581.5601 - val_mse: 17581.5605 - val_mae: 36.6281\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 2021.0258 - mse: 2021.0258 - mae: 30.6283 - val_loss: 17814.6436 - val_mse: 17814.6445 - val_mae: 36.9023\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4321.2435 - mse: 4321.2441 - mae: 34.4585 - val_loss: 2229.6385 - val_mse: 2229.6384 - val_mae: 31.4252\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4344.9830 - mse: 4344.9834 - mae: 34.9063 - val_loss: 2278.1817 - val_mse: 2278.1814 - val_mae: 31.5971\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4087.0906 - mse: 4087.0903 - mae: 35.1085 - val_loss: 2339.0106 - val_mse: 2339.0105 - val_mae: 31.7876\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 4346.0201 - mse: 4346.0190 - mae: 34.9776 - val_loss: 2354.8288 - val_mse: 2354.8289 - val_mae: 31.8133\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4008.7523 - mse: 4008.7522 - mae: 33.8097 - val_loss: 2286.5217 - val_mse: 2286.5217 - val_mae: 31.5988\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4196.1802 - mse: 4196.1797 - mae: 34.9936 - val_loss: 2398.0199 - val_mse: 2398.0200 - val_mae: 31.9786\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4260.8517 - mse: 4260.8516 - mae: 35.4600 - val_loss: 2375.2355 - val_mse: 2375.2358 - val_mae: 31.9271\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4265.7541 - mse: 4265.7534 - mae: 34.5805 - val_loss: 2369.8847 - val_mse: 2369.8845 - val_mae: 31.9038\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 452us/step - loss: 4015.9295 - mse: 4015.9302 - mae: 34.0598 - val_loss: 2343.6709 - val_mse: 2343.6707 - val_mae: 31.8399\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4154.8787 - mse: 4154.8789 - mae: 34.3258 - val_loss: 2382.3171 - val_mse: 2382.3171 - val_mae: 31.9563\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 446us/step - loss: 4162.1272 - mse: 4162.1274 - mae: 34.3726 - val_loss: 2356.5062 - val_mse: 2356.5061 - val_mae: 31.8793\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 513us/step - loss: 3948.0527 - mse: 3948.0527 - mae: 32.5349 - val_loss: 2312.1141 - val_mse: 2312.1140 - val_mae: 31.7497\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 4183.5496 - mse: 4183.5493 - mae: 34.9661 - val_loss: 2419.2756 - val_mse: 2419.2756 - val_mae: 32.1242\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4139.2225 - mse: 4139.2227 - mae: 34.6619 - val_loss: 2344.6209 - val_mse: 2344.6208 - val_mae: 31.8388\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4123.4615 - mse: 4123.4609 - mae: 33.4671 - val_loss: 2449.5982 - val_mse: 2449.5979 - val_mae: 32.2544\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4170.6538 - mse: 4170.6538 - mae: 33.7949 - val_loss: 2402.5268 - val_mse: 2402.5266 - val_mae: 32.0732\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4211.9698 - mse: 4211.9697 - mae: 34.4177 - val_loss: 2319.6909 - val_mse: 2319.6907 - val_mae: 31.7810\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4259.3904 - mse: 4259.3906 - mae: 34.5714 - val_loss: 2400.4735 - val_mse: 2400.4731 - val_mae: 32.0607\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4149.5332 - mse: 4149.5327 - mae: 33.6378 - val_loss: 2320.8930 - val_mse: 2320.8931 - val_mae: 31.7717\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 3985.5212 - mse: 3985.5215 - mae: 34.1107 - val_loss: 2345.7936 - val_mse: 2345.7937 - val_mae: 31.8903\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 658us/step - loss: 4200.0413 - mse: 4200.0415 - mae: 34.0288 - val_loss: 2375.6029 - val_mse: 2375.6028 - val_mae: 31.9634\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4172.8175 - mse: 4172.8174 - mae: 34.3029 - val_loss: 2342.9312 - val_mse: 2342.9312 - val_mae: 31.8487\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4148.9369 - mse: 4148.9375 - mae: 33.5914 - val_loss: 2338.8343 - val_mse: 2338.8342 - val_mae: 31.8269\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4016.1884 - mse: 4016.1882 - mae: 33.9240 - val_loss: 2437.5711 - val_mse: 2437.5710 - val_mae: 32.1955\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4094.4882 - mse: 4094.4878 - mae: 33.9941 - val_loss: 2334.0014 - val_mse: 2334.0015 - val_mae: 31.8399\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4298.1923 - mse: 4298.1924 - mae: 34.4718 - val_loss: 2391.2068 - val_mse: 2391.2068 - val_mae: 32.0135\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 4163.2046 - mse: 4163.2046 - mae: 33.5499 - val_loss: 2434.4585 - val_mse: 2434.4587 - val_mae: 32.1923\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4014.3640 - mse: 4014.3640 - mae: 32.6942 - val_loss: 2315.9794 - val_mse: 2315.9795 - val_mae: 31.8001\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4068.9721 - mse: 4068.9717 - mae: 33.2929 - val_loss: 2320.6436 - val_mse: 2320.6438 - val_mae: 31.8221\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4053.2779 - mse: 4053.2778 - mae: 33.2219 - val_loss: 2339.7280 - val_mse: 2339.7280 - val_mae: 31.9100\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4134.6296 - mse: 4134.6299 - mae: 33.7783 - val_loss: 2384.2153 - val_mse: 2384.2153 - val_mae: 32.0708\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 4094.2411 - mse: 4094.2405 - mae: 33.1326 - val_loss: 2369.8048 - val_mse: 2369.8044 - val_mae: 32.0253\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 4122.5396 - mse: 4122.5391 - mae: 33.4563 - val_loss: 2424.4347 - val_mse: 2424.4348 - val_mae: 32.2072\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 4121.5703 - mse: 4121.5708 - mae: 33.7093 - val_loss: 2307.8340 - val_mse: 2307.8340 - val_mae: 31.7905\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 4061.5489 - mse: 4061.5491 - mae: 33.5287 - val_loss: 2344.2675 - val_mse: 2344.2676 - val_mae: 31.9205\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 535us/step - loss: 4198.3786 - mse: 4198.3789 - mae: 34.1669 - val_loss: 2393.9128 - val_mse: 2393.9131 - val_mae: 32.0635\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4102.6424 - mse: 4102.6421 - mae: 33.9455 - val_loss: 2358.4759 - val_mse: 2358.4758 - val_mae: 31.9352\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 697us/step - loss: 4191.8509 - mse: 4191.8511 - mae: 33.7680 - val_loss: 2371.6483 - val_mse: 2371.6482 - val_mae: 31.9711\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4065.2075 - mse: 4065.2075 - mae: 33.5705 - val_loss: 2422.0039 - val_mse: 2422.0039 - val_mae: 32.1675\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 515us/step - loss: 4003.8748 - mse: 4003.8748 - mae: 32.6535 - val_loss: 2333.6587 - val_mse: 2333.6584 - val_mae: 31.8660\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 489us/step - loss: 3983.2616 - mse: 3983.2625 - mae: 32.7115 - val_loss: 2337.9517 - val_mse: 2337.9517 - val_mae: 31.8726\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 512us/step - loss: 4093.9546 - mse: 4093.9548 - mae: 33.2957 - val_loss: 2359.2326 - val_mse: 2359.2324 - val_mae: 31.9617\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 454us/step - loss: 4090.9587 - mse: 4090.9592 - mae: 33.1407 - val_loss: 2328.9887 - val_mse: 2328.9888 - val_mae: 31.8506\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 3997.6168 - mse: 3997.6162 - mae: 32.4194 - val_loss: 2287.4008 - val_mse: 2287.4006 - val_mae: 31.7082\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 661us/step - loss: 3985.2788 - mse: 3985.2783 - mae: 33.4147 - val_loss: 2345.6014 - val_mse: 2345.6013 - val_mae: 31.8906\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 3916.2544 - mse: 3916.2542 - mae: 33.4046 - val_loss: 2352.5654 - val_mse: 2352.5654 - val_mae: 31.9468\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4047.7674 - mse: 4047.7681 - mae: 32.8140 - val_loss: 2350.6678 - val_mse: 2350.6677 - val_mae: 31.9501\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4154.4634 - mse: 4154.4634 - mae: 34.1210 - val_loss: 2369.4325 - val_mse: 2369.4326 - val_mae: 31.9989\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4105.9597 - mse: 4105.9590 - mae: 33.4561 - val_loss: 2338.3306 - val_mse: 2338.3303 - val_mae: 31.8871\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 3953.6339 - mse: 3953.6333 - mae: 33.6736 - val_loss: 2331.7590 - val_mse: 2331.7588 - val_mae: 31.8774\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 680us/step - loss: 4006.0090 - mse: 4006.0088 - mae: 32.7893 - val_loss: 2315.8887 - val_mse: 2315.8889 - val_mae: 31.8135\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4017.9254 - mse: 4017.9250 - mae: 33.2301 - val_loss: 2333.7708 - val_mse: 2333.7710 - val_mae: 31.8675\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4065.9248 - mse: 4065.9255 - mae: 33.0310 - val_loss: 2371.2685 - val_mse: 2371.2686 - val_mae: 31.9941\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 658us/step - loss: 4006.2409 - mse: 4006.2410 - mae: 32.4704 - val_loss: 2360.6345 - val_mse: 2360.6345 - val_mae: 31.9674\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 3976.8686 - mse: 3976.8684 - mae: 32.3729 - val_loss: 2394.4203 - val_mse: 2394.4204 - val_mae: 32.0674\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4167.2210 - mse: 4167.2212 - mae: 33.6123 - val_loss: 2352.8995 - val_mse: 2352.8994 - val_mae: 31.9246\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 3777.0314 - mse: 3777.0320 - mae: 32.5357 - val_loss: 2310.2133 - val_mse: 2310.2131 - val_mae: 31.7921\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 3827.9218 - mse: 3827.9216 - mae: 32.5897 - val_loss: 2293.6531 - val_mse: 2293.6533 - val_mae: 31.7145\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 614us/step - loss: 3992.4249 - mse: 3992.4250 - mae: 32.2201 - val_loss: 2292.8654 - val_mse: 2292.8655 - val_mae: 31.7090\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 671us/step - loss: 3907.7227 - mse: 3907.7227 - mae: 32.6985 - val_loss: 2398.7484 - val_mse: 2398.7483 - val_mae: 32.0955\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 3879.9468 - mse: 3879.9465 - mae: 32.1922 - val_loss: 2326.2408 - val_mse: 2326.2407 - val_mae: 31.8534\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4093.5923 - mse: 4093.5923 - mae: 34.6846 - val_loss: 2396.3613 - val_mse: 2396.3613 - val_mae: 32.0815\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 533us/step - loss: 3886.1848 - mse: 3886.1851 - mae: 32.3704 - val_loss: 2385.3127 - val_mse: 2385.3130 - val_mae: 32.0588\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 3915.1732 - mse: 3915.1729 - mae: 33.1142 - val_loss: 2303.5139 - val_mse: 2303.5137 - val_mae: 31.7742\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 568us/step - loss: 3868.2000 - mse: 3868.2004 - mae: 32.2550 - val_loss: 2261.3648 - val_mse: 2261.3647 - val_mae: 31.6571\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 3961.0101 - mse: 3961.0105 - mae: 32.7421 - val_loss: 2374.7324 - val_mse: 2374.7322 - val_mae: 32.0504\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 3962.4710 - mse: 3962.4719 - mae: 31.9631 - val_loss: 2359.9399 - val_mse: 2359.9399 - val_mae: 31.9970\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4231.2438 - mse: 4231.2441 - mae: 34.1369 - val_loss: 2382.4154 - val_mse: 2382.4158 - val_mae: 32.0602\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4086.7288 - mse: 4086.7288 - mae: 33.3018 - val_loss: 2336.2090 - val_mse: 2336.2090 - val_mae: 31.8941\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4013.4008 - mse: 4013.4014 - mae: 32.6094 - val_loss: 2310.8808 - val_mse: 2310.8809 - val_mae: 31.7908\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 535us/step - loss: 3994.4365 - mse: 3994.4363 - mae: 32.7428 - val_loss: 2365.0577 - val_mse: 2365.0581 - val_mae: 31.9835\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4147.5062 - mse: 4147.5063 - mae: 33.1590 - val_loss: 2360.5479 - val_mse: 2360.5479 - val_mae: 31.9607\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4125.6631 - mse: 4125.6636 - mae: 33.9313 - val_loss: 2359.8668 - val_mse: 2359.8667 - val_mae: 31.9607\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 531us/step - loss: 4065.4997 - mse: 4065.4993 - mae: 33.2104 - val_loss: 2373.7702 - val_mse: 2373.7703 - val_mae: 31.9772\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4104.9979 - mse: 4104.9980 - mae: 33.5589 - val_loss: 2317.4395 - val_mse: 2317.4395 - val_mae: 31.7883\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 3955.6542 - mse: 3955.6545 - mae: 31.7712 - val_loss: 2358.3803 - val_mse: 2358.3804 - val_mae: 31.9308\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 526us/step - loss: 4005.0305 - mse: 4005.0300 - mae: 31.9805 - val_loss: 2358.3275 - val_mse: 2358.3274 - val_mae: 31.9344\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4100.6744 - mse: 4100.6738 - mae: 33.1852 - val_loss: 2342.6545 - val_mse: 2342.6545 - val_mae: 31.9083\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 4095.3138 - mse: 4095.3137 - mae: 33.4086 - val_loss: 2364.3250 - val_mse: 2364.3250 - val_mae: 31.9953\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 515us/step - loss: 3731.0695 - mse: 3731.0691 - mae: 32.2983 - val_loss: 2354.3704 - val_mse: 2354.3706 - val_mae: 31.9629\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3438.6329 - mse: 3438.6321 - mae: 32.8044 - val_loss: 1443.0992 - val_mse: 1443.0992 - val_mae: 25.6976\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3431.5867 - mse: 3431.5862 - mae: 32.2276 - val_loss: 1443.2016 - val_mse: 1443.2017 - val_mae: 25.2722\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3293.5223 - mse: 3293.5232 - mae: 31.6825 - val_loss: 1437.5398 - val_mse: 1437.5399 - val_mae: 25.6554\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3359.4718 - mse: 3359.4717 - mae: 32.5228 - val_loss: 1433.4069 - val_mse: 1433.4069 - val_mae: 25.1746\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3275.6479 - mse: 3275.6475 - mae: 31.9739 - val_loss: 1434.0070 - val_mse: 1434.0070 - val_mae: 25.4807\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3242.3406 - mse: 3242.3391 - mae: 31.3087 - val_loss: 1438.5882 - val_mse: 1438.5884 - val_mae: 26.1342\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3310.2510 - mse: 3310.2507 - mae: 32.0397 - val_loss: 1435.1284 - val_mse: 1435.1282 - val_mae: 25.8668\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3327.9775 - mse: 3327.9775 - mae: 32.1816 - val_loss: 1433.6521 - val_mse: 1433.6521 - val_mae: 24.9584\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3365.3419 - mse: 3365.3425 - mae: 31.8323 - val_loss: 1433.2226 - val_mse: 1433.2225 - val_mae: 25.5678\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 706us/step - loss: 3390.9893 - mse: 3390.9897 - mae: 32.5940 - val_loss: 1433.9217 - val_mse: 1433.9216 - val_mae: 25.1570\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3234.5688 - mse: 3234.5681 - mae: 31.9982 - val_loss: 1434.3890 - val_mse: 1434.3889 - val_mae: 25.7164\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3303.8501 - mse: 3303.8501 - mae: 32.3094 - val_loss: 1434.1207 - val_mse: 1434.1206 - val_mae: 25.4749\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 664us/step - loss: 3330.0669 - mse: 3330.0676 - mae: 32.2061 - val_loss: 1435.9390 - val_mse: 1435.9392 - val_mae: 24.8339\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 664us/step - loss: 3395.6714 - mse: 3395.6716 - mae: 32.6118 - val_loss: 1435.4694 - val_mse: 1435.4694 - val_mae: 24.9283\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3467.5610 - mse: 3467.5608 - mae: 32.4574 - val_loss: 1438.9383 - val_mse: 1438.9384 - val_mae: 25.0008\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3356.1044 - mse: 3356.1042 - mae: 32.2945 - val_loss: 1441.2390 - val_mse: 1441.2388 - val_mae: 25.9451\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3228.7472 - mse: 3228.7478 - mae: 31.5933 - val_loss: 1438.5562 - val_mse: 1438.5562 - val_mae: 25.7290\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3217.6805 - mse: 3217.6807 - mae: 32.1553 - val_loss: 1437.8685 - val_mse: 1437.8685 - val_mae: 25.5232\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3240.0275 - mse: 3240.0273 - mae: 32.0933 - val_loss: 1436.4655 - val_mse: 1436.4655 - val_mae: 25.4285\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3389.3769 - mse: 3389.3762 - mae: 32.0632 - val_loss: 1438.2513 - val_mse: 1438.2513 - val_mae: 25.7246\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3328.6706 - mse: 3328.6699 - mae: 32.5732 - val_loss: 1436.9746 - val_mse: 1436.9746 - val_mae: 25.4076\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 898us/step - loss: 3315.5652 - mse: 3315.5657 - mae: 31.8193 - val_loss: 1439.7792 - val_mse: 1439.7793 - val_mae: 25.8548\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 665us/step - loss: 3248.2212 - mse: 3248.2205 - mae: 31.5937 - val_loss: 1438.5503 - val_mse: 1438.5503 - val_mae: 25.5011\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3235.7387 - mse: 3235.7385 - mae: 31.2625 - val_loss: 1438.8714 - val_mse: 1438.8715 - val_mae: 25.6322\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 686us/step - loss: 3344.0934 - mse: 3344.0930 - mae: 32.4837 - val_loss: 1438.9047 - val_mse: 1438.9048 - val_mae: 25.7736\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3342.0445 - mse: 3342.0442 - mae: 32.1135 - val_loss: 1436.8314 - val_mse: 1436.8312 - val_mae: 25.3359\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3226.2416 - mse: 3226.2415 - mae: 31.3025 - val_loss: 1442.9982 - val_mse: 1442.9982 - val_mae: 26.2849\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3355.1621 - mse: 3355.1619 - mae: 31.9215 - val_loss: 1438.6848 - val_mse: 1438.6848 - val_mae: 25.3816\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3237.1122 - mse: 3237.1123 - mae: 31.3202 - val_loss: 1440.4130 - val_mse: 1440.4131 - val_mae: 25.0009\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3303.5376 - mse: 3303.5369 - mae: 32.4077 - val_loss: 1441.7161 - val_mse: 1441.7161 - val_mae: 25.0912\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3366.1093 - mse: 3366.1086 - mae: 32.1048 - val_loss: 1441.4289 - val_mse: 1441.4290 - val_mae: 25.6287\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3267.2354 - mse: 3267.2361 - mae: 31.9121 - val_loss: 1441.5828 - val_mse: 1441.5829 - val_mae: 25.6331\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3272.8057 - mse: 3272.8066 - mae: 32.2617 - val_loss: 1440.4620 - val_mse: 1440.4623 - val_mae: 25.5932\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3269.6985 - mse: 3269.6987 - mae: 31.1761 - val_loss: 1439.2701 - val_mse: 1439.2703 - val_mae: 25.2914\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3267.1449 - mse: 3267.1445 - mae: 31.3673 - val_loss: 1445.6697 - val_mse: 1445.6698 - val_mae: 26.2737\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3200.2739 - mse: 3200.2732 - mae: 30.8812 - val_loss: 1439.5479 - val_mse: 1439.5479 - val_mae: 25.5102\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3263.3813 - mse: 3263.3809 - mae: 31.4738 - val_loss: 1440.3761 - val_mse: 1440.3760 - val_mae: 25.5023\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3341.1187 - mse: 3341.1182 - mae: 32.5298 - val_loss: 1440.4485 - val_mse: 1440.4487 - val_mae: 25.7928\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3281.2781 - mse: 3281.2786 - mae: 31.3362 - val_loss: 1437.9073 - val_mse: 1437.9072 - val_mae: 25.5520\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3184.7836 - mse: 3184.7830 - mae: 30.9443 - val_loss: 1437.9392 - val_mse: 1437.9393 - val_mae: 25.5924\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3253.6478 - mse: 3253.6477 - mae: 31.4056 - val_loss: 1440.5150 - val_mse: 1440.5150 - val_mae: 25.9470\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3301.4401 - mse: 3301.4395 - mae: 31.3966 - val_loss: 1439.0222 - val_mse: 1439.0222 - val_mae: 25.6786\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 659us/step - loss: 3282.9583 - mse: 3282.9590 - mae: 31.5756 - val_loss: 1439.3343 - val_mse: 1439.3346 - val_mae: 25.7047\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3271.0457 - mse: 3271.0454 - mae: 31.3382 - val_loss: 1440.1672 - val_mse: 1440.1671 - val_mae: 25.8441\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3300.1097 - mse: 3300.1101 - mae: 31.6675 - val_loss: 1442.1126 - val_mse: 1442.1127 - val_mae: 25.9190\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3266.9496 - mse: 3266.9492 - mae: 31.3264 - val_loss: 1446.7737 - val_mse: 1446.7737 - val_mae: 26.4449\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 483us/step - loss: 3293.6869 - mse: 3293.6870 - mae: 31.7529 - val_loss: 1440.9831 - val_mse: 1440.9829 - val_mae: 25.6420\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 495us/step - loss: 3215.3974 - mse: 3215.3977 - mae: 31.4467 - val_loss: 1447.7226 - val_mse: 1447.7225 - val_mae: 26.3735\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3288.3172 - mse: 3288.3181 - mae: 31.2960 - val_loss: 1441.3358 - val_mse: 1441.3357 - val_mae: 25.5545\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3259.8576 - mse: 3259.8569 - mae: 31.0028 - val_loss: 1445.2647 - val_mse: 1445.2645 - val_mae: 26.1197\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3187.4581 - mse: 3187.4583 - mae: 30.8534 - val_loss: 1442.6623 - val_mse: 1442.6622 - val_mae: 25.5666\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 472us/step - loss: 3311.7754 - mse: 3311.7759 - mae: 31.6868 - val_loss: 1443.5616 - val_mse: 1443.5616 - val_mae: 25.8286\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3207.6738 - mse: 3207.6733 - mae: 31.1870 - val_loss: 1442.7779 - val_mse: 1442.7780 - val_mae: 25.6870\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3269.2971 - mse: 3269.2976 - mae: 31.4888 - val_loss: 1444.6429 - val_mse: 1444.6428 - val_mae: 26.0100\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3230.6842 - mse: 3230.6851 - mae: 31.0688 - val_loss: 1447.4307 - val_mse: 1447.4307 - val_mae: 26.1944\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3261.2322 - mse: 3261.2322 - mae: 31.4600 - val_loss: 1443.1349 - val_mse: 1443.1350 - val_mae: 25.5195\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3230.9541 - mse: 3230.9539 - mae: 30.9605 - val_loss: 1444.7877 - val_mse: 1444.7877 - val_mae: 25.8213\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3334.7111 - mse: 3334.7109 - mae: 31.7980 - val_loss: 1444.7905 - val_mse: 1444.7904 - val_mae: 25.6896\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 496us/step - loss: 3288.7123 - mse: 3288.7117 - mae: 31.5704 - val_loss: 1448.2671 - val_mse: 1448.2671 - val_mae: 26.2636\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 536us/step - loss: 3192.4774 - mse: 3192.4775 - mae: 31.2137 - val_loss: 1449.4381 - val_mse: 1449.4381 - val_mae: 26.2246\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3297.4204 - mse: 3297.4207 - mae: 31.7231 - val_loss: 1447.2676 - val_mse: 1447.2677 - val_mae: 25.8875\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3219.2740 - mse: 3219.2747 - mae: 31.4780 - val_loss: 1446.4920 - val_mse: 1446.4919 - val_mae: 25.5940\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3134.2572 - mse: 3134.2576 - mae: 30.5625 - val_loss: 1445.8089 - val_mse: 1445.8088 - val_mae: 25.6350\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3364.8619 - mse: 3364.8616 - mae: 31.8432 - val_loss: 1448.0821 - val_mse: 1448.0820 - val_mae: 26.1222\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3096.9232 - mse: 3096.9231 - mae: 30.8084 - val_loss: 1452.6640 - val_mse: 1452.6639 - val_mae: 26.5460\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3147.0424 - mse: 3147.0422 - mae: 31.0880 - val_loss: 1449.0787 - val_mse: 1449.0787 - val_mae: 26.1469\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3329.3450 - mse: 3329.3447 - mae: 31.1042 - val_loss: 1448.6722 - val_mse: 1448.6720 - val_mae: 26.1605\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 531us/step - loss: 3131.4294 - mse: 3131.4287 - mae: 30.8369 - val_loss: 1453.0645 - val_mse: 1453.0645 - val_mae: 26.5037\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3213.7420 - mse: 3213.7417 - mae: 31.1128 - val_loss: 1450.0072 - val_mse: 1450.0072 - val_mae: 26.3271\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3252.8080 - mse: 3252.8083 - mae: 31.4167 - val_loss: 1447.0674 - val_mse: 1447.0674 - val_mae: 25.9411\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3300.2140 - mse: 3300.2134 - mae: 31.4057 - val_loss: 1445.6775 - val_mse: 1445.6775 - val_mae: 25.9133\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3180.4302 - mse: 3180.4307 - mae: 31.4990 - val_loss: 1446.1194 - val_mse: 1446.1195 - val_mae: 26.0524\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3355.9452 - mse: 3355.9458 - mae: 31.7409 - val_loss: 1445.4038 - val_mse: 1445.4037 - val_mae: 25.8960\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3171.8760 - mse: 3171.8762 - mae: 30.6121 - val_loss: 1443.9139 - val_mse: 1443.9139 - val_mae: 25.7142\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 531us/step - loss: 3265.2576 - mse: 3265.2578 - mae: 31.1674 - val_loss: 1446.9460 - val_mse: 1446.9458 - val_mae: 26.1077\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3316.3268 - mse: 3316.3267 - mae: 31.1104 - val_loss: 1444.7220 - val_mse: 1444.7223 - val_mae: 25.6988\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3287.9490 - mse: 3287.9485 - mae: 31.5952 - val_loss: 1445.0764 - val_mse: 1445.0764 - val_mae: 25.6929\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 534us/step - loss: 3167.7135 - mse: 3167.7134 - mae: 31.6285 - val_loss: 1445.8343 - val_mse: 1445.8344 - val_mae: 25.7303\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3250.0728 - mse: 3250.0723 - mae: 31.0010 - val_loss: 1445.4434 - val_mse: 1445.4435 - val_mae: 25.7416\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3248.2991 - mse: 3248.2986 - mae: 31.2552 - val_loss: 1446.1413 - val_mse: 1446.1412 - val_mae: 25.8605\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2980.0346 - mse: 2980.0352 - mae: 31.2150 - val_loss: 1058.6792 - val_mse: 1058.6792 - val_mae: 23.6954\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2857.5547 - mse: 2857.5557 - mae: 30.4684 - val_loss: 1056.4957 - val_mse: 1056.4956 - val_mae: 23.7989\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2834.9144 - mse: 2834.9133 - mae: 30.7183 - val_loss: 1065.4985 - val_mse: 1065.4984 - val_mae: 23.3627\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2940.3813 - mse: 2940.3811 - mae: 30.7172 - val_loss: 1056.1896 - val_mse: 1056.1896 - val_mae: 23.9254\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 528us/step - loss: 2852.8706 - mse: 2852.8708 - mae: 30.3691 - val_loss: 1056.2603 - val_mse: 1056.2604 - val_mae: 23.8711\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2867.2282 - mse: 2867.2280 - mae: 30.8063 - val_loss: 1060.3447 - val_mse: 1060.3447 - val_mae: 23.6101\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2784.1759 - mse: 2784.1765 - mae: 30.3645 - val_loss: 1055.6376 - val_mse: 1055.6376 - val_mae: 23.9376\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2952.6524 - mse: 2952.6528 - mae: 30.9242 - val_loss: 1057.6022 - val_mse: 1057.6022 - val_mae: 23.6274\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2849.6654 - mse: 2849.6658 - mae: 30.2004 - val_loss: 1059.0757 - val_mse: 1059.0756 - val_mae: 23.5632\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2866.9367 - mse: 2866.9365 - mae: 30.4528 - val_loss: 1053.4345 - val_mse: 1053.4346 - val_mae: 24.1489\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2789.9681 - mse: 2789.9678 - mae: 30.3412 - val_loss: 1055.9492 - val_mse: 1055.9491 - val_mae: 23.7118\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2814.1632 - mse: 2814.1626 - mae: 30.2778 - val_loss: 1057.1408 - val_mse: 1057.1409 - val_mae: 23.6653\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2810.6842 - mse: 2810.6853 - mae: 30.3540 - val_loss: 1053.0252 - val_mse: 1053.0253 - val_mae: 24.0488\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 537us/step - loss: 2859.0923 - mse: 2859.0923 - mae: 30.6231 - val_loss: 1053.6781 - val_mse: 1053.6781 - val_mae: 23.9223\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2776.3154 - mse: 2776.3159 - mae: 29.7030 - val_loss: 1052.8582 - val_mse: 1052.8583 - val_mae: 24.0690\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2881.6147 - mse: 2881.6143 - mae: 30.2982 - val_loss: 1051.9874 - val_mse: 1051.9874 - val_mae: 24.0659\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2859.5522 - mse: 2859.5515 - mae: 30.3106 - val_loss: 1051.9374 - val_mse: 1051.9374 - val_mae: 24.2230\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2945.5395 - mse: 2945.5393 - mae: 31.2678 - val_loss: 1054.2153 - val_mse: 1054.2153 - val_mae: 23.7948\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2808.5858 - mse: 2808.5862 - mae: 30.2813 - val_loss: 1053.0757 - val_mse: 1053.0757 - val_mae: 23.8487\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2885.8758 - mse: 2885.8757 - mae: 30.5178 - val_loss: 1055.4764 - val_mse: 1055.4763 - val_mae: 23.6634\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2737.2815 - mse: 2737.2808 - mae: 29.6036 - val_loss: 1051.4926 - val_mse: 1051.4926 - val_mae: 24.3087\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2846.9144 - mse: 2846.9146 - mae: 30.3143 - val_loss: 1052.5346 - val_mse: 1052.5345 - val_mae: 23.8471\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2830.2337 - mse: 2830.2339 - mae: 29.9715 - val_loss: 1052.0102 - val_mse: 1052.0103 - val_mae: 23.9526\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2817.4560 - mse: 2817.4558 - mae: 30.2248 - val_loss: 1055.0864 - val_mse: 1055.0864 - val_mae: 23.6243\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2884.8910 - mse: 2884.8909 - mae: 30.5074 - val_loss: 1051.3532 - val_mse: 1051.3531 - val_mae: 23.8775\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2897.1933 - mse: 2897.1951 - mae: 30.5150 - val_loss: 1051.4438 - val_mse: 1051.4438 - val_mae: 23.8186\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2912.6232 - mse: 2912.6226 - mae: 30.1767 - val_loss: 1049.8206 - val_mse: 1049.8206 - val_mae: 23.9677\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2731.7425 - mse: 2731.7434 - mae: 30.1529 - val_loss: 1049.4669 - val_mse: 1049.4670 - val_mae: 24.3689\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2793.9899 - mse: 2793.9897 - mae: 30.4963 - val_loss: 1049.2587 - val_mse: 1049.2588 - val_mae: 24.0679\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 667us/step - loss: 2724.5531 - mse: 2724.5540 - mae: 29.7458 - val_loss: 1049.8226 - val_mse: 1049.8228 - val_mae: 24.0990\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2819.4932 - mse: 2819.4929 - mae: 29.9322 - val_loss: 1049.9038 - val_mse: 1049.9039 - val_mae: 24.2205\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 441us/step - loss: 2827.4520 - mse: 2827.4524 - mae: 30.1674 - val_loss: 1052.7128 - val_mse: 1052.7129 - val_mae: 23.7627\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2787.5996 - mse: 2787.6001 - mae: 30.4872 - val_loss: 1051.0321 - val_mse: 1051.0320 - val_mae: 23.9420\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2940.1530 - mse: 2940.1526 - mae: 30.7177 - val_loss: 1055.6180 - val_mse: 1055.6180 - val_mae: 23.5489\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2790.4503 - mse: 2790.4495 - mae: 29.7925 - val_loss: 1051.0899 - val_mse: 1051.0900 - val_mae: 24.2866\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2859.6785 - mse: 2859.6785 - mae: 30.8980 - val_loss: 1051.5224 - val_mse: 1051.5225 - val_mae: 24.0498\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2829.1682 - mse: 2829.1687 - mae: 30.1706 - val_loss: 1052.4094 - val_mse: 1052.4094 - val_mae: 23.8936\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2865.6482 - mse: 2865.6484 - mae: 30.0156 - val_loss: 1052.6728 - val_mse: 1052.6727 - val_mae: 23.7808\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2787.8277 - mse: 2787.8267 - mae: 30.3506 - val_loss: 1054.5346 - val_mse: 1054.5348 - val_mae: 23.6771\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 520us/step - loss: 2907.4217 - mse: 2907.4216 - mae: 30.4740 - val_loss: 1051.8566 - val_mse: 1051.8566 - val_mae: 23.8597\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2812.6567 - mse: 2812.6567 - mae: 30.0716 - val_loss: 1052.4039 - val_mse: 1052.4039 - val_mae: 23.8699\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2808.9109 - mse: 2808.9116 - mae: 30.1943 - val_loss: 1050.0804 - val_mse: 1050.0804 - val_mae: 24.0373\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 662us/step - loss: 2877.2730 - mse: 2877.2729 - mae: 30.2525 - val_loss: 1050.3406 - val_mse: 1050.3407 - val_mae: 24.1800\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 523us/step - loss: 2735.3465 - mse: 2735.3462 - mae: 29.7679 - val_loss: 1050.0984 - val_mse: 1050.0984 - val_mae: 24.3535\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2836.1594 - mse: 2836.1604 - mae: 29.8163 - val_loss: 1050.8380 - val_mse: 1050.8380 - val_mae: 24.0732\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2881.8392 - mse: 2881.8396 - mae: 30.9013 - val_loss: 1051.4121 - val_mse: 1051.4121 - val_mae: 23.9533\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 553us/step - loss: 2781.3377 - mse: 2781.3381 - mae: 30.3148 - val_loss: 1053.8242 - val_mse: 1053.8241 - val_mae: 23.6791\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 694us/step - loss: 2812.1473 - mse: 2812.1477 - mae: 30.2322 - val_loss: 1051.9411 - val_mse: 1051.9412 - val_mae: 23.8655\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2843.8912 - mse: 2843.8904 - mae: 30.2133 - val_loss: 1051.2927 - val_mse: 1051.2926 - val_mae: 23.9202\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 487us/step - loss: 2827.3520 - mse: 2827.3513 - mae: 30.2163 - val_loss: 1051.0949 - val_mse: 1051.0950 - val_mae: 23.9265\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2802.4181 - mse: 2802.4175 - mae: 29.5157 - val_loss: 1051.0294 - val_mse: 1051.0294 - val_mae: 23.8286\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2853.6430 - mse: 2853.6426 - mae: 29.9955 - val_loss: 1049.4052 - val_mse: 1049.4053 - val_mae: 24.1805\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2841.9683 - mse: 2841.9673 - mae: 30.4909 - val_loss: 1052.4644 - val_mse: 1052.4645 - val_mae: 23.6617\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2819.6038 - mse: 2819.6042 - mae: 30.2781 - val_loss: 1050.0213 - val_mse: 1050.0211 - val_mae: 24.0646\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2860.7536 - mse: 2860.7537 - mae: 30.4034 - val_loss: 1051.7914 - val_mse: 1051.7916 - val_mae: 23.8479\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2744.0181 - mse: 2744.0190 - mae: 30.1700 - val_loss: 1050.4956 - val_mse: 1050.4956 - val_mae: 24.0005\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 2817.5401 - mse: 2817.5398 - mae: 29.8759 - val_loss: 1050.4649 - val_mse: 1050.4650 - val_mae: 24.0872\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2774.1442 - mse: 2774.1443 - mae: 29.6572 - val_loss: 1048.8924 - val_mse: 1048.8925 - val_mae: 24.0812\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2830.0446 - mse: 2830.0452 - mae: 30.0585 - val_loss: 1048.8724 - val_mse: 1048.8723 - val_mae: 24.2539\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2868.9726 - mse: 2868.9722 - mae: 30.4470 - val_loss: 1048.5061 - val_mse: 1048.5060 - val_mae: 24.1407\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2787.4304 - mse: 2787.4302 - mae: 29.5734 - val_loss: 1048.8561 - val_mse: 1048.8560 - val_mae: 24.2966\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2817.3476 - mse: 2817.3481 - mae: 30.4798 - val_loss: 1048.3682 - val_mse: 1048.3684 - val_mae: 24.0205\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2867.0893 - mse: 2867.0896 - mae: 30.5398 - val_loss: 1048.2450 - val_mse: 1048.2450 - val_mae: 24.1404\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2857.9184 - mse: 2857.9185 - mae: 30.3123 - val_loss: 1048.7363 - val_mse: 1048.7363 - val_mae: 23.8405\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 538us/step - loss: 2771.1881 - mse: 2771.1870 - mae: 29.7920 - val_loss: 1048.0508 - val_mse: 1048.0508 - val_mae: 23.8697\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2832.7672 - mse: 2832.7673 - mae: 30.1152 - val_loss: 1047.2673 - val_mse: 1047.2672 - val_mae: 23.9081\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2817.0895 - mse: 2817.0896 - mae: 30.2083 - val_loss: 1049.1577 - val_mse: 1049.1576 - val_mae: 23.7031\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2778.6149 - mse: 2778.6155 - mae: 29.6537 - val_loss: 1049.4411 - val_mse: 1049.4413 - val_mae: 23.7574\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 529us/step - loss: 2811.8281 - mse: 2811.8281 - mae: 29.8588 - val_loss: 1047.5986 - val_mse: 1047.5984 - val_mae: 24.4447\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 522us/step - loss: 2645.5773 - mse: 2645.5774 - mae: 29.4331 - val_loss: 1046.6531 - val_mse: 1046.6531 - val_mae: 24.2785\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 488us/step - loss: 2842.1364 - mse: 2842.1362 - mae: 30.0996 - val_loss: 1045.2492 - val_mse: 1045.2491 - val_mae: 24.0840\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2815.5235 - mse: 2815.5239 - mae: 30.0861 - val_loss: 1045.5339 - val_mse: 1045.5338 - val_mae: 23.9158\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 683us/step - loss: 2829.4973 - mse: 2829.4973 - mae: 30.1809 - val_loss: 1046.1967 - val_mse: 1046.1965 - val_mae: 23.8149\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 689us/step - loss: 2724.8109 - mse: 2724.8108 - mae: 29.2924 - val_loss: 1044.4042 - val_mse: 1044.4042 - val_mae: 24.1786\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2835.8440 - mse: 2835.8442 - mae: 30.2930 - val_loss: 1045.2758 - val_mse: 1045.2756 - val_mae: 23.8041\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2765.8033 - mse: 2765.8030 - mae: 29.7211 - val_loss: 1046.3448 - val_mse: 1046.3450 - val_mae: 23.8388\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2932.9708 - mse: 2932.9700 - mae: 30.5133 - val_loss: 1045.8996 - val_mse: 1045.8995 - val_mae: 23.8562\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2809.3345 - mse: 2809.3352 - mae: 29.8632 - val_loss: 1044.8490 - val_mse: 1044.8490 - val_mae: 24.0132\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2747.3559 - mse: 2747.3560 - mae: 29.5897 - val_loss: 1044.4967 - val_mse: 1044.4968 - val_mae: 24.1144\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2809.0990 - mse: 2809.0994 - mae: 29.8036 - val_loss: 1044.3802 - val_mse: 1044.3804 - val_mae: 23.7665\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 550us/step - loss: 2476.5952 - mse: 2476.5955 - mae: 29.2443 - val_loss: 1557.7467 - val_mse: 1557.7466 - val_mae: 27.5354\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2536.0741 - mse: 2536.0745 - mae: 29.2305 - val_loss: 1551.4908 - val_mse: 1551.4907 - val_mae: 27.7465\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2559.8614 - mse: 2559.8616 - mae: 29.5076 - val_loss: 1556.1346 - val_mse: 1556.1346 - val_mae: 27.5610\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2466.0290 - mse: 2466.0295 - mae: 29.2315 - val_loss: 1562.1437 - val_mse: 1562.1437 - val_mae: 27.3610\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2480.7083 - mse: 2480.7075 - mae: 29.3759 - val_loss: 1562.0150 - val_mse: 1562.0149 - val_mae: 27.3682\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2553.4518 - mse: 2553.4519 - mae: 29.5228 - val_loss: 1554.4388 - val_mse: 1554.4388 - val_mae: 27.6128\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 544us/step - loss: 2544.5447 - mse: 2544.5442 - mae: 29.6703 - val_loss: 1553.5958 - val_mse: 1553.5956 - val_mae: 27.6303\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 526us/step - loss: 2530.3161 - mse: 2530.3162 - mae: 29.2518 - val_loss: 1557.7191 - val_mse: 1557.7190 - val_mae: 27.4830\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2581.7118 - mse: 2581.7122 - mae: 29.6010 - val_loss: 1573.5919 - val_mse: 1573.5919 - val_mae: 27.0980\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 545us/step - loss: 2506.0070 - mse: 2506.0073 - mae: 29.1181 - val_loss: 1557.9576 - val_mse: 1557.9576 - val_mae: 27.4829\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2532.1934 - mse: 2532.1936 - mae: 29.3703 - val_loss: 1555.1630 - val_mse: 1555.1631 - val_mae: 27.5567\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2467.3416 - mse: 2467.3418 - mae: 28.8800 - val_loss: 1551.7339 - val_mse: 1551.7340 - val_mae: 27.6463\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2472.5671 - mse: 2472.5669 - mae: 29.0261 - val_loss: 1561.1958 - val_mse: 1561.1956 - val_mae: 27.3325\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2507.0967 - mse: 2507.0974 - mae: 29.1212 - val_loss: 1548.4080 - val_mse: 1548.4080 - val_mae: 27.7531\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2496.8896 - mse: 2496.8896 - mae: 29.1133 - val_loss: 1548.8488 - val_mse: 1548.8488 - val_mae: 27.6929\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2589.3564 - mse: 2589.3567 - mae: 29.6168 - val_loss: 1559.0930 - val_mse: 1559.0930 - val_mae: 27.3503\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2520.4578 - mse: 2520.4575 - mae: 29.3058 - val_loss: 1556.9202 - val_mse: 1556.9203 - val_mae: 27.3890\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 530us/step - loss: 2507.2793 - mse: 2507.2791 - mae: 29.6524 - val_loss: 1551.9377 - val_mse: 1551.9377 - val_mae: 27.5492\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2495.8206 - mse: 2495.8213 - mae: 29.2765 - val_loss: 1550.9637 - val_mse: 1550.9636 - val_mae: 27.5700\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2503.1244 - mse: 2503.1250 - mae: 29.2669 - val_loss: 1552.4301 - val_mse: 1552.4299 - val_mae: 27.4992\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2535.5624 - mse: 2535.5627 - mae: 29.1239 - val_loss: 1551.8846 - val_mse: 1551.8845 - val_mae: 27.4867\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2493.0106 - mse: 2493.0107 - mae: 28.7503 - val_loss: 1546.7518 - val_mse: 1546.7517 - val_mae: 27.6724\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2480.0749 - mse: 2480.0752 - mae: 29.0739 - val_loss: 1547.9633 - val_mse: 1547.9634 - val_mae: 27.5640\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 538us/step - loss: 2533.1344 - mse: 2533.1340 - mae: 29.4805 - val_loss: 1563.6990 - val_mse: 1563.6989 - val_mae: 27.0893\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2506.8712 - mse: 2506.8708 - mae: 28.8875 - val_loss: 1552.6367 - val_mse: 1552.6366 - val_mae: 27.3500\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2537.4786 - mse: 2537.4785 - mae: 29.3514 - val_loss: 1546.7337 - val_mse: 1546.7335 - val_mae: 27.5346\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2479.0413 - mse: 2479.0425 - mae: 29.0960 - val_loss: 1553.5065 - val_mse: 1553.5065 - val_mae: 27.3196\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2519.1779 - mse: 2519.1775 - mae: 29.5830 - val_loss: 1551.7575 - val_mse: 1551.7576 - val_mae: 27.3734\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2530.9983 - mse: 2530.9983 - mae: 29.4967 - val_loss: 1547.2254 - val_mse: 1547.2255 - val_mae: 27.4829\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 506us/step - loss: 2560.1020 - mse: 2560.1023 - mae: 29.4954 - val_loss: 1551.1166 - val_mse: 1551.1167 - val_mae: 27.3390\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2512.7638 - mse: 2512.7637 - mae: 29.2873 - val_loss: 1544.0966 - val_mse: 1544.0966 - val_mae: 27.5528\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2465.8577 - mse: 2465.8577 - mae: 28.8950 - val_loss: 1538.8022 - val_mse: 1538.8024 - val_mae: 27.7576\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2517.5733 - mse: 2517.5735 - mae: 29.0237 - val_loss: 1540.3435 - val_mse: 1540.3438 - val_mae: 27.6395\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2401.0055 - mse: 2401.0061 - mae: 28.5269 - val_loss: 1547.5647 - val_mse: 1547.5645 - val_mae: 27.3511\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2512.7302 - mse: 2512.7307 - mae: 29.4965 - val_loss: 1546.0584 - val_mse: 1546.0586 - val_mae: 27.3797\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2480.2088 - mse: 2480.2085 - mae: 28.8071 - val_loss: 1548.6043 - val_mse: 1548.6042 - val_mae: 27.2698\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2515.3227 - mse: 2515.3225 - mae: 29.0718 - val_loss: 1559.8443 - val_mse: 1559.8445 - val_mae: 26.9853\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2511.4257 - mse: 2511.4250 - mae: 29.4865 - val_loss: 1540.7287 - val_mse: 1540.7288 - val_mae: 27.4985\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2512.0677 - mse: 2512.0674 - mae: 28.8499 - val_loss: 1548.0296 - val_mse: 1548.0298 - val_mae: 27.2529\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2418.5338 - mse: 2418.5339 - mae: 28.5972 - val_loss: 1541.8013 - val_mse: 1541.8010 - val_mae: 27.4352\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2490.1217 - mse: 2490.1216 - mae: 28.8435 - val_loss: 1536.6272 - val_mse: 1536.6270 - val_mae: 27.6411\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2471.1566 - mse: 2471.1562 - mae: 28.5784 - val_loss: 1551.7545 - val_mse: 1551.7546 - val_mae: 27.1407\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 521us/step - loss: 2501.3164 - mse: 2501.3164 - mae: 28.8496 - val_loss: 1549.0056 - val_mse: 1549.0057 - val_mae: 27.1622\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2468.7438 - mse: 2468.7432 - mae: 29.2254 - val_loss: 1539.8539 - val_mse: 1539.8540 - val_mae: 27.4130\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2435.7104 - mse: 2435.7107 - mae: 29.0516 - val_loss: 1536.5363 - val_mse: 1536.5363 - val_mae: 27.4925\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2490.8515 - mse: 2490.8513 - mae: 29.1640 - val_loss: 1541.2626 - val_mse: 1541.2627 - val_mae: 27.3348\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 552us/step - loss: 2473.5361 - mse: 2473.5359 - mae: 29.0047 - val_loss: 1538.6020 - val_mse: 1538.6022 - val_mae: 27.4222\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2471.2089 - mse: 2471.2100 - mae: 29.3825 - val_loss: 1549.0968 - val_mse: 1549.0966 - val_mae: 27.0992\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2469.7694 - mse: 2469.7698 - mae: 28.6371 - val_loss: 1542.3715 - val_mse: 1542.3713 - val_mae: 27.2480\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2483.2335 - mse: 2483.2334 - mae: 28.9620 - val_loss: 1538.4485 - val_mse: 1538.4485 - val_mae: 27.3510\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2529.3955 - mse: 2529.3950 - mae: 29.1680 - val_loss: 1536.0080 - val_mse: 1536.0081 - val_mae: 27.4341\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2480.4171 - mse: 2480.4167 - mae: 28.7818 - val_loss: 1544.2567 - val_mse: 1544.2567 - val_mae: 27.1646\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2505.6841 - mse: 2505.6833 - mae: 29.1669 - val_loss: 1540.8577 - val_mse: 1540.8577 - val_mae: 27.2285\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2474.3770 - mse: 2474.3784 - mae: 29.0357 - val_loss: 1536.6662 - val_mse: 1536.6661 - val_mae: 27.3332\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2436.7054 - mse: 2436.7053 - mae: 28.6216 - val_loss: 1534.1201 - val_mse: 1534.1202 - val_mae: 27.4332\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2509.8324 - mse: 2509.8328 - mae: 29.2278 - val_loss: 1535.3031 - val_mse: 1535.3029 - val_mae: 27.3972\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2489.7213 - mse: 2489.7224 - mae: 29.5451 - val_loss: 1541.0008 - val_mse: 1541.0009 - val_mae: 27.1724\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2466.4052 - mse: 2466.4048 - mae: 28.8933 - val_loss: 1543.1222 - val_mse: 1543.1221 - val_mae: 27.0790\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2482.5889 - mse: 2482.5889 - mae: 29.0494 - val_loss: 1535.6171 - val_mse: 1535.6172 - val_mae: 27.3087\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 533us/step - loss: 2422.4168 - mse: 2422.4177 - mae: 28.5352 - val_loss: 1530.3988 - val_mse: 1530.3988 - val_mae: 27.4537\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2495.5857 - mse: 2495.5854 - mae: 28.9103 - val_loss: 1528.0685 - val_mse: 1528.0684 - val_mae: 27.5664\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2405.7856 - mse: 2405.7856 - mae: 28.8444 - val_loss: 1528.7113 - val_mse: 1528.7114 - val_mae: 27.5134\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2519.6917 - mse: 2519.6917 - mae: 28.9727 - val_loss: 1544.8157 - val_mse: 1544.8157 - val_mae: 27.0251\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2531.5715 - mse: 2531.5713 - mae: 29.4058 - val_loss: 1539.7993 - val_mse: 1539.7993 - val_mae: 27.1176\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2430.4083 - mse: 2430.4077 - mae: 28.8933 - val_loss: 1533.0153 - val_mse: 1533.0155 - val_mae: 27.3171\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2490.0718 - mse: 2490.0720 - mae: 28.4518 - val_loss: 1536.0200 - val_mse: 1536.0199 - val_mae: 27.2389\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 670us/step - loss: 2447.0955 - mse: 2447.0964 - mae: 28.9819 - val_loss: 1537.6181 - val_mse: 1537.6182 - val_mae: 27.1905\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2431.5982 - mse: 2431.5974 - mae: 28.2932 - val_loss: 1537.8658 - val_mse: 1537.8657 - val_mae: 27.1869\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2453.6975 - mse: 2453.6975 - mae: 28.9608 - val_loss: 1538.2548 - val_mse: 1538.2548 - val_mae: 27.1606\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2467.2835 - mse: 2467.2849 - mae: 28.8857 - val_loss: 1528.6816 - val_mse: 1528.6816 - val_mae: 27.4453\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2477.3475 - mse: 2477.3479 - mae: 29.2492 - val_loss: 1540.1873 - val_mse: 1540.1873 - val_mae: 27.0897\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2445.0188 - mse: 2445.0195 - mae: 28.8447 - val_loss: 1534.2051 - val_mse: 1534.2051 - val_mae: 27.2272\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2456.3041 - mse: 2456.3037 - mae: 28.6786 - val_loss: 1523.0420 - val_mse: 1523.0421 - val_mae: 27.6482\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2447.5997 - mse: 2447.5994 - mae: 28.5446 - val_loss: 1532.3862 - val_mse: 1532.3862 - val_mae: 27.2469\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2389.2701 - mse: 2389.2700 - mae: 28.3776 - val_loss: 1526.3722 - val_mse: 1526.3724 - val_mae: 27.3972\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2422.3854 - mse: 2422.3853 - mae: 28.8026 - val_loss: 1534.0011 - val_mse: 1534.0012 - val_mae: 27.1353\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2474.2744 - mse: 2474.2747 - mae: 29.0208 - val_loss: 1532.0751 - val_mse: 1532.0751 - val_mae: 27.1707\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2398.4642 - mse: 2398.4644 - mae: 28.4931 - val_loss: 1529.4180 - val_mse: 1529.4178 - val_mae: 27.2664\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2367.3496 - mse: 2367.3499 - mae: 28.8594 - val_loss: 1527.4837 - val_mse: 1527.4835 - val_mae: 27.3376\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2479.8325 - mse: 2479.8318 - mae: 28.8822 - val_loss: 1533.6542 - val_mse: 1533.6544 - val_mae: 27.1217\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2336.7564 - mse: 2336.7559 - mae: 29.1497 - val_loss: 3680.9616 - val_mse: 3680.9614 - val_mae: 24.7473\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2378.5260 - mse: 2378.5264 - mae: 29.4574 - val_loss: 3677.9826 - val_mse: 3677.9822 - val_mae: 24.4175\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2325.4791 - mse: 2325.4785 - mae: 29.2463 - val_loss: 3678.1160 - val_mse: 3678.1162 - val_mae: 24.4943\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 571us/step - loss: 2365.1796 - mse: 2365.1799 - mae: 29.6621 - val_loss: 3675.9638 - val_mse: 3675.9639 - val_mae: 24.1192\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2389.5848 - mse: 2389.5854 - mae: 29.4791 - val_loss: 3674.5346 - val_mse: 3674.5352 - val_mae: 23.8414\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2359.7042 - mse: 2359.7041 - mae: 29.0598 - val_loss: 3676.1700 - val_mse: 3676.1694 - val_mae: 24.2600\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 539us/step - loss: 2394.5879 - mse: 2394.5876 - mae: 29.6991 - val_loss: 3673.9387 - val_mse: 3673.9387 - val_mae: 23.9343\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2304.2096 - mse: 2304.2090 - mae: 29.2759 - val_loss: 3672.9351 - val_mse: 3672.9358 - val_mae: 23.8227\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 659us/step - loss: 2377.3700 - mse: 2377.3699 - mae: 29.6301 - val_loss: 3673.2008 - val_mse: 3673.2004 - val_mae: 23.9056\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2366.0424 - mse: 2366.0425 - mae: 29.2521 - val_loss: 3673.8803 - val_mse: 3673.8801 - val_mae: 24.1663\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2362.9083 - mse: 2362.9082 - mae: 29.2719 - val_loss: 3673.2264 - val_mse: 3673.2258 - val_mae: 24.1821\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2348.5952 - mse: 2348.5950 - mae: 29.1417 - val_loss: 3672.5008 - val_mse: 3672.5002 - val_mae: 24.0285\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2374.9532 - mse: 2374.9529 - mae: 29.4573 - val_loss: 3673.9067 - val_mse: 3673.9077 - val_mae: 24.2871\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2325.0209 - mse: 2325.0212 - mae: 28.8574 - val_loss: 3673.2893 - val_mse: 3673.2900 - val_mae: 24.2362\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2306.0294 - mse: 2306.0291 - mae: 29.1976 - val_loss: 3671.6286 - val_mse: 3671.6284 - val_mae: 23.9462\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2358.1038 - mse: 2358.1035 - mae: 29.0660 - val_loss: 3671.5433 - val_mse: 3671.5435 - val_mae: 23.9670\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2418.7923 - mse: 2418.7915 - mae: 29.2895 - val_loss: 3670.3823 - val_mse: 3670.3843 - val_mae: 23.1580\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2374.6431 - mse: 2374.6431 - mae: 29.0880 - val_loss: 3670.9244 - val_mse: 3670.9236 - val_mae: 23.8591\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2352.6946 - mse: 2352.6941 - mae: 29.2461 - val_loss: 3671.8164 - val_mse: 3671.8162 - val_mae: 24.0384\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2352.6292 - mse: 2352.6289 - mae: 29.2406 - val_loss: 3671.3666 - val_mse: 3671.3655 - val_mae: 24.0297\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2404.1325 - mse: 2404.1321 - mae: 29.7398 - val_loss: 3671.8303 - val_mse: 3671.8303 - val_mae: 24.1458\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2342.8416 - mse: 2342.8408 - mae: 29.5072 - val_loss: 3672.1836 - val_mse: 3672.1838 - val_mae: 24.1878\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2361.2334 - mse: 2361.2329 - mae: 29.4377 - val_loss: 3670.1652 - val_mse: 3670.1658 - val_mae: 23.7200\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 656us/step - loss: 2373.5557 - mse: 2373.5559 - mae: 29.1751 - val_loss: 3669.9486 - val_mse: 3669.9487 - val_mae: 23.5852\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2368.3830 - mse: 2368.3831 - mae: 29.1598 - val_loss: 3671.5436 - val_mse: 3671.5427 - val_mae: 24.0353\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2349.7233 - mse: 2349.7224 - mae: 29.3802 - val_loss: 3671.8323 - val_mse: 3671.8333 - val_mae: 24.1472\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2390.8128 - mse: 2390.8125 - mae: 29.6841 - val_loss: 3668.8651 - val_mse: 3668.8652 - val_mae: 23.4918\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 481us/step - loss: 2349.3096 - mse: 2349.3088 - mae: 28.8667 - val_loss: 3670.7679 - val_mse: 3670.7678 - val_mae: 24.0091\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2339.9999 - mse: 2340.0005 - mae: 29.2638 - val_loss: 3671.0730 - val_mse: 3671.0732 - val_mae: 24.1398\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2361.1491 - mse: 2361.1492 - mae: 29.4176 - val_loss: 3668.4270 - val_mse: 3668.4275 - val_mae: 23.4269\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2382.7696 - mse: 2382.7700 - mae: 29.0648 - val_loss: 3670.6439 - val_mse: 3670.6436 - val_mae: 24.0819\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2327.1832 - mse: 2327.1833 - mae: 29.0931 - val_loss: 3668.7258 - val_mse: 3668.7266 - val_mae: 23.8228\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2392.8167 - mse: 2392.8171 - mae: 29.3353 - val_loss: 3668.0839 - val_mse: 3668.0840 - val_mae: 23.6566\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2344.9142 - mse: 2344.9141 - mae: 29.1179 - val_loss: 3669.1360 - val_mse: 3669.1365 - val_mae: 24.0065\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2351.7996 - mse: 2351.7993 - mae: 29.1107 - val_loss: 3671.3829 - val_mse: 3671.3826 - val_mae: 24.3834\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2365.1977 - mse: 2365.1965 - mae: 29.4094 - val_loss: 3668.9123 - val_mse: 3668.9119 - val_mae: 23.9320\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2400.8563 - mse: 2400.8557 - mae: 29.5061 - val_loss: 3668.5352 - val_mse: 3668.5352 - val_mae: 23.8778\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2330.0953 - mse: 2330.0955 - mae: 28.9213 - val_loss: 3668.1236 - val_mse: 3668.1238 - val_mae: 23.7402\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 546us/step - loss: 2309.7990 - mse: 2309.7991 - mae: 29.0414 - val_loss: 3667.7089 - val_mse: 3667.7092 - val_mae: 23.7569\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2301.7674 - mse: 2301.7664 - mae: 28.5602 - val_loss: 3668.4576 - val_mse: 3668.4575 - val_mae: 23.9126\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2351.5844 - mse: 2351.5850 - mae: 29.4701 - val_loss: 3666.4928 - val_mse: 3666.4929 - val_mae: 23.4136\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2384.8575 - mse: 2384.8577 - mae: 29.3288 - val_loss: 3669.0050 - val_mse: 3669.0046 - val_mae: 24.1038\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2307.5674 - mse: 2307.5669 - mae: 29.2752 - val_loss: 3667.7855 - val_mse: 3667.7861 - val_mae: 23.7119\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2347.3426 - mse: 2347.3418 - mae: 29.0905 - val_loss: 3670.1798 - val_mse: 3670.1812 - val_mae: 24.2446\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2335.3002 - mse: 2335.2993 - mae: 29.1634 - val_loss: 3667.6372 - val_mse: 3667.6375 - val_mae: 23.8368\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 512us/step - loss: 2315.6222 - mse: 2315.6223 - mae: 28.5377 - val_loss: 3671.4171 - val_mse: 3671.4170 - val_mae: 24.5504\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2341.6448 - mse: 2341.6450 - mae: 28.9571 - val_loss: 3672.5497 - val_mse: 3672.5488 - val_mae: 24.6383\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2342.4131 - mse: 2342.4128 - mae: 29.3838 - val_loss: 3667.6863 - val_mse: 3667.6853 - val_mae: 23.9405\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2303.3306 - mse: 2303.3311 - mae: 29.0670 - val_loss: 3669.1481 - val_mse: 3669.1479 - val_mae: 24.2181\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2337.8698 - mse: 2337.8691 - mae: 29.2961 - val_loss: 3667.9882 - val_mse: 3667.9875 - val_mae: 24.1411\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 538us/step - loss: 2270.3756 - mse: 2270.3750 - mae: 28.8622 - val_loss: 3670.0600 - val_mse: 3670.0593 - val_mae: 24.3772\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 527us/step - loss: 2371.8633 - mse: 2371.8635 - mae: 29.0873 - val_loss: 3665.5322 - val_mse: 3665.5315 - val_mae: 23.5417\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2372.0026 - mse: 2372.0024 - mae: 29.4961 - val_loss: 3666.4588 - val_mse: 3666.4585 - val_mae: 23.7296\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2351.2937 - mse: 2351.2932 - mae: 28.7643 - val_loss: 3667.2227 - val_mse: 3667.2227 - val_mae: 23.9216\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2311.6700 - mse: 2311.6704 - mae: 28.9240 - val_loss: 3666.7647 - val_mse: 3666.7644 - val_mae: 23.8244\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2345.2330 - mse: 2345.2329 - mae: 29.1979 - val_loss: 3668.6565 - val_mse: 3668.6562 - val_mae: 24.1425\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2353.4357 - mse: 2353.4360 - mae: 28.8361 - val_loss: 3668.5160 - val_mse: 3668.5151 - val_mae: 24.2132\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 546us/step - loss: 2314.7991 - mse: 2314.7986 - mae: 28.9174 - val_loss: 3668.7335 - val_mse: 3668.7329 - val_mae: 24.2171\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 510us/step - loss: 2330.1955 - mse: 2330.1960 - mae: 28.9019 - val_loss: 3666.4225 - val_mse: 3666.4221 - val_mae: 23.8520\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2355.4444 - mse: 2355.4451 - mae: 29.3085 - val_loss: 3665.7118 - val_mse: 3665.7119 - val_mae: 23.7268\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2345.0213 - mse: 2345.0212 - mae: 28.8284 - val_loss: 3668.1906 - val_mse: 3668.1902 - val_mae: 24.1648\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2378.7550 - mse: 2378.7546 - mae: 29.4567 - val_loss: 3667.8301 - val_mse: 3667.8303 - val_mae: 24.1407\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2335.5523 - mse: 2335.5520 - mae: 29.1533 - val_loss: 3668.0384 - val_mse: 3668.0386 - val_mae: 24.1528\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2345.4684 - mse: 2345.4683 - mae: 29.2337 - val_loss: 3665.0434 - val_mse: 3665.0427 - val_mae: 23.3756\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2326.0132 - mse: 2326.0132 - mae: 28.6953 - val_loss: 3666.9902 - val_mse: 3666.9897 - val_mae: 23.9436\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2389.0425 - mse: 2389.0432 - mae: 29.3802 - val_loss: 3667.3451 - val_mse: 3667.3445 - val_mae: 23.9810\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2297.5001 - mse: 2297.4998 - mae: 28.9908 - val_loss: 3667.8243 - val_mse: 3667.8250 - val_mae: 23.9906\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2354.2958 - mse: 2354.2959 - mae: 29.1639 - val_loss: 3666.8547 - val_mse: 3666.8545 - val_mae: 23.9162\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2336.7501 - mse: 2336.7502 - mae: 28.7607 - val_loss: 3667.9027 - val_mse: 3667.9033 - val_mae: 24.0954\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2352.4210 - mse: 2352.4216 - mae: 29.1489 - val_loss: 3665.5935 - val_mse: 3665.5930 - val_mae: 23.6874\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2319.7573 - mse: 2319.7581 - mae: 29.0820 - val_loss: 3667.7996 - val_mse: 3667.7998 - val_mae: 24.1253\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2349.6851 - mse: 2349.6848 - mae: 28.7621 - val_loss: 3671.3612 - val_mse: 3671.3611 - val_mae: 24.4951\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2292.4467 - mse: 2292.4468 - mae: 28.7295 - val_loss: 3673.6763 - val_mse: 3673.6760 - val_mae: 24.7053\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2359.9720 - mse: 2359.9722 - mae: 29.1176 - val_loss: 3670.6893 - val_mse: 3670.6887 - val_mae: 24.4623\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2255.2847 - mse: 2255.2844 - mae: 28.6979 - val_loss: 3668.8910 - val_mse: 3668.8909 - val_mae: 24.2021\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2294.2684 - mse: 2294.2686 - mae: 28.8143 - val_loss: 3665.8033 - val_mse: 3665.8040 - val_mae: 23.8192\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2326.6460 - mse: 2326.6465 - mae: 29.0452 - val_loss: 3665.3153 - val_mse: 3665.3152 - val_mae: 23.8404\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2288.2052 - mse: 2288.2051 - mae: 28.7788 - val_loss: 3665.1949 - val_mse: 3665.1958 - val_mae: 23.8426\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2312.5918 - mse: 2312.5928 - mae: 28.8403 - val_loss: 3663.1023 - val_mse: 3663.1023 - val_mae: 23.3563\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2344.3910 - mse: 2344.3911 - mae: 29.1382 - val_loss: 3666.2427 - val_mse: 3666.2434 - val_mae: 24.0019\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2678.4458 - mse: 2678.4456 - mae: 28.4960 - val_loss: 2373.3859 - val_mse: 2373.3855 - val_mae: 26.8821\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2679.8560 - mse: 2679.8555 - mae: 28.3257 - val_loss: 2377.5475 - val_mse: 2377.5474 - val_mae: 27.1970\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2742.0117 - mse: 2742.0117 - mae: 28.7542 - val_loss: 2384.3912 - val_mse: 2384.3914 - val_mae: 27.1171\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 525us/step - loss: 2715.3926 - mse: 2715.3933 - mae: 28.4676 - val_loss: 2377.1890 - val_mse: 2377.1892 - val_mae: 27.3219\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2681.2005 - mse: 2681.1995 - mae: 28.0696 - val_loss: 2382.7748 - val_mse: 2382.7747 - val_mae: 27.1251\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2734.6942 - mse: 2734.6936 - mae: 28.3477 - val_loss: 2384.9932 - val_mse: 2384.9929 - val_mae: 27.0185\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2719.9352 - mse: 2719.9351 - mae: 28.7064 - val_loss: 2395.9048 - val_mse: 2395.9050 - val_mae: 26.7197\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2712.5154 - mse: 2712.5151 - mae: 28.7235 - val_loss: 2391.9026 - val_mse: 2391.9026 - val_mae: 27.0829\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2731.0000 - mse: 2730.9988 - mae: 28.3755 - val_loss: 2386.0689 - val_mse: 2386.0691 - val_mae: 27.2258\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2699.7314 - mse: 2699.7322 - mae: 28.5093 - val_loss: 2397.5639 - val_mse: 2397.5640 - val_mae: 26.8927\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2688.2300 - mse: 2688.2302 - mae: 28.4752 - val_loss: 2387.0437 - val_mse: 2387.0437 - val_mae: 27.3469\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 544us/step - loss: 2678.6737 - mse: 2678.6731 - mae: 28.3570 - val_loss: 2391.9206 - val_mse: 2391.9204 - val_mae: 26.8159\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2621.7221 - mse: 2621.7222 - mae: 28.2612 - val_loss: 2385.5539 - val_mse: 2385.5540 - val_mae: 27.3210\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2709.6264 - mse: 2709.6270 - mae: 28.5916 - val_loss: 2389.7963 - val_mse: 2389.7964 - val_mae: 26.9180\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2694.6411 - mse: 2694.6406 - mae: 28.3431 - val_loss: 2380.1105 - val_mse: 2380.1106 - val_mae: 27.1042\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2757.6313 - mse: 2757.6321 - mae: 28.4389 - val_loss: 2396.1237 - val_mse: 2396.1238 - val_mae: 26.4829\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2663.7929 - mse: 2663.7927 - mae: 28.2897 - val_loss: 2393.7777 - val_mse: 2393.7781 - val_mae: 27.2351\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2729.1893 - mse: 2729.1902 - mae: 28.5202 - val_loss: 2405.0374 - val_mse: 2405.0369 - val_mae: 26.8490\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2773.3495 - mse: 2773.3496 - mae: 28.2840 - val_loss: 2398.4666 - val_mse: 2398.4666 - val_mae: 27.1202\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2650.7968 - mse: 2650.7976 - mae: 28.1128 - val_loss: 2401.7528 - val_mse: 2401.7532 - val_mae: 27.2783\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2749.1031 - mse: 2749.1030 - mae: 28.7149 - val_loss: 2407.7844 - val_mse: 2407.7842 - val_mae: 27.2046\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2672.7134 - mse: 2672.7134 - mae: 28.3016 - val_loss: 2407.7181 - val_mse: 2407.7185 - val_mae: 26.9662\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2727.7574 - mse: 2727.7576 - mae: 28.7549 - val_loss: 2408.2223 - val_mse: 2408.2227 - val_mae: 27.0293\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2693.7365 - mse: 2693.7363 - mae: 28.5557 - val_loss: 2401.3446 - val_mse: 2401.3442 - val_mae: 26.9369\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 532us/step - loss: 2667.0660 - mse: 2667.0667 - mae: 28.3781 - val_loss: 2403.0251 - val_mse: 2403.0249 - val_mae: 26.9067\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2670.6386 - mse: 2670.6382 - mae: 28.1797 - val_loss: 2392.6365 - val_mse: 2392.6362 - val_mae: 27.3174\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2683.9625 - mse: 2683.9626 - mae: 28.3918 - val_loss: 2390.2506 - val_mse: 2390.2505 - val_mae: 27.0487\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2751.6487 - mse: 2751.6484 - mae: 29.0133 - val_loss: 2393.8756 - val_mse: 2393.8755 - val_mae: 26.8121\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2675.0805 - mse: 2675.0818 - mae: 28.0089 - val_loss: 2389.8920 - val_mse: 2389.8918 - val_mae: 27.2019\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2714.1981 - mse: 2714.1987 - mae: 28.5506 - val_loss: 2393.7841 - val_mse: 2393.7844 - val_mae: 26.9536\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2712.9818 - mse: 2712.9824 - mae: 28.5189 - val_loss: 2389.6328 - val_mse: 2389.6326 - val_mae: 27.1905\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2667.6217 - mse: 2667.6208 - mae: 28.1649 - val_loss: 2398.6217 - val_mse: 2398.6216 - val_mae: 26.7682\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2709.0585 - mse: 2709.0588 - mae: 28.2592 - val_loss: 2397.1962 - val_mse: 2397.1960 - val_mae: 27.1660\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2703.9114 - mse: 2703.9111 - mae: 28.5472 - val_loss: 2394.7112 - val_mse: 2394.7112 - val_mae: 27.2860\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2672.2158 - mse: 2672.2146 - mae: 28.5430 - val_loss: 2397.2743 - val_mse: 2397.2744 - val_mae: 27.0068\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2708.4265 - mse: 2708.4275 - mae: 28.4348 - val_loss: 2403.7259 - val_mse: 2403.7258 - val_mae: 27.0490\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 668us/step - loss: 2704.1510 - mse: 2704.1516 - mae: 28.3802 - val_loss: 2399.5229 - val_mse: 2399.5232 - val_mae: 26.9029\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2735.1503 - mse: 2735.1501 - mae: 28.4852 - val_loss: 2403.9662 - val_mse: 2403.9666 - val_mae: 26.8489\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2652.6730 - mse: 2652.6741 - mae: 28.2525 - val_loss: 2396.1625 - val_mse: 2396.1624 - val_mae: 27.1962\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 546us/step - loss: 2729.5574 - mse: 2729.5579 - mae: 28.3366 - val_loss: 2393.5682 - val_mse: 2393.5681 - val_mae: 27.1846\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 538us/step - loss: 2709.3926 - mse: 2709.3928 - mae: 28.6030 - val_loss: 2400.2369 - val_mse: 2400.2371 - val_mae: 27.1661\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2691.5717 - mse: 2691.5715 - mae: 28.2678 - val_loss: 2396.9520 - val_mse: 2396.9521 - val_mae: 27.3134\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2689.3102 - mse: 2689.3096 - mae: 28.4918 - val_loss: 2395.6576 - val_mse: 2395.6575 - val_mae: 26.8990\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2658.0177 - mse: 2658.0183 - mae: 28.1796 - val_loss: 2396.0126 - val_mse: 2396.0127 - val_mae: 27.0996\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2676.3643 - mse: 2676.3647 - mae: 28.2303 - val_loss: 2391.5338 - val_mse: 2391.5337 - val_mae: 27.2703\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2693.4176 - mse: 2693.4177 - mae: 28.6177 - val_loss: 2391.1205 - val_mse: 2391.1204 - val_mae: 26.8924\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2688.4645 - mse: 2688.4648 - mae: 28.4448 - val_loss: 2398.5295 - val_mse: 2398.5293 - val_mae: 26.7638\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 517us/step - loss: 2757.0172 - mse: 2757.0168 - mae: 28.1598 - val_loss: 2403.9393 - val_mse: 2403.9390 - val_mae: 26.8390\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2724.1761 - mse: 2724.1768 - mae: 28.1226 - val_loss: 2400.0431 - val_mse: 2400.0432 - val_mae: 26.9466\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 667us/step - loss: 2701.9663 - mse: 2701.9668 - mae: 28.2320 - val_loss: 2403.8100 - val_mse: 2403.8101 - val_mae: 27.0230\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2655.1419 - mse: 2655.1411 - mae: 28.0301 - val_loss: 2391.2604 - val_mse: 2391.2607 - val_mae: 27.5871\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 532us/step - loss: 2670.2041 - mse: 2670.2041 - mae: 28.2491 - val_loss: 2400.6314 - val_mse: 2400.6313 - val_mae: 26.8866\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2667.4627 - mse: 2667.4629 - mae: 28.3306 - val_loss: 2396.1277 - val_mse: 2396.1277 - val_mae: 26.7110\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2713.5743 - mse: 2713.5747 - mae: 28.2109 - val_loss: 2398.3344 - val_mse: 2398.3340 - val_mae: 27.1454\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2677.4788 - mse: 2677.4795 - mae: 28.2920 - val_loss: 2392.2717 - val_mse: 2392.2717 - val_mae: 27.4370\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 649us/step - loss: 2713.9085 - mse: 2713.9072 - mae: 28.4894 - val_loss: 2400.6767 - val_mse: 2400.6770 - val_mae: 26.8785\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2665.2708 - mse: 2665.2705 - mae: 27.9741 - val_loss: 2400.3618 - val_mse: 2400.3618 - val_mae: 27.2562\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2688.4418 - mse: 2688.4407 - mae: 28.3263 - val_loss: 2392.8943 - val_mse: 2392.8943 - val_mae: 26.9299\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2731.0874 - mse: 2731.0872 - mae: 28.5980 - val_loss: 2398.3703 - val_mse: 2398.3704 - val_mae: 26.7025\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2698.0880 - mse: 2698.0884 - mae: 28.3259 - val_loss: 2394.4708 - val_mse: 2394.4707 - val_mae: 27.0680\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2714.6401 - mse: 2714.6401 - mae: 28.6351 - val_loss: 2396.9202 - val_mse: 2396.9199 - val_mae: 26.6559\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2711.5000 - mse: 2711.5007 - mae: 27.9044 - val_loss: 2400.5254 - val_mse: 2400.5251 - val_mae: 26.7930\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2686.3471 - mse: 2686.3459 - mae: 28.3367 - val_loss: 2395.6659 - val_mse: 2395.6655 - val_mae: 27.1525\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2681.0743 - mse: 2681.0750 - mae: 28.3231 - val_loss: 2409.3609 - val_mse: 2409.3608 - val_mae: 26.9094\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2718.5913 - mse: 2718.5911 - mae: 28.4063 - val_loss: 2407.3993 - val_mse: 2407.3994 - val_mae: 26.9011\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2669.9524 - mse: 2669.9524 - mae: 28.0461 - val_loss: 2393.5109 - val_mse: 2393.5105 - val_mae: 27.2510\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2671.8973 - mse: 2671.8970 - mae: 28.4116 - val_loss: 2391.9871 - val_mse: 2391.9871 - val_mae: 26.8809\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2672.2466 - mse: 2672.2458 - mae: 28.2633 - val_loss: 2393.7414 - val_mse: 2393.7412 - val_mae: 26.6608\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2672.4598 - mse: 2672.4592 - mae: 28.0043 - val_loss: 2387.7204 - val_mse: 2387.7207 - val_mae: 26.9100\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2714.5734 - mse: 2714.5732 - mae: 28.4521 - val_loss: 2387.1285 - val_mse: 2387.1289 - val_mae: 27.4550\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2668.0641 - mse: 2668.0642 - mae: 27.8695 - val_loss: 2389.8609 - val_mse: 2389.8608 - val_mae: 27.1765\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2657.7825 - mse: 2657.7839 - mae: 28.1960 - val_loss: 2384.3795 - val_mse: 2384.3794 - val_mae: 27.1067\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2708.1555 - mse: 2708.1543 - mae: 28.0134 - val_loss: 2389.0767 - val_mse: 2389.0767 - val_mae: 27.2259\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2673.1204 - mse: 2673.1201 - mae: 27.8843 - val_loss: 2382.4543 - val_mse: 2382.4546 - val_mae: 27.2169\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2675.4615 - mse: 2675.4617 - mae: 27.9714 - val_loss: 2373.6524 - val_mse: 2373.6526 - val_mae: 27.2790\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2674.2020 - mse: 2674.2026 - mae: 28.1043 - val_loss: 2381.5952 - val_mse: 2381.5950 - val_mae: 27.0103\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2643.2075 - mse: 2643.2075 - mae: 28.2828 - val_loss: 2375.3641 - val_mse: 2375.3640 - val_mae: 27.0729\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2677.0799 - mse: 2677.0803 - mae: 28.3213 - val_loss: 2372.3537 - val_mse: 2372.3538 - val_mae: 27.3373\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2652.9159 - mse: 2652.9153 - mae: 28.3062 - val_loss: 2381.9610 - val_mse: 2381.9612 - val_mae: 27.2390\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2647.9826 - mse: 2647.9832 - mae: 27.9965 - val_loss: 2377.9309 - val_mse: 2377.9309 - val_mae: 27.0515\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 13291.9886 - mse: 13291.9893 - mae: 109.7405 - val_loss: 34547.9058 - val_mse: 34547.9062 - val_mae: 132.4519\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 428us/step - loss: 13055.1310 - mse: 13055.1318 - mae: 108.6619 - val_loss: 34131.2004 - val_mse: 34131.1992 - val_mae: 130.8341\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 270us/step - loss: 12424.5466 - mse: 12424.5459 - mae: 105.7257 - val_loss: 32978.4579 - val_mse: 32978.4570 - val_mae: 126.2365\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 415us/step - loss: 10853.9576 - mse: 10853.9570 - mae: 97.8965 - val_loss: 30055.6169 - val_mse: 30055.6172 - val_mae: 113.6831\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 444us/step - loss: 7529.0852 - mse: 7529.0854 - mae: 77.6651 - val_loss: 24201.7334 - val_mse: 24201.7344 - val_mae: 82.5524\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 3393.5472 - mse: 3393.5474 - mae: 44.6405 - val_loss: 18987.3610 - val_mse: 18987.3613 - val_mae: 47.5591\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 3000.6547 - mse: 3000.6543 - mae: 40.0362 - val_loss: 19052.5545 - val_mse: 19052.5547 - val_mae: 47.8281\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2974.8960 - mse: 2974.8958 - mae: 39.6243 - val_loss: 19365.3126 - val_mse: 19365.3145 - val_mae: 49.5602\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2532.3047 - mse: 2532.3047 - mae: 36.8249 - val_loss: 19062.0791 - val_mse: 19062.0801 - val_mae: 47.6959\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2937.4493 - mse: 2937.4495 - mae: 39.4411 - val_loss: 19105.7760 - val_mse: 19105.7754 - val_mae: 47.8597\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 507us/step - loss: 2818.4445 - mse: 2818.4446 - mae: 38.9259 - val_loss: 19257.0292 - val_mse: 19257.0293 - val_mae: 48.6795\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2641.0742 - mse: 2641.0745 - mae: 38.5180 - val_loss: 18939.3789 - val_mse: 18939.3789 - val_mae: 46.6793\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2842.5281 - mse: 2842.5281 - mae: 39.2606 - val_loss: 19416.9686 - val_mse: 19416.9688 - val_mae: 49.4600\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2788.3551 - mse: 2788.3547 - mae: 39.2667 - val_loss: 19016.5594 - val_mse: 19016.5586 - val_mae: 46.9906\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2735.8966 - mse: 2735.8965 - mae: 38.1859 - val_loss: 18809.1272 - val_mse: 18809.1270 - val_mae: 45.6100\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 689us/step - loss: 2574.2540 - mse: 2574.2542 - mae: 37.3968 - val_loss: 19024.4640 - val_mse: 19024.4668 - val_mae: 46.9001\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 2570.7503 - mse: 2570.7502 - mae: 37.1384 - val_loss: 18783.1064 - val_mse: 18783.1055 - val_mae: 45.3046\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 477us/step - loss: 2499.5589 - mse: 2499.5591 - mae: 37.1529 - val_loss: 18708.2477 - val_mse: 18708.2480 - val_mae: 44.7752\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 650us/step - loss: 2736.9671 - mse: 2736.9675 - mae: 37.8974 - val_loss: 18818.0048 - val_mse: 18818.0039 - val_mae: 45.4046\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 488us/step - loss: 2683.4230 - mse: 2683.4233 - mae: 37.7084 - val_loss: 18685.7825 - val_mse: 18685.7832 - val_mae: 44.4748\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2404.3786 - mse: 2404.3784 - mae: 35.8501 - val_loss: 18642.3112 - val_mse: 18642.3105 - val_mae: 44.1207\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2666.5173 - mse: 2666.5173 - mae: 37.9959 - val_loss: 18933.9368 - val_mse: 18933.9355 - val_mae: 45.9190\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 707us/step - loss: 2267.3435 - mse: 2267.3435 - mae: 34.4586 - val_loss: 18611.9634 - val_mse: 18611.9648 - val_mae: 43.8078\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 693us/step - loss: 2591.4055 - mse: 2591.4053 - mae: 36.6193 - val_loss: 18654.4797 - val_mse: 18654.4805 - val_mae: 44.0082\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2478.7429 - mse: 2478.7424 - mae: 36.5897 - val_loss: 18791.8433 - val_mse: 18791.8438 - val_mae: 44.8260\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 781us/step - loss: 2418.1186 - mse: 2418.1184 - mae: 35.4261 - val_loss: 18530.1403 - val_mse: 18530.1406 - val_mae: 43.1010\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 702us/step - loss: 2443.5994 - mse: 2443.5996 - mae: 35.6792 - val_loss: 18696.9412 - val_mse: 18696.9395 - val_mae: 44.0872\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 733us/step - loss: 2548.5151 - mse: 2548.5151 - mae: 36.3238 - val_loss: 18538.4790 - val_mse: 18538.4805 - val_mae: 43.0268\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 682us/step - loss: 2213.5354 - mse: 2213.5354 - mae: 33.5799 - val_loss: 18379.5543 - val_mse: 18379.5547 - val_mae: 42.1271\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 731us/step - loss: 2416.9524 - mse: 2416.9524 - mae: 35.4309 - val_loss: 18640.8720 - val_mse: 18640.8730 - val_mae: 43.5199\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2408.0754 - mse: 2408.0754 - mae: 34.1389 - val_loss: 18481.6473 - val_mse: 18481.6484 - val_mae: 42.4494\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2508.3000 - mse: 2508.3000 - mae: 35.7087 - val_loss: 18471.9065 - val_mse: 18471.9082 - val_mae: 42.3308\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2226.7860 - mse: 2226.7861 - mae: 32.8698 - val_loss: 18440.7908 - val_mse: 18440.7930 - val_mae: 42.1028\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2358.4171 - mse: 2358.4170 - mae: 35.1650 - val_loss: 18457.6093 - val_mse: 18457.6094 - val_mae: 42.1227\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2423.3007 - mse: 2423.3008 - mae: 34.4914 - val_loss: 18301.0478 - val_mse: 18301.0469 - val_mae: 41.2964\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 2412.0414 - mse: 2412.0415 - mae: 35.3384 - val_loss: 18479.4175 - val_mse: 18479.4180 - val_mae: 42.1232\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2370.1819 - mse: 2370.1819 - mae: 35.3758 - val_loss: 18380.5261 - val_mse: 18380.5254 - val_mae: 41.5129\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 695us/step - loss: 2482.4834 - mse: 2482.4834 - mae: 35.6265 - val_loss: 18379.3068 - val_mse: 18379.3066 - val_mae: 41.4352\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2409.6531 - mse: 2409.6533 - mae: 34.4778 - val_loss: 18364.1701 - val_mse: 18364.1719 - val_mae: 41.2846\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 723us/step - loss: 2224.9396 - mse: 2224.9395 - mae: 33.0036 - val_loss: 18236.8773 - val_mse: 18236.8789 - val_mae: 40.6600\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 755us/step - loss: 2375.3211 - mse: 2375.3210 - mae: 33.5509 - val_loss: 18354.7987 - val_mse: 18354.7988 - val_mae: 41.1082\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 679us/step - loss: 2362.6222 - mse: 2362.6221 - mae: 34.2299 - val_loss: 18265.4409 - val_mse: 18265.4395 - val_mae: 40.6168\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 620us/step - loss: 2397.3436 - mse: 2397.3435 - mae: 33.8135 - val_loss: 18131.3146 - val_mse: 18131.3145 - val_mae: 40.0422\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2317.2606 - mse: 2317.2607 - mae: 33.9483 - val_loss: 18261.4331 - val_mse: 18261.4316 - val_mae: 40.4617\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 659us/step - loss: 2151.3832 - mse: 2151.3833 - mae: 32.9180 - val_loss: 18178.4960 - val_mse: 18178.4961 - val_mae: 40.0740\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 484us/step - loss: 2059.6350 - mse: 2059.6348 - mae: 32.1655 - val_loss: 18311.2117 - val_mse: 18311.2109 - val_mae: 40.5713\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 432us/step - loss: 2318.3918 - mse: 2318.3918 - mae: 33.1522 - val_loss: 18262.2227 - val_mse: 18262.2246 - val_mae: 40.2655\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 438us/step - loss: 2361.8975 - mse: 2361.8975 - mae: 34.3901 - val_loss: 18108.2095 - val_mse: 18108.2070 - val_mae: 39.5974\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 463us/step - loss: 1921.1992 - mse: 1921.1990 - mae: 32.3954 - val_loss: 18185.1230 - val_mse: 18185.1211 - val_mae: 39.8089\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 452us/step - loss: 2036.7095 - mse: 2036.7097 - mae: 32.3947 - val_loss: 18067.7946 - val_mse: 18067.7949 - val_mae: 39.3381\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2135.0703 - mse: 2135.0706 - mae: 33.2632 - val_loss: 18229.2961 - val_mse: 18229.2969 - val_mae: 39.8807\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2269.9582 - mse: 2269.9583 - mae: 32.1745 - val_loss: 18079.7583 - val_mse: 18079.7598 - val_mae: 39.2070\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 2247.9840 - mse: 2247.9841 - mae: 32.1637 - val_loss: 18123.0325 - val_mse: 18123.0332 - val_mae: 39.2901\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 2148.3973 - mse: 2148.3972 - mae: 32.9073 - val_loss: 17953.4891 - val_mse: 17953.4902 - val_mae: 38.7491\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 2302.1453 - mse: 2302.1455 - mae: 33.6035 - val_loss: 18083.5520 - val_mse: 18083.5527 - val_mae: 39.0244\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 629us/step - loss: 2115.6784 - mse: 2115.6782 - mae: 31.3251 - val_loss: 18069.9512 - val_mse: 18069.9512 - val_mae: 38.9412\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 700us/step - loss: 2237.9811 - mse: 2237.9810 - mae: 32.9445 - val_loss: 18122.7223 - val_mse: 18122.7227 - val_mae: 39.1015\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 640us/step - loss: 2230.3503 - mse: 2230.3501 - mae: 32.4695 - val_loss: 18103.0475 - val_mse: 18103.0488 - val_mae: 38.9503\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2182.2156 - mse: 2182.2153 - mae: 31.5663 - val_loss: 18057.3615 - val_mse: 18057.3613 - val_mae: 38.7135\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 660us/step - loss: 2127.1742 - mse: 2127.1741 - mae: 32.6242 - val_loss: 17931.9100 - val_mse: 17931.9102 - val_mae: 38.3103\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 2184.1600 - mse: 2184.1599 - mae: 32.7205 - val_loss: 18124.0745 - val_mse: 18124.0742 - val_mae: 38.9363\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 2220.3926 - mse: 2220.3926 - mae: 32.3597 - val_loss: 18178.7906 - val_mse: 18178.7930 - val_mae: 39.2312\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 556us/step - loss: 2127.0478 - mse: 2127.0479 - mae: 31.9732 - val_loss: 18004.1545 - val_mse: 18004.1562 - val_mae: 38.3302\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 2010.0614 - mse: 2010.0614 - mae: 30.3468 - val_loss: 18097.8315 - val_mse: 18097.8320 - val_mae: 38.7102\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 494us/step - loss: 2035.1869 - mse: 2035.1870 - mae: 30.8371 - val_loss: 17889.3794 - val_mse: 17889.3809 - val_mae: 37.9466\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 682us/step - loss: 2154.7080 - mse: 2154.7083 - mae: 31.5840 - val_loss: 18032.9610 - val_mse: 18032.9629 - val_mae: 38.3209\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 623us/step - loss: 1983.9899 - mse: 1983.9897 - mae: 31.3464 - val_loss: 17921.1931 - val_mse: 17921.1914 - val_mae: 37.9317\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 694us/step - loss: 1987.2348 - mse: 1987.2349 - mae: 30.3215 - val_loss: 17814.0161 - val_mse: 17814.0156 - val_mae: 37.7246\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 2085.6610 - mse: 2085.6609 - mae: 32.2140 - val_loss: 17984.5016 - val_mse: 17984.5000 - val_mae: 38.0586\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2048.9301 - mse: 2048.9302 - mae: 31.2514 - val_loss: 18000.5522 - val_mse: 18000.5527 - val_mae: 38.0927\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 1935.9194 - mse: 1935.9196 - mae: 30.8748 - val_loss: 17906.2426 - val_mse: 17906.2422 - val_mae: 37.7453\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 729us/step - loss: 1900.1810 - mse: 1900.1812 - mae: 30.9565 - val_loss: 17771.2163 - val_mse: 17771.2168 - val_mae: 37.5163\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2070.9301 - mse: 2070.9299 - mae: 31.1789 - val_loss: 17751.6547 - val_mse: 17751.6562 - val_mae: 37.4780\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 2000.4935 - mse: 2000.4937 - mae: 30.3144 - val_loss: 17791.5815 - val_mse: 17791.5801 - val_mae: 37.4329\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2009.8786 - mse: 2009.8785 - mae: 30.5796 - val_loss: 17926.2815 - val_mse: 17926.2793 - val_mae: 37.7035\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2122.9173 - mse: 2122.9175 - mae: 31.9184 - val_loss: 18021.9383 - val_mse: 18021.9375 - val_mae: 38.0665\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 594us/step - loss: 2047.4895 - mse: 2047.4895 - mae: 30.2982 - val_loss: 17712.4711 - val_mse: 17712.4727 - val_mae: 37.3398\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2084.8440 - mse: 2084.8440 - mae: 31.3880 - val_loss: 17851.7501 - val_mse: 17851.7520 - val_mae: 37.3900\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 1948.9427 - mse: 1948.9429 - mae: 30.1445 - val_loss: 18041.3806 - val_mse: 18041.3809 - val_mae: 38.1220\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2063.5760 - mse: 2063.5759 - mae: 31.4062 - val_loss: 18005.4013 - val_mse: 18005.4023 - val_mae: 37.9367\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4382.1499 - mse: 4382.1504 - mae: 34.3248 - val_loss: 2157.5566 - val_mse: 2157.5564 - val_mae: 31.0870\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 4269.9421 - mse: 4269.9419 - mae: 35.6371 - val_loss: 2284.3671 - val_mse: 2284.3669 - val_mae: 31.4809\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4286.4871 - mse: 4286.4863 - mae: 35.2910 - val_loss: 2261.5334 - val_mse: 2261.5332 - val_mae: 31.4188\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 658us/step - loss: 4189.5356 - mse: 4189.5356 - mae: 35.0598 - val_loss: 2294.7132 - val_mse: 2294.7131 - val_mae: 31.5249\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 676us/step - loss: 4114.2073 - mse: 4114.2075 - mae: 33.9032 - val_loss: 2310.8311 - val_mse: 2310.8311 - val_mae: 31.5795\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4352.3799 - mse: 4352.3799 - mae: 35.9742 - val_loss: 2356.3230 - val_mse: 2356.3230 - val_mae: 31.7210\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 4078.9764 - mse: 4078.9756 - mae: 34.0389 - val_loss: 2314.0763 - val_mse: 2314.0759 - val_mae: 31.6079\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 685us/step - loss: 4157.4296 - mse: 4157.4297 - mae: 34.3592 - val_loss: 2314.7607 - val_mse: 2314.7607 - val_mae: 31.6105\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 4111.0752 - mse: 4111.0757 - mae: 34.2153 - val_loss: 2301.4920 - val_mse: 2301.4919 - val_mae: 31.5760\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 714us/step - loss: 4262.5711 - mse: 4262.5713 - mae: 35.4648 - val_loss: 2364.9071 - val_mse: 2364.9072 - val_mae: 31.7771\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 4116.2909 - mse: 4116.2910 - mae: 33.8866 - val_loss: 2308.9402 - val_mse: 2308.9402 - val_mae: 31.6138\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4255.7069 - mse: 4255.7065 - mae: 35.3129 - val_loss: 2382.3457 - val_mse: 2382.3457 - val_mae: 31.8624\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 4182.5416 - mse: 4182.5420 - mae: 34.3190 - val_loss: 2308.8961 - val_mse: 2308.8962 - val_mae: 31.6263\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 519us/step - loss: 4190.7559 - mse: 4190.7554 - mae: 35.1870 - val_loss: 2335.1940 - val_mse: 2335.1941 - val_mae: 31.7044\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 4077.1470 - mse: 4077.1470 - mae: 34.0626 - val_loss: 2328.1904 - val_mse: 2328.1902 - val_mae: 31.6944\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 734us/step - loss: 4208.6698 - mse: 4208.6694 - mae: 34.3141 - val_loss: 2380.0739 - val_mse: 2380.0740 - val_mae: 31.8723\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 716us/step - loss: 4184.1328 - mse: 4184.1328 - mae: 33.6814 - val_loss: 2426.1613 - val_mse: 2426.1619 - val_mae: 32.0495\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4247.6024 - mse: 4247.6025 - mae: 34.9052 - val_loss: 2358.7349 - val_mse: 2358.7351 - val_mae: 31.8334\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 653us/step - loss: 4252.2718 - mse: 4252.2715 - mae: 33.9203 - val_loss: 2370.0410 - val_mse: 2370.0405 - val_mae: 31.8703\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4060.2605 - mse: 4060.2607 - mae: 34.6740 - val_loss: 2336.9511 - val_mse: 2336.9512 - val_mae: 31.7529\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 686us/step - loss: 4369.1898 - mse: 4369.1899 - mae: 35.1622 - val_loss: 2389.1088 - val_mse: 2389.1089 - val_mae: 31.9319\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4061.4265 - mse: 4061.4265 - mae: 34.4864 - val_loss: 2350.2544 - val_mse: 2350.2544 - val_mae: 31.8017\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4193.4021 - mse: 4193.4023 - mae: 34.5154 - val_loss: 2389.0877 - val_mse: 2389.0879 - val_mae: 31.9427\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4080.9275 - mse: 4080.9272 - mae: 34.2305 - val_loss: 2383.0770 - val_mse: 2383.0769 - val_mae: 31.9278\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 764us/step - loss: 4277.9103 - mse: 4277.9102 - mae: 33.9641 - val_loss: 2410.3350 - val_mse: 2410.3350 - val_mae: 32.0244\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 3963.1768 - mse: 3963.1765 - mae: 33.5501 - val_loss: 2341.9998 - val_mse: 2341.9998 - val_mae: 31.7995\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4084.7749 - mse: 4084.7751 - mae: 33.1301 - val_loss: 2361.6757 - val_mse: 2361.6755 - val_mae: 31.8720\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 4087.8657 - mse: 4087.8662 - mae: 33.5070 - val_loss: 2315.4514 - val_mse: 2315.4517 - val_mae: 31.7214\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 4273.4581 - mse: 4273.4585 - mae: 35.8935 - val_loss: 2416.0013 - val_mse: 2416.0012 - val_mae: 32.0617\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4054.6080 - mse: 4054.6082 - mae: 33.1942 - val_loss: 2364.9009 - val_mse: 2364.9009 - val_mae: 31.8872\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4000.9203 - mse: 4000.9202 - mae: 33.7292 - val_loss: 2355.3244 - val_mse: 2355.3242 - val_mae: 31.8576\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4093.8819 - mse: 4093.8816 - mae: 34.4022 - val_loss: 2378.6913 - val_mse: 2378.6912 - val_mae: 31.9419\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4028.9146 - mse: 4028.9150 - mae: 33.3160 - val_loss: 2329.8691 - val_mse: 2329.8691 - val_mae: 31.7773\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4305.0177 - mse: 4305.0176 - mae: 35.2863 - val_loss: 2433.3847 - val_mse: 2433.3845 - val_mae: 32.1337\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4277.3912 - mse: 4277.3906 - mae: 34.6839 - val_loss: 2422.4693 - val_mse: 2422.4692 - val_mae: 32.1047\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 677us/step - loss: 4091.1568 - mse: 4091.1562 - mae: 33.1217 - val_loss: 2337.4386 - val_mse: 2337.4387 - val_mae: 31.8116\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4140.7361 - mse: 4140.7363 - mae: 33.7243 - val_loss: 2362.4925 - val_mse: 2362.4924 - val_mae: 31.8943\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4043.8299 - mse: 4043.8298 - mae: 33.3906 - val_loss: 2320.7969 - val_mse: 2320.7971 - val_mae: 31.7662\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4102.4497 - mse: 4102.4497 - mae: 33.5266 - val_loss: 2336.8831 - val_mse: 2336.8831 - val_mae: 31.8039\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 3996.8337 - mse: 3996.8337 - mae: 33.3347 - val_loss: 2322.9668 - val_mse: 2322.9668 - val_mae: 31.7648\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4168.5582 - mse: 4168.5586 - mae: 34.2964 - val_loss: 2348.8766 - val_mse: 2348.8765 - val_mae: 31.8375\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4129.1308 - mse: 4129.1309 - mae: 33.9183 - val_loss: 2399.5972 - val_mse: 2399.5974 - val_mae: 32.0287\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 3997.5109 - mse: 3997.5105 - mae: 33.6084 - val_loss: 2349.5309 - val_mse: 2349.5310 - val_mae: 31.8649\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 700us/step - loss: 4030.7810 - mse: 4030.7805 - mae: 33.4888 - val_loss: 2369.2692 - val_mse: 2369.2695 - val_mae: 31.9386\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 3983.8820 - mse: 3983.8821 - mae: 33.9342 - val_loss: 2382.0060 - val_mse: 2382.0061 - val_mae: 31.9976\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4158.1870 - mse: 4158.1875 - mae: 33.3426 - val_loss: 2378.1470 - val_mse: 2378.1467 - val_mae: 31.9873\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4064.7479 - mse: 4064.7476 - mae: 33.2500 - val_loss: 2402.1018 - val_mse: 2402.1016 - val_mae: 32.0747\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 657us/step - loss: 4206.0344 - mse: 4206.0337 - mae: 34.8938 - val_loss: 2442.3838 - val_mse: 2442.3838 - val_mae: 32.2170\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 3970.2702 - mse: 3970.2703 - mae: 33.1551 - val_loss: 2372.2837 - val_mse: 2372.2837 - val_mae: 31.9749\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4101.3506 - mse: 4101.3511 - mae: 34.5987 - val_loss: 2396.5358 - val_mse: 2396.5359 - val_mae: 32.0641\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 3967.4037 - mse: 3967.4036 - mae: 33.8279 - val_loss: 2369.2915 - val_mse: 2369.2915 - val_mae: 31.9637\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 3886.3233 - mse: 3886.3235 - mae: 32.5192 - val_loss: 2367.4263 - val_mse: 2367.4263 - val_mae: 31.9636\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4188.9105 - mse: 4188.9102 - mae: 34.3607 - val_loss: 2502.9080 - val_mse: 2502.9080 - val_mae: 32.4409\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4036.3200 - mse: 4036.3203 - mae: 33.8591 - val_loss: 2446.2761 - val_mse: 2446.2759 - val_mae: 32.2493\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4047.1472 - mse: 4047.1475 - mae: 33.5463 - val_loss: 2393.4856 - val_mse: 2393.4856 - val_mae: 32.0735\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 681us/step - loss: 4105.4811 - mse: 4105.4810 - mae: 33.3575 - val_loss: 2374.6329 - val_mse: 2374.6333 - val_mae: 31.9997\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 692us/step - loss: 4125.9067 - mse: 4125.9067 - mae: 33.6455 - val_loss: 2425.4533 - val_mse: 2425.4536 - val_mae: 32.1731\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4165.3684 - mse: 4165.3687 - mae: 33.6337 - val_loss: 2429.2089 - val_mse: 2429.2087 - val_mae: 32.1941\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4023.0926 - mse: 4023.0928 - mae: 32.6848 - val_loss: 2416.2510 - val_mse: 2416.2512 - val_mae: 32.1591\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 614us/step - loss: 4045.2135 - mse: 4045.2136 - mae: 33.2582 - val_loss: 2400.7501 - val_mse: 2400.7502 - val_mae: 32.1118\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 3975.9684 - mse: 3975.9685 - mae: 33.5323 - val_loss: 2340.1216 - val_mse: 2340.1216 - val_mae: 31.9220\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4061.3055 - mse: 4061.3054 - mae: 34.0156 - val_loss: 2417.2693 - val_mse: 2417.2695 - val_mae: 32.1858\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4018.3598 - mse: 4018.3601 - mae: 33.3097 - val_loss: 2455.8699 - val_mse: 2455.8701 - val_mae: 32.3176\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4015.0755 - mse: 4015.0754 - mae: 32.6182 - val_loss: 2425.8421 - val_mse: 2425.8420 - val_mae: 32.2190\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4172.3529 - mse: 4172.3530 - mae: 33.6116 - val_loss: 2464.6056 - val_mse: 2464.6060 - val_mae: 32.3528\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4030.5032 - mse: 4030.5024 - mae: 33.4602 - val_loss: 2404.9089 - val_mse: 2404.9089 - val_mae: 32.1710\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 3886.8588 - mse: 3886.8591 - mae: 32.6288 - val_loss: 2427.1422 - val_mse: 2427.1421 - val_mae: 32.2526\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 669us/step - loss: 3894.9611 - mse: 3894.9617 - mae: 33.1925 - val_loss: 2464.9295 - val_mse: 2464.9292 - val_mae: 32.3848\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4010.4668 - mse: 4010.4666 - mae: 33.0380 - val_loss: 2464.8133 - val_mse: 2464.8130 - val_mae: 32.3845\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 3948.0241 - mse: 3948.0242 - mae: 33.1449 - val_loss: 2412.8783 - val_mse: 2412.8784 - val_mae: 32.2119\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4131.3361 - mse: 4131.3359 - mae: 33.6819 - val_loss: 2494.5595 - val_mse: 2494.5596 - val_mae: 32.4896\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 3908.8596 - mse: 3908.8596 - mae: 33.0897 - val_loss: 2423.0970 - val_mse: 2423.0969 - val_mae: 32.2539\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 3994.0857 - mse: 3994.0854 - mae: 32.6169 - val_loss: 2416.5060 - val_mse: 2416.5061 - val_mae: 32.2289\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 3984.5393 - mse: 3984.5388 - mae: 33.6408 - val_loss: 2436.6914 - val_mse: 2436.6912 - val_mae: 32.2888\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4079.5710 - mse: 4079.5708 - mae: 33.0731 - val_loss: 2436.3919 - val_mse: 2436.3918 - val_mae: 32.2889\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4006.8322 - mse: 4006.8320 - mae: 33.3656 - val_loss: 2469.2276 - val_mse: 2469.2275 - val_mae: 32.3928\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4137.3428 - mse: 4137.3433 - mae: 33.0506 - val_loss: 2482.0654 - val_mse: 2482.0652 - val_mae: 32.4310\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 727us/step - loss: 3888.2168 - mse: 3888.2170 - mae: 32.2313 - val_loss: 2425.5060 - val_mse: 2425.5059 - val_mae: 32.2348\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 556us/step - loss: 3985.4555 - mse: 3985.4551 - mae: 32.9958 - val_loss: 2372.8375 - val_mse: 2372.8376 - val_mae: 32.0648\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4071.4051 - mse: 4071.4050 - mae: 32.9426 - val_loss: 2423.4138 - val_mse: 2423.4138 - val_mae: 32.2407\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3422.4465 - mse: 3422.4468 - mae: 33.2259 - val_loss: 1457.4008 - val_mse: 1457.4008 - val_mae: 24.8673\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 683us/step - loss: 3378.1508 - mse: 3378.1516 - mae: 32.7086 - val_loss: 1456.0762 - val_mse: 1456.0764 - val_mae: 25.4277\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3416.9747 - mse: 3416.9749 - mae: 33.2705 - val_loss: 1457.7405 - val_mse: 1457.7406 - val_mae: 24.8535\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3443.8055 - mse: 3443.8054 - mae: 32.9448 - val_loss: 1456.4165 - val_mse: 1456.4167 - val_mae: 25.3507\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3236.9585 - mse: 3236.9590 - mae: 32.2732 - val_loss: 1456.5699 - val_mse: 1456.5701 - val_mae: 25.4769\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3427.6410 - mse: 3427.6414 - mae: 32.6593 - val_loss: 1459.9669 - val_mse: 1459.9670 - val_mae: 26.1652\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3529.0503 - mse: 3529.0491 - mae: 33.5192 - val_loss: 1456.6084 - val_mse: 1456.6083 - val_mae: 25.4579\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3406.1072 - mse: 3406.1074 - mae: 32.5996 - val_loss: 1457.9779 - val_mse: 1457.9779 - val_mae: 25.8314\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3417.7600 - mse: 3417.7598 - mae: 32.5839 - val_loss: 1459.4251 - val_mse: 1459.4253 - val_mae: 26.0670\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3519.0272 - mse: 3519.0273 - mae: 33.2283 - val_loss: 1456.6551 - val_mse: 1456.6550 - val_mae: 25.2168\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3330.5472 - mse: 3330.5476 - mae: 32.2879 - val_loss: 1459.3738 - val_mse: 1459.3738 - val_mae: 26.1395\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 521us/step - loss: 3308.9629 - mse: 3308.9629 - mae: 31.8842 - val_loss: 1456.8870 - val_mse: 1456.8871 - val_mae: 25.6831\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3428.3075 - mse: 3428.3076 - mae: 32.3084 - val_loss: 1456.7399 - val_mse: 1456.7397 - val_mae: 25.5104\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 531us/step - loss: 3320.2818 - mse: 3320.2815 - mae: 32.5551 - val_loss: 1459.0401 - val_mse: 1459.0399 - val_mae: 26.0481\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 474us/step - loss: 3460.4906 - mse: 3460.4893 - mae: 33.2680 - val_loss: 1456.9468 - val_mse: 1456.9467 - val_mae: 25.1534\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 633us/step - loss: 3449.7471 - mse: 3449.7471 - mae: 32.6598 - val_loss: 1457.3865 - val_mse: 1457.3865 - val_mae: 25.4753\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 677us/step - loss: 3474.2164 - mse: 3474.2161 - mae: 32.9420 - val_loss: 1457.7995 - val_mse: 1457.7994 - val_mae: 25.4296\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3382.0520 - mse: 3382.0522 - mae: 32.6503 - val_loss: 1459.6854 - val_mse: 1459.6855 - val_mae: 25.9328\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3412.9789 - mse: 3412.9790 - mae: 32.2287 - val_loss: 1458.3179 - val_mse: 1458.3179 - val_mae: 25.1702\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3367.1160 - mse: 3367.1157 - mae: 32.3971 - val_loss: 1465.8168 - val_mse: 1465.8169 - val_mae: 26.6664\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3388.0525 - mse: 3388.0527 - mae: 33.0894 - val_loss: 1458.1915 - val_mse: 1458.1917 - val_mae: 25.2218\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3447.2381 - mse: 3447.2385 - mae: 32.7962 - val_loss: 1460.3994 - val_mse: 1460.3994 - val_mae: 26.0613\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3348.7367 - mse: 3348.7371 - mae: 32.1633 - val_loss: 1460.7640 - val_mse: 1460.7639 - val_mae: 26.1203\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3231.6667 - mse: 3231.6672 - mae: 32.2935 - val_loss: 1458.3409 - val_mse: 1458.3409 - val_mae: 25.3509\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3276.5834 - mse: 3276.5847 - mae: 32.6286 - val_loss: 1459.8011 - val_mse: 1459.8011 - val_mae: 25.9713\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3247.2735 - mse: 3247.2739 - mae: 31.7618 - val_loss: 1459.1482 - val_mse: 1459.1481 - val_mae: 25.8850\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3416.1387 - mse: 3416.1389 - mae: 33.5579 - val_loss: 1458.3718 - val_mse: 1458.3718 - val_mae: 25.1693\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3254.1041 - mse: 3254.1047 - mae: 32.4715 - val_loss: 1460.0373 - val_mse: 1460.0372 - val_mae: 25.9662\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3446.7240 - mse: 3446.7249 - mae: 32.9948 - val_loss: 1458.6045 - val_mse: 1458.6046 - val_mae: 25.4284\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3248.2157 - mse: 3248.2153 - mae: 31.6132 - val_loss: 1458.5994 - val_mse: 1458.5992 - val_mae: 25.1764\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3311.6341 - mse: 3311.6340 - mae: 32.0303 - val_loss: 1460.8654 - val_mse: 1460.8655 - val_mae: 26.0695\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3269.1526 - mse: 3269.1531 - mae: 31.5537 - val_loss: 1458.7419 - val_mse: 1458.7418 - val_mae: 25.5933\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3363.4380 - mse: 3363.4382 - mae: 32.5070 - val_loss: 1458.6478 - val_mse: 1458.6477 - val_mae: 25.4059\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3267.0960 - mse: 3267.0957 - mae: 31.9472 - val_loss: 1461.1298 - val_mse: 1461.1300 - val_mae: 26.0807\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 680us/step - loss: 3270.5426 - mse: 3270.5427 - mae: 31.7179 - val_loss: 1460.4364 - val_mse: 1460.4365 - val_mae: 25.9562\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3385.7401 - mse: 3385.7395 - mae: 32.6822 - val_loss: 1459.0409 - val_mse: 1459.0408 - val_mae: 25.3376\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 447us/step - loss: 3405.2602 - mse: 3405.2598 - mae: 32.5159 - val_loss: 1460.2598 - val_mse: 1460.2596 - val_mae: 25.8199\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3283.8590 - mse: 3283.8591 - mae: 31.8291 - val_loss: 1459.6419 - val_mse: 1459.6418 - val_mae: 25.6428\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3308.4490 - mse: 3308.4495 - mae: 32.8374 - val_loss: 1459.6202 - val_mse: 1459.6204 - val_mae: 25.2257\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3278.0147 - mse: 3278.0144 - mae: 31.8546 - val_loss: 1459.7303 - val_mse: 1459.7305 - val_mae: 25.3951\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 678us/step - loss: 3261.7490 - mse: 3261.7498 - mae: 31.5056 - val_loss: 1460.9293 - val_mse: 1460.9292 - val_mae: 25.9066\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3298.6110 - mse: 3298.6113 - mae: 32.1696 - val_loss: 1461.2985 - val_mse: 1461.2985 - val_mae: 25.8796\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3315.3496 - mse: 3315.3496 - mae: 31.6513 - val_loss: 1460.6641 - val_mse: 1460.6641 - val_mae: 25.1433\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3362.7964 - mse: 3362.7966 - mae: 32.0736 - val_loss: 1460.4148 - val_mse: 1460.4149 - val_mae: 25.3100\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3261.8297 - mse: 3261.8289 - mae: 32.1269 - val_loss: 1460.7223 - val_mse: 1460.7223 - val_mae: 25.6693\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3220.8125 - mse: 3220.8130 - mae: 30.9190 - val_loss: 1465.8978 - val_mse: 1465.8979 - val_mae: 26.4849\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3356.4567 - mse: 3356.4565 - mae: 32.4696 - val_loss: 1460.6799 - val_mse: 1460.6798 - val_mae: 25.4100\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3375.2651 - mse: 3375.2661 - mae: 32.1426 - val_loss: 1460.3779 - val_mse: 1460.3778 - val_mae: 25.5671\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3177.2484 - mse: 3177.2478 - mae: 31.0734 - val_loss: 1465.1212 - val_mse: 1465.1210 - val_mae: 26.3988\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3236.8975 - mse: 3236.8967 - mae: 31.9501 - val_loss: 1460.9126 - val_mse: 1460.9125 - val_mae: 25.6788\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3233.4283 - mse: 3233.4280 - mae: 31.7645 - val_loss: 1461.2989 - val_mse: 1461.2990 - val_mae: 25.6669\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3379.9253 - mse: 3379.9250 - mae: 32.4519 - val_loss: 1461.6426 - val_mse: 1461.6426 - val_mae: 25.2903\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3243.8238 - mse: 3243.8245 - mae: 31.4851 - val_loss: 1461.9536 - val_mse: 1461.9537 - val_mae: 25.3915\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3189.7666 - mse: 3189.7673 - mae: 31.5161 - val_loss: 1462.1109 - val_mse: 1462.1108 - val_mae: 25.7873\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 535us/step - loss: 3241.3594 - mse: 3241.3591 - mae: 31.8413 - val_loss: 1462.2431 - val_mse: 1462.2432 - val_mae: 25.7965\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3200.5229 - mse: 3200.5232 - mae: 31.2964 - val_loss: 1461.3691 - val_mse: 1461.3691 - val_mae: 25.5460\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3369.2643 - mse: 3369.2642 - mae: 32.5179 - val_loss: 1463.2774 - val_mse: 1463.2776 - val_mae: 24.9347\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3336.9200 - mse: 3336.9202 - mae: 32.2328 - val_loss: 1462.4875 - val_mse: 1462.4874 - val_mae: 25.2087\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3246.4620 - mse: 3246.4619 - mae: 31.4176 - val_loss: 1462.5763 - val_mse: 1462.5762 - val_mae: 25.6753\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3294.6707 - mse: 3294.6711 - mae: 31.7414 - val_loss: 1463.1513 - val_mse: 1463.1511 - val_mae: 25.8314\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3370.5191 - mse: 3370.5193 - mae: 31.8866 - val_loss: 1462.5555 - val_mse: 1462.5553 - val_mae: 25.2426\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3219.9127 - mse: 3219.9124 - mae: 31.1328 - val_loss: 1464.5513 - val_mse: 1464.5515 - val_mae: 26.0925\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3387.3208 - mse: 3387.3208 - mae: 32.1974 - val_loss: 1463.8757 - val_mse: 1463.8759 - val_mae: 25.9490\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3249.4568 - mse: 3249.4573 - mae: 31.8787 - val_loss: 1462.5632 - val_mse: 1462.5634 - val_mae: 25.6087\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3287.3487 - mse: 3287.3479 - mae: 32.1838 - val_loss: 1462.5079 - val_mse: 1462.5081 - val_mae: 25.7248\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3343.1519 - mse: 3343.1521 - mae: 32.3852 - val_loss: 1463.6710 - val_mse: 1463.6711 - val_mae: 25.9269\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 512us/step - loss: 3348.0467 - mse: 3348.0469 - mae: 31.8456 - val_loss: 1462.8724 - val_mse: 1462.8723 - val_mae: 25.6989\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3308.4730 - mse: 3308.4731 - mae: 31.8268 - val_loss: 1462.7870 - val_mse: 1462.7870 - val_mae: 25.5189\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3264.9608 - mse: 3264.9604 - mae: 31.4625 - val_loss: 1463.3085 - val_mse: 1463.3085 - val_mae: 25.6524\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3240.8797 - mse: 3240.8789 - mae: 31.7806 - val_loss: 1466.4776 - val_mse: 1466.4777 - val_mae: 26.2689\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3306.4063 - mse: 3306.4067 - mae: 31.7834 - val_loss: 1465.5668 - val_mse: 1465.5670 - val_mae: 26.1060\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 694us/step - loss: 3297.8132 - mse: 3297.8127 - mae: 32.0815 - val_loss: 1463.4647 - val_mse: 1463.4646 - val_mae: 25.5646\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3371.3994 - mse: 3371.3994 - mae: 32.0920 - val_loss: 1464.0774 - val_mse: 1464.0774 - val_mae: 25.4863\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3312.2665 - mse: 3312.2659 - mae: 31.6829 - val_loss: 1465.5356 - val_mse: 1465.5356 - val_mae: 26.0507\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 704us/step - loss: 3320.1762 - mse: 3320.1760 - mae: 31.8260 - val_loss: 1466.5176 - val_mse: 1466.5176 - val_mae: 26.1804\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3253.1374 - mse: 3253.1375 - mae: 32.1394 - val_loss: 1464.6728 - val_mse: 1464.6729 - val_mae: 25.6192\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3389.9264 - mse: 3389.9260 - mae: 32.3297 - val_loss: 1465.8542 - val_mse: 1465.8541 - val_mae: 25.0849\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3254.9018 - mse: 3254.9014 - mae: 31.4179 - val_loss: 1466.1222 - val_mse: 1466.1223 - val_mae: 25.8982\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3193.3255 - mse: 3193.3252 - mae: 31.6282 - val_loss: 1465.6513 - val_mse: 1465.6512 - val_mae: 25.8068\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3204.8727 - mse: 3204.8728 - mae: 31.1941 - val_loss: 1465.7418 - val_mse: 1465.7418 - val_mae: 25.9346\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 515us/step - loss: 3033.0172 - mse: 3033.0173 - mae: 31.7631 - val_loss: 1076.5861 - val_mse: 1076.5861 - val_mae: 23.7737\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 641us/step - loss: 2939.8717 - mse: 2939.8708 - mae: 31.3937 - val_loss: 1074.2911 - val_mse: 1074.2911 - val_mae: 24.0369\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2827.0098 - mse: 2827.0098 - mae: 30.3694 - val_loss: 1073.9165 - val_mse: 1073.9165 - val_mae: 24.1180\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2898.5834 - mse: 2898.5835 - mae: 31.1739 - val_loss: 1075.6945 - val_mse: 1075.6943 - val_mae: 23.7303\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2864.5940 - mse: 2864.5947 - mae: 30.7759 - val_loss: 1074.2065 - val_mse: 1074.2065 - val_mae: 23.8929\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 656us/step - loss: 2946.1611 - mse: 2946.1616 - mae: 31.4231 - val_loss: 1078.1118 - val_mse: 1078.1117 - val_mae: 23.5524\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2930.2151 - mse: 2930.2156 - mae: 31.4610 - val_loss: 1072.7132 - val_mse: 1072.7131 - val_mae: 24.1120\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2887.9266 - mse: 2887.9272 - mae: 30.8047 - val_loss: 1073.2569 - val_mse: 1073.2568 - val_mae: 23.9530\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2777.2848 - mse: 2777.2849 - mae: 29.9576 - val_loss: 1072.2944 - val_mse: 1072.2944 - val_mae: 24.2767\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2950.3261 - mse: 2950.3254 - mae: 31.4902 - val_loss: 1072.7771 - val_mse: 1072.7772 - val_mae: 24.4566\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 687us/step - loss: 2923.1676 - mse: 2923.1680 - mae: 31.1445 - val_loss: 1074.1319 - val_mse: 1074.1318 - val_mae: 23.8030\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2888.6311 - mse: 2888.6309 - mae: 30.7562 - val_loss: 1072.0357 - val_mse: 1072.0358 - val_mae: 24.0799\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2913.2160 - mse: 2913.2158 - mae: 30.5370 - val_loss: 1071.9769 - val_mse: 1071.9769 - val_mae: 24.0855\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2866.3116 - mse: 2866.3108 - mae: 30.8666 - val_loss: 1071.9727 - val_mse: 1071.9728 - val_mae: 24.2730\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2923.8242 - mse: 2923.8242 - mae: 30.9592 - val_loss: 1072.8945 - val_mse: 1072.8945 - val_mae: 23.9168\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2908.3085 - mse: 2908.3093 - mae: 31.0204 - val_loss: 1073.2185 - val_mse: 1073.2185 - val_mae: 24.6778\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2905.3721 - mse: 2905.3723 - mae: 31.4375 - val_loss: 1072.0698 - val_mse: 1072.0698 - val_mae: 23.9950\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2898.4122 - mse: 2898.4114 - mae: 30.6621 - val_loss: 1072.1448 - val_mse: 1072.1447 - val_mae: 24.4305\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2873.7273 - mse: 2873.7266 - mae: 30.4259 - val_loss: 1072.2413 - val_mse: 1072.2412 - val_mae: 24.4372\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2946.9096 - mse: 2946.9099 - mae: 31.6589 - val_loss: 1073.0810 - val_mse: 1073.0811 - val_mae: 23.8697\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2906.9806 - mse: 2906.9805 - mae: 30.5278 - val_loss: 1072.9072 - val_mse: 1072.9073 - val_mae: 23.9083\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2793.3504 - mse: 2793.3501 - mae: 30.7738 - val_loss: 1072.0283 - val_mse: 1072.0283 - val_mae: 24.3066\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2928.2744 - mse: 2928.2732 - mae: 30.8505 - val_loss: 1072.1752 - val_mse: 1072.1752 - val_mae: 24.3061\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2847.0519 - mse: 2847.0513 - mae: 30.3864 - val_loss: 1072.0317 - val_mse: 1072.0317 - val_mae: 24.1681\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2819.1853 - mse: 2819.1853 - mae: 30.5403 - val_loss: 1072.0260 - val_mse: 1072.0261 - val_mae: 24.2494\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 664us/step - loss: 2944.9024 - mse: 2944.9026 - mae: 31.2670 - val_loss: 1071.9906 - val_mse: 1071.9905 - val_mae: 24.1256\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 695us/step - loss: 2839.2706 - mse: 2839.2708 - mae: 30.5458 - val_loss: 1072.1151 - val_mse: 1072.1150 - val_mae: 24.3976\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2881.6755 - mse: 2881.6750 - mae: 31.0172 - val_loss: 1072.0978 - val_mse: 1072.0979 - val_mae: 24.0515\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2967.8874 - mse: 2967.8877 - mae: 31.1444 - val_loss: 1073.1537 - val_mse: 1073.1538 - val_mae: 23.9039\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2831.5542 - mse: 2831.5544 - mae: 30.4481 - val_loss: 1072.2233 - val_mse: 1072.2233 - val_mae: 24.3240\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2931.2761 - mse: 2931.2756 - mae: 30.5868 - val_loss: 1071.9439 - val_mse: 1071.9438 - val_mae: 24.2144\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2877.5448 - mse: 2877.5454 - mae: 30.4543 - val_loss: 1072.7638 - val_mse: 1072.7638 - val_mae: 24.5807\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2876.7581 - mse: 2876.7583 - mae: 30.4882 - val_loss: 1072.5775 - val_mse: 1072.5774 - val_mae: 24.4373\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2852.9669 - mse: 2852.9666 - mae: 31.1587 - val_loss: 1072.3937 - val_mse: 1072.3938 - val_mae: 24.3172\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 550us/step - loss: 2870.6764 - mse: 2870.6755 - mae: 30.5427 - val_loss: 1072.5400 - val_mse: 1072.5398 - val_mae: 24.2782\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2893.8600 - mse: 2893.8604 - mae: 30.5175 - val_loss: 1072.4322 - val_mse: 1072.4323 - val_mae: 24.2323\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2824.8564 - mse: 2824.8574 - mae: 30.1706 - val_loss: 1076.3616 - val_mse: 1076.3618 - val_mae: 24.9986\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2922.6467 - mse: 2922.6467 - mae: 31.2272 - val_loss: 1072.6342 - val_mse: 1072.6342 - val_mae: 24.2508\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2925.2606 - mse: 2925.2607 - mae: 31.2673 - val_loss: 1073.6274 - val_mse: 1073.6273 - val_mae: 23.8826\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2807.2286 - mse: 2807.2290 - mae: 30.9973 - val_loss: 1072.2798 - val_mse: 1072.2798 - val_mae: 24.0564\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 2893.0724 - mse: 2893.0735 - mae: 30.8270 - val_loss: 1071.7854 - val_mse: 1071.7854 - val_mae: 24.1747\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2914.7011 - mse: 2914.7007 - mae: 30.8373 - val_loss: 1072.0384 - val_mse: 1072.0385 - val_mae: 24.3577\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2870.4404 - mse: 2870.4402 - mae: 30.6185 - val_loss: 1074.1420 - val_mse: 1074.1421 - val_mae: 24.8021\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2885.8690 - mse: 2885.8691 - mae: 30.7158 - val_loss: 1073.3827 - val_mse: 1073.3828 - val_mae: 24.7179\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2922.8124 - mse: 2922.8120 - mae: 31.2359 - val_loss: 1072.7819 - val_mse: 1072.7820 - val_mae: 23.8773\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 659us/step - loss: 2812.3812 - mse: 2812.3806 - mae: 30.4462 - val_loss: 1071.8235 - val_mse: 1071.8235 - val_mae: 24.4011\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2880.4651 - mse: 2880.4651 - mae: 30.2987 - val_loss: 1071.6623 - val_mse: 1071.6624 - val_mae: 24.2923\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 660us/step - loss: 2888.1736 - mse: 2888.1729 - mae: 30.9421 - val_loss: 1072.2663 - val_mse: 1072.2664 - val_mae: 24.5516\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 678us/step - loss: 2857.0096 - mse: 2857.0100 - mae: 31.0986 - val_loss: 1071.6474 - val_mse: 1071.6473 - val_mae: 24.3824\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2865.1638 - mse: 2865.1633 - mae: 30.5743 - val_loss: 1071.3748 - val_mse: 1071.3749 - val_mae: 24.2276\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2856.2942 - mse: 2856.2939 - mae: 30.4795 - val_loss: 1071.1891 - val_mse: 1071.1891 - val_mae: 24.1970\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2795.0219 - mse: 2795.0217 - mae: 29.7026 - val_loss: 1071.5947 - val_mse: 1071.5947 - val_mae: 24.4733\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 682us/step - loss: 2876.1534 - mse: 2876.1538 - mae: 30.8569 - val_loss: 1071.4684 - val_mse: 1071.4684 - val_mae: 24.3530\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2959.5087 - mse: 2959.5088 - mae: 30.9672 - val_loss: 1071.5615 - val_mse: 1071.5615 - val_mae: 24.0867\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2845.8095 - mse: 2845.8083 - mae: 30.1132 - val_loss: 1071.2376 - val_mse: 1071.2374 - val_mae: 24.3478\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2942.4349 - mse: 2942.4348 - mae: 31.2452 - val_loss: 1073.4386 - val_mse: 1073.4387 - val_mae: 24.7999\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2929.5132 - mse: 2929.5139 - mae: 31.2119 - val_loss: 1071.4159 - val_mse: 1071.4160 - val_mae: 24.1763\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2839.2504 - mse: 2839.2507 - mae: 30.2954 - val_loss: 1073.2404 - val_mse: 1073.2404 - val_mae: 24.7694\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2828.3303 - mse: 2828.3306 - mae: 30.5533 - val_loss: 1071.2492 - val_mse: 1071.2493 - val_mae: 24.2590\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2809.4266 - mse: 2809.4265 - mae: 30.3936 - val_loss: 1072.5423 - val_mse: 1072.5424 - val_mae: 24.6236\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 2790.2547 - mse: 2790.2563 - mae: 29.8714 - val_loss: 1073.4602 - val_mse: 1073.4602 - val_mae: 24.7876\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2845.4879 - mse: 2845.4880 - mae: 30.2875 - val_loss: 1071.1836 - val_mse: 1071.1836 - val_mae: 24.2570\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2895.9741 - mse: 2895.9736 - mae: 30.4651 - val_loss: 1071.5856 - val_mse: 1071.5857 - val_mae: 24.5043\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2875.8606 - mse: 2875.8611 - mae: 30.3053 - val_loss: 1071.4000 - val_mse: 1071.3999 - val_mae: 24.4485\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2878.0036 - mse: 2878.0029 - mae: 30.5171 - val_loss: 1070.6129 - val_mse: 1070.6128 - val_mae: 24.4306\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 530us/step - loss: 2926.2901 - mse: 2926.2898 - mae: 31.1823 - val_loss: 1069.6793 - val_mse: 1069.6794 - val_mae: 24.2231\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2826.9523 - mse: 2826.9524 - mae: 30.4723 - val_loss: 1070.5553 - val_mse: 1070.5551 - val_mae: 24.5000\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2861.2363 - mse: 2861.2363 - mae: 30.5944 - val_loss: 1070.3122 - val_mse: 1070.3121 - val_mae: 24.4438\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 664us/step - loss: 2878.1934 - mse: 2878.1934 - mae: 30.5329 - val_loss: 1070.7113 - val_mse: 1070.7113 - val_mae: 24.4948\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2873.9436 - mse: 2873.9441 - mae: 30.6371 - val_loss: 1071.9971 - val_mse: 1071.9969 - val_mae: 24.7158\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 535us/step - loss: 2916.8385 - mse: 2916.8386 - mae: 31.5589 - val_loss: 1069.9582 - val_mse: 1069.9583 - val_mae: 24.1921\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2855.3346 - mse: 2855.3345 - mae: 30.7388 - val_loss: 1070.8290 - val_mse: 1070.8290 - val_mae: 24.5389\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2915.7902 - mse: 2915.7908 - mae: 30.5064 - val_loss: 1070.5582 - val_mse: 1070.5582 - val_mae: 24.3807\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2833.6952 - mse: 2833.6956 - mae: 30.1360 - val_loss: 1071.5306 - val_mse: 1071.5306 - val_mae: 24.5996\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2897.2559 - mse: 2897.2559 - mae: 30.2088 - val_loss: 1070.9277 - val_mse: 1070.9277 - val_mae: 24.0213\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2898.6683 - mse: 2898.6682 - mae: 30.3630 - val_loss: 1070.7064 - val_mse: 1070.7064 - val_mae: 24.1506\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2841.2609 - mse: 2841.2605 - mae: 30.7313 - val_loss: 1071.0730 - val_mse: 1071.0730 - val_mae: 24.4938\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2767.3461 - mse: 2767.3457 - mae: 29.9427 - val_loss: 1072.8904 - val_mse: 1072.8904 - val_mae: 24.8610\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 518us/step - loss: 2902.0622 - mse: 2902.0627 - mae: 30.8068 - val_loss: 1072.7742 - val_mse: 1072.7742 - val_mae: 24.8702\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2863.5828 - mse: 2863.5828 - mae: 30.4010 - val_loss: 1070.4717 - val_mse: 1070.4717 - val_mae: 24.4898\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2556.6233 - mse: 2556.6228 - mae: 30.0749 - val_loss: 1536.2839 - val_mse: 1536.2839 - val_mae: 28.2326\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2578.1174 - mse: 2578.1177 - mae: 30.0319 - val_loss: 1534.0817 - val_mse: 1534.0815 - val_mae: 28.3428\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2575.9813 - mse: 2575.9812 - mae: 30.1038 - val_loss: 1533.6625 - val_mse: 1533.6625 - val_mae: 28.3245\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 661us/step - loss: 2605.0166 - mse: 2605.0168 - mae: 30.2020 - val_loss: 1539.2686 - val_mse: 1539.2687 - val_mae: 28.0242\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 541us/step - loss: 2580.7292 - mse: 2580.7292 - mae: 29.9544 - val_loss: 1532.8441 - val_mse: 1532.8441 - val_mae: 28.2979\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2557.8180 - mse: 2557.8176 - mae: 29.4355 - val_loss: 1531.7240 - val_mse: 1531.7239 - val_mae: 28.3608\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2471.4135 - mse: 2471.4128 - mae: 29.3258 - val_loss: 1532.7409 - val_mse: 1532.7410 - val_mae: 28.2664\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2601.5590 - mse: 2601.5593 - mae: 29.5244 - val_loss: 1525.8898 - val_mse: 1525.8899 - val_mae: 28.8031\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 512us/step - loss: 2606.6800 - mse: 2606.6797 - mae: 29.7008 - val_loss: 1534.5494 - val_mse: 1534.5493 - val_mae: 28.1919\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2577.7305 - mse: 2577.7305 - mae: 29.9662 - val_loss: 1536.8138 - val_mse: 1536.8137 - val_mae: 28.0649\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 687us/step - loss: 2551.9449 - mse: 2551.9453 - mae: 29.7479 - val_loss: 1529.7873 - val_mse: 1529.7874 - val_mae: 28.3899\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2554.0196 - mse: 2554.0203 - mae: 29.7214 - val_loss: 1530.5708 - val_mse: 1530.5708 - val_mae: 28.3568\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2500.0539 - mse: 2500.0535 - mae: 29.6869 - val_loss: 1531.6894 - val_mse: 1531.6895 - val_mae: 28.3012\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 679us/step - loss: 2578.6854 - mse: 2578.6846 - mae: 29.8509 - val_loss: 1532.9109 - val_mse: 1532.9109 - val_mae: 28.2058\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2490.0437 - mse: 2490.0439 - mae: 29.4818 - val_loss: 1528.3500 - val_mse: 1528.3500 - val_mae: 28.4485\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 483us/step - loss: 2563.1264 - mse: 2563.1265 - mae: 29.7879 - val_loss: 1529.3428 - val_mse: 1529.3427 - val_mae: 28.3533\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2488.5898 - mse: 2488.5901 - mae: 29.0482 - val_loss: 1530.4945 - val_mse: 1530.4944 - val_mae: 28.3040\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2521.6168 - mse: 2521.6172 - mae: 29.6499 - val_loss: 1528.2594 - val_mse: 1528.2595 - val_mae: 28.4443\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2552.3701 - mse: 2552.3696 - mae: 30.0110 - val_loss: 1526.7799 - val_mse: 1526.7798 - val_mae: 28.5692\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2462.7243 - mse: 2462.7246 - mae: 29.6674 - val_loss: 1525.1920 - val_mse: 1525.1919 - val_mae: 28.6169\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2512.1596 - mse: 2512.1594 - mae: 29.7953 - val_loss: 1525.3777 - val_mse: 1525.3779 - val_mae: 28.6197\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2519.9719 - mse: 2519.9724 - mae: 29.7762 - val_loss: 1524.8417 - val_mse: 1524.8419 - val_mae: 28.6529\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2545.6089 - mse: 2545.6091 - mae: 29.6448 - val_loss: 1526.1301 - val_mse: 1526.1302 - val_mae: 28.5486\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2559.2468 - mse: 2559.2466 - mae: 29.8165 - val_loss: 1526.0918 - val_mse: 1526.0919 - val_mae: 28.5785\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 521us/step - loss: 2486.5339 - mse: 2486.5334 - mae: 29.6163 - val_loss: 1523.2814 - val_mse: 1523.2816 - val_mae: 28.8008\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2496.5391 - mse: 2496.5396 - mae: 29.5133 - val_loss: 1522.5376 - val_mse: 1522.5377 - val_mae: 28.9360\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2477.1328 - mse: 2477.1328 - mae: 29.4446 - val_loss: 1525.7504 - val_mse: 1525.7502 - val_mae: 28.5711\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 550us/step - loss: 2613.2164 - mse: 2613.2161 - mae: 30.0549 - val_loss: 1523.1227 - val_mse: 1523.1227 - val_mae: 28.8004\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2627.4865 - mse: 2627.4863 - mae: 30.2879 - val_loss: 1525.7076 - val_mse: 1525.7075 - val_mae: 28.5064\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2535.9146 - mse: 2535.9155 - mae: 29.6683 - val_loss: 1526.3430 - val_mse: 1526.3429 - val_mae: 28.4662\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2595.1738 - mse: 2595.1733 - mae: 29.9622 - val_loss: 1527.3869 - val_mse: 1527.3870 - val_mae: 28.4137\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2555.3150 - mse: 2555.3149 - mae: 30.0405 - val_loss: 1526.4080 - val_mse: 1526.4080 - val_mae: 28.5800\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 531us/step - loss: 2492.6253 - mse: 2492.6262 - mae: 29.3987 - val_loss: 1525.7760 - val_mse: 1525.7760 - val_mae: 28.6542\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2584.5213 - mse: 2584.5205 - mae: 29.6891 - val_loss: 1528.9281 - val_mse: 1528.9283 - val_mae: 28.3469\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2540.9372 - mse: 2540.9373 - mae: 29.7033 - val_loss: 1525.7675 - val_mse: 1525.7675 - val_mae: 28.5232\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 519us/step - loss: 2511.4511 - mse: 2511.4519 - mae: 29.5899 - val_loss: 1524.8750 - val_mse: 1524.8748 - val_mae: 28.6290\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2554.5471 - mse: 2554.5469 - mae: 29.5587 - val_loss: 1523.6384 - val_mse: 1523.6383 - val_mae: 28.7661\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2545.1834 - mse: 2545.1831 - mae: 29.6987 - val_loss: 1530.4552 - val_mse: 1530.4552 - val_mae: 28.1895\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2527.1262 - mse: 2527.1260 - mae: 29.7028 - val_loss: 1531.7237 - val_mse: 1531.7238 - val_mae: 28.0618\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2546.9000 - mse: 2546.9004 - mae: 29.6716 - val_loss: 1526.2787 - val_mse: 1526.2789 - val_mae: 28.4027\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2553.9519 - mse: 2553.9519 - mae: 29.6700 - val_loss: 1523.1132 - val_mse: 1523.1129 - val_mae: 28.6762\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2493.9632 - mse: 2493.9624 - mae: 29.0501 - val_loss: 1522.6209 - val_mse: 1522.6208 - val_mae: 28.6823\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2501.6846 - mse: 2501.6838 - mae: 29.2327 - val_loss: 1524.6234 - val_mse: 1524.6234 - val_mae: 28.5291\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2516.9819 - mse: 2516.9827 - mae: 29.7252 - val_loss: 1522.7432 - val_mse: 1522.7433 - val_mae: 28.6835\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 670us/step - loss: 2568.1865 - mse: 2568.1873 - mae: 29.9596 - val_loss: 1526.1138 - val_mse: 1526.1136 - val_mae: 28.3809\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2527.5169 - mse: 2527.5176 - mae: 29.4701 - val_loss: 1525.4028 - val_mse: 1525.4028 - val_mae: 28.4387\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2452.3127 - mse: 2452.3135 - mae: 29.4412 - val_loss: 1522.0371 - val_mse: 1522.0370 - val_mae: 28.7216\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2527.8107 - mse: 2527.8103 - mae: 29.6614 - val_loss: 1523.9686 - val_mse: 1523.9685 - val_mae: 28.4818\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 518us/step - loss: 2578.6897 - mse: 2578.6897 - mae: 29.8805 - val_loss: 1524.2745 - val_mse: 1524.2745 - val_mae: 28.4655\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2507.9510 - mse: 2507.9507 - mae: 29.4837 - val_loss: 1523.4945 - val_mse: 1523.4944 - val_mae: 28.4704\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2516.1011 - mse: 2516.1016 - mae: 29.2291 - val_loss: 1519.9411 - val_mse: 1519.9412 - val_mae: 28.8707\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2506.3611 - mse: 2506.3611 - mae: 29.3034 - val_loss: 1519.1737 - val_mse: 1519.1737 - val_mae: 29.1244\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2457.9997 - mse: 2457.9998 - mae: 29.0606 - val_loss: 1524.1286 - val_mse: 1524.1285 - val_mae: 28.4838\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2554.0183 - mse: 2554.0183 - mae: 29.4834 - val_loss: 1523.9468 - val_mse: 1523.9467 - val_mae: 28.4599\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2494.9788 - mse: 2494.9790 - mae: 29.5418 - val_loss: 1521.1247 - val_mse: 1521.1248 - val_mae: 28.6645\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2574.2310 - mse: 2574.2314 - mae: 30.0768 - val_loss: 1521.6902 - val_mse: 1521.6901 - val_mae: 28.5872\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 547us/step - loss: 2558.6128 - mse: 2558.6128 - mae: 29.2821 - val_loss: 1521.9842 - val_mse: 1521.9841 - val_mae: 28.5984\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 496us/step - loss: 2505.4454 - mse: 2505.4458 - mae: 29.6116 - val_loss: 1519.9697 - val_mse: 1519.9695 - val_mae: 28.7280\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2527.5937 - mse: 2527.5938 - mae: 29.4809 - val_loss: 1523.7617 - val_mse: 1523.7620 - val_mae: 28.3844\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2514.3330 - mse: 2514.3320 - mae: 29.2342 - val_loss: 1520.7789 - val_mse: 1520.7789 - val_mae: 28.6017\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2547.5640 - mse: 2547.5630 - mae: 29.7786 - val_loss: 1521.8875 - val_mse: 1521.8877 - val_mae: 28.5444\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2491.0393 - mse: 2491.0391 - mae: 29.5119 - val_loss: 1521.4193 - val_mse: 1521.4193 - val_mae: 28.4988\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 669us/step - loss: 2477.6762 - mse: 2477.6770 - mae: 29.1327 - val_loss: 1521.0327 - val_mse: 1521.0327 - val_mae: 28.5432\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2470.4795 - mse: 2470.4790 - mae: 29.2693 - val_loss: 1519.0431 - val_mse: 1519.0432 - val_mae: 28.7263\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2485.2739 - mse: 2485.2742 - mae: 29.2593 - val_loss: 1519.8415 - val_mse: 1519.8414 - val_mae: 28.5914\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 602us/step - loss: 2470.4827 - mse: 2470.4822 - mae: 29.2719 - val_loss: 1520.6073 - val_mse: 1520.6073 - val_mae: 28.5067\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 514us/step - loss: 2506.9317 - mse: 2506.9316 - mae: 29.3299 - val_loss: 1520.6842 - val_mse: 1520.6844 - val_mae: 28.4843\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2516.9053 - mse: 2516.9043 - mae: 29.2858 - val_loss: 1520.0216 - val_mse: 1520.0217 - val_mae: 28.6044\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2495.6565 - mse: 2495.6562 - mae: 29.2582 - val_loss: 1518.4596 - val_mse: 1518.4595 - val_mae: 28.8432\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2500.4544 - mse: 2500.4543 - mae: 29.4879 - val_loss: 1520.1112 - val_mse: 1520.1112 - val_mae: 28.6063\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2518.8443 - mse: 2518.8445 - mae: 29.3469 - val_loss: 1522.3797 - val_mse: 1522.3798 - val_mae: 28.4315\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 476us/step - loss: 2494.0312 - mse: 2494.0308 - mae: 29.5531 - val_loss: 1519.1093 - val_mse: 1519.1095 - val_mae: 28.7885\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2506.0727 - mse: 2506.0735 - mae: 29.1999 - val_loss: 1520.3650 - val_mse: 1520.3649 - val_mae: 28.6784\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2523.5678 - mse: 2523.5681 - mae: 29.6110 - val_loss: 1519.7207 - val_mse: 1519.7207 - val_mae: 28.6963\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2468.8125 - mse: 2468.8130 - mae: 29.2751 - val_loss: 1522.5352 - val_mse: 1522.5352 - val_mae: 28.3991\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2480.9235 - mse: 2480.9236 - mae: 29.2672 - val_loss: 1522.4719 - val_mse: 1522.4719 - val_mae: 28.4142\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2543.4187 - mse: 2543.4182 - mae: 29.4506 - val_loss: 1520.7420 - val_mse: 1520.7422 - val_mae: 28.5350\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 661us/step - loss: 2538.5913 - mse: 2538.5906 - mae: 29.5149 - val_loss: 1523.4134 - val_mse: 1523.4132 - val_mae: 28.3312\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2482.3840 - mse: 2482.3835 - mae: 28.7547 - val_loss: 1516.9098 - val_mse: 1516.9098 - val_mae: 28.9322\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2548.9230 - mse: 2548.9226 - mae: 29.4326 - val_loss: 1523.3262 - val_mse: 1523.3263 - val_mae: 28.3141\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2446.2002 - mse: 2446.2004 - mae: 30.3378 - val_loss: 3713.2662 - val_mse: 3713.2664 - val_mae: 25.3369\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 532us/step - loss: 2395.4482 - mse: 2395.4482 - mae: 29.9512 - val_loss: 3712.6056 - val_mse: 3712.6062 - val_mae: 25.1629\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2429.0231 - mse: 2429.0227 - mae: 30.0279 - val_loss: 3717.2194 - val_mse: 3717.2195 - val_mae: 25.5646\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2358.0203 - mse: 2358.0215 - mae: 29.4537 - val_loss: 3714.7260 - val_mse: 3714.7256 - val_mae: 25.3386\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2367.7294 - mse: 2367.7290 - mae: 29.5243 - val_loss: 3710.4825 - val_mse: 3710.4819 - val_mae: 24.8077\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2396.2914 - mse: 2396.2920 - mae: 29.6624 - val_loss: 3716.3687 - val_mse: 3716.3694 - val_mae: 25.5553\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2342.6446 - mse: 2342.6445 - mae: 29.4813 - val_loss: 3721.7013 - val_mse: 3721.7019 - val_mae: 25.9578\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2399.9359 - mse: 2399.9365 - mae: 29.9870 - val_loss: 3712.6674 - val_mse: 3712.6670 - val_mae: 25.1042\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2393.4311 - mse: 2393.4309 - mae: 29.5657 - val_loss: 3714.5283 - val_mse: 3714.5288 - val_mae: 25.2247\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2428.2243 - mse: 2428.2249 - mae: 29.8933 - val_loss: 3712.4609 - val_mse: 3712.4614 - val_mae: 24.9707\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2422.4841 - mse: 2422.4846 - mae: 29.8055 - val_loss: 3712.8004 - val_mse: 3712.7996 - val_mae: 25.0948\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2394.6555 - mse: 2394.6548 - mae: 30.0465 - val_loss: 3713.3694 - val_mse: 3713.3701 - val_mae: 25.2543\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2401.2659 - mse: 2401.2656 - mae: 29.6169 - val_loss: 3715.2858 - val_mse: 3715.2852 - val_mae: 25.5402\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 571us/step - loss: 2418.1301 - mse: 2418.1299 - mae: 29.7071 - val_loss: 3711.0359 - val_mse: 3711.0359 - val_mae: 25.0228\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 517us/step - loss: 2408.5928 - mse: 2408.5928 - mae: 29.6664 - val_loss: 3712.2059 - val_mse: 3712.2056 - val_mae: 25.2431\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2411.9699 - mse: 2411.9700 - mae: 29.6473 - val_loss: 3709.7740 - val_mse: 3709.7744 - val_mae: 25.0015\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2369.1777 - mse: 2369.1780 - mae: 29.7850 - val_loss: 3713.4593 - val_mse: 3713.4587 - val_mae: 25.4752\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2408.6651 - mse: 2408.6653 - mae: 29.6471 - val_loss: 3713.0693 - val_mse: 3713.0701 - val_mae: 25.3928\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2420.9205 - mse: 2420.9204 - mae: 29.6231 - val_loss: 3712.5140 - val_mse: 3712.5142 - val_mae: 25.4531\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2377.4555 - mse: 2377.4556 - mae: 29.5321 - val_loss: 3707.8083 - val_mse: 3707.8071 - val_mae: 24.9517\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2388.2194 - mse: 2388.2195 - mae: 29.3060 - val_loss: 3707.7921 - val_mse: 3707.7925 - val_mae: 24.9603\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2390.1979 - mse: 2390.1982 - mae: 29.9176 - val_loss: 3707.8040 - val_mse: 3707.8044 - val_mae: 24.8680\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2377.3335 - mse: 2377.3337 - mae: 29.3750 - val_loss: 3710.7275 - val_mse: 3710.7283 - val_mae: 25.2288\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 563us/step - loss: 2375.4431 - mse: 2375.4434 - mae: 29.5624 - val_loss: 3710.7397 - val_mse: 3710.7393 - val_mae: 25.3294\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 523us/step - loss: 2376.0390 - mse: 2376.0391 - mae: 29.2923 - val_loss: 3710.4207 - val_mse: 3710.4207 - val_mae: 25.3134\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2364.4205 - mse: 2364.4199 - mae: 29.5754 - val_loss: 3711.5135 - val_mse: 3711.5137 - val_mae: 25.3774\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 546us/step - loss: 2416.1602 - mse: 2416.1609 - mae: 29.7883 - val_loss: 3709.4119 - val_mse: 3709.4126 - val_mae: 25.2337\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2366.6362 - mse: 2366.6365 - mae: 29.6339 - val_loss: 3708.0617 - val_mse: 3708.0615 - val_mae: 24.9860\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2380.0823 - mse: 2380.0830 - mae: 29.4794 - val_loss: 3715.5502 - val_mse: 3715.5503 - val_mae: 25.8031\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2409.7588 - mse: 2409.7590 - mae: 29.3915 - val_loss: 3711.7518 - val_mse: 3711.7524 - val_mae: 25.5422\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 558us/step - loss: 2379.0360 - mse: 2379.0361 - mae: 29.6446 - val_loss: 3708.7826 - val_mse: 3708.7825 - val_mae: 25.1700\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2355.7261 - mse: 2355.7263 - mae: 29.3558 - val_loss: 3712.1609 - val_mse: 3712.1616 - val_mae: 25.5378\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2349.2442 - mse: 2349.2446 - mae: 29.3787 - val_loss: 3706.3064 - val_mse: 3706.3062 - val_mae: 25.0404\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2440.4216 - mse: 2440.4211 - mae: 29.8619 - val_loss: 3707.3699 - val_mse: 3707.3701 - val_mae: 25.2600\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2429.6283 - mse: 2429.6284 - mae: 29.9438 - val_loss: 3703.7368 - val_mse: 3703.7356 - val_mae: 24.8316\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2370.5519 - mse: 2370.5520 - mae: 29.0338 - val_loss: 3709.8308 - val_mse: 3709.8311 - val_mae: 25.5704\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2322.2838 - mse: 2322.2832 - mae: 29.0522 - val_loss: 3710.2412 - val_mse: 3710.2417 - val_mae: 25.6031\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 521us/step - loss: 2319.2662 - mse: 2319.2659 - mae: 29.2088 - val_loss: 3705.6353 - val_mse: 3705.6355 - val_mae: 25.0298\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2387.4220 - mse: 2387.4226 - mae: 29.5092 - val_loss: 3705.8462 - val_mse: 3705.8467 - val_mae: 24.9912\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2353.2758 - mse: 2353.2751 - mae: 29.8658 - val_loss: 3705.4255 - val_mse: 3705.4260 - val_mae: 24.9922\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2356.6277 - mse: 2356.6274 - mae: 29.3654 - val_loss: 3703.0627 - val_mse: 3703.0632 - val_mae: 24.7661\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 525us/step - loss: 2365.8195 - mse: 2365.8196 - mae: 29.3195 - val_loss: 3708.2711 - val_mse: 3708.2725 - val_mae: 25.5014\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2386.6332 - mse: 2386.6331 - mae: 29.3027 - val_loss: 3703.9575 - val_mse: 3703.9578 - val_mae: 25.1643\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2381.8522 - mse: 2381.8518 - mae: 29.4825 - val_loss: 3703.1431 - val_mse: 3703.1433 - val_mae: 25.1502\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2345.9046 - mse: 2345.9045 - mae: 29.3012 - val_loss: 3702.3472 - val_mse: 3702.3477 - val_mae: 25.0543\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2405.6295 - mse: 2405.6292 - mae: 30.0854 - val_loss: 3699.9515 - val_mse: 3699.9504 - val_mae: 24.6678\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 686us/step - loss: 2435.8913 - mse: 2435.8914 - mae: 30.0734 - val_loss: 3704.6733 - val_mse: 3704.6731 - val_mae: 25.2681\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2360.4723 - mse: 2360.4717 - mae: 29.1813 - val_loss: 3703.5586 - val_mse: 3703.5576 - val_mae: 25.1252\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2325.0209 - mse: 2325.0203 - mae: 29.0520 - val_loss: 3705.5145 - val_mse: 3705.5142 - val_mae: 25.3940\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2357.6187 - mse: 2357.6189 - mae: 28.9432 - val_loss: 3704.9043 - val_mse: 3704.9045 - val_mae: 25.4272\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2311.7021 - mse: 2311.7014 - mae: 29.5438 - val_loss: 3700.9054 - val_mse: 3700.9053 - val_mae: 24.9139\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2395.7246 - mse: 2395.7249 - mae: 29.3356 - val_loss: 3702.5598 - val_mse: 3702.5603 - val_mae: 25.0257\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2372.9993 - mse: 2372.9998 - mae: 29.5976 - val_loss: 3700.4770 - val_mse: 3700.4766 - val_mae: 24.9185\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2339.9111 - mse: 2339.9119 - mae: 29.4131 - val_loss: 3701.8667 - val_mse: 3701.8674 - val_mae: 25.1072\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2388.2257 - mse: 2388.2263 - mae: 29.5522 - val_loss: 3699.8976 - val_mse: 3699.8977 - val_mae: 24.9658\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2362.2400 - mse: 2362.2402 - mae: 29.3969 - val_loss: 3700.7796 - val_mse: 3700.7791 - val_mae: 25.1354\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2375.1042 - mse: 2375.1042 - mae: 29.4324 - val_loss: 3700.1226 - val_mse: 3700.1228 - val_mae: 25.0731\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2362.5341 - mse: 2362.5337 - mae: 29.0438 - val_loss: 3701.0673 - val_mse: 3701.0669 - val_mae: 25.3051\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2370.3636 - mse: 2370.3640 - mae: 29.3663 - val_loss: 3697.8824 - val_mse: 3697.8818 - val_mae: 24.9168\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2379.4930 - mse: 2379.4927 - mae: 29.1613 - val_loss: 3700.2893 - val_mse: 3700.2891 - val_mae: 25.2041\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2409.0723 - mse: 2409.0730 - mae: 29.7540 - val_loss: 3698.7852 - val_mse: 3698.7859 - val_mae: 25.0094\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 645us/step - loss: 2385.2558 - mse: 2385.2561 - mae: 29.4229 - val_loss: 3695.9490 - val_mse: 3695.9492 - val_mae: 24.6081\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2398.4390 - mse: 2398.4390 - mae: 28.9647 - val_loss: 3698.6031 - val_mse: 3698.6030 - val_mae: 24.9770\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 524us/step - loss: 2337.9067 - mse: 2337.9062 - mae: 29.4075 - val_loss: 3697.2287 - val_mse: 3697.2290 - val_mae: 24.8797\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2370.4888 - mse: 2370.4883 - mae: 29.2182 - val_loss: 3701.0675 - val_mse: 3701.0679 - val_mae: 25.2193\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2366.6507 - mse: 2366.6501 - mae: 29.4249 - val_loss: 3699.3123 - val_mse: 3699.3115 - val_mae: 25.0373\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2379.1245 - mse: 2379.1252 - mae: 29.2356 - val_loss: 3704.7313 - val_mse: 3704.7310 - val_mae: 25.5723\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2340.4829 - mse: 2340.4822 - mae: 28.9229 - val_loss: 3696.5164 - val_mse: 3696.5156 - val_mae: 24.7111\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2408.6029 - mse: 2408.6033 - mae: 29.3513 - val_loss: 3694.6371 - val_mse: 3694.6370 - val_mae: 24.6354\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2339.8086 - mse: 2339.8079 - mae: 29.2214 - val_loss: 3697.9020 - val_mse: 3697.9019 - val_mae: 25.1163\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2332.9714 - mse: 2332.9712 - mae: 28.7998 - val_loss: 3700.0097 - val_mse: 3700.0098 - val_mae: 25.3825\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2341.7740 - mse: 2341.7737 - mae: 29.5422 - val_loss: 3696.8681 - val_mse: 3696.8679 - val_mae: 24.9530\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2318.6194 - mse: 2318.6191 - mae: 28.9741 - val_loss: 3696.7402 - val_mse: 3696.7402 - val_mae: 24.9370\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2337.4209 - mse: 2337.4209 - mae: 29.0945 - val_loss: 3698.0686 - val_mse: 3698.0686 - val_mae: 25.1537\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2352.8004 - mse: 2352.8003 - mae: 29.3923 - val_loss: 3693.9145 - val_mse: 3693.9153 - val_mae: 24.6492\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2252.2242 - mse: 2252.2249 - mae: 28.3201 - val_loss: 3697.7596 - val_mse: 3697.7598 - val_mae: 25.0953\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 541us/step - loss: 2409.8076 - mse: 2409.8071 - mae: 29.4243 - val_loss: 3696.1292 - val_mse: 3696.1292 - val_mae: 25.0371\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2309.9806 - mse: 2309.9805 - mae: 28.9709 - val_loss: 3696.2722 - val_mse: 3696.2710 - val_mae: 24.9697\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2345.3583 - mse: 2345.3572 - mae: 29.2349 - val_loss: 3700.1143 - val_mse: 3700.1145 - val_mae: 25.4560\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2326.4425 - mse: 2326.4424 - mae: 29.2816 - val_loss: 3701.6635 - val_mse: 3701.6636 - val_mae: 25.6154\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2737.2159 - mse: 2737.2163 - mae: 29.1135 - val_loss: 2342.4574 - val_mse: 2342.4575 - val_mae: 26.8439\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2749.7859 - mse: 2749.7854 - mae: 28.8120 - val_loss: 2338.3799 - val_mse: 2338.3801 - val_mae: 27.3926\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2755.2458 - mse: 2755.2456 - mae: 29.0603 - val_loss: 2339.8078 - val_mse: 2339.8079 - val_mae: 27.4110\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2741.0352 - mse: 2741.0349 - mae: 28.7801 - val_loss: 2338.7066 - val_mse: 2338.7068 - val_mae: 27.4323\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 538us/step - loss: 2736.0722 - mse: 2736.0715 - mae: 28.8502 - val_loss: 2344.6929 - val_mse: 2344.6931 - val_mae: 27.1663\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2731.8050 - mse: 2731.8057 - mae: 28.4381 - val_loss: 2341.7813 - val_mse: 2341.7812 - val_mae: 27.6425\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2782.8386 - mse: 2782.8379 - mae: 29.1950 - val_loss: 2344.3117 - val_mse: 2344.3115 - val_mae: 27.3605\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2741.0824 - mse: 2741.0820 - mae: 28.8159 - val_loss: 2356.7164 - val_mse: 2356.7166 - val_mae: 27.0265\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2723.2853 - mse: 2723.2854 - mae: 29.2368 - val_loss: 2347.6896 - val_mse: 2347.6895 - val_mae: 27.2716\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2750.7834 - mse: 2750.7842 - mae: 29.0352 - val_loss: 2357.1801 - val_mse: 2357.1802 - val_mae: 27.2908\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2706.0225 - mse: 2706.0217 - mae: 28.4039 - val_loss: 2349.2056 - val_mse: 2349.2053 - val_mae: 27.6639\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2746.8748 - mse: 2746.8743 - mae: 28.9195 - val_loss: 2351.2640 - val_mse: 2351.2639 - val_mae: 27.3251\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2717.1368 - mse: 2717.1372 - mae: 28.6716 - val_loss: 2356.2737 - val_mse: 2356.2734 - val_mae: 27.1019\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2732.9129 - mse: 2732.9131 - mae: 29.0900 - val_loss: 2347.7259 - val_mse: 2347.7258 - val_mae: 27.3486\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2755.4046 - mse: 2755.4055 - mae: 29.0093 - val_loss: 2353.9354 - val_mse: 2353.9353 - val_mae: 27.1496\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2753.9665 - mse: 2753.9668 - mae: 28.7350 - val_loss: 2347.6959 - val_mse: 2347.6956 - val_mae: 27.3410\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2765.8615 - mse: 2765.8613 - mae: 29.3129 - val_loss: 2347.1497 - val_mse: 2347.1497 - val_mae: 27.5035\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 549us/step - loss: 2742.7927 - mse: 2742.7927 - mae: 28.4623 - val_loss: 2349.8069 - val_mse: 2349.8069 - val_mae: 27.4121\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 674us/step - loss: 2744.6275 - mse: 2744.6272 - mae: 28.8824 - val_loss: 2352.2764 - val_mse: 2352.2761 - val_mae: 27.2198\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 663us/step - loss: 2697.8789 - mse: 2697.8799 - mae: 28.4788 - val_loss: 2361.1470 - val_mse: 2361.1465 - val_mae: 27.0279\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2774.2580 - mse: 2774.2583 - mae: 28.6589 - val_loss: 2371.8348 - val_mse: 2371.8350 - val_mae: 27.1318\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2730.5269 - mse: 2730.5264 - mae: 28.5839 - val_loss: 2376.2814 - val_mse: 2376.2817 - val_mae: 27.1416\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2741.7273 - mse: 2741.7273 - mae: 28.8087 - val_loss: 2368.7087 - val_mse: 2368.7090 - val_mae: 27.3717\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2729.8286 - mse: 2729.8281 - mae: 28.4694 - val_loss: 2363.2551 - val_mse: 2363.2549 - val_mae: 27.3755\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2698.8174 - mse: 2698.8181 - mae: 28.6999 - val_loss: 2364.4751 - val_mse: 2364.4749 - val_mae: 27.2906\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2741.9914 - mse: 2741.9917 - mae: 28.8720 - val_loss: 2368.1073 - val_mse: 2368.1072 - val_mae: 27.3188\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2697.7364 - mse: 2697.7363 - mae: 28.6243 - val_loss: 2358.8186 - val_mse: 2358.8186 - val_mae: 27.5737\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2739.4281 - mse: 2739.4285 - mae: 28.8057 - val_loss: 2361.5718 - val_mse: 2361.5718 - val_mae: 27.4840\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2721.2593 - mse: 2721.2593 - mae: 28.9066 - val_loss: 2359.0752 - val_mse: 2359.0750 - val_mae: 27.4981\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2746.8176 - mse: 2746.8179 - mae: 28.7747 - val_loss: 2363.4429 - val_mse: 2363.4431 - val_mae: 27.2139\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2709.0207 - mse: 2709.0208 - mae: 28.9459 - val_loss: 2369.2667 - val_mse: 2369.2666 - val_mae: 27.2659\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2758.9073 - mse: 2758.9080 - mae: 28.6018 - val_loss: 2369.3951 - val_mse: 2369.3950 - val_mae: 27.2645\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2710.0479 - mse: 2710.0471 - mae: 28.5063 - val_loss: 2377.0079 - val_mse: 2377.0076 - val_mae: 26.9862\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2742.3901 - mse: 2742.3896 - mae: 28.6841 - val_loss: 2365.4524 - val_mse: 2365.4521 - val_mae: 27.0168\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2727.8509 - mse: 2727.8511 - mae: 28.6040 - val_loss: 2363.6339 - val_mse: 2363.6338 - val_mae: 27.0513\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 535us/step - loss: 2748.3301 - mse: 2748.3301 - mae: 28.6250 - val_loss: 2365.0035 - val_mse: 2365.0032 - val_mae: 27.4860\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 662us/step - loss: 2736.5262 - mse: 2736.5266 - mae: 29.0441 - val_loss: 2373.1391 - val_mse: 2373.1392 - val_mae: 27.1274\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2722.0294 - mse: 2722.0281 - mae: 28.4835 - val_loss: 2364.8560 - val_mse: 2364.8562 - val_mae: 27.3675\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2714.9440 - mse: 2714.9434 - mae: 29.0112 - val_loss: 2360.0211 - val_mse: 2360.0210 - val_mae: 27.4121\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2678.2455 - mse: 2678.2456 - mae: 28.6080 - val_loss: 2356.4238 - val_mse: 2356.4241 - val_mae: 27.6040\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2736.3731 - mse: 2736.3728 - mae: 28.7404 - val_loss: 2359.4991 - val_mse: 2359.4990 - val_mae: 27.1345\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2702.9986 - mse: 2702.9988 - mae: 28.4023 - val_loss: 2355.8229 - val_mse: 2355.8230 - val_mae: 27.1322\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2750.5001 - mse: 2750.5005 - mae: 28.3885 - val_loss: 2354.6670 - val_mse: 2354.6675 - val_mae: 27.2456\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2722.4765 - mse: 2722.4773 - mae: 28.6304 - val_loss: 2360.0732 - val_mse: 2360.0732 - val_mae: 27.1438\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2724.7347 - mse: 2724.7336 - mae: 28.4076 - val_loss: 2357.8197 - val_mse: 2357.8196 - val_mae: 27.1631\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2727.9590 - mse: 2727.9592 - mae: 28.7667 - val_loss: 2357.3366 - val_mse: 2357.3367 - val_mae: 27.4096\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2723.6653 - mse: 2723.6658 - mae: 28.5969 - val_loss: 2362.0346 - val_mse: 2362.0349 - val_mae: 27.5799\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2739.5900 - mse: 2739.5896 - mae: 28.7082 - val_loss: 2372.7610 - val_mse: 2372.7610 - val_mae: 26.9030\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 672us/step - loss: 2709.4368 - mse: 2709.4358 - mae: 28.4417 - val_loss: 2371.6532 - val_mse: 2371.6531 - val_mae: 27.2880\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2785.7165 - mse: 2785.7163 - mae: 28.8948 - val_loss: 2373.0644 - val_mse: 2373.0645 - val_mae: 27.2709\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2739.9990 - mse: 2739.9995 - mae: 28.7409 - val_loss: 2375.8526 - val_mse: 2375.8523 - val_mae: 27.1930\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 497us/step - loss: 2718.8176 - mse: 2718.8174 - mae: 28.5956 - val_loss: 2376.0573 - val_mse: 2376.0574 - val_mae: 27.4366\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2704.6892 - mse: 2704.6887 - mae: 28.5791 - val_loss: 2381.0275 - val_mse: 2381.0276 - val_mae: 27.2124\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2710.3799 - mse: 2710.3811 - mae: 28.7660 - val_loss: 2368.9932 - val_mse: 2368.9929 - val_mae: 27.3048\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2715.1015 - mse: 2715.1008 - mae: 28.6117 - val_loss: 2374.1336 - val_mse: 2374.1335 - val_mae: 27.1702\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2687.1703 - mse: 2687.1704 - mae: 28.5741 - val_loss: 2369.9593 - val_mse: 2369.9592 - val_mae: 27.3095\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2712.0822 - mse: 2712.0835 - mae: 28.3139 - val_loss: 2365.2594 - val_mse: 2365.2595 - val_mae: 27.4049\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2730.0079 - mse: 2730.0085 - mae: 29.0266 - val_loss: 2365.7143 - val_mse: 2365.7144 - val_mae: 27.0455\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 502us/step - loss: 2677.6893 - mse: 2677.6892 - mae: 28.2609 - val_loss: 2356.2060 - val_mse: 2356.2061 - val_mae: 27.4195\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2692.0379 - mse: 2692.0374 - mae: 28.3132 - val_loss: 2360.3494 - val_mse: 2360.3491 - val_mae: 27.4858\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2680.9524 - mse: 2680.9524 - mae: 28.2407 - val_loss: 2363.0060 - val_mse: 2363.0056 - val_mae: 27.3873\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2721.3226 - mse: 2721.3225 - mae: 28.2651 - val_loss: 2370.2451 - val_mse: 2370.2449 - val_mae: 27.1653\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2742.5479 - mse: 2742.5483 - mae: 28.6383 - val_loss: 2374.3392 - val_mse: 2374.3391 - val_mae: 27.1204\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2735.1883 - mse: 2735.1877 - mae: 28.8722 - val_loss: 2363.3135 - val_mse: 2363.3130 - val_mae: 27.5509\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2708.7169 - mse: 2708.7168 - mae: 28.8908 - val_loss: 2354.8766 - val_mse: 2354.8765 - val_mae: 27.3201\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2751.5606 - mse: 2751.5601 - mae: 28.7031 - val_loss: 2356.3826 - val_mse: 2356.3823 - val_mae: 27.4872\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2725.1292 - mse: 2725.1287 - mae: 28.8581 - val_loss: 2350.3444 - val_mse: 2350.3442 - val_mae: 27.6210\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2714.1365 - mse: 2714.1370 - mae: 28.7519 - val_loss: 2343.0125 - val_mse: 2343.0127 - val_mae: 27.1018\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2663.4744 - mse: 2663.4744 - mae: 28.4049 - val_loss: 2339.6059 - val_mse: 2339.6057 - val_mae: 27.8977\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2741.4424 - mse: 2741.4419 - mae: 28.5934 - val_loss: 2348.9770 - val_mse: 2348.9766 - val_mae: 27.2450\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2712.7516 - mse: 2712.7522 - mae: 28.7005 - val_loss: 2344.9133 - val_mse: 2344.9136 - val_mae: 27.4701\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2702.9802 - mse: 2702.9805 - mae: 28.7047 - val_loss: 2349.8824 - val_mse: 2349.8821 - val_mae: 27.2135\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2691.8969 - mse: 2691.8967 - mae: 28.4366 - val_loss: 2347.7441 - val_mse: 2347.7439 - val_mae: 27.7015\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 531us/step - loss: 2699.1380 - mse: 2699.1379 - mae: 28.6926 - val_loss: 2349.4950 - val_mse: 2349.4946 - val_mae: 27.4478\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 515us/step - loss: 2723.9664 - mse: 2723.9663 - mae: 28.3543 - val_loss: 2347.7306 - val_mse: 2347.7307 - val_mae: 27.4611\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2700.4036 - mse: 2700.4031 - mae: 28.2947 - val_loss: 2350.2424 - val_mse: 2350.2422 - val_mae: 27.6800\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2739.4590 - mse: 2739.4597 - mae: 28.9831 - val_loss: 2363.4498 - val_mse: 2363.4497 - val_mae: 26.7589\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2730.8593 - mse: 2730.8594 - mae: 28.5782 - val_loss: 2359.6966 - val_mse: 2359.6965 - val_mae: 27.0689\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 661us/step - loss: 2698.4678 - mse: 2698.4680 - mae: 28.6960 - val_loss: 2354.4600 - val_mse: 2354.4600 - val_mae: 27.3272\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2729.0479 - mse: 2729.0474 - mae: 28.5308 - val_loss: 2351.9715 - val_mse: 2351.9714 - val_mae: 27.6563\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 13308.5045 - mse: 13308.5049 - mae: 109.8123 - val_loss: 34566.4407 - val_mse: 34566.4414 - val_mae: 132.5268\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 13078.8855 - mse: 13078.8857 - mae: 108.7637 - val_loss: 34095.6891 - val_mse: 34095.6914 - val_mae: 130.7204\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 404us/step - loss: 12362.3742 - mse: 12362.3740 - mae: 105.3963 - val_loss: 32720.0868 - val_mse: 32720.0859 - val_mae: 125.2868\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 415us/step - loss: 10485.5172 - mse: 10485.5176 - mae: 95.9877 - val_loss: 28996.5835 - val_mse: 28996.5840 - val_mae: 109.2160\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 397us/step - loss: 6254.3077 - mse: 6254.3091 - mae: 69.4760 - val_loss: 21837.9317 - val_mse: 21837.9316 - val_mae: 68.1843\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 3167.5484 - mse: 3167.5483 - mae: 41.96 - 0s 512us/step - loss: 3091.3897 - mse: 3091.3894 - mae: 41.4523 - val_loss: 18223.2231 - val_mse: 18223.2246 - val_mae: 38.4374\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 444us/step - loss: 2895.1608 - mse: 2895.1609 - mae: 38.7520 - val_loss: 18424.3330 - val_mse: 18424.3340 - val_mae: 39.1756\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2568.4203 - mse: 2568.4202 - mae: 35.7396 - val_loss: 18456.5432 - val_mse: 18456.5430 - val_mae: 39.3235\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2609.4139 - mse: 2609.4141 - mae: 37.2593 - val_loss: 18188.6705 - val_mse: 18188.6719 - val_mae: 38.3466\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 539us/step - loss: 2396.8696 - mse: 2396.8696 - mae: 36.2636 - val_loss: 18118.9879 - val_mse: 18118.9883 - val_mae: 38.2126\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 457us/step - loss: 2462.4902 - mse: 2462.4902 - mae: 35.9011 - val_loss: 18253.7815 - val_mse: 18253.7812 - val_mae: 38.4792\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 448us/step - loss: 2587.7612 - mse: 2587.7612 - mae: 36.2049 - val_loss: 18279.9002 - val_mse: 18279.9004 - val_mae: 38.5664\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 382us/step - loss: 2451.9532 - mse: 2451.9536 - mae: 34.6152 - val_loss: 18078.0211 - val_mse: 18078.0215 - val_mae: 38.1092\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 470us/step - loss: 2620.6420 - mse: 2620.6418 - mae: 36.4004 - val_loss: 18210.2016 - val_mse: 18210.1992 - val_mae: 38.3454\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 430us/step - loss: 2379.4106 - mse: 2379.4104 - mae: 34.4633 - val_loss: 18215.0526 - val_mse: 18215.0547 - val_mae: 38.3498\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 410us/step - loss: 2316.2046 - mse: 2316.2046 - mae: 35.3160 - val_loss: 18230.6851 - val_mse: 18230.6855 - val_mae: 38.3816\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 471us/step - loss: 2521.9883 - mse: 2521.9880 - mae: 35.5249 - val_loss: 18396.9123 - val_mse: 18396.9102 - val_mae: 39.0371\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 2195.0596 - mse: 2195.0596 - mae: 35.4745 - val_loss: 18230.0418 - val_mse: 18230.0430 - val_mae: 38.3698\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 477us/step - loss: 2440.6388 - mse: 2440.6389 - mae: 36.1214 - val_loss: 18497.4645 - val_mse: 18497.4648 - val_mae: 39.5515\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2471.0430 - mse: 2471.0430 - mae: 36.6073 - val_loss: 18297.2542 - val_mse: 18297.2539 - val_mae: 38.5971\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 453us/step - loss: 2363.8749 - mse: 2363.8748 - mae: 35.7178 - val_loss: 18123.9176 - val_mse: 18123.9160 - val_mae: 38.1271\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2540.6015 - mse: 2540.6018 - mae: 35.7326 - val_loss: 18064.3733 - val_mse: 18064.3770 - val_mae: 38.0095\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 483us/step - loss: 2244.6136 - mse: 2244.6138 - mae: 32.9882 - val_loss: 18130.1501 - val_mse: 18130.1504 - val_mae: 38.1220\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 483us/step - loss: 2652.6013 - mse: 2652.6016 - mae: 36.5061 - val_loss: 18360.5385 - val_mse: 18360.5371 - val_mae: 38.8529\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 482us/step - loss: 2338.6271 - mse: 2338.6274 - mae: 34.4658 - val_loss: 18029.5523 - val_mse: 18029.5547 - val_mae: 37.9130\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 2422.9231 - mse: 2422.9231 - mae: 35.6294 - val_loss: 18309.9863 - val_mse: 18309.9863 - val_mae: 38.6140\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 689us/step - loss: 2466.4241 - mse: 2466.4241 - mae: 35.9267 - val_loss: 18560.1958 - val_mse: 18560.1934 - val_mae: 40.0382\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2344.6111 - mse: 2344.6113 - mae: 34.7826 - val_loss: 18289.4274 - val_mse: 18289.4258 - val_mae: 38.5230\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 686us/step - loss: 2314.4476 - mse: 2314.4475 - mae: 34.3055 - val_loss: 18154.2579 - val_mse: 18154.2598 - val_mae: 38.1049\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 699us/step - loss: 2327.8260 - mse: 2327.8259 - mae: 34.0276 - val_loss: 18068.2619 - val_mse: 18068.2617 - val_mae: 37.9402\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 640us/step - loss: 2345.5683 - mse: 2345.5686 - mae: 33.9665 - val_loss: 17997.7713 - val_mse: 17997.7695 - val_mae: 37.8287\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 474us/step - loss: 2259.5831 - mse: 2259.5833 - mae: 33.5930 - val_loss: 18078.5697 - val_mse: 18078.5703 - val_mae: 37.9452\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 641us/step - loss: 2432.5272 - mse: 2432.5273 - mae: 34.5864 - val_loss: 18136.7508 - val_mse: 18136.7520 - val_mae: 38.0437\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 2534.0136 - mse: 2534.0137 - mae: 35.5419 - val_loss: 18159.0828 - val_mse: 18159.0820 - val_mae: 38.0766\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2225.7511 - mse: 2225.7512 - mae: 33.4236 - val_loss: 18096.9415 - val_mse: 18096.9414 - val_mae: 37.9477\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 1986.2226 - mse: 1986.2227 - mae: 32.1433 - val_loss: 17951.5953 - val_mse: 17951.5938 - val_mae: 37.7592\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2352.6493 - mse: 2352.6492 - mae: 34.8729 - val_loss: 18222.5509 - val_mse: 18222.5527 - val_mae: 38.2418\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 684us/step - loss: 2015.5738 - mse: 2015.5737 - mae: 32.4905 - val_loss: 17888.4336 - val_mse: 17888.4316 - val_mae: 37.7904\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2116.5913 - mse: 2116.5916 - mae: 31.8371 - val_loss: 18124.1267 - val_mse: 18124.1289 - val_mae: 37.9656\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 678us/step - loss: 2331.0201 - mse: 2331.0200 - mae: 32.8897 - val_loss: 18099.6661 - val_mse: 18099.6660 - val_mae: 37.9099\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 694us/step - loss: 2332.0780 - mse: 2332.0779 - mae: 34.2269 - val_loss: 18168.8040 - val_mse: 18168.8047 - val_mae: 38.0516\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 2153.2539 - mse: 2153.2539 - mae: 31.7195 - val_loss: 18011.3758 - val_mse: 18011.3730 - val_mae: 37.7223\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2252.6548 - mse: 2252.6545 - mae: 33.2947 - val_loss: 17981.0492 - val_mse: 17981.0508 - val_mae: 37.6770\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 1817.9107 - mse: 1817.9108 - mae: 29.9735 - val_loss: 17886.6341 - val_mse: 17886.6348 - val_mae: 37.7052\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 2111.0987 - mse: 2111.0986 - mae: 32.5085 - val_loss: 18122.3326 - val_mse: 18122.3320 - val_mae: 37.9018\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 2019.7238 - mse: 2019.7239 - mae: 32.5521 - val_loss: 18053.7654 - val_mse: 18053.7637 - val_mae: 37.7619\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 2138.8036 - mse: 2138.8035 - mae: 32.2590 - val_loss: 18052.1563 - val_mse: 18052.1582 - val_mae: 37.7508\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 620us/step - loss: 2382.1400 - mse: 2382.1399 - mae: 33.9902 - val_loss: 18386.0612 - val_mse: 18386.0605 - val_mae: 39.0092\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2183.1096 - mse: 2183.1096 - mae: 33.1380 - val_loss: 17832.2811 - val_mse: 17832.2793 - val_mae: 37.7251\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 704us/step - loss: 2191.8393 - mse: 2191.8394 - mae: 33.3810 - val_loss: 18135.1096 - val_mse: 18135.1113 - val_mae: 37.9008\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 534us/step - loss: 2205.7934 - mse: 2205.7935 - mae: 32.5342 - val_loss: 17924.8542 - val_mse: 17924.8535 - val_mae: 37.5713\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 2095.8158 - mse: 2095.8157 - mae: 31.6150 - val_loss: 18025.0790 - val_mse: 18025.0781 - val_mae: 37.6560\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 609us/step - loss: 1994.7202 - mse: 1994.7200 - mae: 31.1813 - val_loss: 18023.9978 - val_mse: 18023.9961 - val_mae: 37.6456\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2175.7763 - mse: 2175.7764 - mae: 32.0029 - val_loss: 18108.8744 - val_mse: 18108.8730 - val_mae: 37.8047\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 1951.5485 - mse: 1951.5486 - mae: 31.3786 - val_loss: 17893.4831 - val_mse: 17893.4844 - val_mae: 37.5484\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 2263.9273 - mse: 2263.9272 - mae: 33.8089 - val_loss: 18122.1081 - val_mse: 18122.1074 - val_mae: 37.8218\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2331.1383 - mse: 2331.1384 - mae: 33.8261 - val_loss: 17900.2581 - val_mse: 17900.2578 - val_mae: 37.5035\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 1959.8756 - mse: 1959.8755 - mae: 31.3923 - val_loss: 17951.3699 - val_mse: 17951.3672 - val_mae: 37.4821\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 2003.1827 - mse: 2003.1827 - mae: 30.8763 - val_loss: 17991.8968 - val_mse: 17991.8965 - val_mae: 37.5243\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2230.9726 - mse: 2230.9727 - mae: 32.6673 - val_loss: 18158.1370 - val_mse: 18158.1367 - val_mae: 37.9014\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 2118.7944 - mse: 2118.7944 - mae: 31.2383 - val_loss: 18010.5373 - val_mse: 18010.5371 - val_mae: 37.5406\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 671us/step - loss: 2027.7557 - mse: 2027.7556 - mae: 31.4785 - val_loss: 17914.0316 - val_mse: 17914.0312 - val_mae: 37.4280\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2085.3964 - mse: 2085.3965 - mae: 30.7208 - val_loss: 17803.7699 - val_mse: 17803.7695 - val_mae: 37.6041\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 2213.5299 - mse: 2213.5300 - mae: 33.0200 - val_loss: 18084.7907 - val_mse: 18084.7891 - val_mae: 37.6680\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 628us/step - loss: 2070.1744 - mse: 2070.1743 - mae: 31.5514 - val_loss: 17838.9552 - val_mse: 17838.9531 - val_mae: 37.4975\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 2049.8677 - mse: 2049.8677 - mae: 31.8397 - val_loss: 18024.6009 - val_mse: 18024.5996 - val_mae: 37.5239\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 667us/step - loss: 2063.8868 - mse: 2063.8870 - mae: 32.1961 - val_loss: 17989.7054 - val_mse: 17989.7051 - val_mae: 37.4506\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 718us/step - loss: 1980.0614 - mse: 1980.0615 - mae: 30.3214 - val_loss: 18038.8821 - val_mse: 18038.8828 - val_mae: 37.5349\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 692us/step - loss: 2083.9313 - mse: 2083.9312 - mae: 31.1675 - val_loss: 17943.8563 - val_mse: 17943.8555 - val_mae: 37.3616\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 672us/step - loss: 2144.0897 - mse: 2144.0896 - mae: 31.7135 - val_loss: 18280.0765 - val_mse: 18280.0762 - val_mae: 38.5045\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 448us/step - loss: 2011.5874 - mse: 2011.5876 - mae: 30.8350 - val_loss: 17815.0041 - val_mse: 17815.0039 - val_mae: 37.4843\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 1974.8865 - mse: 1974.8866 - mae: 30.4380 - val_loss: 17913.3349 - val_mse: 17913.3359 - val_mae: 37.3135\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 1937.9583 - mse: 1937.9581 - mae: 30.7759 - val_loss: 17840.0882 - val_mse: 17840.0879 - val_mae: 37.3941\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 705us/step - loss: 2053.2212 - mse: 2053.2209 - mae: 31.3786 - val_loss: 18120.1504 - val_mse: 18120.1504 - val_mae: 37.7152\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2003.9324 - mse: 2003.9324 - mae: 30.6910 - val_loss: 17884.4266 - val_mse: 17884.4277 - val_mae: 37.2892\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 710us/step - loss: 2069.4585 - mse: 2069.4585 - mae: 31.6007 - val_loss: 17990.6428 - val_mse: 17990.6426 - val_mae: 37.3522\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2073.4221 - mse: 2073.4224 - mae: 31.6353 - val_loss: 17923.1373 - val_mse: 17923.1367 - val_mae: 37.2507\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 1942.4196 - mse: 1942.4198 - mae: 30.3454 - val_loss: 17951.7738 - val_mse: 17951.7734 - val_mae: 37.2541\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2025.5616 - mse: 2025.5614 - mae: 31.1582 - val_loss: 17893.8360 - val_mse: 17893.8359 - val_mae: 37.2194\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 1766.3232 - mse: 1766.3230 - mae: 30.5649 - val_loss: 17904.8852 - val_mse: 17904.8867 - val_mae: 37.2056\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4182.3896 - mse: 4182.3901 - mae: 35.0982 - val_loss: 2234.7060 - val_mse: 2234.7061 - val_mae: 32.7952\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 519us/step - loss: 4294.1339 - mse: 4294.1343 - mae: 35.6365 - val_loss: 2327.3662 - val_mse: 2327.3660 - val_mae: 33.0242\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4139.7037 - mse: 4139.7031 - mae: 34.4020 - val_loss: 2303.4758 - val_mse: 2303.4758 - val_mae: 32.9329\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 492us/step - loss: 4444.5140 - mse: 4444.5142 - mae: 36.5765 - val_loss: 2331.8764 - val_mse: 2331.8765 - val_mae: 32.9950\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 679us/step - loss: 4242.1973 - mse: 4242.1982 - mae: 34.1091 - val_loss: 2329.5266 - val_mse: 2329.5266 - val_mae: 32.9658\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4326.5526 - mse: 4326.5527 - mae: 36.0450 - val_loss: 2292.6377 - val_mse: 2292.6377 - val_mae: 32.8446\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4101.8934 - mse: 4101.8936 - mae: 33.8805 - val_loss: 2375.8903 - val_mse: 2375.8904 - val_mae: 33.0658\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 676us/step - loss: 4349.0395 - mse: 4349.0405 - mae: 35.4252 - val_loss: 2386.4761 - val_mse: 2386.4763 - val_mae: 33.0802\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 4260.3982 - mse: 4260.3984 - mae: 34.4672 - val_loss: 2324.7111 - val_mse: 2324.7109 - val_mae: 32.8813\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4320.1556 - mse: 4320.1558 - mae: 36.1019 - val_loss: 2328.4855 - val_mse: 2328.4856 - val_mae: 32.8817\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4256.8301 - mse: 4256.8306 - mae: 35.9815 - val_loss: 2387.0772 - val_mse: 2387.0771 - val_mae: 33.0376\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4198.2171 - mse: 4198.2173 - mae: 34.8510 - val_loss: 2437.3488 - val_mse: 2437.3484 - val_mae: 33.1641\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4370.8083 - mse: 4370.8076 - mae: 34.6697 - val_loss: 2390.8139 - val_mse: 2390.8137 - val_mae: 33.0116\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 4361.5105 - mse: 4361.5107 - mae: 34.3467 - val_loss: 2350.0583 - val_mse: 2350.0581 - val_mae: 32.8774\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4296.7005 - mse: 4296.7007 - mae: 34.6681 - val_loss: 2386.6760 - val_mse: 2386.6760 - val_mae: 32.9678\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4169.4828 - mse: 4169.4829 - mae: 34.4743 - val_loss: 2362.3817 - val_mse: 2362.3816 - val_mae: 32.8833\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4013.2089 - mse: 4013.2080 - mae: 34.2108 - val_loss: 2332.6487 - val_mse: 2332.6484 - val_mae: 32.7947\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4131.4211 - mse: 4131.4209 - mae: 34.3053 - val_loss: 2375.6005 - val_mse: 2375.6006 - val_mae: 32.8896\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4083.7019 - mse: 4083.7017 - mae: 33.4420 - val_loss: 2300.9655 - val_mse: 2300.9656 - val_mae: 32.6802\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4254.4074 - mse: 4254.4072 - mae: 35.2339 - val_loss: 2330.7050 - val_mse: 2330.7053 - val_mae: 32.7393\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4239.5139 - mse: 4239.5142 - mae: 34.8096 - val_loss: 2301.6042 - val_mse: 2301.6045 - val_mae: 32.6485\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 653us/step - loss: 4218.6853 - mse: 4218.6855 - mae: 35.0126 - val_loss: 2383.2120 - val_mse: 2383.2119 - val_mae: 32.8441\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 649us/step - loss: 4092.2210 - mse: 4092.2214 - mae: 34.0216 - val_loss: 2272.4458 - val_mse: 2272.4460 - val_mae: 32.5369\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4341.6243 - mse: 4341.6240 - mae: 34.9619 - val_loss: 2343.3370 - val_mse: 2343.3369 - val_mae: 32.7087\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4234.0996 - mse: 4234.0996 - mae: 34.2622 - val_loss: 2349.9315 - val_mse: 2349.9312 - val_mae: 32.7105\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 4101.2886 - mse: 4101.2891 - mae: 34.4384 - val_loss: 2346.0022 - val_mse: 2346.0022 - val_mae: 32.6830\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4061.1484 - mse: 4061.1477 - mae: 33.1831 - val_loss: 2277.1371 - val_mse: 2277.1372 - val_mae: 32.4960\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4146.1905 - mse: 4146.1904 - mae: 34.2492 - val_loss: 2306.1333 - val_mse: 2306.1333 - val_mae: 32.5581\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4205.1442 - mse: 4205.1445 - mae: 33.5599 - val_loss: 2358.7378 - val_mse: 2358.7375 - val_mae: 32.6863\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4074.3039 - mse: 4074.3042 - mae: 33.6361 - val_loss: 2350.0132 - val_mse: 2350.0132 - val_mae: 32.6470\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4139.4650 - mse: 4139.4648 - mae: 33.6869 - val_loss: 2304.2019 - val_mse: 2304.2019 - val_mae: 32.5159\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4191.1299 - mse: 4191.1299 - mae: 33.9575 - val_loss: 2427.7631 - val_mse: 2427.7632 - val_mae: 32.8302\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4095.3127 - mse: 4095.3125 - mae: 33.0648 - val_loss: 2297.6729 - val_mse: 2297.6726 - val_mae: 32.4751\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 4055.0723 - mse: 4055.0730 - mae: 34.3912 - val_loss: 2444.1890 - val_mse: 2444.1892 - val_mae: 32.8433\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4164.2675 - mse: 4164.2676 - mae: 34.8169 - val_loss: 2342.4300 - val_mse: 2342.4302 - val_mae: 32.5619\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 694us/step - loss: 4052.1349 - mse: 4052.1350 - mae: 34.4707 - val_loss: 2336.3505 - val_mse: 2336.3506 - val_mae: 32.5322\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 4100.9848 - mse: 4100.9849 - mae: 34.0805 - val_loss: 2333.0902 - val_mse: 2333.0903 - val_mae: 32.5073\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 681us/step - loss: 4098.9867 - mse: 4098.9868 - mae: 33.8593 - val_loss: 2331.2646 - val_mse: 2331.2644 - val_mae: 32.4914\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 684us/step - loss: 4062.6422 - mse: 4062.6418 - mae: 33.9143 - val_loss: 2377.8830 - val_mse: 2377.8831 - val_mae: 32.6088\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4223.4775 - mse: 4223.4780 - mae: 33.8934 - val_loss: 2334.3230 - val_mse: 2334.3230 - val_mae: 32.4820\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 498us/step - loss: 4143.5738 - mse: 4143.5742 - mae: 34.0757 - val_loss: 2344.4982 - val_mse: 2344.4980 - val_mae: 32.5014\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4092.6122 - mse: 4092.6118 - mae: 33.5374 - val_loss: 2328.1895 - val_mse: 2328.1895 - val_mae: 32.4512\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 525us/step - loss: 4078.8204 - mse: 4078.8208 - mae: 34.2219 - val_loss: 2347.0335 - val_mse: 2347.0332 - val_mae: 32.4898\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 498us/step - loss: 4220.1987 - mse: 4220.1987 - mae: 34.0569 - val_loss: 2398.8294 - val_mse: 2398.8293 - val_mae: 32.6157\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 3884.7506 - mse: 3884.7507 - mae: 33.4326 - val_loss: 2307.4874 - val_mse: 2307.4873 - val_mae: 32.3740\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 4219.7769 - mse: 4219.7764 - mae: 34.4415 - val_loss: 2320.8109 - val_mse: 2320.8110 - val_mae: 32.4023\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 679us/step - loss: 4021.8245 - mse: 4021.8242 - mae: 34.4729 - val_loss: 2344.5567 - val_mse: 2344.5564 - val_mae: 32.4515\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4087.1274 - mse: 4087.1277 - mae: 33.4567 - val_loss: 2375.2583 - val_mse: 2375.2581 - val_mae: 32.5261\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4125.4517 - mse: 4125.4517 - mae: 33.8757 - val_loss: 2362.3136 - val_mse: 2362.3137 - val_mae: 32.4876\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4331.0504 - mse: 4331.0508 - mae: 33.6351 - val_loss: 2377.2886 - val_mse: 2377.2886 - val_mae: 32.5162\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4050.7628 - mse: 4050.7632 - mae: 33.8547 - val_loss: 2317.5986 - val_mse: 2317.5984 - val_mae: 32.3561\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4106.9505 - mse: 4106.9507 - mae: 33.1377 - val_loss: 2308.0743 - val_mse: 2308.0745 - val_mae: 32.3227\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4184.9533 - mse: 4184.9541 - mae: 34.4486 - val_loss: 2357.7275 - val_mse: 2357.7273 - val_mae: 32.4405\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 4225.7076 - mse: 4225.7075 - mae: 33.6086 - val_loss: 2352.3748 - val_mse: 2352.3750 - val_mae: 32.4147\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4189.9697 - mse: 4189.9702 - mae: 33.9300 - val_loss: 2305.6977 - val_mse: 2305.6978 - val_mae: 32.2873\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4177.0665 - mse: 4177.0669 - mae: 34.2613 - val_loss: 2348.0579 - val_mse: 2348.0579 - val_mae: 32.3846\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 3911.3535 - mse: 3911.3538 - mae: 33.0278 - val_loss: 2268.8219 - val_mse: 2268.8220 - val_mae: 32.1773\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4108.5550 - mse: 4108.5557 - mae: 34.4733 - val_loss: 2367.1411 - val_mse: 2367.1411 - val_mae: 32.4221\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 460us/step - loss: 4282.4397 - mse: 4282.4404 - mae: 34.4437 - val_loss: 2363.4847 - val_mse: 2363.4849 - val_mae: 32.4061\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 475us/step - loss: 4133.8169 - mse: 4133.8169 - mae: 33.5583 - val_loss: 2350.5357 - val_mse: 2350.5356 - val_mae: 32.3620\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 460us/step - loss: 4070.1041 - mse: 4070.1038 - mae: 33.1230 - val_loss: 2377.2067 - val_mse: 2377.2068 - val_mae: 32.4239\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4010.6838 - mse: 4010.6836 - mae: 33.7992 - val_loss: 2347.3196 - val_mse: 2347.3198 - val_mae: 32.3380\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4020.9156 - mse: 4020.9153 - mae: 34.2064 - val_loss: 2341.6846 - val_mse: 2341.6848 - val_mae: 32.3124\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 3983.3162 - mse: 3983.3159 - mae: 32.2930 - val_loss: 2288.9963 - val_mse: 2288.9963 - val_mae: 32.1695\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 476us/step - loss: 4080.5910 - mse: 4080.5908 - mae: 33.8571 - val_loss: 2298.3656 - val_mse: 2298.3655 - val_mae: 32.1887\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 3900.4068 - mse: 3900.4067 - mae: 34.3139 - val_loss: 2297.1892 - val_mse: 2297.1890 - val_mae: 32.1789\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4058.6974 - mse: 4058.6970 - mae: 34.6075 - val_loss: 2385.3735 - val_mse: 2385.3733 - val_mae: 32.3906\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4127.6103 - mse: 4127.6099 - mae: 32.8411 - val_loss: 2373.8867 - val_mse: 2373.8870 - val_mae: 32.3551\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4010.5942 - mse: 4010.5947 - mae: 33.4554 - val_loss: 2366.5094 - val_mse: 2366.5093 - val_mae: 32.3282\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 3962.6085 - mse: 3962.6086 - mae: 33.3671 - val_loss: 2348.4894 - val_mse: 2348.4895 - val_mae: 32.2699\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 622us/step - loss: 3894.6815 - mse: 3894.6816 - mae: 32.2103 - val_loss: 2317.8891 - val_mse: 2317.8892 - val_mae: 32.1949\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4026.0334 - mse: 4026.0334 - mae: 33.0071 - val_loss: 2354.2250 - val_mse: 2354.2251 - val_mae: 32.2731\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 3791.0357 - mse: 3791.0359 - mae: 32.7137 - val_loss: 2278.5733 - val_mse: 2278.5732 - val_mae: 32.0833\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 3983.7293 - mse: 3983.7292 - mae: 33.8377 - val_loss: 2340.8944 - val_mse: 2340.8943 - val_mae: 32.2285\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4071.1943 - mse: 4071.1946 - mae: 33.4773 - val_loss: 2386.5830 - val_mse: 2386.5830 - val_mae: 32.3349\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4029.3293 - mse: 4029.3293 - mae: 32.7334 - val_loss: 2358.2370 - val_mse: 2358.2368 - val_mae: 32.2618\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 751us/step - loss: 3862.0904 - mse: 3862.0898 - mae: 32.9476 - val_loss: 2363.8525 - val_mse: 2363.8525 - val_mae: 32.2675\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 731us/step - loss: 4092.8911 - mse: 4092.8906 - mae: 33.8477 - val_loss: 2286.3122 - val_mse: 2286.3123 - val_mae: 32.0755\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4029.0410 - mse: 4029.0410 - mae: 32.8661 - val_loss: 2305.8741 - val_mse: 2305.8740 - val_mae: 32.1194\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 547us/step - loss: 4052.8366 - mse: 4052.8364 - mae: 33.4917 - val_loss: 2371.6359 - val_mse: 2371.6357 - val_mae: 32.2655\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 472us/step - loss: 3545.8532 - mse: 3545.8528 - mae: 34.4361 - val_loss: 1449.8389 - val_mse: 1449.8389 - val_mae: 25.8838\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 469us/step - loss: 3433.5483 - mse: 3433.5479 - mae: 33.2224 - val_loss: 1448.9257 - val_mse: 1448.9258 - val_mae: 25.6040\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3500.8765 - mse: 3500.8767 - mae: 33.6422 - val_loss: 1450.7440 - val_mse: 1450.7440 - val_mae: 25.9287\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3407.3419 - mse: 3407.3420 - mae: 32.9062 - val_loss: 1449.2917 - val_mse: 1449.2916 - val_mae: 25.2076\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3529.8564 - mse: 3529.8562 - mae: 33.3516 - val_loss: 1453.0005 - val_mse: 1453.0005 - val_mae: 26.1720\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3454.3434 - mse: 3454.3428 - mae: 33.6766 - val_loss: 1450.2753 - val_mse: 1450.2754 - val_mae: 25.6062\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 676us/step - loss: 3449.0081 - mse: 3449.0078 - mae: 33.0755 - val_loss: 1450.7620 - val_mse: 1450.7621 - val_mae: 25.6325\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 532us/step - loss: 3474.2526 - mse: 3474.2524 - mae: 32.8342 - val_loss: 1454.3494 - val_mse: 1454.3492 - val_mae: 26.2186\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3422.1727 - mse: 3422.1729 - mae: 33.5350 - val_loss: 1451.3740 - val_mse: 1451.3740 - val_mae: 25.4312\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 527us/step - loss: 3308.5588 - mse: 3308.5581 - mae: 32.7376 - val_loss: 1458.1220 - val_mse: 1458.1218 - val_mae: 26.5299\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3264.0890 - mse: 3264.0886 - mae: 32.4987 - val_loss: 1456.0344 - val_mse: 1456.0343 - val_mae: 26.2640\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3438.5782 - mse: 3438.5779 - mae: 33.3852 - val_loss: 1452.3094 - val_mse: 1452.3094 - val_mae: 25.4483\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 636us/step - loss: 3366.2220 - mse: 3366.2224 - mae: 33.1008 - val_loss: 1453.6402 - val_mse: 1453.6400 - val_mae: 25.8180\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3343.6644 - mse: 3343.6646 - mae: 32.7029 - val_loss: 1454.9944 - val_mse: 1454.9945 - val_mae: 25.9934\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3376.3372 - mse: 3376.3367 - mae: 32.6350 - val_loss: 1453.7848 - val_mse: 1453.7848 - val_mae: 25.6760\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3387.3277 - mse: 3387.3279 - mae: 32.7641 - val_loss: 1456.1370 - val_mse: 1456.1368 - val_mae: 26.0764\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3272.3809 - mse: 3272.3809 - mae: 31.6982 - val_loss: 1456.2644 - val_mse: 1456.2644 - val_mae: 26.0462\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3368.8367 - mse: 3368.8364 - mae: 32.5597 - val_loss: 1454.3899 - val_mse: 1454.3899 - val_mae: 25.4809\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3380.7513 - mse: 3380.7520 - mae: 32.5493 - val_loss: 1457.2100 - val_mse: 1457.2101 - val_mae: 26.1071\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 500us/step - loss: 3290.0700 - mse: 3290.0703 - mae: 32.7187 - val_loss: 1455.6742 - val_mse: 1455.6741 - val_mae: 25.7011\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3457.9411 - mse: 3457.9412 - mae: 33.5127 - val_loss: 1458.4224 - val_mse: 1458.4222 - val_mae: 26.1638\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3254.4852 - mse: 3254.4846 - mae: 32.1411 - val_loss: 1458.1608 - val_mse: 1458.1608 - val_mae: 26.0585\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3433.7688 - mse: 3433.7688 - mae: 33.2797 - val_loss: 1457.8315 - val_mse: 1457.8314 - val_mae: 25.9249\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3414.9612 - mse: 3414.9602 - mae: 32.5230 - val_loss: 1459.3137 - val_mse: 1459.3138 - val_mae: 26.1437\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3357.4354 - mse: 3357.4351 - mae: 33.4598 - val_loss: 1457.5206 - val_mse: 1457.5204 - val_mae: 25.6594\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3420.0426 - mse: 3420.0437 - mae: 33.2713 - val_loss: 1457.8919 - val_mse: 1457.8918 - val_mae: 25.6116\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3291.2939 - mse: 3291.2944 - mae: 32.7073 - val_loss: 1458.2219 - val_mse: 1458.2219 - val_mae: 25.3375\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3343.1176 - mse: 3343.1169 - mae: 32.1645 - val_loss: 1458.2886 - val_mse: 1458.2886 - val_mae: 25.6062\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3300.9112 - mse: 3300.9111 - mae: 32.2997 - val_loss: 1459.4027 - val_mse: 1459.4026 - val_mae: 25.8925\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3441.7123 - mse: 3441.7122 - mae: 33.3957 - val_loss: 1459.3146 - val_mse: 1459.3147 - val_mae: 25.2267\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3266.5302 - mse: 3266.5305 - mae: 32.5820 - val_loss: 1461.8929 - val_mse: 1461.8928 - val_mae: 26.2124\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 507us/step - loss: 3346.1780 - mse: 3346.1780 - mae: 32.0497 - val_loss: 1459.9732 - val_mse: 1459.9733 - val_mae: 25.7587\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3329.5819 - mse: 3329.5820 - mae: 31.8592 - val_loss: 1460.4151 - val_mse: 1460.4152 - val_mae: 25.8199\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3349.8370 - mse: 3349.8367 - mae: 32.9429 - val_loss: 1461.8423 - val_mse: 1461.8424 - val_mae: 26.0555\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 711us/step - loss: 3468.4640 - mse: 3468.4641 - mae: 32.5218 - val_loss: 1461.6344 - val_mse: 1461.6345 - val_mae: 25.9197\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 673us/step - loss: 3360.3039 - mse: 3360.3044 - mae: 32.2941 - val_loss: 1463.6381 - val_mse: 1463.6381 - val_mae: 26.2198\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 676us/step - loss: 3298.0158 - mse: 3298.0161 - mae: 31.7632 - val_loss: 1466.7540 - val_mse: 1466.7540 - val_mae: 26.5829\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3384.0564 - mse: 3384.0566 - mae: 33.1619 - val_loss: 1461.8973 - val_mse: 1461.8973 - val_mae: 25.8412\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3400.9154 - mse: 3400.9150 - mae: 32.4653 - val_loss: 1461.5959 - val_mse: 1461.5959 - val_mae: 25.5679\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3388.4586 - mse: 3388.4585 - mae: 32.2976 - val_loss: 1464.1273 - val_mse: 1464.1276 - val_mae: 26.1563\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3256.0683 - mse: 3256.0681 - mae: 31.3231 - val_loss: 1467.9132 - val_mse: 1467.9132 - val_mae: 26.6053\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3290.0144 - mse: 3290.0154 - mae: 32.6291 - val_loss: 1462.7682 - val_mse: 1462.7682 - val_mae: 25.6867\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 659us/step - loss: 3429.1989 - mse: 3429.2000 - mae: 33.0142 - val_loss: 1463.0136 - val_mse: 1463.0137 - val_mae: 25.3416\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3402.8359 - mse: 3402.8362 - mae: 32.4326 - val_loss: 1465.3901 - val_mse: 1465.3899 - val_mae: 26.2020\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3334.1046 - mse: 3334.1050 - mae: 32.1949 - val_loss: 1463.9012 - val_mse: 1463.9011 - val_mae: 25.8942\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3365.9008 - mse: 3365.9011 - mae: 33.0157 - val_loss: 1465.4180 - val_mse: 1465.4180 - val_mae: 26.1612\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3354.3313 - mse: 3354.3325 - mae: 32.3712 - val_loss: 1466.5293 - val_mse: 1466.5294 - val_mae: 26.2769\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3268.9116 - mse: 3268.9116 - mae: 32.2362 - val_loss: 1465.0835 - val_mse: 1465.0835 - val_mae: 25.9983\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3335.3716 - mse: 3335.3706 - mae: 32.9914 - val_loss: 1464.6544 - val_mse: 1464.6544 - val_mae: 25.4518\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3354.9535 - mse: 3354.9539 - mae: 32.6900 - val_loss: 1467.0581 - val_mse: 1467.0580 - val_mae: 26.2357\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3350.5779 - mse: 3350.5776 - mae: 32.3900 - val_loss: 1465.0375 - val_mse: 1465.0375 - val_mae: 25.5505\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3183.8174 - mse: 3183.8171 - mae: 31.6525 - val_loss: 1471.6623 - val_mse: 1471.6622 - val_mae: 26.6866\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3370.0508 - mse: 3370.0505 - mae: 32.0493 - val_loss: 1468.2222 - val_mse: 1468.2223 - val_mae: 26.2662\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 491us/step - loss: 3321.2926 - mse: 3321.2927 - mae: 31.6413 - val_loss: 1469.4943 - val_mse: 1469.4943 - val_mae: 26.4264\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 467us/step - loss: 3310.4601 - mse: 3310.4595 - mae: 32.9327 - val_loss: 1474.2778 - val_mse: 1474.2776 - val_mae: 26.8856\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3319.1311 - mse: 3319.1309 - mae: 32.4248 - val_loss: 1466.2107 - val_mse: 1466.2106 - val_mae: 25.5105\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3299.1129 - mse: 3299.1128 - mae: 32.3342 - val_loss: 1467.1945 - val_mse: 1467.1945 - val_mae: 25.9697\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3464.6316 - mse: 3464.6311 - mae: 32.8302 - val_loss: 1467.7849 - val_mse: 1467.7849 - val_mae: 26.0063\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 653us/step - loss: 3369.5946 - mse: 3369.5950 - mae: 31.7969 - val_loss: 1467.7537 - val_mse: 1467.7537 - val_mae: 25.9217\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3225.0393 - mse: 3225.0396 - mae: 31.6085 - val_loss: 1468.7684 - val_mse: 1468.7686 - val_mae: 26.1131\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3205.5178 - mse: 3205.5183 - mae: 31.3809 - val_loss: 1483.3841 - val_mse: 1483.3840 - val_mae: 27.4426\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3376.2503 - mse: 3376.2512 - mae: 32.6243 - val_loss: 1467.9579 - val_mse: 1467.9580 - val_mae: 25.8475\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3318.3356 - mse: 3318.3354 - mae: 32.1304 - val_loss: 1470.2165 - val_mse: 1470.2166 - val_mae: 26.2753\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3404.8695 - mse: 3404.8689 - mae: 32.2322 - val_loss: 1468.9892 - val_mse: 1468.9891 - val_mae: 26.0603\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 675us/step - loss: 3342.9258 - mse: 3342.9258 - mae: 32.8728 - val_loss: 1469.9294 - val_mse: 1469.9294 - val_mae: 26.2108\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3365.7329 - mse: 3365.7339 - mae: 32.3394 - val_loss: 1468.6545 - val_mse: 1468.6545 - val_mae: 25.7893\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 676us/step - loss: 3301.7960 - mse: 3301.7969 - mae: 32.5802 - val_loss: 1469.8417 - val_mse: 1469.8418 - val_mae: 26.0816\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 518us/step - loss: 3321.2718 - mse: 3321.2722 - mae: 31.9736 - val_loss: 1471.2679 - val_mse: 1471.2678 - val_mae: 26.2922\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3215.5717 - mse: 3215.5713 - mae: 31.4936 - val_loss: 1472.2478 - val_mse: 1472.2477 - val_mae: 26.4172\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3256.8833 - mse: 3256.8835 - mae: 31.6004 - val_loss: 1472.1943 - val_mse: 1472.1943 - val_mae: 26.3940\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3267.6062 - mse: 3267.6060 - mae: 32.2571 - val_loss: 1469.1731 - val_mse: 1469.1731 - val_mae: 25.7866\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3377.9928 - mse: 3377.9922 - mae: 32.3660 - val_loss: 1469.7061 - val_mse: 1469.7061 - val_mae: 25.9225\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3280.2730 - mse: 3280.2725 - mae: 31.8691 - val_loss: 1469.2372 - val_mse: 1469.2373 - val_mae: 25.6194\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3282.4836 - mse: 3282.4846 - mae: 31.4692 - val_loss: 1469.4854 - val_mse: 1469.4855 - val_mae: 25.7620\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 501us/step - loss: 3324.6998 - mse: 3324.7000 - mae: 32.1395 - val_loss: 1469.6538 - val_mse: 1469.6537 - val_mae: 25.4703\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3383.0131 - mse: 3383.0137 - mae: 31.9953 - val_loss: 1469.6701 - val_mse: 1469.6702 - val_mae: 25.7899\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3249.7677 - mse: 3249.7686 - mae: 31.9135 - val_loss: 1469.6043 - val_mse: 1469.6042 - val_mae: 25.7339\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 544us/step - loss: 3367.9038 - mse: 3367.9036 - mae: 31.9973 - val_loss: 1473.3707 - val_mse: 1473.3708 - val_mae: 26.4801\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3195.8470 - mse: 3195.8469 - mae: 31.0700 - val_loss: 1470.4201 - val_mse: 1470.4199 - val_mae: 25.9592\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3423.6891 - mse: 3423.6897 - mae: 32.1185 - val_loss: 1472.5212 - val_mse: 1472.5212 - val_mae: 26.3488\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2973.6973 - mse: 2973.6963 - mae: 31.2968 - val_loss: 1088.3373 - val_mse: 1088.3373 - val_mae: 23.4690\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2924.3135 - mse: 2924.3135 - mae: 31.5182 - val_loss: 1084.6739 - val_mse: 1084.6737 - val_mae: 23.6025\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2985.2901 - mse: 2985.2891 - mae: 31.2559 - val_loss: 1083.5663 - val_mse: 1083.5664 - val_mae: 23.6583\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2989.4932 - mse: 2989.4929 - mae: 31.6279 - val_loss: 1090.0952 - val_mse: 1090.0952 - val_mae: 23.4200\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2968.6250 - mse: 2968.6252 - mae: 31.3281 - val_loss: 1087.1346 - val_mse: 1087.1345 - val_mae: 23.5089\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2949.7464 - mse: 2949.7466 - mae: 31.1717 - val_loss: 1082.1231 - val_mse: 1082.1232 - val_mae: 23.7218\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 3033.4083 - mse: 3033.4084 - mae: 31.7320 - val_loss: 1078.5824 - val_mse: 1078.5824 - val_mae: 24.0943\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2873.2365 - mse: 2873.2358 - mae: 31.2414 - val_loss: 1079.7183 - val_mse: 1079.7183 - val_mae: 23.8933\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2965.6433 - mse: 2965.6438 - mae: 30.9384 - val_loss: 1089.0333 - val_mse: 1089.0333 - val_mae: 23.4517\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2886.3423 - mse: 2886.3428 - mae: 30.3362 - val_loss: 1078.6630 - val_mse: 1078.6631 - val_mae: 24.0558\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2977.3708 - mse: 2977.3701 - mae: 31.7037 - val_loss: 1084.3246 - val_mse: 1084.3246 - val_mae: 23.6117\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2982.9548 - mse: 2982.9548 - mae: 31.6221 - val_loss: 1086.0716 - val_mse: 1086.0717 - val_mae: 23.5507\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2888.8822 - mse: 2888.8821 - mae: 30.8940 - val_loss: 1079.6611 - val_mse: 1079.6611 - val_mae: 23.9511\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2927.4961 - mse: 2927.4958 - mae: 31.8272 - val_loss: 1086.3785 - val_mse: 1086.3785 - val_mae: 23.5397\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2923.7974 - mse: 2923.7976 - mae: 31.5781 - val_loss: 1080.9265 - val_mse: 1080.9265 - val_mae: 23.8276\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2916.9148 - mse: 2916.9155 - mae: 31.3148 - val_loss: 1080.3776 - val_mse: 1080.3778 - val_mae: 23.8626\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2935.7307 - mse: 2935.7302 - mae: 31.4251 - val_loss: 1079.8023 - val_mse: 1079.8021 - val_mae: 23.9175\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2957.1909 - mse: 2957.1909 - mae: 31.1409 - val_loss: 1078.5861 - val_mse: 1078.5862 - val_mae: 24.1409\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2911.9496 - mse: 2911.9492 - mae: 31.3896 - val_loss: 1079.6488 - val_mse: 1079.6487 - val_mae: 23.9474\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2902.3171 - mse: 2902.3162 - mae: 31.1328 - val_loss: 1079.7553 - val_mse: 1079.7554 - val_mae: 23.9232\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2913.4279 - mse: 2913.4285 - mae: 30.8320 - val_loss: 1078.2998 - val_mse: 1078.2997 - val_mae: 24.1477\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2935.5894 - mse: 2935.5891 - mae: 31.0708 - val_loss: 1079.0735 - val_mse: 1079.0736 - val_mae: 23.9854\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2933.2457 - mse: 2933.2449 - mae: 30.5967 - val_loss: 1081.4127 - val_mse: 1081.4126 - val_mae: 23.7952\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2978.1767 - mse: 2978.1772 - mae: 31.5297 - val_loss: 1088.7379 - val_mse: 1088.7379 - val_mae: 23.4856\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 526us/step - loss: 2985.0620 - mse: 2985.0618 - mae: 31.0819 - val_loss: 1080.1635 - val_mse: 1080.1636 - val_mae: 23.9120\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2939.2132 - mse: 2939.2129 - mae: 31.5640 - val_loss: 1080.6688 - val_mse: 1080.6688 - val_mae: 23.8541\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2915.8903 - mse: 2915.8906 - mae: 31.0665 - val_loss: 1079.3030 - val_mse: 1079.3031 - val_mae: 24.0127\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2959.7514 - mse: 2959.7520 - mae: 31.1434 - val_loss: 1082.2745 - val_mse: 1082.2744 - val_mae: 23.7454\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2939.2250 - mse: 2939.2249 - mae: 31.3427 - val_loss: 1081.2348 - val_mse: 1081.2347 - val_mae: 23.7611\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2902.5234 - mse: 2902.5232 - mae: 30.8089 - val_loss: 1077.8978 - val_mse: 1077.8978 - val_mae: 24.1023\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2945.4240 - mse: 2945.4236 - mae: 31.6575 - val_loss: 1081.0967 - val_mse: 1081.0968 - val_mae: 23.7340\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2946.9116 - mse: 2946.9114 - mae: 30.5561 - val_loss: 1079.4723 - val_mse: 1079.4722 - val_mae: 23.8239\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 537us/step - loss: 2921.0674 - mse: 2921.0662 - mae: 30.9535 - val_loss: 1080.2936 - val_mse: 1080.2937 - val_mae: 23.7257\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2914.6552 - mse: 2914.6553 - mae: 30.8997 - val_loss: 1076.2061 - val_mse: 1076.2062 - val_mae: 24.3382\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2905.8632 - mse: 2905.8633 - mae: 30.9580 - val_loss: 1079.8013 - val_mse: 1079.8014 - val_mae: 23.7308\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2870.4229 - mse: 2870.4226 - mae: 30.8899 - val_loss: 1076.1801 - val_mse: 1076.1801 - val_mae: 24.1209\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2947.0150 - mse: 2947.0146 - mae: 30.9613 - val_loss: 1078.7876 - val_mse: 1078.7877 - val_mae: 23.7959\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2926.8683 - mse: 2926.8677 - mae: 30.9742 - val_loss: 1076.1520 - val_mse: 1076.1520 - val_mae: 24.1557\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 554us/step - loss: 2944.4521 - mse: 2944.4531 - mae: 31.0114 - val_loss: 1075.6037 - val_mse: 1075.6038 - val_mae: 24.4867\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2863.9152 - mse: 2863.9155 - mae: 30.6541 - val_loss: 1075.4316 - val_mse: 1075.4316 - val_mae: 24.1493\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 538us/step - loss: 2932.5171 - mse: 2932.5166 - mae: 30.9864 - val_loss: 1075.9049 - val_mse: 1075.9048 - val_mae: 23.9981\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 699us/step - loss: 2934.7270 - mse: 2934.7280 - mae: 31.0402 - val_loss: 1076.4651 - val_mse: 1076.4651 - val_mae: 23.8704\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2945.9468 - mse: 2945.9468 - mae: 31.3473 - val_loss: 1078.2759 - val_mse: 1078.2759 - val_mae: 23.7107\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2850.5344 - mse: 2850.5354 - mae: 30.9013 - val_loss: 1074.5046 - val_mse: 1074.5046 - val_mae: 24.0641\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2874.0177 - mse: 2874.0178 - mae: 30.9040 - val_loss: 1074.4168 - val_mse: 1074.4169 - val_mae: 23.9598\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2863.6140 - mse: 2863.6143 - mae: 30.4738 - val_loss: 1073.7290 - val_mse: 1073.7291 - val_mae: 24.0841\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2939.5646 - mse: 2939.5640 - mae: 31.1567 - val_loss: 1073.7915 - val_mse: 1073.7916 - val_mae: 24.6837\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2826.4038 - mse: 2826.4038 - mae: 30.8833 - val_loss: 1075.7615 - val_mse: 1075.7614 - val_mae: 23.7740\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2967.6240 - mse: 2967.6248 - mae: 31.2088 - val_loss: 1074.4131 - val_mse: 1074.4131 - val_mae: 23.8971\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2803.4387 - mse: 2803.4385 - mae: 30.6517 - val_loss: 1072.9614 - val_mse: 1072.9613 - val_mae: 24.2385\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 525us/step - loss: 2982.8998 - mse: 2982.8992 - mae: 31.3727 - val_loss: 1074.5619 - val_mse: 1074.5620 - val_mae: 23.8406\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2803.8587 - mse: 2803.8586 - mae: 30.1659 - val_loss: 1072.2996 - val_mse: 1072.2997 - val_mae: 24.2954\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2880.8654 - mse: 2880.8650 - mae: 30.2485 - val_loss: 1072.5091 - val_mse: 1072.5092 - val_mae: 24.2099\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2833.8449 - mse: 2833.8452 - mae: 30.6810 - val_loss: 1072.4636 - val_mse: 1072.4635 - val_mae: 24.1481\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 651us/step - loss: 2886.2405 - mse: 2886.2412 - mae: 30.5341 - val_loss: 1073.2282 - val_mse: 1073.2280 - val_mae: 23.9651\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 640us/step - loss: 2932.8338 - mse: 2932.8337 - mae: 31.4458 - val_loss: 1072.3569 - val_mse: 1072.3569 - val_mae: 24.0885\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2877.2209 - mse: 2877.2209 - mae: 30.3609 - val_loss: 1072.5317 - val_mse: 1072.5317 - val_mae: 24.0001\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2938.9429 - mse: 2938.9431 - mae: 30.9708 - val_loss: 1072.7999 - val_mse: 1072.7998 - val_mae: 23.9112\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2911.5914 - mse: 2911.5913 - mae: 31.2505 - val_loss: 1072.2529 - val_mse: 1072.2529 - val_mae: 23.9421\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2840.5153 - mse: 2840.5159 - mae: 29.9944 - val_loss: 1070.4434 - val_mse: 1070.4435 - val_mae: 24.2709\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2812.2650 - mse: 2812.2656 - mae: 30.7406 - val_loss: 1072.4404 - val_mse: 1072.4404 - val_mae: 23.8457\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2898.2103 - mse: 2898.2095 - mae: 30.9754 - val_loss: 1074.8335 - val_mse: 1074.8335 - val_mae: 23.6508\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2909.3262 - mse: 2909.3264 - mae: 30.9384 - val_loss: 1073.3519 - val_mse: 1073.3519 - val_mae: 23.7426\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2929.0405 - mse: 2929.0405 - mae: 31.3785 - val_loss: 1071.1528 - val_mse: 1071.1528 - val_mae: 23.9349\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2781.0963 - mse: 2781.0967 - mae: 29.9433 - val_loss: 1069.8380 - val_mse: 1069.8380 - val_mae: 24.4784\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 548us/step - loss: 2826.2173 - mse: 2826.2161 - mae: 30.8705 - val_loss: 1070.4231 - val_mse: 1070.4231 - val_mae: 23.9944\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 641us/step - loss: 2855.8201 - mse: 2855.8196 - mae: 30.2372 - val_loss: 1069.6343 - val_mse: 1069.6343 - val_mae: 24.2752\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2874.3118 - mse: 2874.3125 - mae: 30.6549 - val_loss: 1069.5886 - val_mse: 1069.5885 - val_mae: 24.1999\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2835.5730 - mse: 2835.5730 - mae: 30.0820 - val_loss: 1069.7440 - val_mse: 1069.7439 - val_mae: 24.0546\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2829.1160 - mse: 2829.1155 - mae: 30.6527 - val_loss: 1069.0149 - val_mse: 1069.0149 - val_mae: 24.2104\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2793.5720 - mse: 2793.5718 - mae: 30.7983 - val_loss: 1069.4713 - val_mse: 1069.4713 - val_mae: 24.0248\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2897.8924 - mse: 2897.8926 - mae: 31.0481 - val_loss: 1069.8186 - val_mse: 1069.8186 - val_mae: 23.9573\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 641us/step - loss: 2882.7595 - mse: 2882.7593 - mae: 30.6699 - val_loss: 1068.5255 - val_mse: 1068.5254 - val_mae: 24.2845\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2942.6034 - mse: 2942.6023 - mae: 30.8540 - val_loss: 1069.0983 - val_mse: 1069.0983 - val_mae: 23.9826\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2893.3798 - mse: 2893.3806 - mae: 31.0881 - val_loss: 1068.8337 - val_mse: 1068.8335 - val_mae: 24.6155\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2832.0120 - mse: 2832.0117 - mae: 30.3396 - val_loss: 1067.9685 - val_mse: 1067.9684 - val_mae: 24.0836\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2770.4049 - mse: 2770.4045 - mae: 30.4071 - val_loss: 1067.5715 - val_mse: 1067.5715 - val_mae: 24.3120\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2775.0481 - mse: 2775.0486 - mae: 30.1552 - val_loss: 1067.3830 - val_mse: 1067.3831 - val_mae: 24.3629\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2882.7535 - mse: 2882.7534 - mae: 30.5802 - val_loss: 1067.2495 - val_mse: 1067.2495 - val_mae: 24.1255\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2869.6627 - mse: 2869.6624 - mae: 30.9609 - val_loss: 1066.7511 - val_mse: 1066.7510 - val_mae: 24.2304\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2535.9996 - mse: 2535.9993 - mae: 29.8115 - val_loss: 1546.0847 - val_mse: 1546.0845 - val_mae: 28.1272\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 691us/step - loss: 2604.6191 - mse: 2604.6184 - mae: 30.0762 - val_loss: 1547.8122 - val_mse: 1547.8121 - val_mae: 28.0449\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2568.6588 - mse: 2568.6597 - mae: 29.9709 - val_loss: 1563.5614 - val_mse: 1563.5613 - val_mae: 27.4610\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2605.4324 - mse: 2605.4304 - mae: 30.1805 - val_loss: 1546.7969 - val_mse: 1546.7969 - val_mae: 28.0147\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2575.6031 - mse: 2575.6035 - mae: 29.8779 - val_loss: 1555.5111 - val_mse: 1555.5112 - val_mae: 27.6659\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2565.0445 - mse: 2565.0447 - mae: 29.8861 - val_loss: 1553.9439 - val_mse: 1553.9440 - val_mae: 27.7193\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2594.3037 - mse: 2594.3047 - mae: 29.9374 - val_loss: 1548.8563 - val_mse: 1548.8561 - val_mae: 27.9162\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 518us/step - loss: 2536.6791 - mse: 2536.6787 - mae: 29.7674 - val_loss: 1548.7728 - val_mse: 1548.7728 - val_mae: 27.8539\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2584.5484 - mse: 2584.5483 - mae: 29.9101 - val_loss: 1547.1097 - val_mse: 1547.1096 - val_mae: 27.8870\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 647us/step - loss: 2555.0207 - mse: 2555.0203 - mae: 29.7234 - val_loss: 1545.1901 - val_mse: 1545.1903 - val_mae: 27.9707\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2470.2472 - mse: 2470.2466 - mae: 29.1974 - val_loss: 1543.2464 - val_mse: 1543.2463 - val_mae: 28.0447\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2605.0645 - mse: 2605.0649 - mae: 30.1347 - val_loss: 1547.9448 - val_mse: 1547.9451 - val_mae: 27.8100\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2589.2786 - mse: 2589.2776 - mae: 30.4107 - val_loss: 1542.3215 - val_mse: 1542.3217 - val_mae: 28.0260\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2608.2063 - mse: 2608.2058 - mae: 30.1261 - val_loss: 1543.3207 - val_mse: 1543.3209 - val_mae: 27.9431\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 544us/step - loss: 2586.1796 - mse: 2586.1797 - mae: 29.7777 - val_loss: 1549.0109 - val_mse: 1549.0110 - val_mae: 27.7183\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2637.3825 - mse: 2637.3833 - mae: 30.4507 - val_loss: 1544.4399 - val_mse: 1544.4401 - val_mae: 27.8627\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2510.2275 - mse: 2510.2278 - mae: 29.6544 - val_loss: 1536.9491 - val_mse: 1536.9490 - val_mae: 28.1978\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2585.4058 - mse: 2585.4060 - mae: 30.3275 - val_loss: 1554.6733 - val_mse: 1554.6731 - val_mae: 27.5159\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 519us/step - loss: 2522.7901 - mse: 2522.7900 - mae: 29.6942 - val_loss: 1541.9870 - val_mse: 1541.9871 - val_mae: 27.9570\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 561us/step - loss: 2523.0289 - mse: 2523.0295 - mae: 29.9825 - val_loss: 1544.4848 - val_mse: 1544.4849 - val_mae: 27.8120\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2605.7919 - mse: 2605.7925 - mae: 30.3997 - val_loss: 1548.8707 - val_mse: 1548.8706 - val_mae: 27.6436\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2571.1816 - mse: 2571.1814 - mae: 29.8161 - val_loss: 1555.8638 - val_mse: 1555.8636 - val_mae: 27.3939\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2547.0738 - mse: 2547.0737 - mae: 29.7578 - val_loss: 1551.1725 - val_mse: 1551.1724 - val_mae: 27.4945\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2570.3156 - mse: 2570.3154 - mae: 29.9015 - val_loss: 1539.5684 - val_mse: 1539.5686 - val_mae: 27.9104\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2491.1215 - mse: 2491.1206 - mae: 30.1796 - val_loss: 1541.9925 - val_mse: 1541.9924 - val_mae: 27.8126\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2459.3641 - mse: 2459.3640 - mae: 29.0974 - val_loss: 1543.6627 - val_mse: 1543.6627 - val_mae: 27.7298\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2638.1938 - mse: 2638.1946 - mae: 30.4676 - val_loss: 1549.8243 - val_mse: 1549.8242 - val_mae: 27.5559\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2570.7469 - mse: 2570.7471 - mae: 29.7983 - val_loss: 1539.1646 - val_mse: 1539.1647 - val_mae: 27.8563\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2592.6420 - mse: 2592.6418 - mae: 29.6568 - val_loss: 1544.5509 - val_mse: 1544.5505 - val_mae: 27.6270\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2564.9011 - mse: 2564.9009 - mae: 29.8755 - val_loss: 1546.0576 - val_mse: 1546.0574 - val_mae: 27.5824\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2574.5518 - mse: 2574.5518 - mae: 29.8829 - val_loss: 1538.3438 - val_mse: 1538.3438 - val_mae: 27.9123\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2491.8230 - mse: 2491.8235 - mae: 29.8498 - val_loss: 1541.1411 - val_mse: 1541.1410 - val_mae: 27.7835\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2545.2706 - mse: 2545.2710 - mae: 29.7246 - val_loss: 1542.0052 - val_mse: 1542.0051 - val_mae: 27.7519\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 505us/step - loss: 2571.9903 - mse: 2571.9910 - mae: 29.7608 - val_loss: 1539.8718 - val_mse: 1539.8719 - val_mae: 27.8287\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2579.9169 - mse: 2579.9170 - mae: 29.7975 - val_loss: 1548.2310 - val_mse: 1548.2311 - val_mae: 27.5457\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2522.4658 - mse: 2522.4656 - mae: 29.3743 - val_loss: 1544.9913 - val_mse: 1544.9913 - val_mae: 27.5930\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 512us/step - loss: 2619.4142 - mse: 2619.4143 - mae: 29.9140 - val_loss: 1546.1673 - val_mse: 1546.1675 - val_mae: 27.5858\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2546.1905 - mse: 2546.1907 - mae: 29.6308 - val_loss: 1539.7246 - val_mse: 1539.7246 - val_mae: 27.7735\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2493.4105 - mse: 2493.4099 - mae: 29.5149 - val_loss: 1546.3767 - val_mse: 1546.3767 - val_mae: 27.5417\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2537.2490 - mse: 2537.2480 - mae: 29.9071 - val_loss: 1533.1974 - val_mse: 1533.1975 - val_mae: 28.0276\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2654.6010 - mse: 2654.6011 - mae: 30.2553 - val_loss: 1552.0808 - val_mse: 1552.0808 - val_mae: 27.3560\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2555.9520 - mse: 2555.9521 - mae: 29.6879 - val_loss: 1536.0101 - val_mse: 1536.0101 - val_mae: 27.8828\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2624.9697 - mse: 2624.9700 - mae: 29.7836 - val_loss: 1542.6871 - val_mse: 1542.6873 - val_mae: 27.6366\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2545.0575 - mse: 2545.0571 - mae: 29.5267 - val_loss: 1544.7540 - val_mse: 1544.7543 - val_mae: 27.5875\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 683us/step - loss: 2559.2239 - mse: 2559.2239 - mae: 29.9730 - val_loss: 1537.6830 - val_mse: 1537.6827 - val_mae: 27.7861\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2543.6057 - mse: 2543.6050 - mae: 29.8393 - val_loss: 1535.7450 - val_mse: 1535.7451 - val_mae: 27.7996\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2552.4611 - mse: 2552.4604 - mae: 29.6515 - val_loss: 1536.5934 - val_mse: 1536.5934 - val_mae: 27.7491\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2497.4080 - mse: 2497.4077 - mae: 29.4331 - val_loss: 1527.6020 - val_mse: 1527.6019 - val_mae: 28.1303\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2655.4767 - mse: 2655.4771 - mae: 30.2292 - val_loss: 1532.4397 - val_mse: 1532.4397 - val_mae: 27.9427\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2562.9018 - mse: 2562.9023 - mae: 29.9300 - val_loss: 1530.0603 - val_mse: 1530.0604 - val_mae: 28.0261\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2572.6586 - mse: 2572.6582 - mae: 29.6826 - val_loss: 1541.1860 - val_mse: 1541.1859 - val_mae: 27.5611\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 675us/step - loss: 2556.3066 - mse: 2556.3066 - mae: 29.7648 - val_loss: 1539.8472 - val_mse: 1539.8474 - val_mae: 27.6086\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2552.3387 - mse: 2552.3386 - mae: 29.8522 - val_loss: 1538.5295 - val_mse: 1538.5295 - val_mae: 27.6397\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2486.9674 - mse: 2486.9680 - mae: 29.7217 - val_loss: 1537.0663 - val_mse: 1537.0663 - val_mae: 27.7227\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2585.9525 - mse: 2585.9529 - mae: 29.7856 - val_loss: 1540.5461 - val_mse: 1540.5459 - val_mae: 27.6232\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2480.9825 - mse: 2480.9829 - mae: 29.2792 - val_loss: 1535.0150 - val_mse: 1535.0150 - val_mae: 27.8126\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 665us/step - loss: 2472.7911 - mse: 2472.7910 - mae: 29.6305 - val_loss: 1531.8732 - val_mse: 1531.8732 - val_mae: 27.9440\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2455.9129 - mse: 2455.9126 - mae: 29.2861 - val_loss: 1529.6834 - val_mse: 1529.6833 - val_mae: 28.0104\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 693us/step - loss: 2569.1342 - mse: 2569.1345 - mae: 29.7565 - val_loss: 1529.5024 - val_mse: 1529.5024 - val_mae: 28.0034\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2585.6632 - mse: 2585.6638 - mae: 30.2127 - val_loss: 1538.5804 - val_mse: 1538.5802 - val_mae: 27.6480\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2572.3311 - mse: 2572.3308 - mae: 29.5882 - val_loss: 1540.2085 - val_mse: 1540.2086 - val_mae: 27.5765\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2531.4365 - mse: 2531.4365 - mae: 29.7227 - val_loss: 1541.0511 - val_mse: 1541.0510 - val_mae: 27.5695\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2533.0254 - mse: 2533.0261 - mae: 29.5607 - val_loss: 1534.2408 - val_mse: 1534.2406 - val_mae: 27.7772\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2561.6094 - mse: 2561.6091 - mae: 29.7202 - val_loss: 1533.1334 - val_mse: 1533.1332 - val_mae: 27.8237\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2504.5187 - mse: 2504.5188 - mae: 29.7897 - val_loss: 1532.9480 - val_mse: 1532.9480 - val_mae: 27.7953\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2483.6976 - mse: 2483.6970 - mae: 29.3612 - val_loss: 1531.9418 - val_mse: 1531.9419 - val_mae: 27.8691\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2533.9287 - mse: 2533.9292 - mae: 29.6155 - val_loss: 1531.6454 - val_mse: 1531.6455 - val_mae: 27.8515\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2568.9574 - mse: 2568.9575 - mae: 30.1566 - val_loss: 1530.1832 - val_mse: 1530.1832 - val_mae: 27.8460\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2512.1514 - mse: 2512.1501 - mae: 29.7332 - val_loss: 1528.5388 - val_mse: 1528.5388 - val_mae: 27.9207\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2535.3531 - mse: 2535.3535 - mae: 29.7753 - val_loss: 1528.3222 - val_mse: 1528.3223 - val_mae: 27.9256\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2513.8972 - mse: 2513.8972 - mae: 29.6189 - val_loss: 1535.5462 - val_mse: 1535.5461 - val_mae: 27.6262\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - ETA: 0s - loss: 2558.5880 - mse: 2558.5884 - mae: 29.19 - 1s 580us/step - loss: 2505.8369 - mse: 2505.8374 - mae: 29.0012 - val_loss: 1530.5829 - val_mse: 1530.5829 - val_mae: 27.7920\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2503.8943 - mse: 2503.8943 - mae: 29.4969 - val_loss: 1528.5288 - val_mse: 1528.5288 - val_mae: 27.8628\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2518.2021 - mse: 2518.2019 - mae: 29.4815 - val_loss: 1532.9400 - val_mse: 1532.9401 - val_mae: 27.6580\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2493.1461 - mse: 2493.1455 - mae: 29.7378 - val_loss: 1533.7279 - val_mse: 1533.7278 - val_mae: 27.6373\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2457.2275 - mse: 2457.2275 - mae: 29.6355 - val_loss: 1522.3015 - val_mse: 1522.3013 - val_mae: 28.1162\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2597.6361 - mse: 2597.6362 - mae: 30.2772 - val_loss: 1530.0283 - val_mse: 1530.0281 - val_mae: 27.7436\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2490.9851 - mse: 2490.9851 - mae: 28.8531 - val_loss: 1525.9908 - val_mse: 1525.9907 - val_mae: 27.8579\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2447.1708 - mse: 2447.1714 - mae: 29.2892 - val_loss: 1526.9471 - val_mse: 1526.9470 - val_mae: 27.7740\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2519.2012 - mse: 2519.2014 - mae: 29.2694 - val_loss: 1526.1902 - val_mse: 1526.1901 - val_mae: 27.8055\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2408.1557 - mse: 2408.1553 - mae: 29.7879 - val_loss: 3690.4723 - val_mse: 3690.4719 - val_mae: 24.3310\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2413.5962 - mse: 2413.5962 - mae: 29.9712 - val_loss: 3689.5283 - val_mse: 3689.5288 - val_mae: 24.0193\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2434.8443 - mse: 2434.8447 - mae: 29.6856 - val_loss: 3690.7282 - val_mse: 3690.7290 - val_mae: 24.3240\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2377.5219 - mse: 2377.5220 - mae: 29.7122 - val_loss: 3690.0254 - val_mse: 3690.0254 - val_mae: 24.1553\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2361.5024 - mse: 2361.5027 - mae: 29.7684 - val_loss: 3692.9193 - val_mse: 3692.9189 - val_mae: 24.7098\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2375.2205 - mse: 2375.2207 - mae: 29.5664 - val_loss: 3690.7429 - val_mse: 3690.7434 - val_mae: 24.4365\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2427.0050 - mse: 2427.0044 - mae: 29.9351 - val_loss: 3690.2741 - val_mse: 3690.2742 - val_mae: 24.3526\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2364.5962 - mse: 2364.5964 - mae: 29.4205 - val_loss: 3689.2540 - val_mse: 3689.2546 - val_mae: 24.1378\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2406.2649 - mse: 2406.2644 - mae: 30.0205 - val_loss: 3689.0333 - val_mse: 3689.0327 - val_mae: 24.0414\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2386.1699 - mse: 2386.1694 - mae: 29.8921 - val_loss: 3693.0570 - val_mse: 3693.0566 - val_mae: 24.7202\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2423.8015 - mse: 2423.8020 - mae: 29.7264 - val_loss: 3689.2473 - val_mse: 3689.2476 - val_mae: 23.7747\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 503us/step - loss: 2391.1929 - mse: 2391.1929 - mae: 29.7746 - val_loss: 3691.2643 - val_mse: 3691.2642 - val_mae: 24.4249\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2361.7933 - mse: 2361.7927 - mae: 29.2142 - val_loss: 3689.7031 - val_mse: 3689.7031 - val_mae: 24.1112\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 517us/step - loss: 2464.9690 - mse: 2464.9690 - mae: 29.6241 - val_loss: 3688.8525 - val_mse: 3688.8518 - val_mae: 23.8374\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2417.5374 - mse: 2417.5381 - mae: 30.0650 - val_loss: 3688.9981 - val_mse: 3688.9973 - val_mae: 23.7181\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2431.3329 - mse: 2431.3318 - mae: 29.3409 - val_loss: 3691.9724 - val_mse: 3691.9729 - val_mae: 24.5334\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2452.9176 - mse: 2452.9177 - mae: 30.0305 - val_loss: 3690.4369 - val_mse: 3690.4370 - val_mae: 24.1758\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2373.5884 - mse: 2373.5881 - mae: 29.7389 - val_loss: 3694.7688 - val_mse: 3694.7681 - val_mae: 24.9285\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2409.2367 - mse: 2409.2361 - mae: 29.7947 - val_loss: 3689.3445 - val_mse: 3689.3450 - val_mae: 24.1090\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2413.2462 - mse: 2413.2461 - mae: 30.2259 - val_loss: 3689.8867 - val_mse: 3689.8862 - val_mae: 24.2462\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - ETA: 0s - loss: 2452.0620 - mse: 2452.0615 - mae: 29.91 - 2s 597us/step - loss: 2453.6441 - mse: 2453.6436 - mae: 29.9757 - val_loss: 3688.6306 - val_mse: 3688.6306 - val_mae: 23.9636\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2411.9857 - mse: 2411.9858 - mae: 29.4335 - val_loss: 3688.4465 - val_mse: 3688.4470 - val_mae: 24.0194\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2455.7010 - mse: 2455.7017 - mae: 29.8764 - val_loss: 3689.8532 - val_mse: 3689.8528 - val_mae: 24.4293\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2371.4903 - mse: 2371.4910 - mae: 29.6500 - val_loss: 3689.5248 - val_mse: 3689.5244 - val_mae: 24.2334\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2467.1387 - mse: 2467.1392 - mae: 29.8546 - val_loss: 3689.3157 - val_mse: 3689.3157 - val_mae: 24.0993\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2401.1388 - mse: 2401.1387 - mae: 29.9335 - val_loss: 3689.0922 - val_mse: 3689.0920 - val_mae: 23.7345\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2386.2997 - mse: 2386.2996 - mae: 29.3158 - val_loss: 3695.2036 - val_mse: 3695.2039 - val_mae: 24.9689\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2364.0473 - mse: 2364.0479 - mae: 29.7740 - val_loss: 3688.8639 - val_mse: 3688.8635 - val_mae: 24.0395\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2441.0513 - mse: 2441.0513 - mae: 29.9688 - val_loss: 3692.5901 - val_mse: 3692.5896 - val_mae: 24.7167\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2357.1851 - mse: 2357.1848 - mae: 29.8021 - val_loss: 3688.8550 - val_mse: 3688.8547 - val_mae: 23.9654\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2326.5706 - mse: 2326.5715 - mae: 29.5260 - val_loss: 3688.7131 - val_mse: 3688.7131 - val_mae: 23.7863\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 675us/step - loss: 2466.5363 - mse: 2466.5359 - mae: 29.7080 - val_loss: 3688.6121 - val_mse: 3688.6121 - val_mae: 23.7868\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2363.0310 - mse: 2363.0310 - mae: 29.1552 - val_loss: 3690.8626 - val_mse: 3690.8621 - val_mae: 24.3145\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2429.7793 - mse: 2429.7795 - mae: 29.7888 - val_loss: 3689.5076 - val_mse: 3689.5081 - val_mae: 24.0951\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2436.7730 - mse: 2436.7725 - mae: 30.0042 - val_loss: 3689.8072 - val_mse: 3689.8071 - val_mae: 24.1300\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2399.1060 - mse: 2399.1050 - mae: 29.8939 - val_loss: 3688.9564 - val_mse: 3688.9565 - val_mae: 23.7438\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2368.8860 - mse: 2368.8853 - mae: 29.2399 - val_loss: 3690.9208 - val_mse: 3690.9207 - val_mae: 24.3082\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2398.1721 - mse: 2398.1716 - mae: 30.0672 - val_loss: 3688.8085 - val_mse: 3688.8096 - val_mae: 23.6444\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2437.4577 - mse: 2437.4570 - mae: 29.9874 - val_loss: 3689.6296 - val_mse: 3689.6299 - val_mae: 23.8933\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2373.5067 - mse: 2373.5078 - mae: 29.7502 - val_loss: 3690.0886 - val_mse: 3690.0891 - val_mae: 23.9155\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2410.1382 - mse: 2410.1384 - mae: 30.0556 - val_loss: 3692.6508 - val_mse: 3692.6509 - val_mae: 24.5057\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2379.3210 - mse: 2379.3210 - mae: 29.5284 - val_loss: 3690.3121 - val_mse: 3690.3123 - val_mae: 24.0339\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2409.7647 - mse: 2409.7646 - mae: 29.5816 - val_loss: 3691.4474 - val_mse: 3691.4468 - val_mae: 24.3611\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2387.8477 - mse: 2387.8481 - mae: 29.8158 - val_loss: 3688.0307 - val_mse: 3688.0308 - val_mae: 23.8218\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 668us/step - loss: 2367.7254 - mse: 2367.7253 - mae: 29.5580 - val_loss: 3689.0863 - val_mse: 3689.0867 - val_mae: 24.1546\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2389.4146 - mse: 2389.4146 - mae: 29.5731 - val_loss: 3690.5954 - val_mse: 3690.5945 - val_mae: 24.2481\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2414.9324 - mse: 2414.9329 - mae: 30.2035 - val_loss: 3689.9340 - val_mse: 3689.9338 - val_mae: 23.9769\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2389.4649 - mse: 2389.4639 - mae: 30.0634 - val_loss: 3690.5345 - val_mse: 3690.5349 - val_mae: 24.0382\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2355.2646 - mse: 2355.2654 - mae: 29.3459 - val_loss: 3690.1635 - val_mse: 3690.1631 - val_mae: 24.0704\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2327.1791 - mse: 2327.1797 - mae: 28.9268 - val_loss: 3690.5461 - val_mse: 3690.5454 - val_mae: 24.1424\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2352.9667 - mse: 2352.9670 - mae: 29.5186 - val_loss: 3692.1831 - val_mse: 3692.1833 - val_mae: 24.3648\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 674us/step - loss: 2369.4865 - mse: 2369.4873 - mae: 29.3643 - val_loss: 3689.6827 - val_mse: 3689.6831 - val_mae: 23.5921\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2404.3982 - mse: 2404.3979 - mae: 29.8788 - val_loss: 3690.7829 - val_mse: 3690.7832 - val_mae: 24.0990\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 686us/step - loss: 2361.0917 - mse: 2361.0913 - mae: 29.6834 - val_loss: 3691.5874 - val_mse: 3691.5886 - val_mae: 24.2314\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2397.2455 - mse: 2397.2458 - mae: 29.8154 - val_loss: 3689.5462 - val_mse: 3689.5471 - val_mae: 23.6247\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2413.1327 - mse: 2413.1328 - mae: 29.8818 - val_loss: 3689.5850 - val_mse: 3689.5847 - val_mae: 23.6613\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2354.6276 - mse: 2354.6277 - mae: 29.0332 - val_loss: 3689.6031 - val_mse: 3689.6030 - val_mae: 23.8941\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2376.0328 - mse: 2376.0332 - mae: 29.6733 - val_loss: 3688.7887 - val_mse: 3688.7886 - val_mae: 23.4945\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2382.2037 - mse: 2382.2029 - mae: 29.2618 - val_loss: 3692.4197 - val_mse: 3692.4199 - val_mae: 24.5838\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2342.5346 - mse: 2342.5342 - mae: 29.4213 - val_loss: 3687.9642 - val_mse: 3687.9651 - val_mae: 23.5933\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2393.8902 - mse: 2393.8901 - mae: 29.7207 - val_loss: 3688.6614 - val_mse: 3688.6614 - val_mae: 23.9330\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2363.7560 - mse: 2363.7559 - mae: 29.6357 - val_loss: 3688.9673 - val_mse: 3688.9666 - val_mae: 23.9348\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2321.8041 - mse: 2321.8040 - mae: 29.3679 - val_loss: 3693.2411 - val_mse: 3693.2402 - val_mae: 24.4834\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2335.6413 - mse: 2335.6416 - mae: 29.0682 - val_loss: 3690.2093 - val_mse: 3690.2087 - val_mae: 23.8328\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2341.5481 - mse: 2341.5486 - mae: 29.3976 - val_loss: 3691.2979 - val_mse: 3691.2979 - val_mae: 24.1174\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2353.5963 - mse: 2353.5972 - mae: 29.3119 - val_loss: 3689.9566 - val_mse: 3689.9573 - val_mae: 23.8364\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 692us/step - loss: 2376.0434 - mse: 2376.0432 - mae: 29.6744 - val_loss: 3691.2803 - val_mse: 3691.2798 - val_mae: 24.3822\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2375.8860 - mse: 2375.8862 - mae: 29.2571 - val_loss: 3689.0283 - val_mse: 3689.0281 - val_mae: 24.1733\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2378.8478 - mse: 2378.8484 - mae: 29.6110 - val_loss: 3688.8950 - val_mse: 3688.8955 - val_mae: 24.0638\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2398.8390 - mse: 2398.8394 - mae: 29.7987 - val_loss: 3688.6984 - val_mse: 3688.6987 - val_mae: 24.0337\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2369.1251 - mse: 2369.1257 - mae: 29.2365 - val_loss: 3688.9143 - val_mse: 3688.9153 - val_mae: 23.9329\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2374.4171 - mse: 2374.4172 - mae: 29.4097 - val_loss: 3688.6379 - val_mse: 3688.6379 - val_mae: 23.7019\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2344.0263 - mse: 2344.0264 - mae: 29.3156 - val_loss: 3688.9281 - val_mse: 3688.9280 - val_mae: 23.7921\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2405.2983 - mse: 2405.2986 - mae: 29.6837 - val_loss: 3691.2783 - val_mse: 3691.2781 - val_mae: 24.3185\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2365.7562 - mse: 2365.7561 - mae: 29.5703 - val_loss: 3688.8591 - val_mse: 3688.8599 - val_mae: 23.7587\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2342.9552 - mse: 2342.9556 - mae: 29.0847 - val_loss: 3688.9021 - val_mse: 3688.9016 - val_mae: 23.8196\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2401.3005 - mse: 2401.3008 - mae: 29.7405 - val_loss: 3687.6986 - val_mse: 3687.6982 - val_mae: 23.6352\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2327.4651 - mse: 2327.4646 - mae: 29.3778 - val_loss: 3688.5563 - val_mse: 3688.5559 - val_mae: 23.8995\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2351.0495 - mse: 2351.0498 - mae: 29.4455 - val_loss: 3690.7982 - val_mse: 3690.7976 - val_mae: 24.3061\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2420.1665 - mse: 2420.1667 - mae: 29.8548 - val_loss: 3687.9821 - val_mse: 3687.9819 - val_mae: 23.6389\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2754.6767 - mse: 2754.6770 - mae: 28.9488 - val_loss: 2345.5668 - val_mse: 2345.5667 - val_mae: 26.2821\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2780.3413 - mse: 2780.3418 - mae: 29.0435 - val_loss: 2350.2998 - val_mse: 2350.3003 - val_mae: 26.6702\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2763.6631 - mse: 2763.6643 - mae: 28.7595 - val_loss: 2363.8147 - val_mse: 2363.8145 - val_mae: 26.6944\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2747.7191 - mse: 2747.7188 - mae: 28.9822 - val_loss: 2351.7904 - val_mse: 2351.7905 - val_mae: 27.3049\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2763.2977 - mse: 2763.2969 - mae: 29.0331 - val_loss: 2365.5438 - val_mse: 2365.5435 - val_mae: 26.6299\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2733.9096 - mse: 2733.9094 - mae: 29.0494 - val_loss: 2366.4511 - val_mse: 2366.4512 - val_mae: 26.7644\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2726.2277 - mse: 2726.2278 - mae: 28.6559 - val_loss: 2378.9580 - val_mse: 2378.9578 - val_mae: 26.3972\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2736.1622 - mse: 2736.1621 - mae: 28.9094 - val_loss: 2363.8745 - val_mse: 2363.8748 - val_mae: 26.6736\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2728.4269 - mse: 2728.4272 - mae: 28.8099 - val_loss: 2354.9370 - val_mse: 2354.9365 - val_mae: 27.0557\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2764.9365 - mse: 2764.9365 - mae: 29.0766 - val_loss: 2363.6360 - val_mse: 2363.6360 - val_mae: 26.5910\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2702.8208 - mse: 2702.8206 - mae: 28.5191 - val_loss: 2355.7214 - val_mse: 2355.7212 - val_mae: 26.8613\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2771.4669 - mse: 2771.4670 - mae: 28.8719 - val_loss: 2363.5156 - val_mse: 2363.5154 - val_mae: 26.7549\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2721.5083 - mse: 2721.5085 - mae: 28.8921 - val_loss: 2361.9984 - val_mse: 2361.9985 - val_mae: 26.6835\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2710.8225 - mse: 2710.8223 - mae: 28.8045 - val_loss: 2346.8473 - val_mse: 2346.8474 - val_mae: 26.9698\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2730.4869 - mse: 2730.4871 - mae: 29.0203 - val_loss: 2349.3723 - val_mse: 2349.3723 - val_mae: 26.8188\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2748.4703 - mse: 2748.4700 - mae: 28.9017 - val_loss: 2356.4816 - val_mse: 2356.4812 - val_mae: 26.8109\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2696.0308 - mse: 2696.0305 - mae: 28.7278 - val_loss: 2353.5395 - val_mse: 2353.5398 - val_mae: 27.1766\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 508us/step - loss: 2696.7264 - mse: 2696.7268 - mae: 28.7552 - val_loss: 2352.4365 - val_mse: 2352.4370 - val_mae: 26.8375\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2791.1353 - mse: 2791.1357 - mae: 28.9293 - val_loss: 2365.3163 - val_mse: 2365.3164 - val_mae: 26.7771\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 671us/step - loss: 2735.7279 - mse: 2735.7280 - mae: 28.9269 - val_loss: 2373.5059 - val_mse: 2373.5059 - val_mae: 26.5924\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2709.1393 - mse: 2709.1394 - mae: 28.5043 - val_loss: 2360.9939 - val_mse: 2360.9937 - val_mae: 27.2060\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2716.4733 - mse: 2716.4736 - mae: 28.7961 - val_loss: 2357.2316 - val_mse: 2357.2317 - val_mae: 27.5174\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2717.9980 - mse: 2717.9978 - mae: 28.7025 - val_loss: 2365.7351 - val_mse: 2365.7349 - val_mae: 26.7031\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2750.2082 - mse: 2750.2080 - mae: 28.7558 - val_loss: 2365.6167 - val_mse: 2365.6167 - val_mae: 26.5592\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2722.6564 - mse: 2722.6565 - mae: 28.9371 - val_loss: 2360.8569 - val_mse: 2360.8567 - val_mae: 26.8816\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2684.2874 - mse: 2684.2866 - mae: 28.3427 - val_loss: 2371.4279 - val_mse: 2371.4277 - val_mae: 26.5988\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2759.4111 - mse: 2759.4104 - mae: 29.0363 - val_loss: 2363.3032 - val_mse: 2363.3030 - val_mae: 26.8655\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 662us/step - loss: 2731.7834 - mse: 2731.7825 - mae: 28.6446 - val_loss: 2372.8615 - val_mse: 2372.8618 - val_mae: 26.4407\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2782.3240 - mse: 2782.3230 - mae: 28.9465 - val_loss: 2361.9117 - val_mse: 2361.9116 - val_mae: 27.2714\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2748.0160 - mse: 2748.0151 - mae: 28.9258 - val_loss: 2366.6532 - val_mse: 2366.6536 - val_mae: 26.4590\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2697.2253 - mse: 2697.2249 - mae: 28.5121 - val_loss: 2352.8190 - val_mse: 2352.8193 - val_mae: 26.9339\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2739.4022 - mse: 2739.4023 - mae: 28.8328 - val_loss: 2362.4144 - val_mse: 2362.4146 - val_mae: 26.7970\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2747.2728 - mse: 2747.2722 - mae: 28.9048 - val_loss: 2363.9551 - val_mse: 2363.9553 - val_mae: 27.0250\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2757.9939 - mse: 2757.9937 - mae: 29.0621 - val_loss: 2355.0855 - val_mse: 2355.0857 - val_mae: 27.2679\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2728.5927 - mse: 2728.5923 - mae: 28.7003 - val_loss: 2368.2268 - val_mse: 2368.2268 - val_mae: 26.6871\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 531us/step - loss: 2721.6333 - mse: 2721.6335 - mae: 28.8860 - val_loss: 2367.5758 - val_mse: 2367.5759 - val_mae: 26.7379\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2717.0426 - mse: 2717.0420 - mae: 28.8267 - val_loss: 2357.1916 - val_mse: 2357.1914 - val_mae: 27.1942\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2705.1468 - mse: 2705.1467 - mae: 28.5417 - val_loss: 2358.0506 - val_mse: 2358.0505 - val_mae: 26.9476\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2754.5078 - mse: 2754.5078 - mae: 29.0282 - val_loss: 2375.3521 - val_mse: 2375.3518 - val_mae: 26.6764\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2733.0995 - mse: 2733.0996 - mae: 28.8706 - val_loss: 2367.1135 - val_mse: 2367.1133 - val_mae: 27.0984\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2688.8706 - mse: 2688.8708 - mae: 28.6802 - val_loss: 2370.9139 - val_mse: 2370.9141 - val_mae: 26.7948\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2743.0393 - mse: 2743.0381 - mae: 28.8844 - val_loss: 2360.7590 - val_mse: 2360.7588 - val_mae: 27.5425\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2746.3008 - mse: 2746.3010 - mae: 29.1719 - val_loss: 2374.2628 - val_mse: 2374.2629 - val_mae: 26.7592\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 649us/step - loss: 2734.9984 - mse: 2734.9980 - mae: 28.8119 - val_loss: 2373.6826 - val_mse: 2373.6826 - val_mae: 26.8141\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2748.5849 - mse: 2748.5845 - mae: 28.7994 - val_loss: 2373.6367 - val_mse: 2373.6365 - val_mae: 26.8482\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2741.5152 - mse: 2741.5149 - mae: 28.4041 - val_loss: 2367.5850 - val_mse: 2367.5850 - val_mae: 26.9552\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2709.7143 - mse: 2709.7139 - mae: 28.4627 - val_loss: 2373.3300 - val_mse: 2373.3303 - val_mae: 27.0305\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 661us/step - loss: 2765.9107 - mse: 2765.9104 - mae: 28.4692 - val_loss: 2371.0897 - val_mse: 2371.0896 - val_mae: 27.0411\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2706.2033 - mse: 2706.2034 - mae: 28.6029 - val_loss: 2378.4875 - val_mse: 2378.4875 - val_mae: 26.8627\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2771.8227 - mse: 2771.8223 - mae: 28.9478 - val_loss: 2378.7727 - val_mse: 2378.7727 - val_mae: 26.7622\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2746.3957 - mse: 2746.3955 - mae: 28.7851 - val_loss: 2364.0596 - val_mse: 2364.0593 - val_mae: 27.2700\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 484us/step - loss: 2655.0797 - mse: 2655.0798 - mae: 28.3540 - val_loss: 2364.6766 - val_mse: 2364.6770 - val_mae: 27.1260\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2700.4689 - mse: 2700.4688 - mae: 28.5973 - val_loss: 2360.1655 - val_mse: 2360.1658 - val_mae: 27.1324\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2737.3413 - mse: 2737.3420 - mae: 28.8373 - val_loss: 2357.2589 - val_mse: 2357.2583 - val_mae: 27.3046\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 547us/step - loss: 2696.1132 - mse: 2696.1130 - mae: 28.7824 - val_loss: 2355.0738 - val_mse: 2355.0742 - val_mae: 27.5864\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2740.0659 - mse: 2740.0662 - mae: 29.0673 - val_loss: 2356.5277 - val_mse: 2356.5278 - val_mae: 26.9958\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2695.8312 - mse: 2695.8315 - mae: 28.7210 - val_loss: 2354.0317 - val_mse: 2354.0317 - val_mae: 26.8016\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2693.9029 - mse: 2693.9036 - mae: 28.5564 - val_loss: 2353.4656 - val_mse: 2353.4651 - val_mae: 27.0042\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2720.1014 - mse: 2720.1011 - mae: 28.8292 - val_loss: 2349.4453 - val_mse: 2349.4451 - val_mae: 27.3883\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2705.4064 - mse: 2705.4060 - mae: 28.6278 - val_loss: 2360.0557 - val_mse: 2360.0562 - val_mae: 26.8454\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 504us/step - loss: 2764.4736 - mse: 2764.4741 - mae: 28.9845 - val_loss: 2362.2552 - val_mse: 2362.2556 - val_mae: 26.9887\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 509us/step - loss: 2710.8718 - mse: 2710.8711 - mae: 28.4999 - val_loss: 2368.2542 - val_mse: 2368.2539 - val_mae: 26.9313\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2698.2615 - mse: 2698.2622 - mae: 28.4441 - val_loss: 2360.4208 - val_mse: 2360.4207 - val_mae: 27.4114\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2660.9767 - mse: 2660.9768 - mae: 28.4998 - val_loss: 2356.5565 - val_mse: 2356.5564 - val_mae: 27.4441\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2756.9130 - mse: 2756.9136 - mae: 28.8509 - val_loss: 2365.4593 - val_mse: 2365.4592 - val_mae: 27.3423\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2745.8835 - mse: 2745.8838 - mae: 28.7039 - val_loss: 2369.3029 - val_mse: 2369.3025 - val_mae: 27.1085\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2745.2542 - mse: 2745.2539 - mae: 28.5868 - val_loss: 2374.5539 - val_mse: 2374.5542 - val_mae: 27.1127\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2707.6541 - mse: 2707.6541 - mae: 28.6666 - val_loss: 2376.6321 - val_mse: 2376.6321 - val_mae: 26.9118\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2686.4046 - mse: 2686.4053 - mae: 28.3286 - val_loss: 2364.3535 - val_mse: 2364.3540 - val_mae: 27.5043\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2729.3913 - mse: 2729.3909 - mae: 28.8242 - val_loss: 2369.8727 - val_mse: 2369.8728 - val_mae: 27.0866\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2715.9790 - mse: 2715.9785 - mae: 28.7516 - val_loss: 2372.5007 - val_mse: 2372.5010 - val_mae: 27.0891\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2732.0355 - mse: 2732.0361 - mae: 28.6449 - val_loss: 2378.3510 - val_mse: 2378.3508 - val_mae: 27.2410\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2722.9162 - mse: 2722.9165 - mae: 28.6169 - val_loss: 2376.4422 - val_mse: 2376.4421 - val_mae: 27.2772\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 537us/step - loss: 2689.0737 - mse: 2689.0735 - mae: 28.7045 - val_loss: 2371.4228 - val_mse: 2371.4229 - val_mae: 27.8697\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2727.9081 - mse: 2727.9077 - mae: 28.3058 - val_loss: 2379.2643 - val_mse: 2379.2642 - val_mae: 27.3366\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2697.5863 - mse: 2697.5869 - mae: 28.7014 - val_loss: 2373.9732 - val_mse: 2373.9731 - val_mae: 27.4257\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 671us/step - loss: 2697.8055 - mse: 2697.8049 - mae: 28.9772 - val_loss: 2382.4065 - val_mse: 2382.4067 - val_mae: 26.8779\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 514us/step - loss: 2707.6429 - mse: 2707.6431 - mae: 28.6987 - val_loss: 2379.6155 - val_mse: 2379.6155 - val_mae: 27.1034\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2745.1106 - mse: 2745.1099 - mae: 28.8932 - val_loss: 2379.9886 - val_mse: 2379.9885 - val_mae: 27.1467\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 548us/step - loss: 2704.6206 - mse: 2704.6199 - mae: 28.6846 - val_loss: 2378.6534 - val_mse: 2378.6533 - val_mae: 26.9114\n"
     ]
    }
   ],
   "source": [
    "# features list; order made according to Linear Regression FS\n",
    "features_list = ['APXP', \n",
    "                 'LOLP',  \n",
    "                 'In_gen',\n",
    "                 'Ren_R',\n",
    "                 'DA_imb_France', \n",
    "                 'Rene',\n",
    "                 'ratio_offers_vol',\n",
    "                 'DA_price_france',\n",
    "                 'TSDF',\n",
    "                 'dino_bin',\n",
    "                 'DA_margin',\n",
    "                 'Im_Pr']\n",
    "\n",
    "best_score = rmse_normal\n",
    "\n",
    "for i in features_list: \n",
    "    \n",
    "   # X_recovery = X_.copy()\n",
    "    X_recovery = X_\n",
    "    \n",
    "    X_ = pd.concat([X_, X.loc[:,i]], axis = 1)\n",
    "                    \n",
    "    X_.fillna(method = 'ffill', inplace = True)\n",
    "    y.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "    X_ = X_.astype('float64')\n",
    "    X_ = X_.round(20)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_, y, test_size = 0.15, shuffle = False)\n",
    "    \n",
    "    sc_X = MinMaxScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "\n",
    "    # possible debug\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "\n",
    "    def regressor_tunning(n_hidden = 5, \n",
    "                          n_neurons = 40, \n",
    "                          kernel_initializer = \"he_normal\",\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = n_neurons, input_dim = len(X_.columns)))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.3))\n",
    "        for layer in range(n_hidden):\n",
    "            model.add(Dense(units = n_neurons))\n",
    "            model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(rate = 0.3))\n",
    "        model.add(Dense(units = 1, activation = 'linear'))\n",
    "        optimizer = optimizers.Adamax(lr = 0.001)\n",
    "        model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "        return model\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits = splits)    \n",
    "    regressor = regressor_tunning()\n",
    "\n",
    "    # train model\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "        y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "        regressor.fit(X_train_split, y_train_split,  \n",
    "                             shuffle = False, \n",
    "                             validation_split = 0.2,\n",
    "                             batch_size = 20, \n",
    "                             epochs = epochs)\n",
    "\n",
    "    # make predictions and evaluate for all regions\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRICS EVALUATION (1) for the whole test set\n",
    "    # =============================================================================\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    # calculate metrics\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "\n",
    "    # append to list\n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRICS EVALUATION (2) on spike regions\n",
    "    # =============================================================================\n",
    "\n",
    "    # download spike indication binary set\n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "    # append ot lists\n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRIC EVALUATION (3) on normal regions\n",
    "    # =============================================================================\n",
    "\n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "    # append to list\n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "\n",
    "    # condition of improvement for FS\n",
    "    if best_score < rmse_nor[-1]:\n",
    "        X_ = X_recovery\n",
    "    else:\n",
    "        X_ = X_\n",
    "        best_score = rmse_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PrevDay</th>\n",
       "      <td>30.726699</td>\n",
       "      <td>21.294778</td>\n",
       "      <td>72.114891</td>\n",
       "      <td>61.039473</td>\n",
       "      <td>17.782678</td>\n",
       "      <td>15.431658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APXP</th>\n",
       "      <td>30.429759</td>\n",
       "      <td>21.596119</td>\n",
       "      <td>69.591476</td>\n",
       "      <td>57.999739</td>\n",
       "      <td>18.658391</td>\n",
       "      <td>16.225872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOLP</th>\n",
       "      <td>30.605767</td>\n",
       "      <td>20.877213</td>\n",
       "      <td>72.383195</td>\n",
       "      <td>61.226702</td>\n",
       "      <td>17.377950</td>\n",
       "      <td>14.924873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In_gen</th>\n",
       "      <td>30.602583</td>\n",
       "      <td>21.084103</td>\n",
       "      <td>71.982150</td>\n",
       "      <td>60.891433</td>\n",
       "      <td>17.615632</td>\n",
       "      <td>15.211743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ren_R</th>\n",
       "      <td>30.707674</td>\n",
       "      <td>20.765516</td>\n",
       "      <td>72.611698</td>\n",
       "      <td>61.299822</td>\n",
       "      <td>17.443496</td>\n",
       "      <td>14.785912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_imb_France</th>\n",
       "      <td>30.591610</td>\n",
       "      <td>21.202803</td>\n",
       "      <td>71.791700</td>\n",
       "      <td>60.706185</td>\n",
       "      <td>17.708170</td>\n",
       "      <td>15.375280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rene</th>\n",
       "      <td>30.616220</td>\n",
       "      <td>20.965014</td>\n",
       "      <td>72.360681</td>\n",
       "      <td>61.261438</td>\n",
       "      <td>17.412876</td>\n",
       "      <td>15.020503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_offers_vol</th>\n",
       "      <td>31.283237</td>\n",
       "      <td>21.222581</td>\n",
       "      <td>73.264279</td>\n",
       "      <td>61.346841</td>\n",
       "      <td>18.198226</td>\n",
       "      <td>15.303467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_price_france</th>\n",
       "      <td>30.572388</td>\n",
       "      <td>20.439204</td>\n",
       "      <td>73.139244</td>\n",
       "      <td>62.081320</td>\n",
       "      <td>16.835066</td>\n",
       "      <td>14.296177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSDF</th>\n",
       "      <td>30.379316</td>\n",
       "      <td>21.180851</td>\n",
       "      <td>69.203709</td>\n",
       "      <td>57.352566</td>\n",
       "      <td>18.776453</td>\n",
       "      <td>15.844815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dino_bin</th>\n",
       "      <td>30.575084</td>\n",
       "      <td>20.812832</td>\n",
       "      <td>72.008006</td>\n",
       "      <td>60.801475</td>\n",
       "      <td>17.545107</td>\n",
       "      <td>14.913725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_margin</th>\n",
       "      <td>30.776047</td>\n",
       "      <td>21.535122</td>\n",
       "      <td>72.107956</td>\n",
       "      <td>61.205578</td>\n",
       "      <td>17.884461</td>\n",
       "      <td>15.682953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Im_Pr</th>\n",
       "      <td>30.449524</td>\n",
       "      <td>20.290533</td>\n",
       "      <td>73.173766</td>\n",
       "      <td>62.019863</td>\n",
       "      <td>16.555087</td>\n",
       "      <td>14.134639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rmse_general  mae_general  rmse_spike  mae_spike  \\\n",
       "PrevDay              30.726699    21.294778   72.114891  61.039473   \n",
       "APXP                 30.429759    21.596119   69.591476  57.999739   \n",
       "LOLP                 30.605767    20.877213   72.383195  61.226702   \n",
       "In_gen               30.602583    21.084103   71.982150  60.891433   \n",
       "Ren_R                30.707674    20.765516   72.611698  61.299822   \n",
       "DA_imb_France        30.591610    21.202803   71.791700  60.706185   \n",
       "Rene                 30.616220    20.965014   72.360681  61.261438   \n",
       "ratio_offers_vol     31.283237    21.222581   73.264279  61.346841   \n",
       "DA_price_france      30.572388    20.439204   73.139244  62.081320   \n",
       "TSDF                 30.379316    21.180851   69.203709  57.352566   \n",
       "dino_bin             30.575084    20.812832   72.008006  60.801475   \n",
       "DA_margin            30.776047    21.535122   72.107956  61.205578   \n",
       "Im_Pr                30.449524    20.290533   73.173766  62.019863   \n",
       "\n",
       "                  rmse_normal  mae_normal  \n",
       "PrevDay             17.782678   15.431658  \n",
       "APXP                18.658391   16.225872  \n",
       "LOLP                17.377950   14.924873  \n",
       "In_gen              17.615632   15.211743  \n",
       "Ren_R               17.443496   14.785912  \n",
       "DA_imb_France       17.708170   15.375280  \n",
       "Rene                17.412876   15.020503  \n",
       "ratio_offers_vol    18.198226   15.303467  \n",
       "DA_price_france     16.835066   14.296177  \n",
       "TSDF                18.776453   15.844815  \n",
       "dino_bin            17.545107   14.913725  \n",
       "DA_margin           17.884461   15.682953  \n",
       "Im_Pr               16.555087   14.134639  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list_index =   ['PrevDay',\n",
    "                         'APXP', \n",
    "                         'LOLP',  \n",
    "                         'In_gen',\n",
    "                         'Ren_R',\n",
    "                         'DA_imb_France', \n",
    "                         'Rene',\n",
    "                         'ratio_offers_vol',\n",
    "                         'DA_price_france',\n",
    "                         'TSDF',\n",
    "                         'dino_bin',\n",
    "                         'DA_margin',\n",
    "                         'Im_Pr']\n",
    "\n",
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,}, index = features_list_index)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PrevDay', 'LOLP', 'DA_price_france', 'Im_Pr'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.555087250883144"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
