{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply FS in ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (1.18.1)\n",
      "Requirement already satisfied: sklearn in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# empty list to append metric values\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data & treat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > 2018090000, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential # to initialise the NN\n",
    "from keras.layers import Dense # to create layers\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "\n",
    "# parameters for ANN\n",
    "splits = 7\n",
    "epochs = 80\n",
    "\n",
    "# divide into features and label\n",
    "X = data.iloc[:, 0:14]\n",
    "y = data.loc[:, 'Offers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do first prediciton with first feature in list from Linear Regression FS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13329.3499 - mse: 13329.3496 - mae: 109.9059 - val_loss: 34606.3645 - val_mse: 34606.3633 - val_mae: 132.6837\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 412us/step - loss: 13160.9637 - mse: 13160.9639 - mae: 109.1406 - val_loss: 34267.3100 - val_mse: 34267.3125 - val_mae: 131.4041\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 12643.7500 - mse: 12643.7490 - mae: 106.7763 - val_loss: 33244.0137 - val_mse: 33244.0156 - val_mae: 127.4637\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 421us/step - loss: 11311.3102 - mse: 11311.3096 - mae: 100.2896 - val_loss: 30429.6087 - val_mse: 30429.6055 - val_mae: 115.9401\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 389us/step - loss: 7972.2339 - mse: 7972.2339 - mae: 80.9776 - val_loss: 24349.9543 - val_mse: 24349.9531 - val_mae: 85.9359\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 356us/step - loss: 3361.8219 - mse: 3361.8223 - mae: 43.2888 - val_loss: 17954.8006 - val_mse: 17954.8008 - val_mae: 38.1104\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 408us/step - loss: 2296.9526 - mse: 2296.9521 - mae: 35.6714 - val_loss: 17999.9748 - val_mse: 17999.9746 - val_mae: 38.3538\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 353us/step - loss: 2382.2517 - mse: 2382.2520 - mae: 34.0189 - val_loss: 17812.0481 - val_mse: 17812.0488 - val_mae: 37.4804\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 424us/step - loss: 2317.7759 - mse: 2317.7764 - mae: 35.4635 - val_loss: 17932.2517 - val_mse: 17932.2520 - val_mae: 37.9943\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 422us/step - loss: 2240.6576 - mse: 2240.6572 - mae: 33.6379 - val_loss: 17851.7820 - val_mse: 17851.7812 - val_mae: 37.6434\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 411us/step - loss: 2271.7942 - mse: 2271.7942 - mae: 34.3240 - val_loss: 17905.7401 - val_mse: 17905.7402 - val_mae: 37.8761\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2194.2242 - mse: 2194.2241 - mae: 33.4960 - val_loss: 17778.4080 - val_mse: 17778.4082 - val_mae: 37.3529\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 353us/step - loss: 2437.3741 - mse: 2437.3743 - mae: 34.5904 - val_loss: 18094.2642 - val_mse: 18094.2637 - val_mae: 38.9055\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 2440.5785 - mse: 2440.5784 - mae: 35.0140 - val_loss: 18100.1469 - val_mse: 18100.1465 - val_mae: 38.9376\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 387us/step - loss: 2134.6607 - mse: 2134.6606 - mae: 32.9782 - val_loss: 17974.2035 - val_mse: 17974.2012 - val_mae: 38.2024\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 409us/step - loss: 2258.5289 - mse: 2258.5291 - mae: 32.9686 - val_loss: 17627.2578 - val_mse: 17627.2578 - val_mae: 37.1740\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 391us/step - loss: 2308.6452 - mse: 2308.6453 - mae: 33.3760 - val_loss: 17802.9593 - val_mse: 17802.9609 - val_mae: 37.4373\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 441us/step - loss: 2312.5467 - mse: 2312.5471 - mae: 34.4681 - val_loss: 17820.5747 - val_mse: 17820.5742 - val_mae: 37.5034\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 382us/step - loss: 2026.5929 - mse: 2026.5931 - mae: 32.9603 - val_loss: 17865.8760 - val_mse: 17865.8770 - val_mae: 37.6922\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 361us/step - loss: 2362.9181 - mse: 2362.9182 - mae: 35.0386 - val_loss: 18159.7682 - val_mse: 18159.7656 - val_mae: 39.2686\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 2310.7937 - mse: 2310.7935 - mae: 34.2998 - val_loss: 17737.2560 - val_mse: 17737.2559 - val_mae: 37.2146\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 2135.6503 - mse: 2135.6504 - mae: 33.1624 - val_loss: 17955.2098 - val_mse: 17955.2109 - val_mae: 38.0882\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 380us/step - loss: 2104.9811 - mse: 2104.9810 - mae: 32.6767 - val_loss: 17825.9493 - val_mse: 17825.9492 - val_mae: 37.5174\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 377us/step - loss: 2170.3237 - mse: 2170.3240 - mae: 32.7370 - val_loss: 17791.1034 - val_mse: 17791.1035 - val_mae: 37.3851\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 377us/step - loss: 2175.1358 - mse: 2175.1357 - mae: 32.4819 - val_loss: 17738.5771 - val_mse: 17738.5762 - val_mae: 37.2117\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 375us/step - loss: 2022.8234 - mse: 2022.8231 - mae: 32.5385 - val_loss: 17683.8492 - val_mse: 17683.8496 - val_mae: 37.1575\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 375us/step - loss: 2220.6298 - mse: 2220.6296 - mae: 32.9227 - val_loss: 17808.9726 - val_mse: 17808.9707 - val_mae: 37.4476\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 371us/step - loss: 2095.7995 - mse: 2095.7996 - mae: 31.5331 - val_loss: 17912.3166 - val_mse: 17912.3164 - val_mae: 37.8767\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 370us/step - loss: 2042.4550 - mse: 2042.4551 - mae: 32.5067 - val_loss: 17791.9770 - val_mse: 17791.9766 - val_mae: 37.3795\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 378us/step - loss: 2180.2484 - mse: 2180.2485 - mae: 32.6675 - val_loss: 17813.9110 - val_mse: 17813.9121 - val_mae: 37.4627\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 376us/step - loss: 2168.6275 - mse: 2168.6274 - mae: 33.1725 - val_loss: 17837.7331 - val_mse: 17837.7344 - val_mae: 37.5522\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 370us/step - loss: 2199.8539 - mse: 2199.8540 - mae: 33.1407 - val_loss: 17869.1839 - val_mse: 17869.1836 - val_mae: 37.6802\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 368us/step - loss: 2126.3855 - mse: 2126.3855 - mae: 31.9336 - val_loss: 17719.4685 - val_mse: 17719.4688 - val_mae: 37.1772\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 2069.9522 - mse: 2069.9521 - mae: 32.1538 - val_loss: 17584.0064 - val_mse: 17584.0078 - val_mae: 37.2485\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 377us/step - loss: 2441.8652 - mse: 2441.8652 - mae: 35.3988 - val_loss: 17819.8318 - val_mse: 17819.8320 - val_mae: 37.4761\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 373us/step - loss: 1973.8667 - mse: 1973.8666 - mae: 31.1754 - val_loss: 17669.1117 - val_mse: 17669.1113 - val_mae: 37.1439\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 375us/step - loss: 2066.2167 - mse: 2066.2168 - mae: 31.9707 - val_loss: 17815.2267 - val_mse: 17815.2266 - val_mae: 37.4525\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 369us/step - loss: 2117.4258 - mse: 2117.4258 - mae: 31.4207 - val_loss: 17709.7461 - val_mse: 17709.7461 - val_mae: 37.1596\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 381us/step - loss: 1979.5959 - mse: 1979.5958 - mae: 31.2358 - val_loss: 17717.6153 - val_mse: 17717.6133 - val_mae: 37.1669\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 370us/step - loss: 2234.9109 - mse: 2234.9106 - mae: 33.0931 - val_loss: 17961.9686 - val_mse: 17961.9688 - val_mae: 38.0842\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 381us/step - loss: 2277.5949 - mse: 2277.5950 - mae: 33.7007 - val_loss: 17913.5993 - val_mse: 17913.5996 - val_mae: 37.8552\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 379us/step - loss: 2004.4056 - mse: 2004.4056 - mae: 31.5639 - val_loss: 17743.2874 - val_mse: 17743.2871 - val_mae: 37.1921\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 376us/step - loss: 1971.8076 - mse: 1971.8076 - mae: 30.0958 - val_loss: 17829.7539 - val_mse: 17829.7539 - val_mae: 37.4965\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 379us/step - loss: 2001.2541 - mse: 2001.2542 - mae: 31.3724 - val_loss: 17650.3513 - val_mse: 17650.3516 - val_mae: 37.1394\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 381us/step - loss: 1984.9167 - mse: 1984.9167 - mae: 31.4518 - val_loss: 17722.3619 - val_mse: 17722.3613 - val_mae: 37.1648\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 1744.3205 - mse: 1744.3206 - mae: 30.68 - 0s 384us/step - loss: 2008.1143 - mse: 2008.1144 - mae: 31.0952 - val_loss: 17664.0138 - val_mse: 17664.0117 - val_mae: 37.1350\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 373us/step - loss: 2072.7146 - mse: 2072.7144 - mae: 31.9524 - val_loss: 17742.6870 - val_mse: 17742.6855 - val_mae: 37.1860\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 445us/step - loss: 2082.5227 - mse: 2082.5229 - mae: 31.2148 - val_loss: 17608.5676 - val_mse: 17608.5684 - val_mae: 37.1865\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 370us/step - loss: 1933.4689 - mse: 1933.4690 - mae: 30.8793 - val_loss: 17695.4129 - val_mse: 17695.4121 - val_mae: 37.1347\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 356us/step - loss: 2036.3210 - mse: 2036.3212 - mae: 30.6548 - val_loss: 17645.7333 - val_mse: 17645.7324 - val_mae: 37.1358\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 372us/step - loss: 1996.0253 - mse: 1996.0253 - mae: 30.8402 - val_loss: 17777.9855 - val_mse: 17777.9863 - val_mae: 37.2893\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 378us/step - loss: 2081.1466 - mse: 2081.1465 - mae: 31.5953 - val_loss: 17612.2239 - val_mse: 17612.2266 - val_mae: 37.1776\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 373us/step - loss: 1993.3626 - mse: 1993.3624 - mae: 30.4387 - val_loss: 17786.3739 - val_mse: 17786.3730 - val_mae: 37.3198\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 370us/step - loss: 2085.6162 - mse: 2085.6160 - mae: 31.7688 - val_loss: 17713.3637 - val_mse: 17713.3613 - val_mae: 37.1459\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 367us/step - loss: 2064.4006 - mse: 2064.4004 - mae: 31.7091 - val_loss: 17641.5491 - val_mse: 17641.5508 - val_mae: 37.1372\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 364us/step - loss: 1963.2645 - mse: 1963.2644 - mae: 30.8616 - val_loss: 17679.2376 - val_mse: 17679.2402 - val_mae: 37.1291\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 366us/step - loss: 2082.4265 - mse: 2082.4263 - mae: 31.1689 - val_loss: 17828.3173 - val_mse: 17828.3184 - val_mae: 37.4782\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 1972.3016 - mse: 1972.3018 - mae: 30.2417 - val_loss: 17677.4235 - val_mse: 17677.4238 - val_mae: 37.1279\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 436us/step - loss: 1689.1106 - mse: 1689.1105 - mae: 28.4075 - val_loss: 17675.3546 - val_mse: 17675.3555 - val_mae: 37.1270\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 376us/step - loss: 1829.3172 - mse: 1829.3174 - mae: 29.2112 - val_loss: 17748.0610 - val_mse: 17748.0625 - val_mae: 37.1839\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 386us/step - loss: 1929.2908 - mse: 1929.2910 - mae: 30.1140 - val_loss: 17584.4853 - val_mse: 17584.4844 - val_mae: 37.2418\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 335us/step - loss: 2052.4846 - mse: 2052.4846 - mae: 30.1616 - val_loss: 17772.8540 - val_mse: 17772.8535 - val_mae: 37.2582\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 368us/step - loss: 1821.8031 - mse: 1821.8031 - mae: 29.4849 - val_loss: 17788.3493 - val_mse: 17788.3496 - val_mae: 37.3183\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 396us/step - loss: 1911.4702 - mse: 1911.4702 - mae: 30.2167 - val_loss: 17610.4446 - val_mse: 17610.4434 - val_mae: 37.1801\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 437us/step - loss: 2159.0841 - mse: 2159.0842 - mae: 31.5393 - val_loss: 17817.0831 - val_mse: 17817.0840 - val_mae: 37.4240\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 1936.5574 - mse: 1936.5573 - mae: 30.5522 - val_loss: 17609.1040 - val_mse: 17609.1035 - val_mae: 37.1831\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 1918.8291 - mse: 1918.8291 - mae: 29.6770 - val_loss: 17757.9093 - val_mse: 17757.9082 - val_mae: 37.1921\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 380us/step - loss: 1903.1832 - mse: 1903.1832 - mae: 30.8165 - val_loss: 17692.0842 - val_mse: 17692.0840 - val_mae: 37.1203\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 363us/step - loss: 1892.2724 - mse: 1892.2723 - mae: 30.1563 - val_loss: 17646.4957 - val_mse: 17646.4961 - val_mae: 37.1264\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 394us/step - loss: 2053.1477 - mse: 2053.1477 - mae: 32.6120 - val_loss: 17868.6278 - val_mse: 17868.6270 - val_mae: 37.6188\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 443us/step - loss: 1994.0670 - mse: 1994.0670 - mae: 30.8633 - val_loss: 17760.9710 - val_mse: 17760.9707 - val_mae: 37.2010\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 412us/step - loss: 1851.5776 - mse: 1851.5778 - mae: 29.4773 - val_loss: 17717.6650 - val_mse: 17717.6660 - val_mae: 37.1365\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 1861.2071 - mse: 1861.2072 - mae: 29.1644 - val_loss: 17670.7550 - val_mse: 17670.7559 - val_mae: 37.1181\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 367us/step - loss: 1818.6626 - mse: 1818.6625 - mae: 30.1078 - val_loss: 17608.7501 - val_mse: 17608.7480 - val_mae: 37.1827\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 368us/step - loss: 1980.2706 - mse: 1980.2705 - mae: 29.8549 - val_loss: 17790.5575 - val_mse: 17790.5566 - val_mae: 37.3133\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 364us/step - loss: 1895.7777 - mse: 1895.7778 - mae: 29.7507 - val_loss: 17736.3550 - val_mse: 17736.3535 - val_mae: 37.1556\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 364us/step - loss: 1855.2879 - mse: 1855.2878 - mae: 29.5564 - val_loss: 17504.5880 - val_mse: 17504.5898 - val_mae: 37.4614\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 1964.9676 - mse: 1964.9674 - mae: 30.1416 - val_loss: 17647.9858 - val_mse: 17647.9863 - val_mae: 37.1237\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 359us/step - loss: 1830.9408 - mse: 1830.9409 - mae: 29.7899 - val_loss: 17729.8199 - val_mse: 17729.8203 - val_mae: 37.1480\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 356us/step - loss: 1912.3351 - mse: 1912.3350 - mae: 29.7616 - val_loss: 17587.1012 - val_mse: 17587.0996 - val_mae: 37.2342\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 365us/step - loss: 4255.2497 - mse: 4255.2505 - mae: 34.6318 - val_loss: 2163.9405 - val_mse: 2163.9404 - val_mae: 31.0318\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 358us/step - loss: 4179.3863 - mse: 4179.3862 - mae: 35.2626 - val_loss: 2295.4349 - val_mse: 2295.4348 - val_mae: 31.3979\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 457us/step - loss: 3970.8356 - mse: 3970.8354 - mae: 32.8651 - val_loss: 2279.0593 - val_mse: 2279.0593 - val_mae: 31.3574\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 344us/step - loss: 3899.1706 - mse: 3899.1707 - mae: 33.7155 - val_loss: 2267.6765 - val_mse: 2267.6768 - val_mae: 31.3260\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 339us/step - loss: 4312.9005 - mse: 4312.9009 - mae: 35.1936 - val_loss: 2393.2722 - val_mse: 2393.2725 - val_mae: 31.6805\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 348us/step - loss: 4261.0042 - mse: 4261.0039 - mae: 34.2305 - val_loss: 2267.2027 - val_mse: 2267.2026 - val_mae: 31.3310\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 370us/step - loss: 4111.6737 - mse: 4111.6729 - mae: 34.3881 - val_loss: 2304.1690 - val_mse: 2304.1687 - val_mae: 31.4367\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 441us/step - loss: 4219.7640 - mse: 4219.7642 - mae: 34.9418 - val_loss: 2341.2697 - val_mse: 2341.2695 - val_mae: 31.5347\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 451us/step - loss: 4158.7974 - mse: 4158.7974 - mae: 33.8770 - val_loss: 2257.9425 - val_mse: 2257.9429 - val_mae: 31.3048\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 366us/step - loss: 4133.4147 - mse: 4133.4141 - mae: 33.6107 - val_loss: 2288.4550 - val_mse: 2288.4553 - val_mae: 31.3929\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 346us/step - loss: 4002.0507 - mse: 4002.0508 - mae: 33.4339 - val_loss: 2317.4030 - val_mse: 2317.4031 - val_mae: 31.4735\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 406us/step - loss: 4221.7358 - mse: 4221.7358 - mae: 34.9572 - val_loss: 2325.4522 - val_mse: 2325.4521 - val_mae: 31.4985\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4085.7105 - mse: 4085.7107 - mae: 33.9045 - val_loss: 2280.5458 - val_mse: 2280.5457 - val_mae: 31.3776\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4024.8161 - mse: 4024.8159 - mae: 33.7259 - val_loss: 2281.5966 - val_mse: 2281.5964 - val_mae: 31.3811\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 354us/step - loss: 4051.0342 - mse: 4051.0347 - mae: 33.4255 - val_loss: 2285.7538 - val_mse: 2285.7537 - val_mae: 31.3941\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 355us/step - loss: 4242.6189 - mse: 4242.6187 - mae: 34.2792 - val_loss: 2322.6869 - val_mse: 2322.6870 - val_mae: 31.4978\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 358us/step - loss: 4107.3192 - mse: 4107.3188 - mae: 34.2266 - val_loss: 2323.9597 - val_mse: 2323.9597 - val_mae: 31.5056\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4098.4931 - mse: 4098.4927 - mae: 34.0880 - val_loss: 2292.3712 - val_mse: 2292.3713 - val_mae: 31.4216\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 392us/step - loss: 4198.6923 - mse: 4198.6924 - mae: 34.9833 - val_loss: 2364.4233 - val_mse: 2364.4236 - val_mae: 31.6175\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 379us/step - loss: 4111.3365 - mse: 4111.3374 - mae: 33.8809 - val_loss: 2279.6964 - val_mse: 2279.6965 - val_mae: 31.3913\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 399us/step - loss: 4074.9831 - mse: 4074.9827 - mae: 33.7613 - val_loss: 2339.6447 - val_mse: 2339.6448 - val_mae: 31.5559\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 394us/step - loss: 4024.8341 - mse: 4024.8345 - mae: 33.3466 - val_loss: 2228.6063 - val_mse: 2228.6062 - val_mae: 31.2573\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4146.9793 - mse: 4146.9800 - mae: 34.6037 - val_loss: 2328.6712 - val_mse: 2328.6711 - val_mae: 31.5285\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 363us/step - loss: 4154.7532 - mse: 4154.7534 - mae: 34.7140 - val_loss: 2356.0640 - val_mse: 2356.0640 - val_mae: 31.6043\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 360us/step - loss: 3980.1022 - mse: 3980.1016 - mae: 33.1243 - val_loss: 2301.2593 - val_mse: 2301.2593 - val_mae: 31.4595\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 363us/step - loss: 4156.9496 - mse: 4156.9497 - mae: 33.7996 - val_loss: 2334.3253 - val_mse: 2334.3252 - val_mae: 31.5493\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 352us/step - loss: 4188.8314 - mse: 4188.8320 - mae: 34.3775 - val_loss: 2329.1752 - val_mse: 2329.1755 - val_mae: 31.5401\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 351us/step - loss: 4200.3496 - mse: 4200.3506 - mae: 34.9299 - val_loss: 2315.9724 - val_mse: 2315.9724 - val_mae: 31.5078\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 360us/step - loss: 4117.2634 - mse: 4117.2632 - mae: 33.9297 - val_loss: 2279.0583 - val_mse: 2279.0583 - val_mae: 31.4064\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 340us/step - loss: 4138.2647 - mse: 4138.2646 - mae: 33.8422 - val_loss: 2295.0833 - val_mse: 2295.0833 - val_mae: 31.4529\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 351us/step - loss: 3886.6891 - mse: 3886.6892 - mae: 33.3007 - val_loss: 2281.0688 - val_mse: 2281.0688 - val_mae: 31.4149\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 354us/step - loss: 4208.5312 - mse: 4208.5317 - mae: 34.9381 - val_loss: 2353.4403 - val_mse: 2353.4402 - val_mae: 31.6139\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 345us/step - loss: 4076.9212 - mse: 4076.9209 - mae: 33.9021 - val_loss: 2306.0339 - val_mse: 2306.0337 - val_mae: 31.4886\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 352us/step - loss: 4161.3446 - mse: 4161.3447 - mae: 34.9570 - val_loss: 2314.8544 - val_mse: 2314.8542 - val_mae: 31.5148\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 361us/step - loss: 4028.8646 - mse: 4028.8643 - mae: 33.6526 - val_loss: 2313.4744 - val_mse: 2313.4744 - val_mae: 31.5130\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 360us/step - loss: 4200.0024 - mse: 4200.0029 - mae: 33.2681 - val_loss: 2348.6819 - val_mse: 2348.6819 - val_mae: 31.6081\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 376us/step - loss: 4035.3390 - mse: 4035.3389 - mae: 33.7067 - val_loss: 2268.4788 - val_mse: 2268.4788 - val_mae: 31.3930\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 368us/step - loss: 4042.1724 - mse: 4042.1726 - mae: 34.3488 - val_loss: 2305.7529 - val_mse: 2305.7529 - val_mae: 31.4966\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4013.5825 - mse: 4013.5828 - mae: 33.7998 - val_loss: 2303.1563 - val_mse: 2303.1562 - val_mae: 31.4923\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 366us/step - loss: 3955.7507 - mse: 3955.7507 - mae: 32.7837 - val_loss: 2283.9025 - val_mse: 2283.9028 - val_mae: 31.4399\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 360us/step - loss: 3987.4289 - mse: 3987.4290 - mae: 33.0585 - val_loss: 2304.4924 - val_mse: 2304.4922 - val_mae: 31.4985\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 414us/step - loss: 4083.4959 - mse: 4083.4951 - mae: 33.7120 - val_loss: 2302.9418 - val_mse: 2302.9417 - val_mae: 31.4961\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4096.6975 - mse: 4096.6978 - mae: 33.6519 - val_loss: 2306.3923 - val_mse: 2306.3923 - val_mae: 31.5078\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 365us/step - loss: 3954.2110 - mse: 3954.2112 - mae: 32.1722 - val_loss: 2273.2748 - val_mse: 2273.2747 - val_mae: 31.4178\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4167.7191 - mse: 4167.7197 - mae: 33.5294 - val_loss: 2307.7940 - val_mse: 2307.7939 - val_mae: 31.5174\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 4235.1087 - mse: 4235.1094 - mae: 34.0791 - val_loss: 2324.6312 - val_mse: 2324.6313 - val_mae: 31.5649\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 357us/step - loss: 4063.2813 - mse: 4063.2812 - mae: 33.7338 - val_loss: 2279.4399 - val_mse: 2279.4397 - val_mae: 31.4432\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 364us/step - loss: 4025.3567 - mse: 4025.3567 - mae: 33.8114 - val_loss: 2263.8065 - val_mse: 2263.8064 - val_mae: 31.4046\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 400us/step - loss: 4169.9793 - mse: 4169.9790 - mae: 34.3227 - val_loss: 2393.7152 - val_mse: 2393.7153 - val_mae: 31.7719\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 352us/step - loss: 3958.4007 - mse: 3958.4011 - mae: 33.1649 - val_loss: 2318.9219 - val_mse: 2318.9221 - val_mae: 31.5565\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 370us/step - loss: 3971.2608 - mse: 3971.2610 - mae: 33.2518 - val_loss: 2305.2536 - val_mse: 2305.2537 - val_mae: 31.5202\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 365us/step - loss: 4169.4436 - mse: 4169.4434 - mae: 34.3942 - val_loss: 2302.9071 - val_mse: 2302.9072 - val_mae: 31.5131\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 391us/step - loss: 3996.3318 - mse: 3996.3323 - mae: 33.3595 - val_loss: 2317.3641 - val_mse: 2317.3643 - val_mae: 31.5529\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 345us/step - loss: 4238.5737 - mse: 4238.5732 - mae: 34.5426 - val_loss: 2333.7210 - val_mse: 2333.7214 - val_mae: 31.5988\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 442us/step - loss: 3988.7492 - mse: 3988.7488 - mae: 33.2266 - val_loss: 2298.2289 - val_mse: 2298.2288 - val_mae: 31.5048\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 372us/step - loss: 4112.1704 - mse: 4112.1709 - mae: 33.5849 - val_loss: 2298.2486 - val_mse: 2298.2485 - val_mae: 31.5056\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 363us/step - loss: 4108.8025 - mse: 4108.8027 - mae: 33.1870 - val_loss: 2337.2886 - val_mse: 2337.2886 - val_mae: 31.6154\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 363us/step - loss: 4128.4296 - mse: 4128.4287 - mae: 33.2762 - val_loss: 2290.0387 - val_mse: 2290.0386 - val_mae: 31.4854\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 4143.2018 - mse: 4143.2012 - mae: 33.4575 - val_loss: 2370.4711 - val_mse: 2370.4709 - val_mae: 31.7159\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 329us/step - loss: 3997.1416 - mse: 3997.1414 - mae: 33.3080 - val_loss: 2289.5496 - val_mse: 2289.5496 - val_mae: 31.4918\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 384us/step - loss: 4037.3854 - mse: 4037.3855 - mae: 33.1549 - val_loss: 2233.4857 - val_mse: 2233.4858 - val_mae: 31.3502\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 3764.1767 - mse: 3764.1775 - mae: 33.0975 - val_loss: 2285.0321 - val_mse: 2285.0320 - val_mae: 31.4821\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 411us/step - loss: 4184.4386 - mse: 4184.4385 - mae: 34.2164 - val_loss: 2381.1178 - val_mse: 2381.1179 - val_mae: 31.7646\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 383us/step - loss: 4094.8788 - mse: 4094.8789 - mae: 32.8204 - val_loss: 2330.6877 - val_mse: 2330.6880 - val_mae: 31.6211\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 384us/step - loss: 3845.7457 - mse: 3845.7458 - mae: 32.4718 - val_loss: 2288.1174 - val_mse: 2288.1172 - val_mae: 31.4968\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 343us/step - loss: 4195.0380 - mse: 4195.0386 - mae: 33.5477 - val_loss: 2350.4024 - val_mse: 2350.4026 - val_mae: 31.6836\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 354us/step - loss: 4019.8253 - mse: 4019.8257 - mae: 32.8185 - val_loss: 2328.4955 - val_mse: 2328.4956 - val_mae: 31.6196\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4043.6996 - mse: 4043.7002 - mae: 33.4843 - val_loss: 2305.7804 - val_mse: 2305.7805 - val_mae: 31.5506\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4068.9898 - mse: 4068.9897 - mae: 33.5044 - val_loss: 2338.9975 - val_mse: 2338.9978 - val_mae: 31.6543\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 373us/step - loss: 3861.2232 - mse: 3861.2229 - mae: 32.4436 - val_loss: 2311.9322 - val_mse: 2311.9321 - val_mae: 31.5767\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 360us/step - loss: 3927.4713 - mse: 3927.4712 - mae: 32.9858 - val_loss: 2273.3712 - val_mse: 2273.3713 - val_mae: 31.4671\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 3960.3371 - mse: 3960.3374 - mae: 33.8190 - val_loss: 2300.7234 - val_mse: 2300.7234 - val_mae: 31.5405\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 3989.1238 - mse: 3989.1240 - mae: 33.2343 - val_loss: 2290.7426 - val_mse: 2290.7424 - val_mae: 31.5109\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 3968.2986 - mse: 3968.2986 - mae: 33.4544 - val_loss: 2338.8635 - val_mse: 2338.8635 - val_mae: 31.6574\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 4050.7179 - mse: 4050.7183 - mae: 32.7235 - val_loss: 2330.5644 - val_mse: 2330.5642 - val_mae: 31.6359\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 364us/step - loss: 3922.9872 - mse: 3922.9875 - mae: 32.6130 - val_loss: 2271.6051 - val_mse: 2271.6050 - val_mae: 31.4676\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 370us/step - loss: 3996.6307 - mse: 3996.6309 - mae: 33.3526 - val_loss: 2357.0285 - val_mse: 2357.0286 - val_mae: 31.7200\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 369us/step - loss: 4036.6222 - mse: 4036.6221 - mae: 32.6361 - val_loss: 2309.7681 - val_mse: 2309.7681 - val_mae: 31.5843\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 373us/step - loss: 3974.0776 - mse: 3974.0779 - mae: 32.9114 - val_loss: 2284.6371 - val_mse: 2284.6372 - val_mae: 31.5112\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 370us/step - loss: 4045.1855 - mse: 4045.1858 - mae: 33.1063 - val_loss: 2349.5259 - val_mse: 2349.5256 - val_mae: 31.7048\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 360us/step - loss: 3284.3357 - mse: 3284.3362 - mae: 33.0362 - val_loss: 1460.2585 - val_mse: 1460.2585 - val_mae: 25.5291\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 357us/step - loss: 3377.9813 - mse: 3377.9817 - mae: 32.5371 - val_loss: 1460.7863 - val_mse: 1460.7863 - val_mae: 25.8314\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 376us/step - loss: 3356.7724 - mse: 3356.7720 - mae: 32.6974 - val_loss: 1460.7465 - val_mse: 1460.7467 - val_mae: 25.3685\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3229.1973 - mse: 3229.1978 - mae: 32.1846 - val_loss: 1461.7072 - val_mse: 1461.7069 - val_mae: 26.0326\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 384us/step - loss: 3409.7672 - mse: 3409.7673 - mae: 33.3078 - val_loss: 1462.2329 - val_mse: 1462.2329 - val_mae: 25.1165\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 357us/step - loss: 3298.8479 - mse: 3298.8474 - mae: 32.5462 - val_loss: 1460.9801 - val_mse: 1460.9801 - val_mae: 25.8525\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 361us/step - loss: 3307.1037 - mse: 3307.1033 - mae: 32.2032 - val_loss: 1460.4141 - val_mse: 1460.4141 - val_mae: 25.7141\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 346us/step - loss: 3423.2340 - mse: 3423.2334 - mae: 33.1582 - val_loss: 1460.5667 - val_mse: 1460.5667 - val_mae: 25.7803\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 362us/step - loss: 3360.2566 - mse: 3360.2559 - mae: 33.0129 - val_loss: 1460.3353 - val_mse: 1460.3352 - val_mae: 25.5715\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 362us/step - loss: 3355.3846 - mse: 3355.3845 - mae: 32.2789 - val_loss: 1462.2984 - val_mse: 1462.2985 - val_mae: 26.1362\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3388.2784 - mse: 3388.2783 - mae: 33.0702 - val_loss: 1460.7684 - val_mse: 1460.7684 - val_mae: 25.2974\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 357us/step - loss: 3338.2256 - mse: 3338.2253 - mae: 32.0864 - val_loss: 1463.6264 - val_mse: 1463.6263 - val_mae: 26.3517\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3343.7201 - mse: 3343.7200 - mae: 32.2142 - val_loss: 1460.3614 - val_mse: 1460.3615 - val_mae: 25.8083\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3474.5727 - mse: 3474.5732 - mae: 32.7849 - val_loss: 1460.3984 - val_mse: 1460.3987 - val_mae: 25.7755\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 363us/step - loss: 3371.4937 - mse: 3371.4944 - mae: 32.5608 - val_loss: 1464.4542 - val_mse: 1464.4543 - val_mae: 26.4823\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 369us/step - loss: 3274.2284 - mse: 3274.2288 - mae: 32.2114 - val_loss: 1460.1200 - val_mse: 1460.1201 - val_mae: 25.8588\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 372us/step - loss: 3458.4064 - mse: 3458.4053 - mae: 32.7013 - val_loss: 1461.5061 - val_mse: 1461.5062 - val_mae: 25.0933\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 397us/step - loss: 3424.2084 - mse: 3424.2090 - mae: 33.0464 - val_loss: 1459.9743 - val_mse: 1459.9742 - val_mae: 25.4633\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 376us/step - loss: 3310.2311 - mse: 3310.2312 - mae: 32.0247 - val_loss: 1459.8757 - val_mse: 1459.8756 - val_mae: 25.6688\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 346us/step - loss: 3378.6832 - mse: 3378.6833 - mae: 32.8270 - val_loss: 1461.1714 - val_mse: 1461.1715 - val_mae: 26.0197\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 354us/step - loss: 3214.5000 - mse: 3214.5000 - mae: 31.7898 - val_loss: 1463.3117 - val_mse: 1463.3116 - val_mae: 26.2990\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 401us/step - loss: 3502.2359 - mse: 3502.2358 - mae: 33.9393 - val_loss: 1461.6090 - val_mse: 1461.6091 - val_mae: 26.0464\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 361us/step - loss: 3281.7561 - mse: 3281.7559 - mae: 32.1027 - val_loss: 1460.4338 - val_mse: 1460.4340 - val_mae: 25.3950\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3336.6342 - mse: 3336.6340 - mae: 32.5240 - val_loss: 1461.7793 - val_mse: 1461.7794 - val_mae: 26.0896\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 357us/step - loss: 3379.1016 - mse: 3379.1013 - mae: 32.4962 - val_loss: 1460.4914 - val_mse: 1460.4915 - val_mae: 25.3629\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 350us/step - loss: 3197.0687 - mse: 3197.0679 - mae: 31.2436 - val_loss: 1460.7499 - val_mse: 1460.7501 - val_mae: 25.8687\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3228.1597 - mse: 3228.1602 - mae: 31.2736 - val_loss: 1464.9448 - val_mse: 1464.9448 - val_mae: 26.5053\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 343us/step - loss: 3234.7270 - mse: 3234.7268 - mae: 31.6723 - val_loss: 1462.8520 - val_mse: 1462.8521 - val_mae: 26.2559\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 392us/step - loss: 3230.9331 - mse: 3230.9321 - mae: 32.2254 - val_loss: 1460.2855 - val_mse: 1460.2854 - val_mae: 25.6859\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 340us/step - loss: 3305.2420 - mse: 3305.2412 - mae: 31.7681 - val_loss: 1461.4513 - val_mse: 1461.4515 - val_mae: 25.9392\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3298.4047 - mse: 3298.4050 - mae: 31.9327 - val_loss: 1462.7038 - val_mse: 1462.7035 - val_mae: 26.2000\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3312.5630 - mse: 3312.5630 - mae: 32.3556 - val_loss: 1460.7161 - val_mse: 1460.7159 - val_mae: 25.8368\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3346.0343 - mse: 3346.0354 - mae: 32.5631 - val_loss: 1461.5729 - val_mse: 1461.5729 - val_mae: 26.0577\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 387us/step - loss: 3368.7984 - mse: 3368.7986 - mae: 32.3386 - val_loss: 1460.1926 - val_mse: 1460.1929 - val_mae: 25.5891\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 377us/step - loss: 3391.6564 - mse: 3391.6567 - mae: 32.3090 - val_loss: 1460.2166 - val_mse: 1460.2166 - val_mae: 25.4178\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 354us/step - loss: 3345.7629 - mse: 3345.7629 - mae: 32.3309 - val_loss: 1460.3625 - val_mse: 1460.3624 - val_mae: 25.8100\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 357us/step - loss: 3267.7343 - mse: 3267.7341 - mae: 31.9048 - val_loss: 1461.2518 - val_mse: 1461.2517 - val_mae: 26.0319\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 353us/step - loss: 3298.9515 - mse: 3298.9504 - mae: 32.6604 - val_loss: 1461.9483 - val_mse: 1461.9484 - val_mae: 26.1738\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 354us/step - loss: 3327.7759 - mse: 3327.7764 - mae: 32.4388 - val_loss: 1460.9490 - val_mse: 1460.9487 - val_mae: 25.9969\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 351us/step - loss: 3231.0868 - mse: 3231.0869 - mae: 31.5070 - val_loss: 1461.8671 - val_mse: 1461.8669 - val_mae: 26.1996\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 356us/step - loss: 3401.0395 - mse: 3401.0396 - mae: 32.6243 - val_loss: 1459.2693 - val_mse: 1459.2690 - val_mae: 25.4964\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3264.4501 - mse: 3264.4500 - mae: 31.1753 - val_loss: 1468.9348 - val_mse: 1468.9348 - val_mae: 26.9234\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 354us/step - loss: 3240.9496 - mse: 3240.9502 - mae: 32.2511 - val_loss: 1459.2593 - val_mse: 1459.2594 - val_mae: 25.4720\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 363us/step - loss: 3268.8927 - mse: 3268.8923 - mae: 31.8009 - val_loss: 1459.9299 - val_mse: 1459.9298 - val_mae: 25.9067\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3253.8152 - mse: 3253.8154 - mae: 32.1382 - val_loss: 1459.4967 - val_mse: 1459.4968 - val_mae: 25.3504\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 361us/step - loss: 3238.1401 - mse: 3238.1404 - mae: 31.6013 - val_loss: 1459.4700 - val_mse: 1459.4700 - val_mae: 25.7983\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 356us/step - loss: 3376.4282 - mse: 3376.4285 - mae: 32.4530 - val_loss: 1459.9107 - val_mse: 1459.9108 - val_mae: 25.3067\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3238.4076 - mse: 3238.4077 - mae: 31.2883 - val_loss: 1466.1821 - val_mse: 1466.1820 - val_mae: 26.6842\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 358us/step - loss: 3319.2689 - mse: 3319.2688 - mae: 32.0730 - val_loss: 1459.7604 - val_mse: 1459.7605 - val_mae: 25.3367\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 356us/step - loss: 3325.2164 - mse: 3325.2158 - mae: 32.2174 - val_loss: 1459.6126 - val_mse: 1459.6127 - val_mae: 25.5225\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 361us/step - loss: 3210.5845 - mse: 3210.5835 - mae: 31.6872 - val_loss: 1461.6431 - val_mse: 1461.6432 - val_mae: 26.1754\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 361us/step - loss: 3296.1674 - mse: 3296.1682 - mae: 31.8022 - val_loss: 1461.3365 - val_mse: 1461.3365 - val_mae: 26.1492\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3384.6309 - mse: 3384.6304 - mae: 31.9945 - val_loss: 1460.7310 - val_mse: 1460.7310 - val_mae: 26.0217\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 368us/step - loss: 3264.1749 - mse: 3264.1753 - mae: 31.7708 - val_loss: 1459.5024 - val_mse: 1459.5026 - val_mae: 25.7997\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 368us/step - loss: 3287.4195 - mse: 3287.4197 - mae: 31.1057 - val_loss: 1464.1273 - val_mse: 1464.1273 - val_mae: 26.5247\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 369us/step - loss: 3325.7031 - mse: 3325.7026 - mae: 31.9177 - val_loss: 1459.7609 - val_mse: 1459.7610 - val_mae: 25.9350\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3198.6185 - mse: 3198.6194 - mae: 31.3280 - val_loss: 1466.9111 - val_mse: 1466.9109 - val_mae: 26.7809\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 358us/step - loss: 3325.2474 - mse: 3325.2468 - mae: 31.8261 - val_loss: 1459.6278 - val_mse: 1459.6277 - val_mae: 25.9020\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 341us/step - loss: 3263.6539 - mse: 3263.6543 - mae: 31.4802 - val_loss: 1460.9618 - val_mse: 1460.9619 - val_mae: 26.1865\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 344us/step - loss: 3276.7090 - mse: 3276.7097 - mae: 31.8030 - val_loss: 1460.8080 - val_mse: 1460.8080 - val_mae: 26.1576\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 327us/step - loss: 3317.9954 - mse: 3317.9961 - mae: 31.8100 - val_loss: 1459.1516 - val_mse: 1459.1516 - val_mae: 25.8446\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 371us/step - loss: 3397.5178 - mse: 3397.5188 - mae: 32.4739 - val_loss: 1459.4874 - val_mse: 1459.4873 - val_mae: 25.9169\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 366us/step - loss: 3154.7660 - mse: 3154.7666 - mae: 31.1097 - val_loss: 1458.6070 - val_mse: 1458.6069 - val_mae: 25.7020\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3261.4134 - mse: 3261.4131 - mae: 31.9573 - val_loss: 1462.0444 - val_mse: 1462.0444 - val_mae: 26.3668\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 358us/step - loss: 3380.3314 - mse: 3380.3308 - mae: 32.4873 - val_loss: 1460.0392 - val_mse: 1460.0391 - val_mae: 26.0309\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 357us/step - loss: 3251.0434 - mse: 3251.0427 - mae: 31.1659 - val_loss: 1458.8928 - val_mse: 1458.8928 - val_mae: 25.7423\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 355us/step - loss: 3426.9585 - mse: 3426.9592 - mae: 32.0892 - val_loss: 1458.7463 - val_mse: 1458.7462 - val_mae: 25.5308\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 354us/step - loss: 3267.3011 - mse: 3267.3010 - mae: 31.4512 - val_loss: 1458.7593 - val_mse: 1458.7592 - val_mae: 25.6407\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 357us/step - loss: 3229.1012 - mse: 3229.1011 - mae: 31.0770 - val_loss: 1458.9262 - val_mse: 1458.9263 - val_mae: 25.6607\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 345us/step - loss: 3209.8869 - mse: 3209.8877 - mae: 31.4491 - val_loss: 1459.1703 - val_mse: 1459.1702 - val_mae: 25.8248\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 358us/step - loss: 3444.7187 - mse: 3444.7197 - mae: 32.2949 - val_loss: 1458.6837 - val_mse: 1458.6837 - val_mae: 25.6667\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 408us/step - loss: 3192.6162 - mse: 3192.6160 - mae: 31.0550 - val_loss: 1458.7597 - val_mse: 1458.7596 - val_mae: 25.7778\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 388us/step - loss: 3195.7847 - mse: 3195.7849 - mae: 31.4193 - val_loss: 1458.5584 - val_mse: 1458.5583 - val_mae: 25.7214\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3159.4700 - mse: 3159.4690 - mae: 31.1547 - val_loss: 1460.3004 - val_mse: 1460.3004 - val_mae: 26.1680\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 363us/step - loss: 3232.2898 - mse: 3232.2896 - mae: 31.1991 - val_loss: 1459.5175 - val_mse: 1459.5176 - val_mae: 26.0355\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3237.6654 - mse: 3237.6655 - mae: 31.2550 - val_loss: 1459.6044 - val_mse: 1459.6042 - val_mae: 26.0897\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 368us/step - loss: 3228.4892 - mse: 3228.4880 - mae: 31.3894 - val_loss: 1458.1519 - val_mse: 1458.1519 - val_mae: 25.8215\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 398us/step - loss: 3225.9407 - mse: 3225.9412 - mae: 31.4430 - val_loss: 1457.7044 - val_mse: 1457.7043 - val_mae: 25.4745\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 352us/step - loss: 3362.7251 - mse: 3362.7253 - mae: 31.9679 - val_loss: 1459.1294 - val_mse: 1459.1294 - val_mae: 26.0632\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 354us/step - loss: 3236.3491 - mse: 3236.3491 - mae: 31.1354 - val_loss: 1458.3331 - val_mse: 1458.3329 - val_mae: 25.9468\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 360us/step - loss: 2898.4979 - mse: 2898.4983 - mae: 30.9945 - val_loss: 1066.7747 - val_mse: 1066.7748 - val_mae: 23.7550\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 360us/step - loss: 2960.2523 - mse: 2960.2522 - mae: 31.2445 - val_loss: 1063.4015 - val_mse: 1063.4014 - val_mae: 24.1254\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 358us/step - loss: 2861.5928 - mse: 2861.5918 - mae: 30.7938 - val_loss: 1063.5510 - val_mse: 1063.5510 - val_mae: 24.0735\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 348us/step - loss: 2859.0829 - mse: 2859.0830 - mae: 31.1431 - val_loss: 1063.7678 - val_mse: 1063.7678 - val_mae: 24.0090\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 343us/step - loss: 2934.7471 - mse: 2934.7473 - mae: 31.0542 - val_loss: 1063.7346 - val_mse: 1063.7346 - val_mae: 24.0438\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2855.7883 - mse: 2855.7876 - mae: 30.7710 - val_loss: 1063.3395 - val_mse: 1063.3395 - val_mae: 24.0911\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 390us/step - loss: 2867.5649 - mse: 2867.5647 - mae: 31.1894 - val_loss: 1062.7307 - val_mse: 1062.7306 - val_mae: 24.1463\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 352us/step - loss: 2944.3732 - mse: 2944.3728 - mae: 30.9826 - val_loss: 1063.1481 - val_mse: 1063.1479 - val_mae: 24.1214\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2866.5736 - mse: 2866.5735 - mae: 30.8112 - val_loss: 1064.2774 - val_mse: 1064.2773 - val_mae: 23.8840\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 372us/step - loss: 2934.1438 - mse: 2934.1445 - mae: 31.1198 - val_loss: 1063.0973 - val_mse: 1063.0972 - val_mae: 24.0289\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 372us/step - loss: 2923.9335 - mse: 2923.9333 - mae: 31.1240 - val_loss: 1061.9633 - val_mse: 1061.9631 - val_mae: 24.2747\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 352us/step - loss: 2864.4073 - mse: 2864.4077 - mae: 30.5867 - val_loss: 1062.7228 - val_mse: 1062.7228 - val_mae: 23.9777\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 370us/step - loss: 2972.4124 - mse: 2972.4126 - mae: 31.5111 - val_loss: 1063.5253 - val_mse: 1063.5254 - val_mae: 23.8527\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 359us/step - loss: 2873.6726 - mse: 2873.6733 - mae: 31.2718 - val_loss: 1061.1556 - val_mse: 1061.1556 - val_mae: 24.3147\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 374us/step - loss: 2927.8873 - mse: 2927.8872 - mae: 31.4583 - val_loss: 1062.6781 - val_mse: 1062.6782 - val_mae: 23.8839\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 363us/step - loss: 2869.5636 - mse: 2869.5632 - mae: 30.2080 - val_loss: 1060.3188 - val_mse: 1060.3187 - val_mae: 24.2434\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 384us/step - loss: 2817.7119 - mse: 2817.7122 - mae: 30.6670 - val_loss: 1060.6803 - val_mse: 1060.6802 - val_mae: 23.9938\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2877.5213 - mse: 2877.5210 - mae: 31.0157 - val_loss: 1059.7054 - val_mse: 1059.7056 - val_mae: 24.3320\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 362us/step - loss: 2929.3709 - mse: 2929.3708 - mae: 31.3547 - val_loss: 1061.8607 - val_mse: 1061.8607 - val_mae: 23.8186\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 343us/step - loss: 2938.1389 - mse: 2938.1396 - mae: 30.9919 - val_loss: 1059.1438 - val_mse: 1059.1438 - val_mae: 24.2435\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 396us/step - loss: 2969.6009 - mse: 2969.6016 - mae: 31.1737 - val_loss: 1058.9976 - val_mse: 1058.9977 - val_mae: 24.3738\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 364us/step - loss: 2926.9635 - mse: 2926.9631 - mae: 31.0831 - val_loss: 1058.6165 - val_mse: 1058.6163 - val_mae: 24.3031\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 363us/step - loss: 2929.6522 - mse: 2929.6523 - mae: 31.3438 - val_loss: 1062.5553 - val_mse: 1062.5552 - val_mae: 23.7236\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 358us/step - loss: 2896.4375 - mse: 2896.4370 - mae: 30.8983 - val_loss: 1060.0366 - val_mse: 1060.0367 - val_mae: 23.9000\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 360us/step - loss: 2889.9881 - mse: 2889.9883 - mae: 30.4287 - val_loss: 1058.3686 - val_mse: 1058.3687 - val_mae: 24.1560\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2943.0422 - mse: 2943.0417 - mae: 31.5895 - val_loss: 1061.3801 - val_mse: 1061.3800 - val_mae: 23.7697\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 371us/step - loss: 2877.3445 - mse: 2877.3430 - mae: 30.5659 - val_loss: 1057.9966 - val_mse: 1057.9965 - val_mae: 24.1039\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 340us/step - loss: 2913.7773 - mse: 2913.7771 - mae: 30.8825 - val_loss: 1060.4878 - val_mse: 1060.4877 - val_mae: 23.8000\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 384us/step - loss: 2955.4879 - mse: 2955.4880 - mae: 31.0947 - val_loss: 1059.1208 - val_mse: 1059.1208 - val_mae: 23.9193\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 383us/step - loss: 2838.1865 - mse: 2838.1877 - mae: 30.7367 - val_loss: 1058.9393 - val_mse: 1058.9393 - val_mae: 23.9184\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 337us/step - loss: 2875.6037 - mse: 2875.6052 - mae: 31.0646 - val_loss: 1057.4581 - val_mse: 1057.4580 - val_mae: 24.2729\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 372us/step - loss: 2870.1628 - mse: 2870.1621 - mae: 30.5474 - val_loss: 1057.6428 - val_mse: 1057.6428 - val_mae: 24.1410\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 375us/step - loss: 2845.0395 - mse: 2845.0398 - mae: 30.1550 - val_loss: 1057.1555 - val_mse: 1057.1555 - val_mae: 24.3144\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 362us/step - loss: 2828.3027 - mse: 2828.3030 - mae: 30.5379 - val_loss: 1057.4492 - val_mse: 1057.4491 - val_mae: 24.0469\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 351us/step - loss: 2847.9312 - mse: 2847.9312 - mae: 30.2499 - val_loss: 1059.9280 - val_mse: 1059.9281 - val_mae: 23.8133\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 369us/step - loss: 2905.9579 - mse: 2905.9583 - mae: 31.0056 - val_loss: 1058.1195 - val_mse: 1058.1195 - val_mae: 23.9601\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 367us/step - loss: 2830.9676 - mse: 2830.9683 - mae: 30.6590 - val_loss: 1056.9667 - val_mse: 1056.9666 - val_mae: 24.0485\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 356us/step - loss: 2868.8579 - mse: 2868.8582 - mae: 30.2757 - val_loss: 1057.2500 - val_mse: 1057.2500 - val_mae: 23.9629\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 348us/step - loss: 2872.6578 - mse: 2872.6575 - mae: 30.4567 - val_loss: 1056.4779 - val_mse: 1056.4779 - val_mae: 24.1012\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 340us/step - loss: 2837.6555 - mse: 2837.6550 - mae: 30.0383 - val_loss: 1055.8013 - val_mse: 1055.8014 - val_mae: 24.3250\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2931.5594 - mse: 2931.5591 - mae: 31.3308 - val_loss: 1056.4202 - val_mse: 1056.4200 - val_mae: 23.9550\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 369us/step - loss: 2938.7362 - mse: 2938.7354 - mae: 31.2950 - val_loss: 1058.0148 - val_mse: 1058.0149 - val_mae: 23.7797\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 366us/step - loss: 2893.9143 - mse: 2893.9136 - mae: 30.4176 - val_loss: 1056.4881 - val_mse: 1056.4882 - val_mae: 24.0433\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 370us/step - loss: 2894.4695 - mse: 2894.4690 - mae: 30.9893 - val_loss: 1057.7562 - val_mse: 1057.7562 - val_mae: 23.8599\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2904.9661 - mse: 2904.9661 - mae: 30.8160 - val_loss: 1056.0366 - val_mse: 1056.0369 - val_mae: 24.2230\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 364us/step - loss: 2919.0729 - mse: 2919.0723 - mae: 30.9064 - val_loss: 1057.4746 - val_mse: 1057.4747 - val_mae: 23.8739\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 326us/step - loss: 2842.7879 - mse: 2842.7878 - mae: 30.5496 - val_loss: 1055.9088 - val_mse: 1055.9091 - val_mae: 24.3451\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 402us/step - loss: 2855.9474 - mse: 2855.9465 - mae: 30.2111 - val_loss: 1058.0488 - val_mse: 1058.0487 - val_mae: 23.8643\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 352us/step - loss: 2847.5973 - mse: 2847.5977 - mae: 30.9142 - val_loss: 1056.9757 - val_mse: 1056.9758 - val_mae: 24.0382\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 332us/step - loss: 2887.0271 - mse: 2887.0264 - mae: 30.6235 - val_loss: 1056.4753 - val_mse: 1056.4753 - val_mae: 24.1999\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 357us/step - loss: 2797.5878 - mse: 2797.5879 - mae: 30.3454 - val_loss: 1056.4219 - val_mse: 1056.4219 - val_mae: 24.1542\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 364us/step - loss: 2854.2779 - mse: 2854.2788 - mae: 30.4794 - val_loss: 1056.2226 - val_mse: 1056.2225 - val_mae: 24.1659\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 365us/step - loss: 2849.6808 - mse: 2849.6807 - mae: 30.6812 - val_loss: 1057.7625 - val_mse: 1057.7625 - val_mae: 23.9234\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 346us/step - loss: 2794.6305 - mse: 2794.6311 - mae: 30.6869 - val_loss: 1056.8816 - val_mse: 1056.8816 - val_mae: 23.9524\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 357us/step - loss: 2883.4874 - mse: 2883.4875 - mae: 30.7733 - val_loss: 1056.5351 - val_mse: 1056.5352 - val_mae: 23.9764\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 329us/step - loss: 2870.8680 - mse: 2870.8689 - mae: 30.6932 - val_loss: 1056.1649 - val_mse: 1056.1649 - val_mae: 23.9455\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 366us/step - loss: 2873.9633 - mse: 2873.9626 - mae: 30.6599 - val_loss: 1055.0601 - val_mse: 1055.0603 - val_mae: 24.3241\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 362us/step - loss: 2807.7244 - mse: 2807.7241 - mae: 30.5576 - val_loss: 1054.6072 - val_mse: 1054.6071 - val_mae: 24.3215\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 351us/step - loss: 2855.9094 - mse: 2855.9097 - mae: 30.5134 - val_loss: 1054.6801 - val_mse: 1054.6799 - val_mae: 24.0619\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 370us/step - loss: 2819.2905 - mse: 2819.2900 - mae: 30.7022 - val_loss: 1054.5703 - val_mse: 1054.5704 - val_mae: 23.9606\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 378us/step - loss: 2935.5712 - mse: 2935.5698 - mae: 31.1307 - val_loss: 1053.9350 - val_mse: 1053.9349 - val_mae: 24.0219\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 390us/step - loss: 2790.2552 - mse: 2790.2551 - mae: 30.2568 - val_loss: 1054.0095 - val_mse: 1054.0095 - val_mae: 24.0647\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 337us/step - loss: 2881.4374 - mse: 2881.4382 - mae: 30.4026 - val_loss: 1055.5557 - val_mse: 1055.5555 - val_mae: 23.8301\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 363us/step - loss: 2898.9652 - mse: 2898.9651 - mae: 30.9226 - val_loss: 1054.7948 - val_mse: 1054.7947 - val_mae: 23.9180\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 364us/step - loss: 2772.3264 - mse: 2772.3264 - mae: 30.1750 - val_loss: 1053.3852 - val_mse: 1053.3853 - val_mae: 24.0618\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 354us/step - loss: 2796.1482 - mse: 2796.1484 - mae: 30.0321 - val_loss: 1052.9761 - val_mse: 1052.9761 - val_mae: 24.3166\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 363us/step - loss: 2828.3551 - mse: 2828.3550 - mae: 30.8757 - val_loss: 1052.8614 - val_mse: 1052.8615 - val_mae: 24.1258\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2838.0784 - mse: 2838.0786 - mae: 29.9434 - val_loss: 1052.5877 - val_mse: 1052.5876 - val_mae: 24.2089\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 351us/step - loss: 2944.6389 - mse: 2944.6394 - mae: 31.1662 - val_loss: 1053.4331 - val_mse: 1053.4331 - val_mae: 23.9030\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 364us/step - loss: 2810.0332 - mse: 2810.0334 - mae: 30.2467 - val_loss: 1052.7175 - val_mse: 1052.7177 - val_mae: 24.0020\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 346us/step - loss: 2827.2276 - mse: 2827.2271 - mae: 30.5684 - val_loss: 1052.4382 - val_mse: 1052.4381 - val_mae: 24.0638\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 358us/step - loss: 2825.6241 - mse: 2825.6243 - mae: 30.5045 - val_loss: 1051.9639 - val_mse: 1051.9641 - val_mae: 24.2670\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 354us/step - loss: 2875.3037 - mse: 2875.3035 - mae: 30.5102 - val_loss: 1051.7167 - val_mse: 1051.7166 - val_mae: 24.1676\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 348us/step - loss: 2830.4368 - mse: 2830.4373 - mae: 30.3186 - val_loss: 1051.7657 - val_mse: 1051.7656 - val_mae: 24.3788\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 393us/step - loss: 2870.0201 - mse: 2870.0203 - mae: 30.5371 - val_loss: 1052.0734 - val_mse: 1052.0732 - val_mae: 23.9776\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 374us/step - loss: 2818.7975 - mse: 2818.7976 - mae: 30.3210 - val_loss: 1051.3089 - val_mse: 1051.3088 - val_mae: 24.4285\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 363us/step - loss: 2821.1228 - mse: 2821.1221 - mae: 30.3510 - val_loss: 1052.1682 - val_mse: 1052.1682 - val_mae: 24.0060\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 362us/step - loss: 2774.6921 - mse: 2774.6912 - mae: 30.0679 - val_loss: 1052.0240 - val_mse: 1052.0239 - val_mae: 24.0442\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 354us/step - loss: 2879.9947 - mse: 2879.9951 - mae: 30.5236 - val_loss: 1052.0922 - val_mse: 1052.0922 - val_mae: 24.1401\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2798.7931 - mse: 2798.7935 - mae: 29.7268 - val_loss: 1053.0506 - val_mse: 1053.0507 - val_mae: 24.7344\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 399us/step - loss: 2610.9013 - mse: 2610.9011 - mae: 30.4542 - val_loss: 1547.1609 - val_mse: 1547.1608 - val_mae: 27.7986\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 381us/step - loss: 2552.3454 - mse: 2552.3447 - mae: 29.5931 - val_loss: 1552.5779 - val_mse: 1552.5778 - val_mae: 27.5873\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 383us/step - loss: 2509.8256 - mse: 2509.8247 - mae: 29.4809 - val_loss: 1551.5947 - val_mse: 1551.5946 - val_mae: 27.6035\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 347us/step - loss: 2533.2312 - mse: 2533.2302 - mae: 29.4570 - val_loss: 1541.8334 - val_mse: 1541.8331 - val_mae: 27.9482\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 387us/step - loss: 2538.6270 - mse: 2538.6279 - mae: 29.8351 - val_loss: 1543.9733 - val_mse: 1543.9733 - val_mae: 27.8414\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2564.1612 - mse: 2564.1609 - mae: 29.8500 - val_loss: 1541.5362 - val_mse: 1541.5363 - val_mae: 27.9212\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2483.7593 - mse: 2483.7585 - mae: 29.4838 - val_loss: 1541.1049 - val_mse: 1541.1049 - val_mae: 27.9135\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 355us/step - loss: 2481.0858 - mse: 2481.0859 - mae: 29.3108 - val_loss: 1541.0834 - val_mse: 1541.0835 - val_mae: 27.8948\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 386us/step - loss: 2576.5295 - mse: 2576.5295 - mae: 29.8834 - val_loss: 1543.2547 - val_mse: 1543.2548 - val_mae: 27.8043\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 387us/step - loss: 2528.0131 - mse: 2528.0120 - mae: 29.6031 - val_loss: 1540.6878 - val_mse: 1540.6876 - val_mae: 27.9162\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 440us/step - loss: 2551.0033 - mse: 2551.0029 - mae: 29.7133 - val_loss: 1539.8148 - val_mse: 1539.8148 - val_mae: 28.0091\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 368us/step - loss: 2512.5977 - mse: 2512.5974 - mae: 30.2149 - val_loss: 1545.6850 - val_mse: 1545.6849 - val_mae: 27.7548\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2455.4495 - mse: 2455.4497 - mae: 29.4875 - val_loss: 1549.6731 - val_mse: 1549.6732 - val_mae: 27.6242\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 370us/step - loss: 2507.1806 - mse: 2507.1816 - mae: 29.4158 - val_loss: 1543.9840 - val_mse: 1543.9841 - val_mae: 27.7884\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 354us/step - loss: 2546.9265 - mse: 2546.9263 - mae: 29.5459 - val_loss: 1553.4398 - val_mse: 1553.4398 - val_mae: 27.4649\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 359us/step - loss: 2575.1088 - mse: 2575.1091 - mae: 29.7585 - val_loss: 1539.4899 - val_mse: 1539.4900 - val_mae: 27.9644\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 382us/step - loss: 2450.9089 - mse: 2450.9087 - mae: 29.4301 - val_loss: 1540.5390 - val_mse: 1540.5388 - val_mae: 27.8991\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 350us/step - loss: 2539.7432 - mse: 2539.7437 - mae: 29.9933 - val_loss: 1544.2408 - val_mse: 1544.2407 - val_mae: 27.7564\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 366us/step - loss: 2580.6484 - mse: 2580.6484 - mae: 29.7180 - val_loss: 1545.5656 - val_mse: 1545.5658 - val_mae: 27.7114\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 370us/step - loss: 2518.8571 - mse: 2518.8560 - mae: 29.9539 - val_loss: 1537.3750 - val_mse: 1537.3749 - val_mae: 28.0440\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 388us/step - loss: 2567.1146 - mse: 2567.1140 - mae: 29.8772 - val_loss: 1548.3504 - val_mse: 1548.3505 - val_mae: 27.5461\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 361us/step - loss: 2491.9726 - mse: 2491.9719 - mae: 29.3524 - val_loss: 1540.4720 - val_mse: 1540.4720 - val_mae: 27.8659\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 347us/step - loss: 2479.1654 - mse: 2479.1653 - mae: 29.7923 - val_loss: 1537.1539 - val_mse: 1537.1539 - val_mae: 27.9571\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2553.8066 - mse: 2553.8066 - mae: 29.9123 - val_loss: 1541.6712 - val_mse: 1541.6711 - val_mae: 27.7048\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 371us/step - loss: 2545.0506 - mse: 2545.0508 - mae: 29.7211 - val_loss: 1535.2696 - val_mse: 1535.2694 - val_mae: 27.9586\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 364us/step - loss: 2506.3991 - mse: 2506.3994 - mae: 29.6928 - val_loss: 1545.0325 - val_mse: 1545.0325 - val_mae: 27.5727\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 365us/step - loss: 2573.5209 - mse: 2573.5208 - mae: 29.7971 - val_loss: 1548.2602 - val_mse: 1548.2603 - val_mae: 27.4508\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 390us/step - loss: 2521.5567 - mse: 2521.5569 - mae: 29.3900 - val_loss: 1542.6380 - val_mse: 1542.6378 - val_mae: 27.6282\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2576.2394 - mse: 2576.2393 - mae: 29.7141 - val_loss: 1547.7664 - val_mse: 1547.7666 - val_mae: 27.4713\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2524.0519 - mse: 2524.0525 - mae: 29.6718 - val_loss: 1538.6392 - val_mse: 1538.6392 - val_mae: 27.7891\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2515.9759 - mse: 2515.9746 - mae: 29.7103 - val_loss: 1546.7270 - val_mse: 1546.7271 - val_mae: 27.4969\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2509.2600 - mse: 2509.2603 - mae: 29.4322 - val_loss: 1538.8723 - val_mse: 1538.8723 - val_mae: 27.7536\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 265us/step - loss: 2544.4746 - mse: 2544.4756 - mae: 29.5766 - val_loss: 1533.8656 - val_mse: 1533.8655 - val_mae: 28.0488\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2525.7602 - mse: 2525.7603 - mae: 29.6843 - val_loss: 1537.8432 - val_mse: 1537.8431 - val_mae: 27.8005\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2603.4458 - mse: 2603.4463 - mae: 30.0233 - val_loss: 1534.3309 - val_mse: 1534.3308 - val_mae: 27.9713\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 355us/step - loss: 2487.6389 - mse: 2487.6387 - mae: 29.7841 - val_loss: 1534.0191 - val_mse: 1534.0190 - val_mae: 28.0112\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2467.4312 - mse: 2467.4312 - mae: 29.5484 - val_loss: 1538.5567 - val_mse: 1538.5566 - val_mae: 27.7984\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2517.0290 - mse: 2517.0283 - mae: 29.4632 - val_loss: 1536.5960 - val_mse: 1536.5958 - val_mae: 27.9168\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2477.8375 - mse: 2477.8374 - mae: 28.9491 - val_loss: 1539.7691 - val_mse: 1539.7694 - val_mae: 27.7766\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 273us/step - loss: 2554.3010 - mse: 2554.3003 - mae: 29.3829 - val_loss: 1533.7741 - val_mse: 1533.7740 - val_mae: 28.0553\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2553.3932 - mse: 2553.3933 - mae: 29.9341 - val_loss: 1535.2919 - val_mse: 1535.2920 - val_mae: 27.9523\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2478.5365 - mse: 2478.5364 - mae: 28.6658 - val_loss: 1533.9005 - val_mse: 1533.9005 - val_mae: 28.0516\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2511.2381 - mse: 2511.2378 - mae: 29.8026 - val_loss: 1535.1737 - val_mse: 1535.1737 - val_mae: 27.9314\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 277us/step - loss: 2535.2698 - mse: 2535.2695 - mae: 29.8447 - val_loss: 1544.4957 - val_mse: 1544.4958 - val_mae: 27.5468\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2523.2558 - mse: 2523.2556 - mae: 29.1722 - val_loss: 1538.2681 - val_mse: 1538.2681 - val_mae: 27.7741\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2471.9191 - mse: 2471.9202 - mae: 29.0623 - val_loss: 1533.9641 - val_mse: 1533.9642 - val_mae: 27.9823\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 324us/step - loss: 2494.7457 - mse: 2494.7456 - mae: 29.7489 - val_loss: 1534.5748 - val_mse: 1534.5747 - val_mae: 27.9164\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2551.9424 - mse: 2551.9409 - mae: 29.9281 - val_loss: 1540.8328 - val_mse: 1540.8329 - val_mae: 27.6526\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 261us/step - loss: 2526.2554 - mse: 2526.2559 - mae: 29.2695 - val_loss: 1542.9264 - val_mse: 1542.9265 - val_mae: 27.5926\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2555.7685 - mse: 2555.7686 - mae: 29.6392 - val_loss: 1542.1943 - val_mse: 1542.1943 - val_mae: 27.6347\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 338us/step - loss: 2563.9512 - mse: 2563.9514 - mae: 29.1696 - val_loss: 1540.9386 - val_mse: 1540.9388 - val_mae: 27.6579\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2524.7468 - mse: 2524.7466 - mae: 29.6506 - val_loss: 1544.2624 - val_mse: 1544.2623 - val_mae: 27.4755\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2472.6807 - mse: 2472.6809 - mae: 29.1750 - val_loss: 1533.8609 - val_mse: 1533.8607 - val_mae: 27.9894\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2523.7964 - mse: 2523.7959 - mae: 29.6460 - val_loss: 1547.3369 - val_mse: 1547.3370 - val_mae: 27.4185\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 253us/step - loss: 2537.9875 - mse: 2537.9873 - mae: 29.6887 - val_loss: 1543.9789 - val_mse: 1543.9789 - val_mae: 27.5335\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 279us/step - loss: 2543.9703 - mse: 2543.9712 - mae: 29.9066 - val_loss: 1544.3493 - val_mse: 1544.3492 - val_mae: 27.5358\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2482.5946 - mse: 2482.5950 - mae: 29.2686 - val_loss: 1536.9878 - val_mse: 1536.9878 - val_mae: 27.8341\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 351us/step - loss: 2482.3989 - mse: 2482.3989 - mae: 29.2368 - val_loss: 1540.7113 - val_mse: 1540.7114 - val_mae: 27.6863\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2461.3698 - mse: 2461.3699 - mae: 29.2799 - val_loss: 1543.5851 - val_mse: 1543.5851 - val_mae: 27.5430\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2506.3157 - mse: 2506.3164 - mae: 29.3263 - val_loss: 1536.8202 - val_mse: 1536.8201 - val_mae: 27.8132\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2468.7631 - mse: 2468.7639 - mae: 29.2196 - val_loss: 1537.6034 - val_mse: 1537.6035 - val_mae: 27.7998\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 330us/step - loss: 2536.4470 - mse: 2536.4475 - mae: 29.9065 - val_loss: 1536.0552 - val_mse: 1536.0552 - val_mae: 27.8184\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 271us/step - loss: 2501.7972 - mse: 2501.7974 - mae: 29.9300 - val_loss: 1541.7743 - val_mse: 1541.7739 - val_mae: 27.5650\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2466.2587 - mse: 2466.2593 - mae: 28.9087 - val_loss: 1539.9234 - val_mse: 1539.9232 - val_mae: 27.6113\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2500.6022 - mse: 2500.6023 - mae: 29.2264 - val_loss: 1536.7256 - val_mse: 1536.7258 - val_mae: 27.7699\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2554.4166 - mse: 2554.4167 - mae: 29.3436 - val_loss: 1529.9417 - val_mse: 1529.9418 - val_mae: 28.1223\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2513.4061 - mse: 2513.4058 - mae: 29.4805 - val_loss: 1535.5767 - val_mse: 1535.5768 - val_mae: 27.8275\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2505.3060 - mse: 2505.3064 - mae: 29.4530 - val_loss: 1539.0195 - val_mse: 1539.0195 - val_mae: 27.6671\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2472.7227 - mse: 2472.7227 - mae: 29.2513 - val_loss: 1535.2094 - val_mse: 1535.2095 - val_mae: 27.8041\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2551.9658 - mse: 2551.9661 - mae: 29.5457 - val_loss: 1542.2994 - val_mse: 1542.2994 - val_mae: 27.4999\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 276us/step - loss: 2541.2863 - mse: 2541.2854 - mae: 29.4246 - val_loss: 1538.2487 - val_mse: 1538.2488 - val_mae: 27.6658\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 271us/step - loss: 2457.1497 - mse: 2457.1499 - mae: 29.3315 - val_loss: 1537.6899 - val_mse: 1537.6899 - val_mae: 27.6896\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2466.1939 - mse: 2466.1929 - mae: 29.1315 - val_loss: 1530.7712 - val_mse: 1530.7712 - val_mae: 28.0555\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 248us/step - loss: 2508.1252 - mse: 2508.1255 - mae: 29.8843 - val_loss: 1534.3945 - val_mse: 1534.3944 - val_mae: 27.8069\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2493.7143 - mse: 2493.7148 - mae: 29.3443 - val_loss: 1529.9833 - val_mse: 1529.9832 - val_mae: 28.0246\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2518.3030 - mse: 2518.3037 - mae: 29.0269 - val_loss: 1532.1648 - val_mse: 1532.1648 - val_mae: 27.9166\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2457.8852 - mse: 2457.8853 - mae: 29.4707 - val_loss: 1534.8275 - val_mse: 1534.8275 - val_mae: 27.7854\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2480.8159 - mse: 2480.8154 - mae: 29.3916 - val_loss: 1539.2135 - val_mse: 1539.2135 - val_mae: 27.5898\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2506.6379 - mse: 2506.6377 - mae: 29.3295 - val_loss: 1536.4752 - val_mse: 1536.4751 - val_mae: 27.7362\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2516.4919 - mse: 2516.4919 - mae: 29.3613 - val_loss: 1536.7119 - val_mse: 1536.7120 - val_mae: 27.7522\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 259us/step - loss: 2412.4329 - mse: 2412.4326 - mae: 29.7971 - val_loss: 3702.7193 - val_mse: 3702.7195 - val_mae: 24.7979\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2371.2306 - mse: 2371.2310 - mae: 29.7039 - val_loss: 3701.1732 - val_mse: 3701.1731 - val_mae: 24.3307\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2391.5503 - mse: 2391.5505 - mae: 29.8392 - val_loss: 3703.0982 - val_mse: 3703.0984 - val_mae: 24.8398\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2407.8029 - mse: 2407.8020 - mae: 29.7614 - val_loss: 3702.9460 - val_mse: 3702.9465 - val_mae: 24.8250\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 340us/step - loss: 2438.1149 - mse: 2438.1140 - mae: 29.9546 - val_loss: 3705.8234 - val_mse: 3705.8242 - val_mae: 25.2740\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2425.4895 - mse: 2425.4902 - mae: 29.9656 - val_loss: 3701.2504 - val_mse: 3701.2502 - val_mae: 24.4780\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2381.8614 - mse: 2381.8613 - mae: 29.3377 - val_loss: 3702.6700 - val_mse: 3702.6704 - val_mae: 24.7755\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2412.7948 - mse: 2412.7949 - mae: 29.4623 - val_loss: 3704.1234 - val_mse: 3704.1243 - val_mae: 25.0056\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2407.6967 - mse: 2407.6965 - mae: 29.9063 - val_loss: 3705.1082 - val_mse: 3705.1084 - val_mae: 25.1327\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2411.7472 - mse: 2411.7468 - mae: 29.6966 - val_loss: 3701.9971 - val_mse: 3701.9973 - val_mae: 24.7145\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 275us/step - loss: 2453.2951 - mse: 2453.2949 - mae: 30.1721 - val_loss: 3701.6326 - val_mse: 3701.6318 - val_mae: 24.6979\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2343.8739 - mse: 2343.8743 - mae: 29.5541 - val_loss: 3702.1279 - val_mse: 3702.1279 - val_mae: 24.7287\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2455.2368 - mse: 2455.2373 - mae: 29.6639 - val_loss: 3705.7569 - val_mse: 3705.7571 - val_mae: 25.3140\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2406.1013 - mse: 2406.1008 - mae: 30.0429 - val_loss: 3702.6583 - val_mse: 3702.6572 - val_mae: 24.9352\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 357us/step - loss: 2396.3129 - mse: 2396.3130 - mae: 29.8101 - val_loss: 3701.6880 - val_mse: 3701.6892 - val_mae: 24.7580\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2422.9557 - mse: 2422.9556 - mae: 29.8114 - val_loss: 3701.0939 - val_mse: 3701.0940 - val_mae: 24.6046\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2425.6742 - mse: 2425.6743 - mae: 29.7723 - val_loss: 3702.0013 - val_mse: 3702.0012 - val_mae: 24.8031\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2370.8493 - mse: 2370.8494 - mae: 29.3197 - val_loss: 3703.8547 - val_mse: 3703.8545 - val_mae: 25.0344\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 248us/step - loss: 2408.8383 - mse: 2408.8381 - mae: 29.6247 - val_loss: 3704.6044 - val_mse: 3704.6045 - val_mae: 24.9775\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2367.8151 - mse: 2367.8157 - mae: 29.6835 - val_loss: 3707.1154 - val_mse: 3707.1147 - val_mae: 25.3292\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2393.4158 - mse: 2393.4158 - mae: 29.5117 - val_loss: 3706.2546 - val_mse: 3706.2544 - val_mae: 25.2284\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 330us/step - loss: 2366.9157 - mse: 2366.9155 - mae: 29.2967 - val_loss: 3703.1175 - val_mse: 3703.1172 - val_mae: 24.7414\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2355.6509 - mse: 2355.6511 - mae: 29.4082 - val_loss: 3706.3119 - val_mse: 3706.3120 - val_mae: 25.2055\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2408.1690 - mse: 2408.1689 - mae: 29.8990 - val_loss: 3704.7129 - val_mse: 3704.7131 - val_mae: 24.9527\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 250us/step - loss: 2359.8993 - mse: 2359.8997 - mae: 29.8704 - val_loss: 3705.6650 - val_mse: 3705.6646 - val_mae: 25.1863\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2417.2320 - mse: 2417.2324 - mae: 30.0702 - val_loss: 3704.3744 - val_mse: 3704.3752 - val_mae: 24.9862\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2398.0025 - mse: 2398.0024 - mae: 29.4940 - val_loss: 3706.3622 - val_mse: 3706.3621 - val_mae: 25.1450\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2431.8048 - mse: 2431.8049 - mae: 29.7385 - val_loss: 3702.3644 - val_mse: 3702.3635 - val_mae: 24.4323\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 249us/step - loss: 2382.6722 - mse: 2382.6721 - mae: 29.9341 - val_loss: 3702.6603 - val_mse: 3702.6597 - val_mae: 24.6442\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2374.0296 - mse: 2374.0305 - mae: 29.6259 - val_loss: 3706.3519 - val_mse: 3706.3518 - val_mae: 25.1948\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2330.5768 - mse: 2330.5774 - mae: 29.1549 - val_loss: 3708.8638 - val_mse: 3708.8630 - val_mae: 25.5086\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2382.0283 - mse: 2382.0288 - mae: 29.4320 - val_loss: 3704.6013 - val_mse: 3704.6008 - val_mae: 25.0325\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2368.1403 - mse: 2368.1401 - mae: 29.5321 - val_loss: 3702.3837 - val_mse: 3702.3843 - val_mae: 24.7261\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 256us/step - loss: 2394.5120 - mse: 2394.5120 - mae: 29.6902 - val_loss: 3707.4189 - val_mse: 3707.4185 - val_mae: 25.4149\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 255us/step - loss: 2354.7459 - mse: 2354.7463 - mae: 29.4187 - val_loss: 3705.2064 - val_mse: 3705.2061 - val_mae: 25.0723\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 242us/step - loss: 2446.0849 - mse: 2446.0845 - mae: 30.2092 - val_loss: 3704.4998 - val_mse: 3704.5000 - val_mae: 25.0327\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2318.2950 - mse: 2318.2952 - mae: 29.2313 - val_loss: 3704.2202 - val_mse: 3704.2195 - val_mae: 25.0536\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2401.6281 - mse: 2401.6282 - mae: 29.9478 - val_loss: 3700.1506 - val_mse: 3700.1506 - val_mae: 24.2805\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2413.6180 - mse: 2413.6187 - mae: 29.6826 - val_loss: 3701.1598 - val_mse: 3701.1594 - val_mae: 24.5974\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 365us/step - loss: 2359.2046 - mse: 2359.2046 - mae: 29.6678 - val_loss: 3705.3228 - val_mse: 3705.3230 - val_mae: 25.1250\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2321.7191 - mse: 2321.7195 - mae: 29.2321 - val_loss: 3704.1557 - val_mse: 3704.1562 - val_mae: 24.9795\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 267us/step - loss: 2372.8523 - mse: 2372.8523 - mae: 29.7604 - val_loss: 3705.2892 - val_mse: 3705.2898 - val_mae: 25.2291\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2350.7952 - mse: 2350.7957 - mae: 29.4025 - val_loss: 3702.1129 - val_mse: 3702.1145 - val_mae: 24.8389\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2345.3467 - mse: 2345.3459 - mae: 29.3928 - val_loss: 3703.7719 - val_mse: 3703.7720 - val_mae: 24.9794\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2340.4202 - mse: 2340.4207 - mae: 29.7682 - val_loss: 3704.0224 - val_mse: 3704.0220 - val_mae: 25.0142\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2407.5368 - mse: 2407.5369 - mae: 29.6017 - val_loss: 3699.8141 - val_mse: 3699.8149 - val_mae: 24.3625\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2386.1100 - mse: 2386.1099 - mae: 29.5733 - val_loss: 3702.2206 - val_mse: 3702.2209 - val_mae: 24.9294\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 263us/step - loss: 2393.0836 - mse: 2393.0830 - mae: 29.5174 - val_loss: 3702.2807 - val_mse: 3702.2808 - val_mae: 24.8825\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2365.0085 - mse: 2365.0088 - mae: 29.6910 - val_loss: 3703.7471 - val_mse: 3703.7473 - val_mae: 25.0567\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2433.6694 - mse: 2433.6694 - mae: 29.9217 - val_loss: 3703.1844 - val_mse: 3703.1838 - val_mae: 25.0385\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2363.5604 - mse: 2363.5598 - mae: 29.3962 - val_loss: 3702.5129 - val_mse: 3702.5125 - val_mae: 24.9163\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2331.6027 - mse: 2331.6023 - mae: 29.3535 - val_loss: 3702.0504 - val_mse: 3702.0503 - val_mae: 24.8159\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 263us/step - loss: 2378.1701 - mse: 2378.1709 - mae: 29.7415 - val_loss: 3704.0410 - val_mse: 3704.0408 - val_mae: 25.0625\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2331.3218 - mse: 2331.3215 - mae: 29.5683 - val_loss: 3707.5072 - val_mse: 3707.5078 - val_mae: 25.4834\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 362us/step - loss: 2384.8994 - mse: 2384.8999 - mae: 29.6312 - val_loss: 3703.1620 - val_mse: 3703.1631 - val_mae: 24.9736\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2438.1341 - mse: 2438.1343 - mae: 29.8949 - val_loss: 3700.6025 - val_mse: 3700.6021 - val_mae: 24.6469\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 366us/step - loss: 2388.7664 - mse: 2388.7671 - mae: 29.2779 - val_loss: 3705.7425 - val_mse: 3705.7417 - val_mae: 25.3834\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2364.4334 - mse: 2364.4329 - mae: 29.5101 - val_loss: 3700.5807 - val_mse: 3700.5811 - val_mae: 24.5474\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2331.1583 - mse: 2331.1594 - mae: 29.3264 - val_loss: 3701.4706 - val_mse: 3701.4714 - val_mae: 24.8297\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2346.6913 - mse: 2346.6917 - mae: 29.5680 - val_loss: 3702.8086 - val_mse: 3702.8083 - val_mae: 25.0193\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 266us/step - loss: 2346.5111 - mse: 2346.5122 - mae: 29.2486 - val_loss: 3706.4995 - val_mse: 3706.4998 - val_mae: 25.4564\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2378.2781 - mse: 2378.2791 - mae: 29.4752 - val_loss: 3702.5611 - val_mse: 3702.5608 - val_mae: 24.9265\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2393.9945 - mse: 2393.9944 - mae: 29.4794 - val_loss: 3701.6897 - val_mse: 3701.6897 - val_mae: 24.8631\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 331us/step - loss: 2339.9997 - mse: 2340.0000 - mae: 29.4460 - val_loss: 3703.2830 - val_mse: 3703.2837 - val_mae: 25.1287\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2334.4492 - mse: 2334.4482 - mae: 29.2676 - val_loss: 3703.0971 - val_mse: 3703.0981 - val_mae: 25.1014\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2323.3604 - mse: 2323.3608 - mae: 29.3682 - val_loss: 3704.5774 - val_mse: 3704.5774 - val_mae: 25.2494\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2367.7025 - mse: 2367.7029 - mae: 29.7426 - val_loss: 3704.2502 - val_mse: 3704.2502 - val_mae: 25.1666\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2382.3126 - mse: 2382.3120 - mae: 29.2846 - val_loss: 3702.1978 - val_mse: 3702.1968 - val_mae: 24.8172\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2361.1539 - mse: 2361.1528 - mae: 29.3384 - val_loss: 3705.5855 - val_mse: 3705.5857 - val_mae: 25.3259\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 253us/step - loss: 2370.0148 - mse: 2370.0142 - mae: 29.5991 - val_loss: 3705.1148 - val_mse: 3705.1145 - val_mae: 25.2658\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 362us/step - loss: 2360.4778 - mse: 2360.4775 - mae: 29.3938 - val_loss: 3704.0026 - val_mse: 3704.0027 - val_mae: 25.1491\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2366.5013 - mse: 2366.5015 - mae: 29.2873 - val_loss: 3706.7615 - val_mse: 3706.7620 - val_mae: 25.4908\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2280.1540 - mse: 2280.1536 - mae: 28.9217 - val_loss: 3707.6485 - val_mse: 3707.6479 - val_mae: 25.5931\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2359.8225 - mse: 2359.8215 - mae: 29.3155 - val_loss: 3700.1638 - val_mse: 3700.1636 - val_mae: 24.6639\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 244us/step - loss: 2362.9328 - mse: 2362.9326 - mae: 29.4432 - val_loss: 3703.4045 - val_mse: 3703.4050 - val_mae: 25.2273\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2322.7104 - mse: 2322.7104 - mae: 29.3532 - val_loss: 3705.7008 - val_mse: 3705.7007 - val_mae: 25.5636\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2319.6704 - mse: 2319.6699 - mae: 29.3764 - val_loss: 3702.1377 - val_mse: 3702.1375 - val_mae: 24.9620\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2302.9678 - mse: 2302.9678 - mae: 29.2764 - val_loss: 3704.7791 - val_mse: 3704.7788 - val_mae: 25.3124\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2393.9964 - mse: 2393.9968 - mae: 29.4395 - val_loss: 3703.9730 - val_mse: 3703.9731 - val_mae: 25.1742\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2315.2721 - mse: 2315.2715 - mae: 29.0956 - val_loss: 3704.5121 - val_mse: 3704.5125 - val_mae: 25.2028\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2656.6900 - mse: 2656.6904 - mae: 28.3660 - val_loss: 2479.7848 - val_mse: 2479.7847 - val_mae: 27.7212\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2741.0219 - mse: 2741.0232 - mae: 29.3255 - val_loss: 2487.5211 - val_mse: 2487.5212 - val_mae: 27.7560\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 331us/step - loss: 2742.6287 - mse: 2742.6284 - mae: 28.8203 - val_loss: 2495.9497 - val_mse: 2495.9492 - val_mae: 27.3656\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2743.0623 - mse: 2743.0623 - mae: 28.9191 - val_loss: 2492.4461 - val_mse: 2492.4460 - val_mae: 27.6747\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2708.0191 - mse: 2708.0190 - mae: 28.9556 - val_loss: 2490.0914 - val_mse: 2490.0918 - val_mae: 27.8256\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2759.5053 - mse: 2759.5054 - mae: 29.1598 - val_loss: 2507.3430 - val_mse: 2507.3428 - val_mae: 27.2090\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2794.0927 - mse: 2794.0933 - mae: 28.8309 - val_loss: 2502.3762 - val_mse: 2502.3765 - val_mae: 27.8038\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2775.2252 - mse: 2775.2241 - mae: 29.0129 - val_loss: 2515.8583 - val_mse: 2515.8582 - val_mae: 27.3130\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2751.8683 - mse: 2751.8684 - mae: 28.6347 - val_loss: 2504.0386 - val_mse: 2504.0383 - val_mae: 27.6525\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 249us/step - loss: 2740.7671 - mse: 2740.7678 - mae: 28.9096 - val_loss: 2505.6479 - val_mse: 2505.6477 - val_mae: 27.6668\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2746.6770 - mse: 2746.6763 - mae: 28.8563 - val_loss: 2503.5428 - val_mse: 2503.5425 - val_mae: 27.6871\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2702.2560 - mse: 2702.2554 - mae: 28.6233 - val_loss: 2510.7346 - val_mse: 2510.7346 - val_mae: 27.4096\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2743.9553 - mse: 2743.9548 - mae: 28.9096 - val_loss: 2509.8426 - val_mse: 2509.8425 - val_mae: 27.5838\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2733.1585 - mse: 2733.1582 - mae: 28.9474 - val_loss: 2512.3874 - val_mse: 2512.3877 - val_mae: 27.5954\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2707.0367 - mse: 2707.0374 - mae: 28.6789 - val_loss: 2504.1139 - val_mse: 2504.1145 - val_mae: 27.8682\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2724.3173 - mse: 2724.3164 - mae: 28.7070 - val_loss: 2507.0500 - val_mse: 2507.0496 - val_mae: 27.4985\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 338us/step - loss: 2762.3198 - mse: 2762.3198 - mae: 29.1401 - val_loss: 2507.8858 - val_mse: 2507.8862 - val_mae: 27.3985\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2726.6672 - mse: 2726.6687 - mae: 29.1782 - val_loss: 2506.2240 - val_mse: 2506.2239 - val_mae: 27.8207\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 257us/step - loss: 2744.0995 - mse: 2744.0994 - mae: 29.1075 - val_loss: 2507.9789 - val_mse: 2507.9792 - val_mae: 27.6626\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 329us/step - loss: 2694.3796 - mse: 2694.3784 - mae: 28.6833 - val_loss: 2503.0191 - val_mse: 2503.0190 - val_mae: 28.0252\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2764.1502 - mse: 2764.1509 - mae: 29.0608 - val_loss: 2504.0763 - val_mse: 2504.0762 - val_mae: 27.8622\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2757.6468 - mse: 2757.6470 - mae: 29.1898 - val_loss: 2506.8759 - val_mse: 2506.8760 - val_mae: 27.5335\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2732.0246 - mse: 2732.0244 - mae: 29.0075 - val_loss: 2506.9794 - val_mse: 2506.9792 - val_mae: 27.4153\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2726.6779 - mse: 2726.6777 - mae: 29.0525 - val_loss: 2498.5667 - val_mse: 2498.5674 - val_mae: 27.8103\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2759.8661 - mse: 2759.8669 - mae: 29.0538 - val_loss: 2503.6359 - val_mse: 2503.6360 - val_mae: 27.5385\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2757.1403 - mse: 2757.1406 - mae: 28.7657 - val_loss: 2501.5226 - val_mse: 2501.5227 - val_mae: 27.5765\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2692.5636 - mse: 2692.5635 - mae: 28.8718 - val_loss: 2496.3306 - val_mse: 2496.3306 - val_mae: 27.7171\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2724.0408 - mse: 2724.0408 - mae: 28.9207 - val_loss: 2494.9462 - val_mse: 2494.9463 - val_mae: 27.9896\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 344us/step - loss: 2760.6869 - mse: 2760.6873 - mae: 28.9512 - val_loss: 2510.5924 - val_mse: 2510.5923 - val_mae: 27.2543\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2744.3784 - mse: 2744.3787 - mae: 29.1113 - val_loss: 2509.6563 - val_mse: 2509.6565 - val_mae: 27.2976\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2748.1145 - mse: 2748.1143 - mae: 28.9048 - val_loss: 2506.4175 - val_mse: 2506.4177 - val_mae: 27.7948\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2717.3119 - mse: 2717.3110 - mae: 28.8266 - val_loss: 2510.8526 - val_mse: 2510.8523 - val_mae: 27.4896\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2755.1260 - mse: 2755.1260 - mae: 29.0628 - val_loss: 2514.1398 - val_mse: 2514.1399 - val_mae: 27.4939\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2732.5349 - mse: 2732.5347 - mae: 28.9353 - val_loss: 2514.9270 - val_mse: 2514.9270 - val_mae: 27.4390\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2704.4063 - mse: 2704.4055 - mae: 28.6862 - val_loss: 2507.6258 - val_mse: 2507.6260 - val_mae: 27.4754\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2733.2293 - mse: 2733.2295 - mae: 28.5574 - val_loss: 2504.1032 - val_mse: 2504.1033 - val_mae: 27.5230\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 260us/step - loss: 2736.8864 - mse: 2736.8867 - mae: 28.7810 - val_loss: 2512.9020 - val_mse: 2512.9023 - val_mae: 27.5124\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2763.1053 - mse: 2763.1050 - mae: 29.0608 - val_loss: 2503.2050 - val_mse: 2503.2056 - val_mae: 27.8957\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2699.3022 - mse: 2699.3022 - mae: 28.8082 - val_loss: 2503.1032 - val_mse: 2503.1035 - val_mae: 27.7903\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2719.2220 - mse: 2719.2219 - mae: 28.7917 - val_loss: 2511.5690 - val_mse: 2511.5691 - val_mae: 27.5187\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 242us/step - loss: 2703.2411 - mse: 2703.2405 - mae: 28.8774 - val_loss: 2508.9624 - val_mse: 2508.9622 - val_mae: 27.7183\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2742.2274 - mse: 2742.2273 - mae: 28.8222 - val_loss: 2507.3897 - val_mse: 2507.3899 - val_mae: 27.7331\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2739.0241 - mse: 2739.0234 - mae: 28.5770 - val_loss: 2514.2475 - val_mse: 2514.2473 - val_mae: 27.5692\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2779.2616 - mse: 2779.2615 - mae: 29.1014 - val_loss: 2504.3064 - val_mse: 2504.3059 - val_mae: 27.9335\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2788.1196 - mse: 2788.1206 - mae: 29.3268 - val_loss: 2510.3901 - val_mse: 2510.3901 - val_mae: 27.8094\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 265us/step - loss: 2717.0118 - mse: 2717.0117 - mae: 28.6793 - val_loss: 2507.3760 - val_mse: 2507.3760 - val_mae: 27.8532\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2747.2045 - mse: 2747.2034 - mae: 28.7431 - val_loss: 2510.9049 - val_mse: 2510.9048 - val_mae: 27.6130\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2713.3288 - mse: 2713.3296 - mae: 28.5044 - val_loss: 2504.9009 - val_mse: 2504.9006 - val_mae: 27.9446\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2692.3147 - mse: 2692.3147 - mae: 28.8958 - val_loss: 2509.0363 - val_mse: 2509.0364 - val_mae: 27.7465\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2770.0588 - mse: 2770.0588 - mae: 29.2015 - val_loss: 2511.5272 - val_mse: 2511.5273 - val_mae: 27.6472\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 327us/step - loss: 2742.5618 - mse: 2742.5625 - mae: 29.0120 - val_loss: 2516.6980 - val_mse: 2516.6978 - val_mae: 27.4812\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2752.1438 - mse: 2752.1436 - mae: 28.8500 - val_loss: 2514.8240 - val_mse: 2514.8237 - val_mae: 27.6581\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 257us/step - loss: 2688.1613 - mse: 2688.1621 - mae: 28.7592 - val_loss: 2514.7749 - val_mse: 2514.7744 - val_mae: 27.6612\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2706.5490 - mse: 2706.5486 - mae: 28.6384 - val_loss: 2506.1310 - val_mse: 2506.1311 - val_mae: 27.9603\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2692.9391 - mse: 2692.9392 - mae: 28.4857 - val_loss: 2513.8004 - val_mse: 2513.8013 - val_mae: 27.4884\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2715.1198 - mse: 2715.1191 - mae: 28.6471 - val_loss: 2510.0889 - val_mse: 2510.0891 - val_mae: 27.5700\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2704.3235 - mse: 2704.3235 - mae: 28.7613 - val_loss: 2507.5271 - val_mse: 2507.5276 - val_mae: 27.7210\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2754.4336 - mse: 2754.4343 - mae: 28.6931 - val_loss: 2504.1742 - val_mse: 2504.1743 - val_mae: 27.5452\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 346us/step - loss: 2751.5490 - mse: 2751.5483 - mae: 28.9261 - val_loss: 2507.6631 - val_mse: 2507.6633 - val_mae: 27.8186\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2715.6432 - mse: 2715.6438 - mae: 28.6611 - val_loss: 2506.4859 - val_mse: 2506.4858 - val_mae: 27.8669\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2735.4853 - mse: 2735.4856 - mae: 28.8042 - val_loss: 2506.4426 - val_mse: 2506.4426 - val_mae: 27.7640\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2720.4881 - mse: 2720.4883 - mae: 28.8447 - val_loss: 2508.0806 - val_mse: 2508.0803 - val_mae: 27.7001\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 265us/step - loss: 2740.9801 - mse: 2740.9805 - mae: 28.6924 - val_loss: 2504.4431 - val_mse: 2504.4431 - val_mae: 27.6050\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2749.4999 - mse: 2749.4998 - mae: 28.7783 - val_loss: 2511.6382 - val_mse: 2511.6384 - val_mae: 27.4608\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2746.4572 - mse: 2746.4570 - mae: 28.9149 - val_loss: 2515.8404 - val_mse: 2515.8408 - val_mae: 27.4778\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2774.9002 - mse: 2774.8997 - mae: 29.0608 - val_loss: 2519.1196 - val_mse: 2519.1199 - val_mae: 27.4207\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 269us/step - loss: 2719.5048 - mse: 2719.5046 - mae: 28.5539 - val_loss: 2514.6975 - val_mse: 2514.6973 - val_mae: 27.6420\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2688.3911 - mse: 2688.3909 - mae: 28.5099 - val_loss: 2511.9785 - val_mse: 2511.9788 - val_mae: 27.8929\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 271us/step - loss: 2720.2633 - mse: 2720.2646 - mae: 28.6385 - val_loss: 2516.5695 - val_mse: 2516.5696 - val_mae: 27.6680\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2731.4120 - mse: 2731.4121 - mae: 28.8386 - val_loss: 2515.0056 - val_mse: 2515.0056 - val_mae: 27.8472\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2768.6183 - mse: 2768.6184 - mae: 28.7068 - val_loss: 2514.1916 - val_mse: 2514.1912 - val_mae: 27.4911\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2764.4146 - mse: 2764.4146 - mae: 28.9684 - val_loss: 2517.4053 - val_mse: 2517.4050 - val_mae: 27.6001\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2730.3608 - mse: 2730.3606 - mae: 28.7411 - val_loss: 2515.2607 - val_mse: 2515.2610 - val_mae: 27.5913\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2687.0164 - mse: 2687.0161 - mae: 28.6030 - val_loss: 2510.5474 - val_mse: 2510.5466 - val_mae: 27.7633\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2708.8141 - mse: 2708.8140 - mae: 29.0197 - val_loss: 2509.5914 - val_mse: 2509.5916 - val_mae: 27.7645\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2731.5592 - mse: 2731.5596 - mae: 28.6879 - val_loss: 2515.3920 - val_mse: 2515.3921 - val_mae: 27.4963\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2698.9167 - mse: 2698.9165 - mae: 28.4171 - val_loss: 2514.0863 - val_mse: 2514.0867 - val_mae: 27.8974\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2678.0725 - mse: 2678.0732 - mae: 28.5544 - val_loss: 2519.1120 - val_mse: 2519.1118 - val_mae: 27.7960\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2713.2374 - mse: 2713.2378 - mae: 28.7324 - val_loss: 2514.0482 - val_mse: 2514.0488 - val_mae: 27.8168\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2694.2026 - mse: 2694.2019 - mae: 28.4512 - val_loss: 2512.4204 - val_mse: 2512.4204 - val_mae: 27.9385\n"
     ]
    }
   ],
   "source": [
    "# data set to append here (start with first feature added)\n",
    "X_ = X.loc[:,'PrevDay']\n",
    "\n",
    "# do first prediction\n",
    "X_.fillna(method = 'ffill', inplace = True)\n",
    "y.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "X_ = X_.astype('float64')\n",
    "X_ = X_.round(20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_, y, test_size = 0.15, shuffle = False)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# possible debug\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "def regressor_tunning(n_hidden = 5, \n",
    "                      n_neurons = 40, \n",
    "                      kernel_initializer = \"he_normal\",\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = n_neurons, input_dim = 1))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dropout(rate = 0.3))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(units = n_neurons))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.3))\n",
    "    model.add(Dense(units = 1, activation = 'linear'))\n",
    "    optimizer = optimizers.Adamax(lr = 0.001)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = splits)    \n",
    "regressor = regressor_tunning()\n",
    "\n",
    "# train model\n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "    y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "    regressor.fit(X_train_split, y_train_split,  \n",
    "                         shuffle = False, \n",
    "                         validation_split = 0.2,\n",
    "                         batch_size = 20, \n",
    "                         epochs = epochs)\n",
    "\n",
    "# make predictions and evaluate for all regions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS EVALUATION (1) for the whole test set\n",
    "# =============================================================================\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "# calculate metrics\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mae_error = mae(y_test, y_pred)\n",
    "\n",
    "# append to list\n",
    "rmse_gen.append(rmse_error)\n",
    "mae_gen.append(mae_error)\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS EVALUATION (2) on spike regions\n",
    "# =============================================================================\n",
    "\n",
    "# download spike indication binary set\n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "# append ot lists\n",
    "rmse_spi.append(rmse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "\n",
    "# =============================================================================\n",
    "# METRIC EVALUATION (3) on normal regions\n",
    "# =============================================================================\n",
    "\n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "# append to list\n",
    "rmse_nor.append(rmse_normal)\n",
    "mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply loop for the rest of the list of features with condition of improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13337.7836 - mse: 13337.7832 - mae: 109.9448 - val_loss: 34632.8240 - val_mse: 34632.8203 - val_mae: 132.7837\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 355us/step - loss: 13220.8962 - mse: 13220.8975 - mae: 109.4137 - val_loss: 34403.7675 - val_mse: 34403.7734 - val_mae: 131.9225\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 12880.8811 - mse: 12880.8809 - mae: 107.8659 - val_loss: 33738.0407 - val_mse: 33738.0391 - val_mae: 129.3884\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 362us/step - loss: 12009.0474 - mse: 12009.0459 - mae: 103.7222 - val_loss: 31945.5021 - val_mse: 31945.5020 - val_mae: 122.3053\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 9782.0537 - mse: 9782.0537 - mae: 92.1725 - val_loss: 27696.7352 - val_mse: 27696.7363 - val_mae: 103.6013\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 254us/step - loss: 5612.8874 - mse: 5612.8882 - mae: 64.3959 - val_loss: 20706.4244 - val_mse: 20706.4238 - val_mae: 61.5075\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 254us/step - loss: 2541.5118 - mse: 2541.5117 - mae: 36.7670 - val_loss: 17586.9021 - val_mse: 17586.9023 - val_mae: 35.0488\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 242us/step - loss: 2446.9288 - mse: 2446.9287 - mae: 36.7242 - val_loss: 17913.3597 - val_mse: 17913.3594 - val_mae: 36.1167\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 248us/step - loss: 2837.9517 - mse: 2837.9517 - mae: 38.1292 - val_loss: 18034.9988 - val_mse: 18035.0000 - val_mae: 36.8634\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 267us/step - loss: 2722.9524 - mse: 2722.9524 - mae: 38.2251 - val_loss: 17986.4042 - val_mse: 17986.4043 - val_mae: 36.5478\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2578.1178 - mse: 2578.1174 - mae: 36.5382 - val_loss: 17971.0473 - val_mse: 17971.0488 - val_mae: 36.4523\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2826.0260 - mse: 2826.0259 - mae: 35.8773 - val_loss: 17865.8510 - val_mse: 17865.8496 - val_mae: 35.8729\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 249us/step - loss: 2300.5399 - mse: 2300.5403 - mae: 34.0595 - val_loss: 17823.5390 - val_mse: 17823.5391 - val_mae: 35.6411\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 338us/step - loss: 2574.7900 - mse: 2574.7900 - mae: 35.9023 - val_loss: 17658.5638 - val_mse: 17658.5645 - val_mae: 35.1970\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 353us/step - loss: 2409.1350 - mse: 2409.1353 - mae: 34.6888 - val_loss: 17680.1811 - val_mse: 17680.1816 - val_mae: 35.2303\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2490.9761 - mse: 2490.9763 - mae: 36.3200 - val_loss: 17800.6047 - val_mse: 17800.6055 - val_mae: 35.5338\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 349us/step - loss: 2365.2287 - mse: 2365.2288 - mae: 34.0226 - val_loss: 17870.2489 - val_mse: 17870.2480 - val_mae: 35.9048\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 258us/step - loss: 2624.9698 - mse: 2624.9697 - mae: 37.2496 - val_loss: 17896.1025 - val_mse: 17896.1035 - val_mae: 36.0438\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2374.3686 - mse: 2374.3684 - mae: 34.3379 - val_loss: 17788.3729 - val_mse: 17788.3711 - val_mae: 35.4908\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 372us/step - loss: 2439.8010 - mse: 2439.8010 - mae: 35.3543 - val_loss: 17795.7155 - val_mse: 17795.7168 - val_mae: 35.5251\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2674.8113 - mse: 2674.8113 - mae: 36.1940 - val_loss: 17882.5029 - val_mse: 17882.5020 - val_mae: 35.9698\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 258us/step - loss: 2707.0921 - mse: 2707.0918 - mae: 37.1271 - val_loss: 17995.1724 - val_mse: 17995.1719 - val_mae: 36.6090\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 251us/step - loss: 2182.4472 - mse: 2182.4473 - mae: 32.2692 - val_loss: 17615.5273 - val_mse: 17615.5273 - val_mae: 35.2192\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2450.0224 - mse: 2450.0222 - mae: 34.9606 - val_loss: 17809.2403 - val_mse: 17809.2402 - val_mae: 35.5975\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2503.5574 - mse: 2503.5569 - mae: 35.0963 - val_loss: 17900.8271 - val_mse: 17900.8281 - val_mae: 36.0706\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 361us/step - loss: 2456.0863 - mse: 2456.0864 - mae: 34.1350 - val_loss: 17793.2937 - val_mse: 17793.2949 - val_mae: 35.5380\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 356us/step - loss: 2380.3859 - mse: 2380.3860 - mae: 34.7836 - val_loss: 17735.8508 - val_mse: 17735.8516 - val_mae: 35.3896\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 288us/step - loss: 2410.7253 - mse: 2410.7253 - mae: 35.2144 - val_loss: 17823.0430 - val_mse: 17823.0449 - val_mae: 35.6729\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 279us/step - loss: 2318.3347 - mse: 2318.3347 - mae: 33.4335 - val_loss: 17580.5463 - val_mse: 17580.5488 - val_mae: 35.2380\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 281us/step - loss: 2234.7155 - mse: 2234.7158 - mae: 32.7237 - val_loss: 17653.2102 - val_mse: 17653.2109 - val_mae: 35.2987\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2234.7114 - mse: 2234.7112 - mae: 34.1764 - val_loss: 17679.4483 - val_mse: 17679.4453 - val_mae: 35.3359\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2445.8227 - mse: 2445.8225 - mae: 34.5137 - val_loss: 17981.6942 - val_mse: 17981.6934 - val_mae: 36.5202\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2244.1551 - mse: 2244.1553 - mae: 32.8032 - val_loss: 17790.9580 - val_mse: 17790.9570 - val_mae: 35.5582\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 408us/step - loss: 2486.6293 - mse: 2486.6289 - mae: 34.8257 - val_loss: 17742.2989 - val_mse: 17742.2988 - val_mae: 35.4409\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2235.4549 - mse: 2235.4551 - mae: 33.8763 - val_loss: 17752.9175 - val_mse: 17752.9180 - val_mae: 35.4675\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 349us/step - loss: 1993.5751 - mse: 1993.5751 - mae: 31.6392 - val_loss: 17844.0481 - val_mse: 17844.0488 - val_mae: 35.8006\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 256us/step - loss: 2148.4453 - mse: 2148.4451 - mae: 32.2126 - val_loss: 17802.3303 - val_mse: 17802.3320 - val_mae: 35.6266\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 258us/step - loss: 2300.7494 - mse: 2300.7493 - mae: 33.3915 - val_loss: 17748.3322 - val_mse: 17748.3301 - val_mae: 35.4843\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 288us/step - loss: 2418.9728 - mse: 2418.9727 - mae: 34.1258 - val_loss: 17866.8223 - val_mse: 17866.8223 - val_mae: 35.9207\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2255.9334 - mse: 2255.9333 - mae: 32.4718 - val_loss: 17735.3043 - val_mse: 17735.3047 - val_mae: 35.4806\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2110.7750 - mse: 2110.7749 - mae: 30.3516 - val_loss: 17498.2058 - val_mse: 17498.2051 - val_mae: 35.3867\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 402us/step - loss: 2304.3507 - mse: 2304.3506 - mae: 33.4189 - val_loss: 17746.8366 - val_mse: 17746.8359 - val_mae: 35.5062\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 225us/step - loss: 2291.1354 - mse: 2291.1357 - mae: 33.8162 - val_loss: 17781.4419 - val_mse: 17781.4434 - val_mae: 35.5869\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2271.0686 - mse: 2271.0688 - mae: 33.1883 - val_loss: 17676.4734 - val_mse: 17676.4746 - val_mae: 35.4259\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 243us/step - loss: 2226.1794 - mse: 2226.1794 - mae: 32.1948 - val_loss: 17630.8453 - val_mse: 17630.8438 - val_mae: 35.4048\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 356us/step - loss: 2270.7585 - mse: 2270.7585 - mae: 33.3345 - val_loss: 17803.2690 - val_mse: 17803.2676 - val_mae: 35.6794\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 2098.6892 - mse: 2098.6892 - mae: 32.1124 - val_loss: 17723.0974 - val_mse: 17723.0977 - val_mae: 35.5121\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 349us/step - loss: 2159.9536 - mse: 2159.9536 - mae: 31.4999 - val_loss: 17708.5102 - val_mse: 17708.5117 - val_mae: 35.4978\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 375us/step - loss: 2163.0835 - mse: 2163.0835 - mae: 31.9814 - val_loss: 17915.5487 - val_mse: 17915.5488 - val_mae: 36.1763\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 364us/step - loss: 2080.5934 - mse: 2080.5933 - mae: 32.3674 - val_loss: 17698.8608 - val_mse: 17698.8594 - val_mae: 35.4988\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 245us/step - loss: 2267.5828 - mse: 2267.5828 - mae: 33.3417 - val_loss: 17828.2304 - val_mse: 17828.2305 - val_mae: 35.8007\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 335us/step - loss: 2169.8974 - mse: 2169.8972 - mae: 32.7327 - val_loss: 17726.3671 - val_mse: 17726.3672 - val_mae: 35.5555\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2360.1881 - mse: 2360.1880 - mae: 34.2701 - val_loss: 17790.6868 - val_mse: 17790.6875 - val_mae: 35.6894\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 287us/step - loss: 2022.6460 - mse: 2022.6461 - mae: 32.1602 - val_loss: 17677.9230 - val_mse: 17677.9219 - val_mae: 35.5126\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2368.4712 - mse: 2368.4712 - mae: 33.3835 - val_loss: 17714.9651 - val_mse: 17714.9668 - val_mae: 35.5612\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 290us/step - loss: 2213.4542 - mse: 2213.4543 - mae: 33.0992 - val_loss: 17762.3753 - val_mse: 17762.3730 - val_mae: 35.6366\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 289us/step - loss: 2056.6937 - mse: 2056.6936 - mae: 31.9219 - val_loss: 17610.7628 - val_mse: 17610.7617 - val_mae: 35.4977\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 280us/step - loss: 2020.4698 - mse: 2020.4698 - mae: 31.1124 - val_loss: 17699.3890 - val_mse: 17699.3906 - val_mae: 35.5625\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 227us/step - loss: 1945.9441 - mse: 1945.9442 - mae: 30.6673 - val_loss: 17707.7914 - val_mse: 17707.7891 - val_mae: 35.5786\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 280us/step - loss: 2110.0734 - mse: 2110.0732 - mae: 31.6032 - val_loss: 17737.2965 - val_mse: 17737.2969 - val_mae: 35.6231\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 1884.2347 - mse: 1884.2346 - mae: 31.4323 - val_loss: 17668.2048 - val_mse: 17668.2031 - val_mae: 35.5566\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 385us/step - loss: 2009.4646 - mse: 2009.4645 - mae: 31.2716 - val_loss: 17720.8718 - val_mse: 17720.8711 - val_mae: 35.6101\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 363us/step - loss: 2008.9028 - mse: 2008.9027 - mae: 31.0036 - val_loss: 17694.9167 - val_mse: 17694.9160 - val_mae: 35.5876\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 341us/step - loss: 2053.5771 - mse: 2053.5769 - mae: 31.2031 - val_loss: 17737.0707 - val_mse: 17737.0703 - val_mae: 35.6432\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 267us/step - loss: 2063.3089 - mse: 2063.3088 - mae: 30.8545 - val_loss: 17686.5496 - val_mse: 17686.5508 - val_mae: 35.5963\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 1943.6208 - mse: 1943.6208 - mae: 30.0280 - val_loss: 17694.5492 - val_mse: 17694.5508 - val_mae: 35.6107\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 1951.1956 - mse: 1951.1958 - mae: 29.5659 - val_loss: 17791.0460 - val_mse: 17791.0488 - val_mae: 35.7741\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 225us/step - loss: 1953.5974 - mse: 1953.5973 - mae: 29.9958 - val_loss: 17722.6268 - val_mse: 17722.6289 - val_mae: 35.6492\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 235us/step - loss: 2113.5835 - mse: 2113.5835 - mae: 31.3587 - val_loss: 17718.0879 - val_mse: 17718.0879 - val_mae: 35.6516\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 230us/step - loss: 2062.0920 - mse: 2062.0920 - mae: 31.1981 - val_loss: 17664.4901 - val_mse: 17664.4902 - val_mae: 35.6172\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 1963.0518 - mse: 1963.0516 - mae: 30.2881 - val_loss: 17751.9118 - val_mse: 17751.9121 - val_mae: 35.7058\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 387us/step - loss: 2124.5817 - mse: 2124.5815 - mae: 31.2903 - val_loss: 17624.3025 - val_mse: 17624.3027 - val_mae: 35.6355\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 1944.5811 - mse: 1944.5811 - mae: 30.8847 - val_loss: 17705.3174 - val_mse: 17705.3164 - val_mae: 35.6678\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 233us/step - loss: 2051.9277 - mse: 2051.9277 - mae: 30.5135 - val_loss: 17581.1053 - val_mse: 17581.1055 - val_mae: 35.6784\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 232us/step - loss: 2118.9666 - mse: 2118.9666 - mae: 31.4931 - val_loss: 17662.6837 - val_mse: 17662.6816 - val_mae: 35.6699\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 329us/step - loss: 1985.6041 - mse: 1985.6042 - mae: 28.6884 - val_loss: 17552.9446 - val_mse: 17552.9434 - val_mae: 35.7308\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2000.6563 - mse: 2000.6562 - mae: 31.5074 - val_loss: 17586.5272 - val_mse: 17586.5273 - val_mae: 35.7113\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 2112.9704 - mse: 2112.9702 - mae: 31.7651 - val_loss: 17792.9833 - val_mse: 17792.9844 - val_mae: 35.8577\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 1892.2067 - mse: 1892.2068 - mae: 29.7024 - val_loss: 17653.1456 - val_mse: 17653.1465 - val_mae: 35.7148\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 1947.8931 - mse: 1947.8929 - mae: 30.3453 - val_loss: 17738.3142 - val_mse: 17738.3145 - val_mae: 35.7570\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 395us/step - loss: 4322.1429 - mse: 4322.1431 - mae: 35.3400 - val_loss: 2093.5590 - val_mse: 2093.5591 - val_mae: 31.3652\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 228us/step - loss: 4314.4586 - mse: 4314.4585 - mae: 35.4081 - val_loss: 2181.8081 - val_mse: 2181.8083 - val_mae: 31.5678\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 239us/step - loss: 4226.3836 - mse: 4226.3843 - mae: 35.4888 - val_loss: 2330.2711 - val_mse: 2330.2710 - val_mae: 32.0228\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 239us/step - loss: 4340.6253 - mse: 4340.6260 - mae: 35.5387 - val_loss: 2193.6306 - val_mse: 2193.6309 - val_mae: 31.5931\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4224.2293 - mse: 4224.2290 - mae: 34.8304 - val_loss: 2221.1233 - val_mse: 2221.1233 - val_mae: 31.6633\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4370.1467 - mse: 4370.1465 - mae: 35.4282 - val_loss: 2295.9836 - val_mse: 2295.9839 - val_mae: 31.8797\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 370us/step - loss: 4411.3283 - mse: 4411.3286 - mae: 35.4712 - val_loss: 2255.6422 - val_mse: 2255.6423 - val_mae: 31.7485\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 251us/step - loss: 4323.8317 - mse: 4323.8320 - mae: 35.0132 - val_loss: 2271.2609 - val_mse: 2271.2607 - val_mae: 31.7857\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 232us/step - loss: 4121.7312 - mse: 4121.7314 - mae: 34.6597 - val_loss: 2228.5177 - val_mse: 2228.5178 - val_mae: 31.6701\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4392.7208 - mse: 4392.7212 - mae: 36.0995 - val_loss: 2262.4049 - val_mse: 2262.4048 - val_mae: 31.7552\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4210.7075 - mse: 4210.7075 - mae: 34.9281 - val_loss: 2237.1814 - val_mse: 2237.1814 - val_mae: 31.6891\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4188.2736 - mse: 4188.2739 - mae: 34.7622 - val_loss: 2288.6700 - val_mse: 2288.6702 - val_mae: 31.8337\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4140.9673 - mse: 4140.9673 - mae: 34.9590 - val_loss: 2227.2122 - val_mse: 2227.2122 - val_mae: 31.6597\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 260us/step - loss: 4170.6055 - mse: 4170.6060 - mae: 33.6267 - val_loss: 2241.5716 - val_mse: 2241.5718 - val_mae: 31.6957\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4177.1628 - mse: 4177.1626 - mae: 34.6566 - val_loss: 2240.1386 - val_mse: 2240.1384 - val_mae: 31.6914\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4229.3798 - mse: 4229.3799 - mae: 34.4453 - val_loss: 2261.1867 - val_mse: 2261.1868 - val_mae: 31.7463\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4411.7438 - mse: 4411.7437 - mae: 36.1558 - val_loss: 2248.4324 - val_mse: 2248.4321 - val_mae: 31.7092\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4256.9881 - mse: 4256.9883 - mae: 35.4455 - val_loss: 2297.1044 - val_mse: 2297.1045 - val_mae: 31.8540\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4341.5738 - mse: 4341.5742 - mae: 35.4479 - val_loss: 2245.2953 - val_mse: 2245.2954 - val_mae: 31.6985\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 4205.1268 - mse: 4205.1270 - mae: 34.4976 - val_loss: 2309.7684 - val_mse: 2309.7683 - val_mae: 31.8950\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 357us/step - loss: 4243.0445 - mse: 4243.0444 - mae: 34.9434 - val_loss: 2258.1305 - val_mse: 2258.1306 - val_mae: 31.7327\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 287us/step - loss: 4077.9978 - mse: 4077.9976 - mae: 33.4181 - val_loss: 2205.3677 - val_mse: 2205.3677 - val_mae: 31.5923\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4211.9886 - mse: 4211.9883 - mae: 34.7456 - val_loss: 2208.1411 - val_mse: 2208.1414 - val_mae: 31.5984\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4189.4821 - mse: 4189.4819 - mae: 34.4151 - val_loss: 2242.9278 - val_mse: 2242.9277 - val_mae: 31.6885\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 259us/step - loss: 4192.7025 - mse: 4192.7031 - mae: 34.4903 - val_loss: 2243.8517 - val_mse: 2243.8516 - val_mae: 31.6910\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4268.7983 - mse: 4268.7974 - mae: 34.2511 - val_loss: 2249.9949 - val_mse: 2249.9946 - val_mae: 31.7064\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4321.0317 - mse: 4321.0317 - mae: 35.0091 - val_loss: 2309.0847 - val_mse: 2309.0847 - val_mae: 31.8835\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 253us/step - loss: 4100.4814 - mse: 4100.4810 - mae: 33.4563 - val_loss: 2179.7153 - val_mse: 2179.7153 - val_mae: 31.5293\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4163.6849 - mse: 4163.6855 - mae: 34.6128 - val_loss: 2241.1518 - val_mse: 2241.1519 - val_mae: 31.6804\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4189.2742 - mse: 4189.2739 - mae: 33.7721 - val_loss: 2252.7892 - val_mse: 2252.7893 - val_mae: 31.7102\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 302us/step - loss: 4186.6583 - mse: 4186.6577 - mae: 35.4022 - val_loss: 2283.3655 - val_mse: 2283.3652 - val_mae: 31.7968\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 3961.1682 - mse: 3961.1677 - mae: 33.2487 - val_loss: 2227.3426 - val_mse: 2227.3428 - val_mae: 31.6431\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4263.4690 - mse: 4263.4692 - mae: 34.2227 - val_loss: 2236.6915 - val_mse: 2236.6914 - val_mae: 31.6679\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 328us/step - loss: 4085.9972 - mse: 4085.9966 - mae: 33.9172 - val_loss: 2281.2952 - val_mse: 2281.2952 - val_mae: 31.7862\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4174.1390 - mse: 4174.1396 - mae: 34.2385 - val_loss: 2244.1947 - val_mse: 2244.1948 - val_mae: 31.6835\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 271us/step - loss: 4268.9670 - mse: 4268.9663 - mae: 35.3781 - val_loss: 2307.8885 - val_mse: 2307.8884 - val_mae: 31.8668\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 302us/step - loss: 4162.3927 - mse: 4162.3931 - mae: 33.4295 - val_loss: 2231.6601 - val_mse: 2231.6599 - val_mae: 31.6494\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4259.9392 - mse: 4259.9395 - mae: 35.2296 - val_loss: 2255.3912 - val_mse: 2255.3914 - val_mae: 31.7127\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 279us/step - loss: 4148.2608 - mse: 4148.2612 - mae: 34.7541 - val_loss: 2242.4111 - val_mse: 2242.4109 - val_mae: 31.6766\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4232.1469 - mse: 4232.1465 - mae: 34.6067 - val_loss: 2265.6525 - val_mse: 2265.6523 - val_mae: 31.7380\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4233.7838 - mse: 4233.7832 - mae: 34.2398 - val_loss: 2303.8245 - val_mse: 2303.8245 - val_mae: 31.8453\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4222.4825 - mse: 4222.4824 - mae: 34.0677 - val_loss: 2264.0517 - val_mse: 2264.0518 - val_mae: 31.7332\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 374us/step - loss: 4232.9755 - mse: 4232.9756 - mae: 35.6667 - val_loss: 2316.0645 - val_mse: 2316.0647 - val_mae: 31.8821\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4254.6340 - mse: 4254.6343 - mae: 33.4560 - val_loss: 2273.5842 - val_mse: 2273.5840 - val_mae: 31.7578\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4250.5023 - mse: 4250.5015 - mae: 34.3989 - val_loss: 2272.2578 - val_mse: 2272.2578 - val_mae: 31.7551\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4109.9666 - mse: 4109.9663 - mae: 33.9612 - val_loss: 2259.8206 - val_mse: 2259.8208 - val_mae: 31.7197\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4277.3654 - mse: 4277.3657 - mae: 34.1580 - val_loss: 2266.9026 - val_mse: 2266.9026 - val_mae: 31.7404\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 302us/step - loss: 4160.5152 - mse: 4160.5156 - mae: 33.3121 - val_loss: 2195.7447 - val_mse: 2195.7446 - val_mae: 31.5506\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 228us/step - loss: 4035.9881 - mse: 4035.9883 - mae: 33.6552 - val_loss: 2236.0732 - val_mse: 2236.0732 - val_mae: 31.6531\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 279us/step - loss: 4086.1783 - mse: 4086.1782 - mae: 32.9330 - val_loss: 2211.3426 - val_mse: 2211.3428 - val_mae: 31.5899\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 222us/step - loss: 4131.8387 - mse: 4131.8384 - mae: 33.8399 - val_loss: 2289.8034 - val_mse: 2289.8032 - val_mae: 31.7999\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 227us/step - loss: 4138.1431 - mse: 4138.1431 - mae: 34.1657 - val_loss: 2261.6273 - val_mse: 2261.6272 - val_mae: 31.7217\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4231.3443 - mse: 4231.3447 - mae: 34.7955 - val_loss: 2312.4411 - val_mse: 2312.4409 - val_mae: 31.8657\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 297us/step - loss: 4141.7705 - mse: 4141.7710 - mae: 33.5282 - val_loss: 2225.2141 - val_mse: 2225.2141 - val_mae: 31.6234\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4164.7907 - mse: 4164.7910 - mae: 34.9261 - val_loss: 2327.0563 - val_mse: 2327.0564 - val_mae: 31.9091\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 3802.4415 - mse: 3802.4414 - mae: 33.6042 - val_loss: 2257.7822 - val_mse: 2257.7822 - val_mae: 31.7099\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4014.1585 - mse: 4014.1582 - mae: 33.3933 - val_loss: 2245.9039 - val_mse: 2245.9041 - val_mae: 31.6754\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4033.6861 - mse: 4033.6858 - mae: 33.2588 - val_loss: 2224.8208 - val_mse: 2224.8208 - val_mae: 31.6197\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4182.7147 - mse: 4182.7148 - mae: 33.8357 - val_loss: 2260.5360 - val_mse: 2260.5356 - val_mae: 31.7174\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 4058.5770 - mse: 4058.5774 - mae: 33.1545 - val_loss: 2263.8119 - val_mse: 2263.8118 - val_mae: 31.7248\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 263us/step - loss: 4128.8777 - mse: 4128.8779 - mae: 33.1023 - val_loss: 2271.0372 - val_mse: 2271.0371 - val_mae: 31.7449\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 287us/step - loss: 4059.0905 - mse: 4059.0908 - mae: 32.7411 - val_loss: 2273.5762 - val_mse: 2273.5762 - val_mae: 31.7518\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 247us/step - loss: 4074.5862 - mse: 4074.5862 - mae: 32.9093 - val_loss: 2284.7845 - val_mse: 2284.7844 - val_mae: 31.7831\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 379us/step - loss: 4117.9963 - mse: 4117.9966 - mae: 34.1462 - val_loss: 2284.1916 - val_mse: 2284.1914 - val_mae: 31.7828\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 345us/step - loss: 3911.5440 - mse: 3911.5439 - mae: 32.9667 - val_loss: 2270.6155 - val_mse: 2270.6157 - val_mae: 31.7430\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4151.3584 - mse: 4151.3584 - mae: 33.6774 - val_loss: 2298.4189 - val_mse: 2298.4189 - val_mae: 31.8207\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 338us/step - loss: 4348.6171 - mse: 4348.6172 - mae: 34.7375 - val_loss: 2389.8649 - val_mse: 2389.8650 - val_mae: 32.0937\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 3953.9951 - mse: 3953.9951 - mae: 33.0888 - val_loss: 2219.3510 - val_mse: 2219.3511 - val_mae: 31.6040\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4004.9280 - mse: 4004.9280 - mae: 33.6141 - val_loss: 2269.0706 - val_mse: 2269.0706 - val_mae: 31.7390\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4254.6851 - mse: 4254.6860 - mae: 34.2455 - val_loss: 2289.9803 - val_mse: 2289.9802 - val_mae: 31.7955\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 3928.5395 - mse: 3928.5393 - mae: 32.7821 - val_loss: 2312.6920 - val_mse: 2312.6919 - val_mae: 31.8597\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4054.5513 - mse: 4054.5515 - mae: 34.2187 - val_loss: 2283.4183 - val_mse: 2283.4182 - val_mae: 31.7759\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 3865.9601 - mse: 3865.9602 - mae: 32.4551 - val_loss: 2240.1310 - val_mse: 2240.1311 - val_mae: 31.6537\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4199.6747 - mse: 4199.6753 - mae: 34.4349 - val_loss: 2254.8020 - val_mse: 2254.8020 - val_mae: 31.6926\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4048.9475 - mse: 4048.9480 - mae: 33.2089 - val_loss: 2306.9469 - val_mse: 2306.9468 - val_mae: 31.8399\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4067.0337 - mse: 4067.0339 - mae: 33.9166 - val_loss: 2301.4051 - val_mse: 2301.4053 - val_mae: 31.8247\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 4134.2924 - mse: 4134.2920 - mae: 33.6038 - val_loss: 2276.9227 - val_mse: 2276.9229 - val_mae: 31.7586\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 278us/step - loss: 4077.7677 - mse: 4077.7678 - mae: 34.3369 - val_loss: 2308.6937 - val_mse: 2308.6938 - val_mae: 31.8463\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 362us/step - loss: 4182.4937 - mse: 4182.4937 - mae: 34.2036 - val_loss: 2298.2011 - val_mse: 2298.2009 - val_mae: 31.8156\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 381us/step - loss: 4022.5317 - mse: 4022.5310 - mae: 32.6380 - val_loss: 2245.7474 - val_mse: 2245.7476 - val_mae: 31.6692\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3488.1136 - mse: 3488.1133 - mae: 33.4560 - val_loss: 1429.7257 - val_mse: 1429.7257 - val_mae: 25.3973\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3376.8603 - mse: 3376.8606 - mae: 33.3668 - val_loss: 1431.3429 - val_mse: 1431.3428 - val_mae: 24.8590\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 276us/step - loss: 3507.4756 - mse: 3507.4758 - mae: 33.1320 - val_loss: 1430.6174 - val_mse: 1430.6174 - val_mae: 25.2170\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3435.0293 - mse: 3435.0298 - mae: 33.8340 - val_loss: 1432.5536 - val_mse: 1432.5536 - val_mae: 24.8244\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 234us/step - loss: 3430.4409 - mse: 3430.4404 - mae: 33.0102 - val_loss: 1431.7402 - val_mse: 1431.7401 - val_mae: 25.1642\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3397.3821 - mse: 3397.3816 - mae: 32.3187 - val_loss: 1434.4744 - val_mse: 1434.4742 - val_mae: 25.8982\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3456.2413 - mse: 3456.2417 - mae: 33.9468 - val_loss: 1433.5269 - val_mse: 1433.5269 - val_mae: 24.9099\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3423.2199 - mse: 3423.2200 - mae: 33.1427 - val_loss: 1433.6820 - val_mse: 1433.6820 - val_mae: 25.6234\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3507.4690 - mse: 3507.4690 - mae: 33.7048 - val_loss: 1436.0674 - val_mse: 1436.0674 - val_mae: 24.7247\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3359.1217 - mse: 3359.1216 - mae: 32.59 - 0s 242us/step - loss: 3322.4219 - mse: 3322.4216 - mae: 32.5528 - val_loss: 1433.7198 - val_mse: 1433.7200 - val_mae: 25.2311\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3279.8317 - mse: 3279.8318 - mae: 32.5969 - val_loss: 1435.3822 - val_mse: 1435.3822 - val_mae: 25.7948\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3427.1623 - mse: 3427.1628 - mae: 33.0244 - val_loss: 1435.3760 - val_mse: 1435.3760 - val_mae: 25.7361\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3457.5650 - mse: 3457.5642 - mae: 33.4295 - val_loss: 1436.4925 - val_mse: 1436.4924 - val_mae: 24.8216\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 223us/step - loss: 3385.7722 - mse: 3385.7720 - mae: 32.9543 - val_loss: 1435.3393 - val_mse: 1435.3392 - val_mae: 25.5566\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 262us/step - loss: 3403.8819 - mse: 3403.8813 - mae: 32.6071 - val_loss: 1436.4255 - val_mse: 1436.4254 - val_mae: 25.7136\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 254us/step - loss: 3349.8860 - mse: 3349.8855 - mae: 33.0863 - val_loss: 1438.2438 - val_mse: 1438.2437 - val_mae: 24.8052\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 273us/step - loss: 3341.1750 - mse: 3341.1755 - mae: 32.3243 - val_loss: 1437.1433 - val_mse: 1437.1432 - val_mae: 25.0074\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3375.9419 - mse: 3375.9421 - mae: 33.1024 - val_loss: 1438.8432 - val_mse: 1438.8433 - val_mae: 25.9073\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3306.2083 - mse: 3306.2087 - mae: 32.6519 - val_loss: 1437.4474 - val_mse: 1437.4474 - val_mae: 25.1321\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3354.9030 - mse: 3354.9026 - mae: 32.5663 - val_loss: 1437.7473 - val_mse: 1437.7473 - val_mae: 25.5082\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 327us/step - loss: 3433.2350 - mse: 3433.2351 - mae: 32.8490 - val_loss: 1438.5250 - val_mse: 1438.5250 - val_mae: 25.0786\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 279us/step - loss: 3425.0501 - mse: 3425.0513 - mae: 33.1022 - val_loss: 1438.5664 - val_mse: 1438.5665 - val_mae: 25.2701\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3341.9459 - mse: 3341.9460 - mae: 32.7712 - val_loss: 1439.0965 - val_mse: 1439.0966 - val_mae: 25.2115\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3430.3692 - mse: 3430.3696 - mae: 32.7413 - val_loss: 1439.9725 - val_mse: 1439.9725 - val_mae: 25.0674\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3440.4168 - mse: 3440.4167 - mae: 32.7558 - val_loss: 1439.7919 - val_mse: 1439.7917 - val_mae: 25.1749\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3341.2458 - mse: 3341.2466 - mae: 31.9642 - val_loss: 1440.5054 - val_mse: 1440.5051 - val_mae: 25.6096\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3335.1049 - mse: 3335.1050 - mae: 32.4826 - val_loss: 1441.0219 - val_mse: 1441.0217 - val_mae: 25.2998\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3470.7010 - mse: 3470.7007 - mae: 33.3097 - val_loss: 1442.1409 - val_mse: 1442.1409 - val_mae: 25.6328\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3321.2955 - mse: 3321.2949 - mae: 32.5790 - val_loss: 1442.8827 - val_mse: 1442.8827 - val_mae: 25.7140\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3319.9262 - mse: 3319.9260 - mae: 31.8459 - val_loss: 1442.8703 - val_mse: 1442.8701 - val_mae: 25.3648\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3343.9472 - mse: 3343.9475 - mae: 31.8196 - val_loss: 1443.1382 - val_mse: 1443.1381 - val_mae: 25.4810\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3445.2198 - mse: 3445.2200 - mae: 32.6244 - val_loss: 1447.1783 - val_mse: 1447.1783 - val_mae: 24.7752\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3247.9238 - mse: 3247.9233 - mae: 31.9182 - val_loss: 1444.9566 - val_mse: 1444.9565 - val_mae: 25.7497\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3403.4351 - mse: 3403.4355 - mae: 32.4103 - val_loss: 1444.8574 - val_mse: 1444.8574 - val_mae: 25.3810\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3364.1031 - mse: 3364.1023 - mae: 32.4291 - val_loss: 1445.2559 - val_mse: 1445.2559 - val_mae: 25.5066\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 215us/step - loss: 3313.9260 - mse: 3313.9265 - mae: 32.2723 - val_loss: 1446.0229 - val_mse: 1446.0231 - val_mae: 25.7436\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 327us/step - loss: 3405.1319 - mse: 3405.1331 - mae: 33.0414 - val_loss: 1445.8128 - val_mse: 1445.8129 - val_mae: 25.4702\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 263us/step - loss: 3394.6248 - mse: 3394.6250 - mae: 32.8065 - val_loss: 1446.5178 - val_mse: 1446.5177 - val_mae: 25.1438\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 273us/step - loss: 3302.1499 - mse: 3302.1501 - mae: 31.3295 - val_loss: 1446.3408 - val_mse: 1446.3407 - val_mae: 25.2718\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3350.8089 - mse: 3350.8086 - mae: 32.0025 - val_loss: 1446.6113 - val_mse: 1446.6113 - val_mae: 25.5306\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3415.0390 - mse: 3415.0398 - mae: 32.9794 - val_loss: 1447.0007 - val_mse: 1447.0009 - val_mae: 25.4590\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3307.2045 - mse: 3307.2043 - mae: 32.0933 - val_loss: 1447.6033 - val_mse: 1447.6033 - val_mae: 25.6747\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3434.8307 - mse: 3434.8308 - mae: 33.5403 - val_loss: 1447.4438 - val_mse: 1447.4438 - val_mae: 25.4215\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3323.6992 - mse: 3323.6992 - mae: 32.0461 - val_loss: 1448.6328 - val_mse: 1448.6327 - val_mae: 25.8629\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 261us/step - loss: 3353.1095 - mse: 3353.1096 - mae: 32.7690 - val_loss: 1452.1830 - val_mse: 1452.1831 - val_mae: 26.3062\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3264.9225 - mse: 3264.9221 - mae: 31.9297 - val_loss: 1449.8730 - val_mse: 1449.8730 - val_mae: 25.9506\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3319.5385 - mse: 3319.5386 - mae: 31.8241 - val_loss: 1452.0572 - val_mse: 1452.0571 - val_mae: 26.2396\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 230us/step - loss: 3298.2621 - mse: 3298.2615 - mae: 32.3153 - val_loss: 1450.0828 - val_mse: 1450.0825 - val_mae: 25.8985\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3285.5887 - mse: 3285.5879 - mae: 32.1190 - val_loss: 1452.1321 - val_mse: 1452.1322 - val_mae: 26.1662\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3321.9761 - mse: 3321.9758 - mae: 32.6539 - val_loss: 1450.0496 - val_mse: 1450.0497 - val_mae: 25.3274\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 236us/step - loss: 3318.8973 - mse: 3318.8977 - mae: 32.3805 - val_loss: 1450.6637 - val_mse: 1450.6636 - val_mae: 25.5885\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 241us/step - loss: 3369.4120 - mse: 3369.4121 - mae: 32.9704 - val_loss: 1452.7559 - val_mse: 1452.7559 - val_mae: 26.0660\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 277us/step - loss: 3272.8287 - mse: 3272.8298 - mae: 31.3934 - val_loss: 1451.4447 - val_mse: 1451.4445 - val_mae: 25.2303\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 213us/step - loss: 3253.1447 - mse: 3253.1445 - mae: 31.4963 - val_loss: 1452.4685 - val_mse: 1452.4686 - val_mae: 25.9527\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 212us/step - loss: 3345.9503 - mse: 3345.9509 - mae: 32.4574 - val_loss: 1451.8982 - val_mse: 1451.8981 - val_mae: 25.2628\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 348us/step - loss: 3353.3289 - mse: 3353.3298 - mae: 32.5154 - val_loss: 1451.7967 - val_mse: 1451.7968 - val_mae: 25.5772\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 255us/step - loss: 3375.9612 - mse: 3375.9612 - mae: 31.8056 - val_loss: 1454.6728 - val_mse: 1454.6729 - val_mae: 26.1400\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3303.8209 - mse: 3303.8208 - mae: 32.4403 - val_loss: 1454.6268 - val_mse: 1454.6267 - val_mae: 26.0733\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3157.2923 - mse: 3157.2920 - mae: 31.3692 - val_loss: 1453.5937 - val_mse: 1453.5936 - val_mae: 25.8751\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3288.2639 - mse: 3288.2642 - mae: 31.5923 - val_loss: 1454.8208 - val_mse: 1454.8208 - val_mae: 26.0723\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3282.0199 - mse: 3282.0193 - mae: 32.3334 - val_loss: 1453.7360 - val_mse: 1453.7360 - val_mae: 25.3784\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3319.4211 - mse: 3319.4214 - mae: 32.5792 - val_loss: 1453.8818 - val_mse: 1453.8818 - val_mae: 25.6005\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3160.4215 - mse: 3160.4214 - mae: 30.8006 - val_loss: 1458.7041 - val_mse: 1458.7041 - val_mae: 26.4603\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 278us/step - loss: 3369.6569 - mse: 3369.6567 - mae: 32.1441 - val_loss: 1454.4247 - val_mse: 1454.4248 - val_mae: 25.4712\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3246.8125 - mse: 3246.8130 - mae: 31.7695 - val_loss: 1455.8285 - val_mse: 1455.8287 - val_mae: 25.9609\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3350.2451 - mse: 3350.2451 - mae: 32.5445 - val_loss: 1456.6478 - val_mse: 1456.6477 - val_mae: 26.0897\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3394.3002 - mse: 3394.3000 - mae: 32.0446 - val_loss: 1455.6267 - val_mse: 1455.6267 - val_mae: 25.2815\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 294us/step - loss: 3392.7883 - mse: 3392.7891 - mae: 32.4386 - val_loss: 1455.2203 - val_mse: 1455.2202 - val_mae: 25.6863\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3281.3362 - mse: 3281.3357 - mae: 31.5859 - val_loss: 1455.4446 - val_mse: 1455.4447 - val_mae: 25.6647\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3447.3105 - mse: 3447.3110 - mae: 33.0782 - val_loss: 1455.4327 - val_mse: 1455.4326 - val_mae: 25.4190\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3263.3358 - mse: 3263.3354 - mae: 31.9247 - val_loss: 1456.6331 - val_mse: 1456.6331 - val_mae: 25.9687\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3417.3326 - mse: 3417.3325 - mae: 32.3196 - val_loss: 1456.0216 - val_mse: 1456.0217 - val_mae: 25.5978\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3306.3878 - mse: 3306.3877 - mae: 32.0879 - val_loss: 1456.0763 - val_mse: 1456.0762 - val_mae: 25.7128\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 277us/step - loss: 3337.9006 - mse: 3337.8999 - mae: 31.4333 - val_loss: 1455.8933 - val_mse: 1455.8932 - val_mae: 25.5433\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3215.5295 - mse: 3215.5291 - mae: 31.4667 - val_loss: 1455.8756 - val_mse: 1455.8756 - val_mae: 25.7331\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 269us/step - loss: 3270.5847 - mse: 3270.5847 - mae: 31.8812 - val_loss: 1458.1758 - val_mse: 1458.1758 - val_mae: 26.2081\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 366us/step - loss: 3240.1669 - mse: 3240.1675 - mae: 31.6382 - val_loss: 1455.6863 - val_mse: 1455.6862 - val_mae: 25.6141\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3220.9966 - mse: 3220.9956 - mae: 31.5119 - val_loss: 1455.6507 - val_mse: 1455.6508 - val_mae: 25.5866\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 327us/step - loss: 3232.6227 - mse: 3232.6233 - mae: 31.6416 - val_loss: 1455.8693 - val_mse: 1455.8695 - val_mae: 25.6179\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3197.1905 - mse: 3197.1907 - mae: 31.0289 - val_loss: 1460.2990 - val_mse: 1460.2988 - val_mae: 26.4475\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2948.4116 - mse: 2948.4114 - mae: 31.4302 - val_loss: 1063.3777 - val_mse: 1063.3778 - val_mae: 23.7943\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 333us/step - loss: 2938.5826 - mse: 2938.5828 - mae: 31.2185 - val_loss: 1061.7451 - val_mse: 1061.7450 - val_mae: 23.9567\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2939.2835 - mse: 2939.2837 - mae: 31.8188 - val_loss: 1062.7579 - val_mse: 1062.7579 - val_mae: 23.7125\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 3026.4722 - mse: 3026.4722 - mae: 31.6644 - val_loss: 1064.3601 - val_mse: 1064.3601 - val_mae: 23.5474\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2887.4431 - mse: 2887.4436 - mae: 31.1152 - val_loss: 1061.8011 - val_mse: 1061.8010 - val_mae: 23.7447\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2985.3553 - mse: 2985.3550 - mae: 31.8604 - val_loss: 1061.1788 - val_mse: 1061.1788 - val_mae: 23.8032\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2926.2761 - mse: 2926.2754 - mae: 31.5810 - val_loss: 1060.4047 - val_mse: 1060.4045 - val_mae: 23.9066\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 3046.9409 - mse: 3046.9404 - mae: 31.9252 - val_loss: 1067.4465 - val_mse: 1067.4464 - val_mae: 23.3410\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 258us/step - loss: 2913.6531 - mse: 2913.6533 - mae: 31.0850 - val_loss: 1060.4027 - val_mse: 1060.4026 - val_mae: 23.8329\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 3058.8490 - mse: 3058.8486 - mae: 31.5154 - val_loss: 1060.3486 - val_mse: 1060.3488 - val_mae: 23.8362\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2835.1784 - mse: 2835.1780 - mae: 30.4840 - val_loss: 1058.6908 - val_mse: 1058.6906 - val_mae: 24.1017\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 341us/step - loss: 2952.4276 - mse: 2952.4280 - mae: 30.9638 - val_loss: 1063.2379 - val_mse: 1063.2378 - val_mae: 23.5152\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 342us/step - loss: 2925.1891 - mse: 2925.1890 - mae: 31.0613 - val_loss: 1064.3076 - val_mse: 1064.3075 - val_mae: 23.4676\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2953.8977 - mse: 2953.8984 - mae: 31.4608 - val_loss: 1061.4958 - val_mse: 1061.4958 - val_mae: 23.6665\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2959.3028 - mse: 2959.3025 - mae: 31.4718 - val_loss: 1060.5267 - val_mse: 1060.5267 - val_mae: 23.7308\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2947.5525 - mse: 2947.5525 - mae: 31.6562 - val_loss: 1059.2443 - val_mse: 1059.2441 - val_mae: 23.9563\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 270us/step - loss: 2902.2696 - mse: 2902.2698 - mae: 30.9629 - val_loss: 1065.4706 - val_mse: 1065.4707 - val_mae: 23.3821\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2912.0923 - mse: 2912.0930 - mae: 31.0066 - val_loss: 1061.7835 - val_mse: 1061.7836 - val_mae: 23.5889\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2912.7351 - mse: 2912.7354 - mae: 30.8827 - val_loss: 1060.6306 - val_mse: 1060.6306 - val_mae: 23.7005\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 338us/step - loss: 2924.5007 - mse: 2924.5015 - mae: 31.2565 - val_loss: 1058.5422 - val_mse: 1058.5422 - val_mae: 23.9790\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2844.0516 - mse: 2844.0510 - mae: 31.2655 - val_loss: 1057.2655 - val_mse: 1057.2654 - val_mae: 24.3283\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2930.1024 - mse: 2930.1025 - mae: 31.1663 - val_loss: 1063.6667 - val_mse: 1063.6666 - val_mae: 23.4115\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2831.8102 - mse: 2831.8091 - mae: 30.4993 - val_loss: 1058.2205 - val_mse: 1058.2207 - val_mae: 23.8668\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 339us/step - loss: 2972.3797 - mse: 2972.3804 - mae: 31.9986 - val_loss: 1057.7541 - val_mse: 1057.7543 - val_mae: 23.8622\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2912.5508 - mse: 2912.5508 - mae: 31.3714 - val_loss: 1056.8995 - val_mse: 1056.8995 - val_mae: 24.0647\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 266us/step - loss: 2904.5159 - mse: 2904.5151 - mae: 30.7690 - val_loss: 1057.9215 - val_mse: 1057.9215 - val_mae: 23.8495\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2904.1335 - mse: 2904.1333 - mae: 30.6608 - val_loss: 1056.5354 - val_mse: 1056.5353 - val_mae: 24.0993\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2804.6659 - mse: 2804.6658 - mae: 30.9051 - val_loss: 1058.0662 - val_mse: 1058.0663 - val_mae: 23.8120\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2904.5950 - mse: 2904.5947 - mae: 30.6880 - val_loss: 1056.7184 - val_mse: 1056.7184 - val_mae: 24.0862\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2909.2428 - mse: 2909.2427 - mae: 31.4596 - val_loss: 1060.2962 - val_mse: 1060.2963 - val_mae: 23.6015\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 255us/step - loss: 2885.4269 - mse: 2885.4272 - mae: 30.9280 - val_loss: 1056.8777 - val_mse: 1056.8779 - val_mae: 23.9076\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 0s 245us/step - loss: 2943.1329 - mse: 2943.1323 - mae: 31.1894 - val_loss: 1059.0192 - val_mse: 1059.0192 - val_mae: 23.6454\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2898.9891 - mse: 2898.9888 - mae: 30.8687 - val_loss: 1056.3133 - val_mse: 1056.3132 - val_mae: 23.9711\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2953.9689 - mse: 2953.9690 - mae: 31.1859 - val_loss: 1055.9190 - val_mse: 1055.9191 - val_mae: 24.3552\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 257us/step - loss: 2888.6345 - mse: 2888.6350 - mae: 30.5554 - val_loss: 1058.4966 - val_mse: 1058.4965 - val_mae: 23.6045\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 253us/step - loss: 2849.8028 - mse: 2849.8030 - mae: 30.6533 - val_loss: 1055.5495 - val_mse: 1055.5496 - val_mae: 23.8251\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2927.9607 - mse: 2927.9604 - mae: 30.9786 - val_loss: 1059.4596 - val_mse: 1059.4597 - val_mae: 23.4310\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 0s 248us/step - loss: 2958.0862 - mse: 2958.0872 - mae: 31.3425 - val_loss: 1055.8273 - val_mse: 1055.8273 - val_mae: 23.6930\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2884.9219 - mse: 2884.9219 - mae: 30.9192 - val_loss: 1054.2080 - val_mse: 1054.2080 - val_mae: 23.8862\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2890.9684 - mse: 2890.9695 - mae: 30.5932 - val_loss: 1053.6512 - val_mse: 1053.6511 - val_mae: 24.0592\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2971.7055 - mse: 2971.7048 - mae: 31.3077 - val_loss: 1053.6639 - val_mse: 1053.6639 - val_mae: 24.3658\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2850.4239 - mse: 2850.4238 - mae: 31.0692 - val_loss: 1054.2183 - val_mse: 1054.2181 - val_mae: 23.8074\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2911.8970 - mse: 2911.8977 - mae: 30.9606 - val_loss: 1053.9847 - val_mse: 1053.9846 - val_mae: 23.7377\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2857.3857 - mse: 2857.3865 - mae: 30.5156 - val_loss: 1051.9671 - val_mse: 1051.9672 - val_mae: 23.8983\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 256us/step - loss: 2835.0618 - mse: 2835.0618 - mae: 30.8554 - val_loss: 1053.4627 - val_mse: 1053.4626 - val_mae: 23.6151\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2857.8520 - mse: 2857.8516 - mae: 30.5152 - val_loss: 1052.0585 - val_mse: 1052.0586 - val_mae: 23.7218\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 332us/step - loss: 2890.1284 - mse: 2890.1287 - mae: 31.3415 - val_loss: 1050.7576 - val_mse: 1050.7576 - val_mae: 23.9113\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2917.7176 - mse: 2917.7183 - mae: 30.8018 - val_loss: 1051.5270 - val_mse: 1051.5271 - val_mae: 23.6177\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2870.7497 - mse: 2870.7500 - mae: 30.9984 - val_loss: 1051.5199 - val_mse: 1051.5199 - val_mae: 23.5685\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2896.3240 - mse: 2896.3242 - mae: 30.7544 - val_loss: 1055.7076 - val_mse: 1055.7075 - val_mae: 23.2371\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2849.3845 - mse: 2849.3843 - mae: 30.7101 - val_loss: 1049.4701 - val_mse: 1049.4701 - val_mae: 23.6381\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2798.0862 - mse: 2798.0854 - mae: 30.5296 - val_loss: 1047.4497 - val_mse: 1047.4496 - val_mae: 23.9551\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2829.6530 - mse: 2829.6536 - mae: 30.9197 - val_loss: 1047.6729 - val_mse: 1047.6730 - val_mae: 24.0293\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2843.5954 - mse: 2843.5955 - mae: 30.4716 - val_loss: 1046.9427 - val_mse: 1046.9427 - val_mae: 24.0286\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2882.7827 - mse: 2882.7834 - mae: 30.7306 - val_loss: 1048.8260 - val_mse: 1048.8259 - val_mae: 23.6479\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2822.6731 - mse: 2822.6724 - mae: 30.3816 - val_loss: 1046.7468 - val_mse: 1046.7468 - val_mae: 23.9701\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2800.2005 - mse: 2800.2004 - mae: 30.4831 - val_loss: 1046.4220 - val_mse: 1046.4220 - val_mae: 24.0939\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2819.2998 - mse: 2819.2996 - mae: 30.5159 - val_loss: 1047.7518 - val_mse: 1047.7517 - val_mae: 23.6180\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2899.6749 - mse: 2899.6755 - mae: 30.8364 - val_loss: 1048.7249 - val_mse: 1048.7251 - val_mae: 23.5318\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2917.7790 - mse: 2917.7791 - mae: 30.9449 - val_loss: 1046.4745 - val_mse: 1046.4745 - val_mae: 23.7206\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2884.7125 - mse: 2884.7117 - mae: 30.8342 - val_loss: 1046.7987 - val_mse: 1046.7988 - val_mae: 23.6227\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2917.2337 - mse: 2917.2329 - mae: 30.5402 - val_loss: 1047.4182 - val_mse: 1047.4181 - val_mae: 23.4899\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2794.7806 - mse: 2794.7800 - mae: 29.7901 - val_loss: 1043.1251 - val_mse: 1043.1251 - val_mae: 24.0734\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2872.6883 - mse: 2872.6882 - mae: 30.8597 - val_loss: 1042.3142 - val_mse: 1042.3141 - val_mae: 23.9452\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2850.8337 - mse: 2850.8333 - mae: 30.2927 - val_loss: 1041.4101 - val_mse: 1041.4100 - val_mae: 24.2379\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2857.6240 - mse: 2857.6245 - mae: 30.5899 - val_loss: 1040.7005 - val_mse: 1040.7004 - val_mae: 24.1960\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 329us/step - loss: 2897.0983 - mse: 2897.0979 - mae: 30.8041 - val_loss: 1043.7526 - val_mse: 1043.7526 - val_mae: 23.4704\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2896.4598 - mse: 2896.4590 - mae: 30.9428 - val_loss: 1040.1810 - val_mse: 1040.1809 - val_mae: 23.9521\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2845.4316 - mse: 2845.4314 - mae: 30.5708 - val_loss: 1039.5318 - val_mse: 1039.5317 - val_mae: 23.9471\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 316us/step - loss: 2917.1891 - mse: 2917.1890 - mae: 30.8121 - val_loss: 1042.7472 - val_mse: 1042.7471 - val_mae: 23.5046\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 341us/step - loss: 2872.3676 - mse: 2872.3679 - mae: 30.6175 - val_loss: 1041.6811 - val_mse: 1041.6812 - val_mae: 23.6687\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 333us/step - loss: 2853.8409 - mse: 2853.8403 - mae: 30.6968 - val_loss: 1040.4097 - val_mse: 1040.4098 - val_mae: 23.8629\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2842.8523 - mse: 2842.8525 - mae: 30.4079 - val_loss: 1040.6327 - val_mse: 1040.6326 - val_mae: 23.6455\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2861.2456 - mse: 2861.2451 - mae: 30.2817 - val_loss: 1039.2286 - val_mse: 1039.2285 - val_mae: 23.7994\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2858.5042 - mse: 2858.5037 - mae: 30.3530 - val_loss: 1038.8111 - val_mse: 1038.8110 - val_mae: 23.8427\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2807.3112 - mse: 2807.3110 - mae: 30.2468 - val_loss: 1038.6511 - val_mse: 1038.6511 - val_mae: 23.8182\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2809.3784 - mse: 2809.3799 - mae: 30.0982 - val_loss: 1037.6762 - val_mse: 1037.6763 - val_mae: 23.8612\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2972.8033 - mse: 2972.8040 - mae: 30.9659 - val_loss: 1045.9983 - val_mse: 1045.9984 - val_mae: 23.1359\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2898.1344 - mse: 2898.1340 - mae: 30.6734 - val_loss: 1037.5604 - val_mse: 1037.5604 - val_mae: 23.7871\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2809.1350 - mse: 2809.1345 - mae: 30.0684 - val_loss: 1036.7097 - val_mse: 1036.7097 - val_mae: 24.0155\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 268us/step - loss: 2548.5828 - mse: 2548.5823 - mae: 30.1356 - val_loss: 1523.6741 - val_mse: 1523.6741 - val_mae: 27.5646\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 219us/step - loss: 2500.6687 - mse: 2500.6692 - mae: 29.4651 - val_loss: 1520.2069 - val_mse: 1520.2069 - val_mae: 27.6115\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 361us/step - loss: 2575.8683 - mse: 2575.8679 - mae: 30.0792 - val_loss: 1516.1814 - val_mse: 1516.1816 - val_mae: 27.7040\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2532.6070 - mse: 2532.6074 - mae: 29.7475 - val_loss: 1522.4398 - val_mse: 1522.4401 - val_mae: 27.4442\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2559.3174 - mse: 2559.3174 - mae: 30.1281 - val_loss: 1516.6076 - val_mse: 1516.6077 - val_mae: 27.5558\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 276us/step - loss: 2502.8795 - mse: 2502.8799 - mae: 29.5829 - val_loss: 1516.1841 - val_mse: 1516.1840 - val_mae: 27.5093\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 264us/step - loss: 2547.9546 - mse: 2547.9548 - mae: 29.7335 - val_loss: 1518.1759 - val_mse: 1518.1759 - val_mae: 27.4112\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 332us/step - loss: 2553.4685 - mse: 2553.4690 - mae: 29.4052 - val_loss: 1512.3856 - val_mse: 1512.3855 - val_mae: 27.5754\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2505.5211 - mse: 2505.5212 - mae: 29.2059 - val_loss: 1513.3769 - val_mse: 1513.3767 - val_mae: 27.4990\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2579.6296 - mse: 2579.6296 - mae: 29.7665 - val_loss: 1513.5935 - val_mse: 1513.5935 - val_mae: 27.4568\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2567.1836 - mse: 2567.1836 - mae: 29.5726 - val_loss: 1516.9450 - val_mse: 1516.9451 - val_mae: 27.3127\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2526.1930 - mse: 2526.1934 - mae: 29.5174 - val_loss: 1521.0502 - val_mse: 1521.0500 - val_mae: 27.1565\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2584.8064 - mse: 2584.8071 - mae: 30.0924 - val_loss: 1520.9854 - val_mse: 1520.9855 - val_mae: 27.1201\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2568.5200 - mse: 2568.5198 - mae: 29.9987 - val_loss: 1516.0262 - val_mse: 1516.0261 - val_mae: 27.2292\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2490.9627 - mse: 2490.9624 - mae: 29.6494 - val_loss: 1515.5142 - val_mse: 1515.5140 - val_mae: 27.2106\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2504.7603 - mse: 2504.7607 - mae: 29.8457 - val_loss: 1505.2008 - val_mse: 1505.2007 - val_mae: 27.4945\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 222us/step - loss: 2514.8614 - mse: 2514.8613 - mae: 29.2167 - val_loss: 1499.8667 - val_mse: 1499.8666 - val_mae: 27.6366\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2526.9325 - mse: 2526.9329 - mae: 29.5257 - val_loss: 1509.8815 - val_mse: 1509.8815 - val_mae: 27.2380\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2471.7769 - mse: 2471.7778 - mae: 29.2813 - val_loss: 1509.1701 - val_mse: 1509.1702 - val_mae: 27.1988\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 247us/step - loss: 2559.5268 - mse: 2559.5266 - mae: 29.6130 - val_loss: 1520.1785 - val_mse: 1520.1785 - val_mae: 26.8916\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 274us/step - loss: 2586.0165 - mse: 2586.0164 - mae: 30.0467 - val_loss: 1511.2787 - val_mse: 1511.2787 - val_mae: 27.0884\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 252us/step - loss: 2445.3265 - mse: 2445.3267 - mae: 29.1782 - val_loss: 1495.6917 - val_mse: 1495.6918 - val_mae: 27.5733\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 263us/step - loss: 2526.3870 - mse: 2526.3862 - mae: 29.7037 - val_loss: 1502.8990 - val_mse: 1502.8990 - val_mae: 27.3076\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2573.0220 - mse: 2573.0215 - mae: 29.6194 - val_loss: 1507.9312 - val_mse: 1507.9312 - val_mae: 27.1646\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2538.6305 - mse: 2538.6301 - mae: 29.4988 - val_loss: 1500.7060 - val_mse: 1500.7059 - val_mae: 27.4007\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2544.4633 - mse: 2544.4636 - mae: 29.6579 - val_loss: 1515.6105 - val_mse: 1515.6105 - val_mae: 26.9328\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2505.2513 - mse: 2505.2515 - mae: 29.0898 - val_loss: 1504.9724 - val_mse: 1504.9723 - val_mae: 27.1936\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2521.7873 - mse: 2521.7869 - mae: 29.4781 - val_loss: 1507.3067 - val_mse: 1507.3066 - val_mae: 27.0980\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2448.7081 - mse: 2448.7087 - mae: 29.2474 - val_loss: 1494.9074 - val_mse: 1494.9073 - val_mae: 27.4718\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2481.3977 - mse: 2481.3975 - mae: 29.5712 - val_loss: 1492.2436 - val_mse: 1492.2435 - val_mae: 27.5596\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 276us/step - loss: 2534.9034 - mse: 2534.9026 - mae: 29.6849 - val_loss: 1498.3006 - val_mse: 1498.3008 - val_mae: 27.2695\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 254us/step - loss: 2495.5553 - mse: 2495.5549 - mae: 29.5451 - val_loss: 1500.6308 - val_mse: 1500.6310 - val_mae: 27.2032\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2566.4248 - mse: 2566.4238 - mae: 30.0950 - val_loss: 1507.9463 - val_mse: 1507.9463 - val_mae: 26.9917\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2556.0910 - mse: 2556.0911 - mae: 29.5919 - val_loss: 1499.6126 - val_mse: 1499.6127 - val_mae: 27.1874\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 332us/step - loss: 2584.0681 - mse: 2584.0674 - mae: 29.9303 - val_loss: 1507.2055 - val_mse: 1507.2054 - val_mae: 26.9681\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2453.6609 - mse: 2453.6604 - mae: 29.2717 - val_loss: 1507.1735 - val_mse: 1507.1735 - val_mae: 26.9584\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 345us/step - loss: 2523.2771 - mse: 2523.2769 - mae: 29.4366 - val_loss: 1500.8891 - val_mse: 1500.8890 - val_mae: 27.1326\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2469.3028 - mse: 2469.3025 - mae: 29.3243 - val_loss: 1490.1034 - val_mse: 1490.1034 - val_mae: 27.4296\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 259us/step - loss: 2515.8123 - mse: 2515.8127 - mae: 29.2394 - val_loss: 1486.2933 - val_mse: 1486.2932 - val_mae: 27.4943\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2488.8175 - mse: 2488.8169 - mae: 29.1180 - val_loss: 1491.1639 - val_mse: 1491.1642 - val_mae: 27.2494\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2492.1089 - mse: 2492.1094 - mae: 28.8934 - val_loss: 1491.1095 - val_mse: 1491.1094 - val_mae: 27.2440\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2473.4761 - mse: 2473.4751 - mae: 29.3331 - val_loss: 1485.4708 - val_mse: 1485.4706 - val_mae: 27.3865\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 276us/step - loss: 2528.8938 - mse: 2528.8943 - mae: 29.6279 - val_loss: 1494.0679 - val_mse: 1494.0679 - val_mae: 27.0806\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2504.6781 - mse: 2504.6787 - mae: 29.4169 - val_loss: 1486.1717 - val_mse: 1486.1716 - val_mae: 27.2881\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2563.5104 - mse: 2563.5103 - mae: 30.1709 - val_loss: 1492.6920 - val_mse: 1492.6921 - val_mae: 27.0149\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2480.4239 - mse: 2480.4243 - mae: 29.0270 - val_loss: 1486.3475 - val_mse: 1486.3473 - val_mae: 27.2058\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2496.3459 - mse: 2496.3445 - mae: 29.6903 - val_loss: 1485.0803 - val_mse: 1485.0802 - val_mae: 27.2423\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2533.8557 - mse: 2533.8557 - mae: 29.5717 - val_loss: 1494.0891 - val_mse: 1494.0890 - val_mae: 26.9380\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2510.4411 - mse: 2510.4412 - mae: 29.0232 - val_loss: 1480.4413 - val_mse: 1480.4412 - val_mae: 27.3870\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2502.0464 - mse: 2502.0459 - mae: 29.3277 - val_loss: 1483.1178 - val_mse: 1483.1179 - val_mae: 27.2569\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2520.5755 - mse: 2520.5754 - mae: 29.6282 - val_loss: 1489.6585 - val_mse: 1489.6584 - val_mae: 27.0097\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2522.2031 - mse: 2522.2031 - mae: 29.3322 - val_loss: 1490.4301 - val_mse: 1490.4299 - val_mae: 27.0179\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 280us/step - loss: 2473.4414 - mse: 2473.4417 - mae: 29.1784 - val_loss: 1486.2429 - val_mse: 1486.2430 - val_mae: 27.1155\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2537.9473 - mse: 2537.9473 - mae: 29.0403 - val_loss: 1481.2691 - val_mse: 1481.2690 - val_mae: 27.2488\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2597.2990 - mse: 2597.2988 - mae: 29.9477 - val_loss: 1486.8539 - val_mse: 1486.8539 - val_mae: 27.0362\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2501.5713 - mse: 2501.5708 - mae: 29.2315 - val_loss: 1492.2561 - val_mse: 1492.2565 - val_mae: 26.8628\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 263us/step - loss: 2463.3143 - mse: 2463.3147 - mae: 29.1815 - val_loss: 1480.0785 - val_mse: 1480.0785 - val_mae: 27.2029\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 267us/step - loss: 2521.1578 - mse: 2521.1577 - mae: 29.8190 - val_loss: 1482.3158 - val_mse: 1482.3157 - val_mae: 27.0696\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2533.2222 - mse: 2533.2222 - mae: 29.3031 - val_loss: 1489.7822 - val_mse: 1489.7820 - val_mae: 26.8546\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2474.7560 - mse: 2474.7573 - mae: 28.9314 - val_loss: 1478.7761 - val_mse: 1478.7760 - val_mae: 27.2093\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2513.1811 - mse: 2513.1809 - mae: 29.1764 - val_loss: 1488.0566 - val_mse: 1488.0566 - val_mae: 26.9389\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 249us/step - loss: 2470.8765 - mse: 2470.8765 - mae: 29.4639 - val_loss: 1489.7958 - val_mse: 1489.7959 - val_mae: 26.8482\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2481.5231 - mse: 2481.5237 - mae: 28.7367 - val_loss: 1487.1469 - val_mse: 1487.1467 - val_mae: 26.9321\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 358us/step - loss: 2454.2479 - mse: 2454.2483 - mae: 29.0841 - val_loss: 1482.1210 - val_mse: 1482.1211 - val_mae: 27.0709\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 279us/step - loss: 2456.4225 - mse: 2456.4219 - mae: 29.0802 - val_loss: 1486.3774 - val_mse: 1486.3774 - val_mae: 26.9442\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2487.5729 - mse: 2487.5732 - mae: 29.0933 - val_loss: 1484.1723 - val_mse: 1484.1724 - val_mae: 27.0166\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2504.0407 - mse: 2504.0400 - mae: 29.0215 - val_loss: 1482.7531 - val_mse: 1482.7531 - val_mae: 27.0691\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 340us/step - loss: 2475.7544 - mse: 2475.7544 - mae: 29.4938 - val_loss: 1486.0528 - val_mse: 1486.0529 - val_mae: 26.9325\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2545.9268 - mse: 2545.9270 - mae: 29.5318 - val_loss: 1491.7356 - val_mse: 1491.7356 - val_mae: 26.7828\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 272us/step - loss: 2437.3542 - mse: 2437.3535 - mae: 29.1274 - val_loss: 1482.4480 - val_mse: 1482.4479 - val_mae: 26.9895\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2500.2662 - mse: 2500.2664 - mae: 28.7594 - val_loss: 1479.1706 - val_mse: 1479.1707 - val_mae: 27.1216\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2463.0899 - mse: 2463.0898 - mae: 29.1973 - val_loss: 1473.7414 - val_mse: 1473.7413 - val_mae: 27.2696\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 221us/step - loss: 2516.2269 - mse: 2516.2271 - mae: 29.4097 - val_loss: 1472.2042 - val_mse: 1472.2041 - val_mae: 27.2941\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 261us/step - loss: 2511.3323 - mse: 2511.3323 - mae: 29.1131 - val_loss: 1473.3944 - val_mse: 1473.3944 - val_mae: 27.2203\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2470.5351 - mse: 2470.5359 - mae: 28.9134 - val_loss: 1474.0957 - val_mse: 1474.0958 - val_mae: 27.2056\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 343us/step - loss: 2481.7345 - mse: 2481.7339 - mae: 29.2629 - val_loss: 1476.0915 - val_mse: 1476.0916 - val_mae: 27.1106\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2470.2125 - mse: 2470.2117 - mae: 29.0263 - val_loss: 1474.9015 - val_mse: 1474.9016 - val_mae: 27.2197\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 278us/step - loss: 2509.5776 - mse: 2509.5771 - mae: 29.2633 - val_loss: 1487.0533 - val_mse: 1487.0532 - val_mae: 26.8496\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2568.8154 - mse: 2568.8147 - mae: 29.6953 - val_loss: 1499.0052 - val_mse: 1499.0049 - val_mae: 26.6159\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2478.2275 - mse: 2478.2283 - mae: 29.1982 - val_loss: 1492.7196 - val_mse: 1492.7197 - val_mae: 26.7523\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2408.9031 - mse: 2408.9026 - mae: 29.3551 - val_loss: 3710.4635 - val_mse: 3710.4636 - val_mae: 24.4472\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2417.6043 - mse: 2417.6045 - mae: 29.5190 - val_loss: 3710.2661 - val_mse: 3710.2668 - val_mae: 24.4570\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2364.2179 - mse: 2364.2178 - mae: 29.1947 - val_loss: 3709.3094 - val_mse: 3709.3096 - val_mae: 23.8979\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2418.9759 - mse: 2418.9763 - mae: 29.2833 - val_loss: 3710.3614 - val_mse: 3710.3613 - val_mae: 24.0337\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2358.7841 - mse: 2358.7837 - mae: 29.4705 - val_loss: 3711.9262 - val_mse: 3711.9253 - val_mae: 24.6463\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2329.1733 - mse: 2329.1733 - mae: 28.9126 - val_loss: 3712.2252 - val_mse: 3712.2253 - val_mae: 24.6301\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2394.5555 - mse: 2394.5569 - mae: 29.6351 - val_loss: 3713.6264 - val_mse: 3713.6270 - val_mae: 24.5705\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2364.7372 - mse: 2364.7378 - mae: 29.1017 - val_loss: 3713.2178 - val_mse: 3713.2168 - val_mae: 23.9249\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2319.3088 - mse: 2319.3093 - mae: 28.9699 - val_loss: 3715.1571 - val_mse: 3715.1562 - val_mae: 24.4669\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 272us/step - loss: 2352.5531 - mse: 2352.5532 - mae: 29.5710 - val_loss: 3713.6241 - val_mse: 3713.6243 - val_mae: 23.9823\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2333.8876 - mse: 2333.8884 - mae: 29.0234 - val_loss: 3715.6923 - val_mse: 3715.6917 - val_mae: 24.3773\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2342.6305 - mse: 2342.6309 - mae: 29.5389 - val_loss: 3717.5633 - val_mse: 3717.5632 - val_mae: 24.6834\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2374.6141 - mse: 2374.6143 - mae: 29.8140 - val_loss: 3716.9687 - val_mse: 3716.9685 - val_mae: 24.4271\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2365.7455 - mse: 2365.7451 - mae: 29.6756 - val_loss: 3715.4202 - val_mse: 3715.4194 - val_mae: 23.7286\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 267us/step - loss: 2413.6910 - mse: 2413.6904 - mae: 29.5745 - val_loss: 3715.7957 - val_mse: 3715.7954 - val_mae: 24.2353\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2379.6377 - mse: 2379.6379 - mae: 29.3947 - val_loss: 3716.4515 - val_mse: 3716.4512 - val_mae: 24.2165\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 352us/step - loss: 2372.2693 - mse: 2372.2688 - mae: 29.4048 - val_loss: 3716.0646 - val_mse: 3716.0642 - val_mae: 24.2737\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2421.5547 - mse: 2421.5549 - mae: 29.6267 - val_loss: 3717.6912 - val_mse: 3717.6907 - val_mae: 24.5166\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2384.0962 - mse: 2384.0959 - mae: 29.2580 - val_loss: 3716.2141 - val_mse: 3716.2141 - val_mae: 23.5752\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2408.0970 - mse: 2408.0962 - mae: 29.4163 - val_loss: 3717.1459 - val_mse: 3717.1455 - val_mae: 24.2800\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2437.5679 - mse: 2437.5674 - mae: 29.4865 - val_loss: 3716.5705 - val_mse: 3716.5703 - val_mae: 24.1400\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 269us/step - loss: 2319.8390 - mse: 2319.8391 - mae: 29.1847 - val_loss: 3715.8518 - val_mse: 3715.8518 - val_mae: 24.1537\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2396.4439 - mse: 2396.4438 - mae: 29.5037 - val_loss: 3715.9995 - val_mse: 3716.0002 - val_mae: 24.1926\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2376.9620 - mse: 2376.9624 - mae: 29.4204 - val_loss: 3714.9204 - val_mse: 3714.9204 - val_mae: 24.1405\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2388.8177 - mse: 2388.8184 - mae: 29.2547 - val_loss: 3715.7111 - val_mse: 3715.7119 - val_mae: 24.1423\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2453.5780 - mse: 2453.5779 - mae: 29.9932 - val_loss: 3715.7658 - val_mse: 3715.7651 - val_mae: 23.9543\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2392.3591 - mse: 2392.3591 - mae: 29.3751 - val_loss: 3716.5428 - val_mse: 3716.5425 - val_mae: 24.3628\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2414.0819 - mse: 2414.0828 - mae: 29.7148 - val_loss: 3713.6173 - val_mse: 3713.6172 - val_mae: 23.5954\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2386.0486 - mse: 2386.0483 - mae: 29.2861 - val_loss: 3717.9583 - val_mse: 3717.9583 - val_mae: 24.9251\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2367.8467 - mse: 2367.8469 - mae: 28.9295 - val_loss: 3718.7680 - val_mse: 3718.7681 - val_mae: 24.9123\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2371.7753 - mse: 2371.7751 - mae: 29.1228 - val_loss: 3715.8682 - val_mse: 3715.8679 - val_mae: 24.4925\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 331us/step - loss: 2355.1315 - mse: 2355.1313 - mae: 29.5768 - val_loss: 3715.1475 - val_mse: 3715.1487 - val_mae: 24.2164\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2393.3762 - mse: 2393.3757 - mae: 29.2382 - val_loss: 3715.0494 - val_mse: 3715.0493 - val_mae: 24.3956\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2378.4920 - mse: 2378.4915 - mae: 29.1797 - val_loss: 3714.6876 - val_mse: 3714.6870 - val_mae: 24.5571\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2339.1316 - mse: 2339.1311 - mae: 29.2028 - val_loss: 3716.6606 - val_mse: 3716.6606 - val_mae: 24.9233\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2367.0484 - mse: 2367.0483 - mae: 29.4254 - val_loss: 3712.5550 - val_mse: 3712.5544 - val_mae: 24.1407\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2300.3047 - mse: 2300.3052 - mae: 29.2543 - val_loss: 3713.0940 - val_mse: 3713.0933 - val_mae: 24.2263\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2376.4085 - mse: 2376.4087 - mae: 29.3232 - val_loss: 3713.1121 - val_mse: 3713.1118 - val_mae: 24.1326\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 266us/step - loss: 2339.0125 - mse: 2339.0127 - mae: 29.2976 - val_loss: 3713.7670 - val_mse: 3713.7671 - val_mae: 24.2246\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2345.4435 - mse: 2345.4438 - mae: 28.9071 - val_loss: 3713.8693 - val_mse: 3713.8701 - val_mae: 24.0404\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 328us/step - loss: 2329.1014 - mse: 2329.1013 - mae: 29.1469 - val_loss: 3716.9119 - val_mse: 3716.9119 - val_mae: 24.5862\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2385.2861 - mse: 2385.2854 - mae: 29.5371 - val_loss: 3715.1418 - val_mse: 3715.1423 - val_mae: 24.1395\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2306.8412 - mse: 2306.8428 - mae: 28.8534 - val_loss: 3715.6340 - val_mse: 3715.6345 - val_mae: 24.4933\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2348.3912 - mse: 2348.3916 - mae: 29.4323 - val_loss: 3714.0391 - val_mse: 3714.0396 - val_mae: 24.2085\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 280us/step - loss: 2349.2357 - mse: 2349.2358 - mae: 28.9651 - val_loss: 3714.7055 - val_mse: 3714.7061 - val_mae: 24.2862\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2334.3270 - mse: 2334.3271 - mae: 29.4918 - val_loss: 3715.4224 - val_mse: 3715.4221 - val_mae: 24.3849\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2378.4127 - mse: 2378.4131 - mae: 29.6177 - val_loss: 3716.1470 - val_mse: 3716.1470 - val_mae: 24.5407\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 346us/step - loss: 2377.5455 - mse: 2377.5454 - mae: 29.3721 - val_loss: 3716.2013 - val_mse: 3716.2012 - val_mae: 24.5820\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2350.7786 - mse: 2350.7776 - mae: 29.4624 - val_loss: 3714.0468 - val_mse: 3714.0466 - val_mae: 24.1908\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2335.7058 - mse: 2335.7056 - mae: 28.8968 - val_loss: 3715.5451 - val_mse: 3715.5452 - val_mae: 24.5301\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2360.6290 - mse: 2360.6284 - mae: 29.3505 - val_loss: 3716.7552 - val_mse: 3716.7554 - val_mae: 24.7553\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2358.8227 - mse: 2358.8228 - mae: 29.0474 - val_loss: 3714.0033 - val_mse: 3714.0039 - val_mae: 24.2099\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2376.0434 - mse: 2376.0437 - mae: 29.3491 - val_loss: 3715.6924 - val_mse: 3715.6924 - val_mae: 24.4092\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2325.6881 - mse: 2325.6885 - mae: 29.1126 - val_loss: 3714.5900 - val_mse: 3714.5901 - val_mae: 24.4131\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2368.5152 - mse: 2368.5156 - mae: 29.1338 - val_loss: 3715.6357 - val_mse: 3715.6355 - val_mae: 24.6726\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2345.4531 - mse: 2345.4529 - mae: 29.1989 - val_loss: 3714.9484 - val_mse: 3714.9482 - val_mae: 24.4118\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2361.4219 - mse: 2361.4226 - mae: 29.2690 - val_loss: 3714.4558 - val_mse: 3714.4558 - val_mae: 24.3873\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2342.1556 - mse: 2342.1553 - mae: 29.2641 - val_loss: 3715.0892 - val_mse: 3715.0891 - val_mae: 24.5461\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 244us/step - loss: 2335.8309 - mse: 2335.8306 - mae: 29.2359 - val_loss: 3715.1689 - val_mse: 3715.1689 - val_mae: 24.4598\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2318.3419 - mse: 2318.3418 - mae: 28.9608 - val_loss: 3715.7480 - val_mse: 3715.7483 - val_mae: 24.5742\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2381.9438 - mse: 2381.9438 - mae: 29.4901 - val_loss: 3715.2668 - val_mse: 3715.2668 - val_mae: 24.6205\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2360.0791 - mse: 2360.0796 - mae: 29.2337 - val_loss: 3714.4280 - val_mse: 3714.4280 - val_mae: 24.3584\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2365.7037 - mse: 2365.7039 - mae: 29.4884 - val_loss: 3714.9701 - val_mse: 3714.9700 - val_mae: 24.1125\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2329.9633 - mse: 2329.9629 - mae: 29.2207 - val_loss: 3715.2738 - val_mse: 3715.2737 - val_mae: 24.3612\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2391.8428 - mse: 2391.8430 - mae: 29.2440 - val_loss: 3714.1153 - val_mse: 3714.1152 - val_mae: 24.3749\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2409.8320 - mse: 2409.8328 - mae: 29.3257 - val_loss: 3716.1944 - val_mse: 3716.1941 - val_mae: 24.8156\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2361.6449 - mse: 2361.6455 - mae: 29.3134 - val_loss: 3715.7203 - val_mse: 3715.7205 - val_mae: 24.8144\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2332.9377 - mse: 2332.9385 - mae: 29.1334 - val_loss: 3717.9578 - val_mse: 3717.9573 - val_mae: 24.8845\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2391.9731 - mse: 2391.9727 - mae: 29.1943 - val_loss: 3715.0446 - val_mse: 3715.0442 - val_mae: 24.3316\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2341.8388 - mse: 2341.8389 - mae: 29.1227 - val_loss: 3714.5732 - val_mse: 3714.5730 - val_mae: 24.0980\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2331.5673 - mse: 2331.5674 - mae: 28.7926 - val_loss: 3717.6700 - val_mse: 3717.6699 - val_mae: 24.7701\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2369.9548 - mse: 2369.9546 - mae: 29.0781 - val_loss: 3713.2389 - val_mse: 3713.2390 - val_mae: 24.2158\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2322.9769 - mse: 2322.9771 - mae: 29.2385 - val_loss: 3715.5084 - val_mse: 3715.5081 - val_mae: 24.5894\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 265us/step - loss: 2320.0512 - mse: 2320.0505 - mae: 28.8216 - val_loss: 3715.8883 - val_mse: 3715.8882 - val_mae: 24.7662\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2321.6090 - mse: 2321.6089 - mae: 28.9283 - val_loss: 3714.4875 - val_mse: 3714.4875 - val_mae: 24.4438\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2337.3212 - mse: 2337.3215 - mae: 29.3599 - val_loss: 3715.1498 - val_mse: 3715.1497 - val_mae: 24.2536\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2332.8474 - mse: 2332.8459 - mae: 29.1644 - val_loss: 3713.7207 - val_mse: 3713.7205 - val_mae: 24.0422\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2356.3107 - mse: 2356.3103 - mae: 29.1272 - val_loss: 3713.7102 - val_mse: 3713.7112 - val_mae: 24.4776\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 233us/step - loss: 2298.9037 - mse: 2298.9036 - mae: 29.0450 - val_loss: 3713.4598 - val_mse: 3713.4595 - val_mae: 24.1998\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 266us/step - loss: 2328.7995 - mse: 2328.7998 - mae: 29.2530 - val_loss: 3715.3093 - val_mse: 3715.3096 - val_mae: 24.4966\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2677.8751 - mse: 2677.8752 - mae: 28.5889 - val_loss: 2159.3412 - val_mse: 2159.3413 - val_mae: 26.6547\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2727.3574 - mse: 2727.3582 - mae: 28.4575 - val_loss: 2154.4185 - val_mse: 2154.4185 - val_mae: 26.7769\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 260us/step - loss: 2674.7151 - mse: 2674.7153 - mae: 28.3230 - val_loss: 2159.7380 - val_mse: 2159.7383 - val_mae: 26.4936\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2779.5102 - mse: 2779.5100 - mae: 28.6368 - val_loss: 2166.0529 - val_mse: 2166.0530 - val_mae: 26.5123\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2720.1267 - mse: 2720.1260 - mae: 28.5755 - val_loss: 2177.4380 - val_mse: 2177.4377 - val_mae: 26.2497\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2710.9989 - mse: 2710.9985 - mae: 28.8770 - val_loss: 2163.2326 - val_mse: 2163.2329 - val_mae: 26.6414\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2726.0760 - mse: 2726.0767 - mae: 28.5744 - val_loss: 2155.0167 - val_mse: 2155.0166 - val_mae: 26.8706\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 270us/step - loss: 2745.0082 - mse: 2745.0085 - mae: 28.5301 - val_loss: 2162.6167 - val_mse: 2162.6169 - val_mae: 26.5326\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2755.8372 - mse: 2755.8381 - mae: 28.7647 - val_loss: 2163.8301 - val_mse: 2163.8301 - val_mae: 26.5063\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2704.8750 - mse: 2704.8745 - mae: 28.5458 - val_loss: 2151.2485 - val_mse: 2151.2485 - val_mae: 26.7714\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2746.2075 - mse: 2746.2075 - mae: 28.5107 - val_loss: 2158.1176 - val_mse: 2158.1179 - val_mae: 26.6963\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2727.9081 - mse: 2727.9077 - mae: 28.6098 - val_loss: 2164.6596 - val_mse: 2164.6597 - val_mae: 26.6577\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 254us/step - loss: 2707.0535 - mse: 2707.0535 - mae: 28.3129 - val_loss: 2160.2223 - val_mse: 2160.2224 - val_mae: 27.0318\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2694.5727 - mse: 2694.5728 - mae: 28.5625 - val_loss: 2171.7230 - val_mse: 2171.7234 - val_mae: 26.9141\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2697.7884 - mse: 2697.7883 - mae: 28.5257 - val_loss: 2183.9540 - val_mse: 2183.9543 - val_mae: 26.4841\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2754.5784 - mse: 2754.5791 - mae: 28.5710 - val_loss: 2175.0319 - val_mse: 2175.0320 - val_mae: 26.6932\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2715.2649 - mse: 2715.2654 - mae: 27.9213 - val_loss: 2174.4177 - val_mse: 2174.4177 - val_mae: 26.6633\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2695.0209 - mse: 2695.0200 - mae: 28.1647 - val_loss: 2166.5509 - val_mse: 2166.5510 - val_mae: 26.9786\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2756.3950 - mse: 2756.3948 - mae: 28.6815 - val_loss: 2177.8239 - val_mse: 2177.8240 - val_mae: 26.6984\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 276us/step - loss: 2753.7838 - mse: 2753.7837 - mae: 28.7129 - val_loss: 2160.2783 - val_mse: 2160.2783 - val_mae: 27.0549\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2785.0566 - mse: 2785.0559 - mae: 29.0153 - val_loss: 2183.0372 - val_mse: 2183.0374 - val_mae: 26.5825\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 270us/step - loss: 2711.8824 - mse: 2711.8823 - mae: 28.5723 - val_loss: 2172.3977 - val_mse: 2172.3977 - val_mae: 26.8152\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2743.7198 - mse: 2743.7197 - mae: 28.6979 - val_loss: 2163.8426 - val_mse: 2163.8425 - val_mae: 26.9549\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2686.4148 - mse: 2686.4150 - mae: 28.6222 - val_loss: 2164.5875 - val_mse: 2164.5876 - val_mae: 26.6995\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2717.4272 - mse: 2717.4275 - mae: 28.8166 - val_loss: 2166.9277 - val_mse: 2166.9280 - val_mae: 26.5670\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2743.3389 - mse: 2743.3394 - mae: 28.5725 - val_loss: 2177.9634 - val_mse: 2177.9636 - val_mae: 26.4130\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2682.9022 - mse: 2682.9023 - mae: 28.1973 - val_loss: 2172.3160 - val_mse: 2172.3159 - val_mae: 26.6706\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2740.9901 - mse: 2740.9907 - mae: 28.5808 - val_loss: 2174.2858 - val_mse: 2174.2864 - val_mae: 26.6198\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2697.2750 - mse: 2697.2747 - mae: 28.4399 - val_loss: 2171.8979 - val_mse: 2171.8982 - val_mae: 26.8184\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 256us/step - loss: 2725.7296 - mse: 2725.7290 - mae: 29.0611 - val_loss: 2167.2213 - val_mse: 2167.2212 - val_mae: 26.9363\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2727.2577 - mse: 2727.2581 - mae: 28.3642 - val_loss: 2187.0864 - val_mse: 2187.0867 - val_mae: 26.4534\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 266us/step - loss: 2765.1257 - mse: 2765.1257 - mae: 28.5038 - val_loss: 2188.3793 - val_mse: 2188.3792 - val_mae: 26.6273\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2744.3167 - mse: 2744.3157 - mae: 28.5578 - val_loss: 2192.4383 - val_mse: 2192.4385 - val_mae: 26.6241\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 253us/step - loss: 2719.8402 - mse: 2719.8406 - mae: 28.6250 - val_loss: 2190.5858 - val_mse: 2190.5859 - val_mae: 26.5812\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2736.3504 - mse: 2736.3513 - mae: 28.6346 - val_loss: 2180.4062 - val_mse: 2180.4060 - val_mae: 26.7085\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2647.2274 - mse: 2647.2273 - mae: 27.9651 - val_loss: 2163.1415 - val_mse: 2163.1418 - val_mae: 26.9897\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2662.2277 - mse: 2662.2278 - mae: 28.3895 - val_loss: 2181.2326 - val_mse: 2181.2324 - val_mae: 26.5116\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 246us/step - loss: 2723.0436 - mse: 2723.0444 - mae: 28.5955 - val_loss: 2180.3207 - val_mse: 2180.3208 - val_mae: 26.4420\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 329us/step - loss: 2710.6236 - mse: 2710.6230 - mae: 28.3812 - val_loss: 2175.6727 - val_mse: 2175.6729 - val_mae: 26.6705\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2689.4910 - mse: 2689.4905 - mae: 28.5637 - val_loss: 2168.1780 - val_mse: 2168.1782 - val_mae: 26.8819\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2703.1409 - mse: 2703.1404 - mae: 28.3013 - val_loss: 2174.5006 - val_mse: 2174.5007 - val_mae: 26.6801\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2754.0643 - mse: 2754.0645 - mae: 28.6665 - val_loss: 2172.8170 - val_mse: 2172.8171 - val_mae: 26.8353\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2728.9808 - mse: 2728.9819 - mae: 28.3796 - val_loss: 2153.3845 - val_mse: 2153.3848 - val_mae: 27.0653\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2722.1745 - mse: 2722.1738 - mae: 28.5390 - val_loss: 2165.3894 - val_mse: 2165.3894 - val_mae: 26.6362\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2709.2807 - mse: 2709.2810 - mae: 28.5612 - val_loss: 2169.9626 - val_mse: 2169.9626 - val_mae: 26.6105\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2710.1840 - mse: 2710.1851 - mae: 28.3390 - val_loss: 2186.8215 - val_mse: 2186.8213 - val_mae: 26.3551\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2695.6035 - mse: 2695.6050 - mae: 28.3662 - val_loss: 2183.0991 - val_mse: 2183.0991 - val_mae: 26.4924\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 276us/step - loss: 2733.7632 - mse: 2733.7617 - mae: 28.7060 - val_loss: 2186.1031 - val_mse: 2186.1030 - val_mae: 26.5923\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2707.5237 - mse: 2707.5234 - mae: 28.3847 - val_loss: 2169.5928 - val_mse: 2169.5925 - val_mae: 27.0683\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 264us/step - loss: 2742.7361 - mse: 2742.7351 - mae: 28.5622 - val_loss: 2182.0234 - val_mse: 2182.0234 - val_mae: 26.4522\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2709.7066 - mse: 2709.7073 - mae: 28.5815 - val_loss: 2167.4731 - val_mse: 2167.4734 - val_mae: 26.8607\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2747.6465 - mse: 2747.6462 - mae: 28.4297 - val_loss: 2178.2091 - val_mse: 2178.2092 - val_mae: 26.6301\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2658.9532 - mse: 2658.9524 - mae: 28.3360 - val_loss: 2184.1820 - val_mse: 2184.1821 - val_mae: 26.7250\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2693.1585 - mse: 2693.1582 - mae: 28.4907 - val_loss: 2192.8897 - val_mse: 2192.8894 - val_mae: 26.5077\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2690.9738 - mse: 2690.9749 - mae: 28.4996 - val_loss: 2187.1654 - val_mse: 2187.1655 - val_mae: 26.6742\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 271us/step - loss: 2699.4366 - mse: 2699.4360 - mae: 28.2617 - val_loss: 2185.1666 - val_mse: 2185.1663 - val_mae: 26.7324\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2707.1636 - mse: 2707.1624 - mae: 28.1847 - val_loss: 2181.9390 - val_mse: 2181.9390 - val_mae: 27.0228\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 254us/step - loss: 2740.5287 - mse: 2740.5288 - mae: 28.7343 - val_loss: 2183.7719 - val_mse: 2183.7717 - val_mae: 26.7859\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 262us/step - loss: 2679.0880 - mse: 2679.0881 - mae: 28.1472 - val_loss: 2170.4260 - val_mse: 2170.4258 - val_mae: 27.1653\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2717.5271 - mse: 2717.5264 - mae: 28.6580 - val_loss: 2180.7123 - val_mse: 2180.7119 - val_mae: 26.6165\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2639.5961 - mse: 2639.5962 - mae: 28.4107 - val_loss: 2162.5146 - val_mse: 2162.5146 - val_mae: 26.8880\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2683.6599 - mse: 2683.6606 - mae: 28.0834 - val_loss: 2165.0153 - val_mse: 2165.0151 - val_mae: 26.7406\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2724.4978 - mse: 2724.4988 - mae: 28.6810 - val_loss: 2182.5097 - val_mse: 2182.5095 - val_mae: 26.4642\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2729.2851 - mse: 2729.2849 - mae: 28.3681 - val_loss: 2164.8011 - val_mse: 2164.8013 - val_mae: 26.9743\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2715.6938 - mse: 2715.6938 - mae: 28.3875 - val_loss: 2167.1451 - val_mse: 2167.1450 - val_mae: 26.8132\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2666.5082 - mse: 2666.5090 - mae: 28.3637 - val_loss: 2172.1461 - val_mse: 2172.1460 - val_mae: 26.8809\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2719.8113 - mse: 2719.8115 - mae: 28.1231 - val_loss: 2191.1586 - val_mse: 2191.1589 - val_mae: 26.4930\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 263us/step - loss: 2680.9640 - mse: 2680.9634 - mae: 28.0762 - val_loss: 2179.2142 - val_mse: 2179.2144 - val_mae: 27.0069\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2710.8271 - mse: 2710.8269 - mae: 28.2657 - val_loss: 2181.6332 - val_mse: 2181.6331 - val_mae: 27.0377\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2696.2178 - mse: 2696.2173 - mae: 28.4906 - val_loss: 2183.9358 - val_mse: 2183.9355 - val_mae: 26.9258\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2722.5718 - mse: 2722.5718 - mae: 28.5759 - val_loss: 2183.4278 - val_mse: 2183.4277 - val_mae: 26.7839\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2712.5731 - mse: 2712.5730 - mae: 28.5232 - val_loss: 2182.3479 - val_mse: 2182.3479 - val_mae: 26.7926\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2729.4442 - mse: 2729.4434 - mae: 28.6404 - val_loss: 2172.0753 - val_mse: 2172.0757 - val_mae: 26.7347\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2690.0027 - mse: 2690.0029 - mae: 28.2969 - val_loss: 2171.9620 - val_mse: 2171.9622 - val_mae: 26.7928\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2677.5235 - mse: 2677.5225 - mae: 28.1536 - val_loss: 2173.0639 - val_mse: 2173.0637 - val_mae: 26.5634\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2718.8052 - mse: 2718.8066 - mae: 28.5443 - val_loss: 2182.5999 - val_mse: 2182.5999 - val_mae: 26.6347\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2649.1753 - mse: 2649.1746 - mae: 27.9128 - val_loss: 2157.1116 - val_mse: 2157.1116 - val_mae: 27.2073\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2707.2465 - mse: 2707.2468 - mae: 28.2058 - val_loss: 2170.3662 - val_mse: 2170.3660 - val_mae: 26.8081\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2694.9175 - mse: 2694.9170 - mae: 28.3449 - val_loss: 2166.0113 - val_mse: 2166.0115 - val_mae: 26.9262\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2671.8309 - mse: 2671.8301 - mae: 28.0246 - val_loss: 2182.3318 - val_mse: 2182.3318 - val_mae: 26.5491\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13325.8312 - mse: 13325.8311 - mae: 109.8925 - val_loss: 34610.7325 - val_mse: 34610.7305 - val_mae: 132.7008\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 13153.2450 - mse: 13153.2461 - mae: 109.1255 - val_loss: 34265.0184 - val_mse: 34265.0156 - val_mae: 131.3981\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 241us/step - loss: 12644.9492 - mse: 12644.9502 - mae: 106.7677 - val_loss: 33247.3731 - val_mse: 33247.3750 - val_mae: 127.4869\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 345us/step - loss: 11275.1168 - mse: 11275.1172 - mae: 100.1044 - val_loss: 30494.7257 - val_mse: 30494.7266 - val_mae: 116.2502\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 260us/step - loss: 7979.7639 - mse: 7979.7642 - mae: 81.5305 - val_loss: 24139.0910 - val_mse: 24139.0918 - val_mae: 84.7985\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 3371.3484 - mse: 3371.3486 - mae: 44.8342 - val_loss: 17832.2418 - val_mse: 17832.2422 - val_mae: 35.4506\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 2652.6390 - mse: 2652.6389 - mae: 36.5604 - val_loss: 17728.7850 - val_mse: 17728.7852 - val_mae: 35.0758\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 279us/step - loss: 2552.5702 - mse: 2552.5703 - mae: 35.8618 - val_loss: 17946.2773 - val_mse: 17946.2773 - val_mae: 36.1223\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 402us/step - loss: 2467.8259 - mse: 2467.8259 - mae: 35.5012 - val_loss: 17969.9873 - val_mse: 17969.9883 - val_mae: 36.2716\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 355us/step - loss: 2226.8066 - mse: 2226.8066 - mae: 33.1365 - val_loss: 17803.1032 - val_mse: 17803.1035 - val_mae: 35.3172\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2469.7522 - mse: 2469.7522 - mae: 34.4880 - val_loss: 17928.4583 - val_mse: 17928.4570 - val_mae: 36.0144\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 262us/step - loss: 2432.7881 - mse: 2432.7881 - mae: 34.4840 - val_loss: 17719.4605 - val_mse: 17719.4609 - val_mae: 35.0982\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2244.0842 - mse: 2244.0842 - mae: 33.8659 - val_loss: 17639.1524 - val_mse: 17639.1523 - val_mae: 35.0036\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2596.5340 - mse: 2596.5342 - mae: 36.0402 - val_loss: 18036.2838 - val_mse: 18036.2832 - val_mae: 36.7108\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 385us/step - loss: 2274.8347 - mse: 2274.8345 - mae: 34.3208 - val_loss: 17899.2888 - val_mse: 17899.2871 - val_mae: 35.8453\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2364.7095 - mse: 2364.7095 - mae: 34.3275 - val_loss: 17715.4528 - val_mse: 17715.4531 - val_mae: 35.1251\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2477.0122 - mse: 2477.0125 - mae: 33.7374 - val_loss: 17844.9747 - val_mse: 17844.9766 - val_mae: 35.5400\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 272us/step - loss: 2560.8194 - mse: 2560.8193 - mae: 35.7107 - val_loss: 18073.1531 - val_mse: 18073.1523 - val_mae: 36.9706\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2153.2316 - mse: 2153.2314 - mae: 33.5628 - val_loss: 17743.0939 - val_mse: 17743.0957 - val_mae: 35.2015\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 382us/step - loss: 2142.8608 - mse: 2142.8611 - mae: 33.6498 - val_loss: 17932.7716 - val_mse: 17932.7695 - val_mae: 36.0463\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 368us/step - loss: 2365.8328 - mse: 2365.8325 - mae: 33.4002 - val_loss: 17838.7450 - val_mse: 17838.7461 - val_mae: 35.5197\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 255us/step - loss: 2225.3509 - mse: 2225.3508 - mae: 34.1871 - val_loss: 17770.4812 - val_mse: 17770.4824 - val_mae: 35.2826\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 288us/step - loss: 2202.5334 - mse: 2202.5334 - mae: 32.5753 - val_loss: 17766.9550 - val_mse: 17766.9551 - val_mae: 35.2877\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 392us/step - loss: 2196.4798 - mse: 2196.4795 - mae: 32.3182 - val_loss: 17737.7848 - val_mse: 17737.7832 - val_mae: 35.2489\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2234.3544 - mse: 2234.3542 - mae: 33.7917 - val_loss: 17659.7218 - val_mse: 17659.7227 - val_mae: 35.1680\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2240.4450 - mse: 2240.4453 - mae: 33.0359 - val_loss: 17771.3626 - val_mse: 17771.3613 - val_mae: 35.3280\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2313.4645 - mse: 2313.4648 - mae: 33.9729 - val_loss: 17810.3202 - val_mse: 17810.3203 - val_mae: 35.4424\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 354us/step - loss: 2292.0180 - mse: 2292.0181 - mae: 33.1265 - val_loss: 18005.1807 - val_mse: 18005.1797 - val_mae: 36.5025\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2211.2483 - mse: 2211.2483 - mae: 32.8131 - val_loss: 17753.3432 - val_mse: 17753.3418 - val_mae: 35.3314\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 290us/step - loss: 2281.4651 - mse: 2281.4651 - mae: 33.7908 - val_loss: 17717.7508 - val_mse: 17717.7500 - val_mae: 35.2928\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2279.3423 - mse: 2279.3420 - mae: 33.6556 - val_loss: 17945.1743 - val_mse: 17945.1738 - val_mae: 36.1432\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 2214.6105 - mse: 2214.6108 - mae: 32.6247 - val_loss: 17794.8561 - val_mse: 17794.8555 - val_mae: 35.4440\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 360us/step - loss: 2450.6028 - mse: 2450.6030 - mae: 34.0113 - val_loss: 18107.6907 - val_mse: 18107.6914 - val_mae: 37.2526\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2059.9276 - mse: 2059.9275 - mae: 32.0519 - val_loss: 17670.7480 - val_mse: 17670.7480 - val_mae: 35.2906\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2257.3199 - mse: 2257.3198 - mae: 33.5660 - val_loss: 17764.5546 - val_mse: 17764.5547 - val_mae: 35.4145\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2067.9415 - mse: 2067.9414 - mae: 30.6081 - val_loss: 17763.5214 - val_mse: 17763.5215 - val_mae: 35.4222\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 396us/step - loss: 2189.7273 - mse: 2189.7273 - mae: 33.1966 - val_loss: 17676.0496 - val_mse: 17676.0508 - val_mae: 35.3263\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 2154.7171 - mse: 2154.7173 - mae: 32.8952 - val_loss: 17747.0652 - val_mse: 17747.0645 - val_mae: 35.4216\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 382us/step - loss: 2184.2328 - mse: 2184.2329 - mae: 32.4407 - val_loss: 17604.3767 - val_mse: 17604.3770 - val_mae: 35.3162\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2155.5703 - mse: 2155.5701 - mae: 31.5791 - val_loss: 17745.1025 - val_mse: 17745.1016 - val_mae: 35.4421\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2123.1561 - mse: 2123.1562 - mae: 32.0281 - val_loss: 17740.8157 - val_mse: 17740.8145 - val_mae: 35.4454\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 2150.7078 - mse: 2150.7078 - mae: 32.2713 - val_loss: 17593.6201 - val_mse: 17593.6172 - val_mae: 35.3461\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2214.6468 - mse: 2214.6467 - mae: 32.9440 - val_loss: 17726.1731 - val_mse: 17726.1738 - val_mae: 35.4434\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2307.4325 - mse: 2307.4326 - mae: 33.4981 - val_loss: 17780.6886 - val_mse: 17780.6895 - val_mae: 35.5316\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 284us/step - loss: 1897.4555 - mse: 1897.4556 - mae: 29.8498 - val_loss: 17602.9598 - val_mse: 17602.9609 - val_mae: 35.3885\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 343us/step - loss: 2240.6239 - mse: 2240.6238 - mae: 33.3837 - val_loss: 17901.3202 - val_mse: 17901.3203 - val_mae: 35.9817\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 2153.8220 - mse: 2153.8223 - mae: 32.6382 - val_loss: 17754.6599 - val_mse: 17754.6602 - val_mae: 35.5224\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2239.1456 - mse: 2239.1460 - mae: 32.6600 - val_loss: 17906.7557 - val_mse: 17906.7539 - val_mae: 36.0187\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 275us/step - loss: 1991.8461 - mse: 1991.8459 - mae: 31.0848 - val_loss: 17653.9587 - val_mse: 17653.9570 - val_mae: 35.4478\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 234us/step - loss: 2077.2108 - mse: 2077.2109 - mae: 30.9669 - val_loss: 17624.8720 - val_mse: 17624.8711 - val_mae: 35.4481\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 266us/step - loss: 2315.1286 - mse: 2315.1287 - mae: 33.7355 - val_loss: 17727.5284 - val_mse: 17727.5293 - val_mae: 35.5185\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 2293.9072 - mse: 2293.9072 - mae: 33.0055 - val_loss: 17815.3586 - val_mse: 17815.3574 - val_mae: 35.6961\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 276us/step - loss: 2118.9092 - mse: 2118.9092 - mae: 31.5081 - val_loss: 17747.4252 - val_mse: 17747.4258 - val_mae: 35.5679\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 284us/step - loss: 2132.1697 - mse: 2132.1697 - mae: 31.1721 - val_loss: 17743.5748 - val_mse: 17743.5742 - val_mae: 35.5730\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 289us/step - loss: 2085.3828 - mse: 2085.3826 - mae: 31.0634 - val_loss: 17676.8277 - val_mse: 17676.8262 - val_mae: 35.5284\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2099.0249 - mse: 2099.0247 - mae: 31.8879 - val_loss: 17613.0743 - val_mse: 17613.0742 - val_mae: 35.5434\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 1954.5406 - mse: 1954.5406 - mae: 30.2781 - val_loss: 17651.7023 - val_mse: 17651.7012 - val_mae: 35.5492\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2085.0169 - mse: 2085.0168 - mae: 30.7522 - val_loss: 17682.4522 - val_mse: 17682.4512 - val_mae: 35.5684\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 1990.3238 - mse: 1990.3237 - mae: 30.5115 - val_loss: 17671.2452 - val_mse: 17671.2461 - val_mae: 35.5817\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2132.7356 - mse: 2132.7358 - mae: 32.8794 - val_loss: 17883.3142 - val_mse: 17883.3145 - val_mae: 36.0118\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 1994.4811 - mse: 1994.4812 - mae: 30.7305 - val_loss: 17659.0993 - val_mse: 17659.0977 - val_mae: 35.6101\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 1957.6419 - mse: 1957.6418 - mae: 30.3404 - val_loss: 17736.1382 - val_mse: 17736.1387 - val_mae: 35.6568\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 236us/step - loss: 2005.6824 - mse: 2005.6824 - mae: 30.5580 - val_loss: 17884.8519 - val_mse: 17884.8535 - val_mae: 36.0423\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 1997.7701 - mse: 1997.7701 - mae: 30.9503 - val_loss: 17574.9359 - val_mse: 17574.9355 - val_mae: 35.6950\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 1932.1161 - mse: 1932.1161 - mae: 29.5716 - val_loss: 17615.9584 - val_mse: 17615.9570 - val_mae: 35.6793\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2038.0317 - mse: 2038.0316 - mae: 31.0548 - val_loss: 17856.2092 - val_mse: 17856.2070 - val_mae: 35.9626\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2076.6431 - mse: 2076.6431 - mae: 30.5690 - val_loss: 17623.8739 - val_mse: 17623.8730 - val_mae: 35.7078\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 1823.9221 - mse: 1823.9221 - mae: 29.8703 - val_loss: 17570.8956 - val_mse: 17570.8945 - val_mae: 35.7638\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 1964.3967 - mse: 1964.3967 - mae: 30.8603 - val_loss: 17701.1705 - val_mse: 17701.1719 - val_mae: 35.7199\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 235us/step - loss: 1927.0594 - mse: 1927.0593 - mae: 30.7881 - val_loss: 17696.5608 - val_mse: 17696.5625 - val_mae: 35.7270\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 1861.6540 - mse: 1861.6539 - mae: 29.8251 - val_loss: 17664.2388 - val_mse: 17664.2402 - val_mae: 35.7377\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 344us/step - loss: 1991.3236 - mse: 1991.3236 - mae: 30.6920 - val_loss: 17720.0931 - val_mse: 17720.0957 - val_mae: 35.7569\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 364us/step - loss: 2101.7982 - mse: 2101.7981 - mae: 31.7917 - val_loss: 17961.7140 - val_mse: 17961.7168 - val_mae: 36.4531\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 367us/step - loss: 2089.7345 - mse: 2089.7344 - mae: 30.0859 - val_loss: 17782.7068 - val_mse: 17782.7051 - val_mae: 35.8390\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 241us/step - loss: 1932.7443 - mse: 1932.7444 - mae: 30.0372 - val_loss: 17663.5526 - val_mse: 17663.5527 - val_mae: 35.8009\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 275us/step - loss: 1793.6256 - mse: 1793.6256 - mae: 27.8456 - val_loss: 17532.5399 - val_mse: 17532.5391 - val_mae: 35.9545\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 1963.0742 - mse: 1963.0741 - mae: 30.1457 - val_loss: 17769.7881 - val_mse: 17769.7891 - val_mae: 35.8521\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 1946.6601 - mse: 1946.6602 - mae: 29.7232 - val_loss: 17606.3867 - val_mse: 17606.3867 - val_mae: 35.8660\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 1911.5680 - mse: 1911.5679 - mae: 29.5203 - val_loss: 17626.6968 - val_mse: 17626.6973 - val_mae: 35.8680\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2063.6433 - mse: 2063.6433 - mae: 31.0804 - val_loss: 17766.7400 - val_mse: 17766.7422 - val_mae: 35.8919\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4259.8799 - mse: 4259.8799 - mae: 34.5753 - val_loss: 2108.0460 - val_mse: 2108.0459 - val_mae: 31.3923\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4345.3507 - mse: 4345.3506 - mae: 36.0202 - val_loss: 2241.6460 - val_mse: 2241.6460 - val_mae: 31.7474\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4261.9753 - mse: 4261.9756 - mae: 34.4148 - val_loss: 2222.9575 - val_mse: 2222.9573 - val_mae: 31.6889\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4254.9336 - mse: 4254.9326 - mae: 35.2621 - val_loss: 2200.5743 - val_mse: 2200.5742 - val_mae: 31.6222\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 252us/step - loss: 4074.2982 - mse: 4074.2983 - mae: 33.8262 - val_loss: 2238.9359 - val_mse: 2238.9363 - val_mae: 31.7261\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 344us/step - loss: 4419.2354 - mse: 4419.2354 - mae: 35.1039 - val_loss: 2273.7957 - val_mse: 2273.7957 - val_mae: 31.8177\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4169.0357 - mse: 4169.0356 - mae: 34.0742 - val_loss: 2225.4246 - val_mse: 2225.4248 - val_mae: 31.6834\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 252us/step - loss: 4166.1176 - mse: 4166.1177 - mae: 34.3468 - val_loss: 2263.4030 - val_mse: 2263.4033 - val_mae: 31.7810\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4079.5682 - mse: 4079.5686 - mae: 33.2944 - val_loss: 2160.0864 - val_mse: 2160.0864 - val_mae: 31.5021\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4001.9438 - mse: 4001.9438 - mae: 34.8757 - val_loss: 2271.2760 - val_mse: 2271.2756 - val_mae: 31.7979\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4213.0613 - mse: 4213.0615 - mae: 35.2305 - val_loss: 2314.1552 - val_mse: 2314.1550 - val_mae: 31.9287\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4296.3440 - mse: 4296.3438 - mae: 35.5578 - val_loss: 2240.2073 - val_mse: 2240.2070 - val_mae: 31.7123\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4037.0165 - mse: 4037.0168 - mae: 33.2498 - val_loss: 2212.3925 - val_mse: 2212.3928 - val_mae: 31.6353\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4185.7201 - mse: 4185.7207 - mae: 34.9463 - val_loss: 2286.4542 - val_mse: 2286.4541 - val_mae: 31.8319\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4381.2302 - mse: 4381.2305 - mae: 35.0724 - val_loss: 2336.7397 - val_mse: 2336.7395 - val_mae: 31.9884\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4237.7505 - mse: 4237.7505 - mae: 34.6193 - val_loss: 2334.3016 - val_mse: 2334.3018 - val_mae: 31.9793\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 271us/step - loss: 4136.5420 - mse: 4136.5420 - mae: 34.8911 - val_loss: 2293.5972 - val_mse: 2293.5972 - val_mae: 31.8498\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 362us/step - loss: 4259.5171 - mse: 4259.5176 - mae: 34.7914 - val_loss: 2287.9805 - val_mse: 2287.9805 - val_mae: 31.8323\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 4139.3603 - mse: 4139.3604 - mae: 33.7421 - val_loss: 2253.4047 - val_mse: 2253.4048 - val_mae: 31.7358\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 339us/step - loss: 4265.0659 - mse: 4265.0664 - mae: 34.8785 - val_loss: 2268.9243 - val_mse: 2268.9243 - val_mae: 31.7783\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 4236.3650 - mse: 4236.3647 - mae: 33.9459 - val_loss: 2326.2453 - val_mse: 2326.2456 - val_mae: 31.9398\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4214.0782 - mse: 4214.0776 - mae: 34.1752 - val_loss: 2268.3889 - val_mse: 2268.3889 - val_mae: 31.7730\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4154.7802 - mse: 4154.7803 - mae: 33.9650 - val_loss: 2283.3792 - val_mse: 2283.3792 - val_mae: 31.8154\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4179.7940 - mse: 4179.7944 - mae: 34.4473 - val_loss: 2270.3609 - val_mse: 2270.3611 - val_mae: 31.7758\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4153.9402 - mse: 4153.9399 - mae: 34.0961 - val_loss: 2251.6158 - val_mse: 2251.6157 - val_mae: 31.7214\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 265us/step - loss: 4260.4247 - mse: 4260.4253 - mae: 34.2926 - val_loss: 2360.0400 - val_mse: 2360.0400 - val_mae: 32.0349\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4155.6112 - mse: 4155.6108 - mae: 33.1599 - val_loss: 2305.7409 - val_mse: 2305.7407 - val_mae: 31.8696\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 260us/step - loss: 4235.1503 - mse: 4235.1499 - mae: 34.0526 - val_loss: 2295.9853 - val_mse: 2295.9854 - val_mae: 31.8387\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4193.7766 - mse: 4193.7769 - mae: 33.1140 - val_loss: 2283.1272 - val_mse: 2283.1270 - val_mae: 31.8012\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4179.8299 - mse: 4179.8296 - mae: 33.8744 - val_loss: 2274.7351 - val_mse: 2274.7354 - val_mae: 31.7741\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4063.6060 - mse: 4063.6062 - mae: 33.8354 - val_loss: 2291.6678 - val_mse: 2291.6677 - val_mae: 31.8173\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 366us/step - loss: 4155.7061 - mse: 4155.7061 - mae: 33.5062 - val_loss: 2302.3047 - val_mse: 2302.3049 - val_mae: 31.8476\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 359us/step - loss: 4047.0879 - mse: 4047.0874 - mae: 33.3340 - val_loss: 2246.6644 - val_mse: 2246.6646 - val_mae: 31.6888\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 258us/step - loss: 4094.1782 - mse: 4094.1782 - mae: 33.0176 - val_loss: 2258.3111 - val_mse: 2258.3115 - val_mae: 31.7194\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4173.7943 - mse: 4173.7939 - mae: 34.4653 - val_loss: 2275.1095 - val_mse: 2275.1096 - val_mae: 31.7664\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4108.3403 - mse: 4108.3398 - mae: 33.3100 - val_loss: 2245.5332 - val_mse: 2245.5332 - val_mae: 31.6803\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 4168.8089 - mse: 4168.8096 - mae: 34.0768 - val_loss: 2296.7431 - val_mse: 2296.7432 - val_mae: 31.8232\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4071.0955 - mse: 4071.0950 - mae: 33.7502 - val_loss: 2280.0176 - val_mse: 2280.0178 - val_mae: 31.7785\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 278us/step - loss: 4125.4022 - mse: 4125.4019 - mae: 33.2742 - val_loss: 2268.0594 - val_mse: 2268.0596 - val_mae: 31.7441\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 231us/step - loss: 4196.0500 - mse: 4196.0503 - mae: 33.8549 - val_loss: 2249.3707 - val_mse: 2249.3706 - val_mae: 31.6887\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 361us/step - loss: 4118.2175 - mse: 4118.2178 - mae: 33.2656 - val_loss: 2320.7298 - val_mse: 2320.7297 - val_mae: 31.8884\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4210.0086 - mse: 4210.0083 - mae: 33.9800 - val_loss: 2285.0558 - val_mse: 2285.0557 - val_mae: 31.7853\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 237us/step - loss: 4312.0702 - mse: 4312.0698 - mae: 33.8761 - val_loss: 2284.8642 - val_mse: 2284.8640 - val_mae: 31.7830\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 332us/step - loss: 4150.8405 - mse: 4150.8403 - mae: 33.4707 - val_loss: 2241.1170 - val_mse: 2241.1169 - val_mae: 31.6582\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 362us/step - loss: 4200.7398 - mse: 4200.7402 - mae: 34.5625 - val_loss: 2317.5689 - val_mse: 2317.5691 - val_mae: 31.8689\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 301us/step - loss: 4065.9412 - mse: 4065.9417 - mae: 33.2741 - val_loss: 2312.2622 - val_mse: 2312.2622 - val_mae: 31.8528\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 301us/step - loss: 4329.4368 - mse: 4329.4370 - mae: 34.4098 - val_loss: 2319.2491 - val_mse: 2319.2493 - val_mae: 31.8693\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 3954.9369 - mse: 3954.9365 - mae: 32.6733 - val_loss: 2265.4691 - val_mse: 2265.4692 - val_mae: 31.7144\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 4063.3599 - mse: 4063.3599 - mae: 33.1053 - val_loss: 2282.8509 - val_mse: 2282.8508 - val_mae: 31.7621\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 350us/step - loss: 4092.5775 - mse: 4092.5771 - mae: 33.7878 - val_loss: 2314.4527 - val_mse: 2314.4526 - val_mae: 31.8499\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4155.6887 - mse: 4155.6885 - mae: 34.0121 - val_loss: 2240.9846 - val_mse: 2240.9846 - val_mae: 31.6460\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4150.0768 - mse: 4150.0771 - mae: 33.9973 - val_loss: 2308.1861 - val_mse: 2308.1860 - val_mae: 31.8318\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4071.1018 - mse: 4071.1023 - mae: 33.7208 - val_loss: 2267.8673 - val_mse: 2267.8672 - val_mae: 31.7184\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 3862.3839 - mse: 3862.3838 - mae: 32.5674 - val_loss: 2253.5900 - val_mse: 2253.5901 - val_mae: 31.6811\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4057.6762 - mse: 4057.6763 - mae: 34.5914 - val_loss: 2295.9779 - val_mse: 2295.9780 - val_mae: 31.7982\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4103.5896 - mse: 4103.5898 - mae: 34.0115 - val_loss: 2308.7420 - val_mse: 2308.7419 - val_mae: 31.8326\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4099.6937 - mse: 4099.6938 - mae: 33.5613 - val_loss: 2293.7408 - val_mse: 2293.7405 - val_mae: 31.7911\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 3954.1500 - mse: 3954.1499 - mae: 33.2120 - val_loss: 2293.2625 - val_mse: 2293.2625 - val_mae: 31.7891\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4036.2602 - mse: 4036.2595 - mae: 33.1822 - val_loss: 2292.0319 - val_mse: 2292.0320 - val_mae: 31.7828\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 332us/step - loss: 4020.3213 - mse: 4020.3210 - mae: 33.1842 - val_loss: 2331.8249 - val_mse: 2331.8250 - val_mae: 31.8954\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4125.7989 - mse: 4125.7988 - mae: 33.6435 - val_loss: 2281.0605 - val_mse: 2281.0608 - val_mae: 31.7499\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 265us/step - loss: 3961.0907 - mse: 3961.0908 - mae: 33.2296 - val_loss: 2349.3759 - val_mse: 2349.3760 - val_mae: 31.9433\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 277us/step - loss: 4068.8566 - mse: 4068.8572 - mae: 32.4830 - val_loss: 2301.7815 - val_mse: 2301.7815 - val_mae: 31.8034\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 3896.9716 - mse: 3896.9729 - mae: 32.7664 - val_loss: 2254.6180 - val_mse: 2254.6179 - val_mae: 31.6735\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4123.5457 - mse: 4123.5459 - mae: 33.3450 - val_loss: 2267.8123 - val_mse: 2267.8120 - val_mae: 31.7065\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 3889.5979 - mse: 3889.5984 - mae: 32.4911 - val_loss: 2280.2329 - val_mse: 2280.2332 - val_mae: 31.7365\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 3925.7778 - mse: 3925.7781 - mae: 32.7921 - val_loss: 2322.7732 - val_mse: 2322.7732 - val_mae: 31.8558\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4102.3670 - mse: 4102.3672 - mae: 32.3829 - val_loss: 2256.4818 - val_mse: 2256.4817 - val_mae: 31.6700\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 287us/step - loss: 4162.9056 - mse: 4162.9053 - mae: 33.3464 - val_loss: 2290.3219 - val_mse: 2290.3220 - val_mae: 31.7618\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 229us/step - loss: 4044.5356 - mse: 4044.5354 - mae: 32.2385 - val_loss: 2330.9311 - val_mse: 2330.9312 - val_mae: 31.8781\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 218us/step - loss: 4104.7578 - mse: 4104.7578 - mae: 33.6099 - val_loss: 2357.5588 - val_mse: 2357.5588 - val_mae: 31.9532\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 271us/step - loss: 4130.2311 - mse: 4130.2305 - mae: 33.6075 - val_loss: 2301.6106 - val_mse: 2301.6106 - val_mae: 31.7884\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 359us/step - loss: 4030.5519 - mse: 4030.5522 - mae: 32.8954 - val_loss: 2290.3667 - val_mse: 2290.3665 - val_mae: 31.7581\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 3963.9260 - mse: 3963.9265 - mae: 34.1278 - val_loss: 2286.6098 - val_mse: 2286.6096 - val_mae: 31.7441\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 353us/step - loss: 4067.7649 - mse: 4067.7649 - mae: 32.7750 - val_loss: 2322.8374 - val_mse: 2322.8372 - val_mae: 31.8449\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 3941.2657 - mse: 3941.2654 - mae: 32.7610 - val_loss: 2289.0988 - val_mse: 2289.0986 - val_mae: 31.7493\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 4028.3702 - mse: 4028.3706 - mae: 32.9011 - val_loss: 2288.1487 - val_mse: 2288.1484 - val_mae: 31.7465\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 247us/step - loss: 4130.4405 - mse: 4130.4404 - mae: 32.5644 - val_loss: 2291.0413 - val_mse: 2291.0413 - val_mae: 31.7528\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 3923.7695 - mse: 3923.7698 - mae: 32.5067 - val_loss: 2278.7947 - val_mse: 2278.7947 - val_mae: 31.7194\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 3875.0872 - mse: 3875.0874 - mae: 32.2358 - val_loss: 2268.4171 - val_mse: 2268.4167 - val_mae: 31.6914\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3272.2894 - mse: 3272.2893 - mae: 32.2929 - val_loss: 1457.4482 - val_mse: 1457.4480 - val_mae: 25.8884\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3464.0854 - mse: 3464.0857 - mae: 33.0797 - val_loss: 1457.3757 - val_mse: 1457.3756 - val_mae: 25.9581\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 284us/step - loss: 3514.2110 - mse: 3514.2109 - mae: 33.2283 - val_loss: 1457.3562 - val_mse: 1457.3561 - val_mae: 25.9717\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3263.5689 - mse: 3263.5688 - mae: 31.7489 - val_loss: 1458.0786 - val_mse: 1458.0787 - val_mae: 26.0919\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3455.5942 - mse: 3455.5952 - mae: 33.4744 - val_loss: 1456.5894 - val_mse: 1456.5894 - val_mae: 25.5927\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 239us/step - loss: 3425.8302 - mse: 3425.8301 - mae: 33.0309 - val_loss: 1458.1213 - val_mse: 1458.1213 - val_mae: 25.9767\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3447.9621 - mse: 3447.9619 - mae: 32.9215 - val_loss: 1458.2451 - val_mse: 1458.2451 - val_mae: 25.2170\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3400.9817 - mse: 3400.9822 - mae: 32.6976 - val_loss: 1457.4800 - val_mse: 1457.4800 - val_mae: 25.6871\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3350.3882 - mse: 3350.3879 - mae: 32.8503 - val_loss: 1457.5186 - val_mse: 1457.5187 - val_mae: 25.6377\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3385.1550 - mse: 3385.1553 - mae: 32.3084 - val_loss: 1457.7440 - val_mse: 1457.7440 - val_mae: 25.7680\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3435.0376 - mse: 3435.0378 - mae: 32.8646 - val_loss: 1457.6508 - val_mse: 1457.6508 - val_mae: 25.6787\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3327.9910 - mse: 3327.9917 - mae: 31.8943 - val_loss: 1457.8797 - val_mse: 1457.8796 - val_mae: 25.4045\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3338.8317 - mse: 3338.8320 - mae: 32.4022 - val_loss: 1458.4187 - val_mse: 1458.4186 - val_mae: 25.7765\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3326.9266 - mse: 3326.9265 - mae: 33.0683 - val_loss: 1459.0778 - val_mse: 1459.0778 - val_mae: 25.8447\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3336.3619 - mse: 3336.3625 - mae: 32.1155 - val_loss: 1459.2402 - val_mse: 1459.2404 - val_mae: 25.6105\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 271us/step - loss: 3359.9702 - mse: 3359.9695 - mae: 32.8070 - val_loss: 1458.6681 - val_mse: 1458.6681 - val_mae: 25.4456\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3363.3644 - mse: 3363.3645 - mae: 32.6121 - val_loss: 1460.3993 - val_mse: 1460.3994 - val_mae: 26.0680\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3378.3147 - mse: 3378.3147 - mae: 31.8494 - val_loss: 1460.5720 - val_mse: 1460.5720 - val_mae: 26.0335\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 246us/step - loss: 3271.9942 - mse: 3271.9939 - mae: 32.8764 - val_loss: 1460.1845 - val_mse: 1460.1844 - val_mae: 25.7488\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 225us/step - loss: 3219.4461 - mse: 3219.4465 - mae: 31.9553 - val_loss: 1462.2912 - val_mse: 1462.2911 - val_mae: 26.1259\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 224us/step - loss: 3422.3276 - mse: 3422.3284 - mae: 32.8301 - val_loss: 1461.1064 - val_mse: 1461.1064 - val_mae: 25.7873\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 276us/step - loss: 3395.8720 - mse: 3395.8723 - mae: 32.1160 - val_loss: 1462.0448 - val_mse: 1462.0449 - val_mae: 26.0079\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 219us/step - loss: 3419.3067 - mse: 3419.3064 - mae: 32.6296 - val_loss: 1462.8748 - val_mse: 1462.8749 - val_mae: 26.1621\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3280.1660 - mse: 3280.1658 - mae: 31.9787 - val_loss: 1464.7860 - val_mse: 1464.7861 - val_mae: 26.3561\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3410.3295 - mse: 3410.3301 - mae: 32.3664 - val_loss: 1464.6510 - val_mse: 1464.6511 - val_mae: 26.3127\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 258us/step - loss: 3409.5451 - mse: 3409.5449 - mae: 32.7785 - val_loss: 1467.9352 - val_mse: 1467.9352 - val_mae: 26.6107\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3288.2357 - mse: 3288.2351 - mae: 31.9843 - val_loss: 1463.0978 - val_mse: 1463.0978 - val_mae: 25.7051\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 245us/step - loss: 3305.4359 - mse: 3305.4355 - mae: 31.7433 - val_loss: 1464.6113 - val_mse: 1464.6113 - val_mae: 25.9695\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 243us/step - loss: 3422.4484 - mse: 3422.4492 - mae: 33.3089 - val_loss: 1465.0101 - val_mse: 1465.0103 - val_mae: 26.0657\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 347us/step - loss: 3294.0721 - mse: 3294.0715 - mae: 32.1777 - val_loss: 1464.6746 - val_mse: 1464.6747 - val_mae: 25.7579\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3395.5315 - mse: 3395.5308 - mae: 32.5458 - val_loss: 1465.0970 - val_mse: 1465.0970 - val_mae: 25.6936\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 235us/step - loss: 3346.1691 - mse: 3346.1697 - mae: 32.0772 - val_loss: 1468.4936 - val_mse: 1468.4938 - val_mae: 26.4065\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 331us/step - loss: 3234.4200 - mse: 3234.4194 - mae: 31.9231 - val_loss: 1467.4892 - val_mse: 1467.4895 - val_mae: 26.1760\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 349us/step - loss: 3359.2316 - mse: 3359.2322 - mae: 32.7852 - val_loss: 1465.7346 - val_mse: 1465.7346 - val_mae: 25.7923\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3275.9873 - mse: 3275.9875 - mae: 31.8439 - val_loss: 1466.0952 - val_mse: 1466.0952 - val_mae: 25.9847\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 343us/step - loss: 3327.2534 - mse: 3327.2539 - mae: 32.6978 - val_loss: 1465.4223 - val_mse: 1465.4225 - val_mae: 25.5295\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3326.6403 - mse: 3326.6401 - mae: 32.5311 - val_loss: 1465.6647 - val_mse: 1465.6646 - val_mae: 25.7152\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3248.3654 - mse: 3248.3655 - mae: 32.0956 - val_loss: 1466.0291 - val_mse: 1466.0293 - val_mae: 26.0266\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3393.2858 - mse: 3393.2859 - mae: 32.2503 - val_loss: 1466.9209 - val_mse: 1466.9209 - val_mae: 25.2102\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3265.4832 - mse: 3265.4834 - mae: 31.3202 - val_loss: 1468.3322 - val_mse: 1468.3323 - val_mae: 26.1162\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3315.2687 - mse: 3315.2693 - mae: 31.9523 - val_loss: 1466.2277 - val_mse: 1466.2278 - val_mae: 25.6593\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3336.5899 - mse: 3336.5901 - mae: 31.8268 - val_loss: 1469.1745 - val_mse: 1469.1744 - val_mae: 26.2685\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3267.2916 - mse: 3267.2910 - mae: 32.4619 - val_loss: 1467.9320 - val_mse: 1467.9321 - val_mae: 25.9467\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3317.4188 - mse: 3317.4187 - mae: 31.6355 - val_loss: 1469.5903 - val_mse: 1469.5905 - val_mae: 26.0812\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3388.2054 - mse: 3388.2051 - mae: 32.3419 - val_loss: 1469.1502 - val_mse: 1469.1504 - val_mae: 25.7611\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3236.0357 - mse: 3236.0359 - mae: 31.7742 - val_loss: 1470.1031 - val_mse: 1470.1031 - val_mae: 26.1750\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3395.4612 - mse: 3395.4609 - mae: 32.7906 - val_loss: 1468.7943 - val_mse: 1468.7943 - val_mae: 25.8778\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3347.1681 - mse: 3347.1685 - mae: 32.1948 - val_loss: 1469.8213 - val_mse: 1469.8212 - val_mae: 25.8699\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3336.7604 - mse: 3336.7610 - mae: 31.6789 - val_loss: 1474.6346 - val_mse: 1474.6345 - val_mae: 26.5883\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 249us/step - loss: 3298.9640 - mse: 3298.9639 - mae: 31.6300 - val_loss: 1471.8348 - val_mse: 1471.8348 - val_mae: 26.1377\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 258us/step - loss: 3341.0239 - mse: 3341.0242 - mae: 32.0899 - val_loss: 1470.9050 - val_mse: 1470.9050 - val_mae: 25.9658\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3266.4983 - mse: 3266.4983 - mae: 31.7883 - val_loss: 1473.1979 - val_mse: 1473.1980 - val_mae: 26.3285\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3313.6483 - mse: 3313.6482 - mae: 32.3853 - val_loss: 1470.9579 - val_mse: 1470.9578 - val_mae: 25.8124\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3277.8654 - mse: 3277.8655 - mae: 31.6315 - val_loss: 1471.9845 - val_mse: 1471.9846 - val_mae: 26.1902\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 342us/step - loss: 3242.5262 - mse: 3242.5256 - mae: 32.0125 - val_loss: 1475.0524 - val_mse: 1475.0522 - val_mae: 26.5629\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3378.9780 - mse: 3378.9775 - mae: 32.2935 - val_loss: 1471.5253 - val_mse: 1471.5253 - val_mae: 25.8524\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3416.6859 - mse: 3416.6855 - mae: 32.6464 - val_loss: 1471.6344 - val_mse: 1471.6345 - val_mae: 25.7982\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3279.4087 - mse: 3279.4080 - mae: 31.7526 - val_loss: 1472.9776 - val_mse: 1472.9777 - val_mae: 26.2174\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3312.2977 - mse: 3312.2983 - mae: 31.9171 - val_loss: 1472.8294 - val_mse: 1472.8293 - val_mae: 26.1798\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3257.7615 - mse: 3257.7620 - mae: 31.8770 - val_loss: 1470.5532 - val_mse: 1470.5533 - val_mae: 25.6399\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3311.1771 - mse: 3311.1782 - mae: 31.2458 - val_loss: 1471.2154 - val_mse: 1471.2152 - val_mae: 25.8090\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3140.8661 - mse: 3140.8662 - mae: 31.4774 - val_loss: 1471.5297 - val_mse: 1471.5295 - val_mae: 26.0150\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3296.7574 - mse: 3296.7568 - mae: 31.4327 - val_loss: 1473.7954 - val_mse: 1473.7954 - val_mae: 26.3447\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3302.8039 - mse: 3302.8037 - mae: 31.6495 - val_loss: 1472.8439 - val_mse: 1472.8439 - val_mae: 25.3043\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3233.0902 - mse: 3233.0896 - mae: 30.8556 - val_loss: 1473.8548 - val_mse: 1473.8547 - val_mae: 26.2956\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3196.5974 - mse: 3196.5979 - mae: 31.6024 - val_loss: 1473.1681 - val_mse: 1473.1681 - val_mae: 26.0777\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 276us/step - loss: 3261.9864 - mse: 3261.9863 - mae: 31.7958 - val_loss: 1474.0500 - val_mse: 1474.0502 - val_mae: 26.0676\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3286.2103 - mse: 3286.2107 - mae: 31.2481 - val_loss: 1476.4117 - val_mse: 1476.4117 - val_mae: 26.3765\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3397.3598 - mse: 3397.3599 - mae: 32.0231 - val_loss: 1474.1399 - val_mse: 1474.1399 - val_mae: 25.6764\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3337.9991 - mse: 3337.9993 - mae: 32.0497 - val_loss: 1474.5325 - val_mse: 1474.5326 - val_mae: 26.1366\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3070.9928 - mse: 3070.9922 - mae: 31.2917 - val_loss: 1474.6118 - val_mse: 1474.6118 - val_mae: 26.1488\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3347.8199 - mse: 3347.8198 - mae: 31.6246 - val_loss: 1474.2100 - val_mse: 1474.2101 - val_mae: 25.9730\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3216.8346 - mse: 3216.8347 - mae: 31.8446 - val_loss: 1473.7075 - val_mse: 1473.7075 - val_mae: 25.8481\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3277.7219 - mse: 3277.7224 - mae: 32.2366 - val_loss: 1474.0277 - val_mse: 1474.0277 - val_mae: 25.7559\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 272us/step - loss: 3227.8434 - mse: 3227.8438 - mae: 31.7604 - val_loss: 1474.2437 - val_mse: 1474.2437 - val_mae: 25.7684\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 259us/step - loss: 3241.1177 - mse: 3241.1167 - mae: 31.7898 - val_loss: 1474.9662 - val_mse: 1474.9661 - val_mae: 26.1125\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 233us/step - loss: 3282.6467 - mse: 3282.6465 - mae: 31.6844 - val_loss: 1475.0453 - val_mse: 1475.0453 - val_mae: 26.1009\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 356us/step - loss: 3174.2132 - mse: 3174.2126 - mae: 30.4663 - val_loss: 1475.1531 - val_mse: 1475.1532 - val_mae: 26.2601\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 257us/step - loss: 3188.6296 - mse: 3188.6294 - mae: 31.0998 - val_loss: 1477.2172 - val_mse: 1477.2173 - val_mae: 26.5308\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 278us/step - loss: 3314.3454 - mse: 3314.3455 - mae: 31.4544 - val_loss: 1473.9504 - val_mse: 1473.9506 - val_mae: 26.0666\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 271us/step - loss: 2875.6780 - mse: 2875.6772 - mae: 30.5687 - val_loss: 1073.0336 - val_mse: 1073.0336 - val_mae: 24.1572\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 268us/step - loss: 2931.6834 - mse: 2931.6836 - mae: 31.1658 - val_loss: 1074.8351 - val_mse: 1074.8351 - val_mae: 23.7924\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 0s 235us/step - loss: 2830.2849 - mse: 2830.2856 - mae: 30.6969 - val_loss: 1072.8256 - val_mse: 1072.8258 - val_mae: 24.0337\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2917.4620 - mse: 2917.4624 - mae: 31.0141 - val_loss: 1073.0271 - val_mse: 1073.0272 - val_mae: 23.9392\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2906.8540 - mse: 2906.8550 - mae: 31.1152 - val_loss: 1074.6385 - val_mse: 1074.6385 - val_mae: 23.7505\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 0s 245us/step - loss: 2889.6293 - mse: 2889.6304 - mae: 30.7199 - val_loss: 1071.2996 - val_mse: 1071.2996 - val_mae: 24.1045\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2930.1367 - mse: 2930.1362 - mae: 31.4392 - val_loss: 1070.3229 - val_mse: 1070.3230 - val_mae: 24.5950\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2899.6378 - mse: 2899.6370 - mae: 31.0901 - val_loss: 1070.1305 - val_mse: 1070.1305 - val_mae: 24.0477\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2908.8366 - mse: 2908.8372 - mae: 30.8987 - val_loss: 1068.6250 - val_mse: 1068.6250 - val_mae: 24.2430\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2901.8902 - mse: 2901.8901 - mae: 30.9938 - val_loss: 1068.1054 - val_mse: 1068.1055 - val_mae: 24.1890\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2911.6807 - mse: 2911.6809 - mae: 30.6322 - val_loss: 1070.5687 - val_mse: 1070.5687 - val_mae: 23.7613\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 260us/step - loss: 2941.9355 - mse: 2941.9358 - mae: 31.0448 - val_loss: 1073.6913 - val_mse: 1073.6913 - val_mae: 23.5501\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2956.1052 - mse: 2956.1052 - mae: 30.8505 - val_loss: 1070.8791 - val_mse: 1070.8792 - val_mae: 23.7021\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 349us/step - loss: 2964.1016 - mse: 2964.1011 - mae: 31.5306 - val_loss: 1067.1132 - val_mse: 1067.1133 - val_mae: 24.2964\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2948.1964 - mse: 2948.1965 - mae: 31.8389 - val_loss: 1067.9748 - val_mse: 1067.9749 - val_mae: 23.8573\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2960.1062 - mse: 2960.1064 - mae: 31.1575 - val_loss: 1068.6505 - val_mse: 1068.6505 - val_mae: 23.7568\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2859.2269 - mse: 2859.2261 - mae: 30.9607 - val_loss: 1067.0662 - val_mse: 1067.0663 - val_mae: 23.8433\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2875.0865 - mse: 2875.0862 - mae: 30.9991 - val_loss: 1065.1750 - val_mse: 1065.1749 - val_mae: 24.0423\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2947.5681 - mse: 2947.5688 - mae: 31.3037 - val_loss: 1072.1127 - val_mse: 1072.1127 - val_mae: 23.4351\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 260us/step - loss: 2889.6632 - mse: 2889.6633 - mae: 30.4283 - val_loss: 1065.9997 - val_mse: 1065.9996 - val_mae: 23.8299\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2843.8706 - mse: 2843.8713 - mae: 30.5648 - val_loss: 1064.5068 - val_mse: 1064.5068 - val_mae: 24.0151\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2931.3577 - mse: 2931.3579 - mae: 30.8738 - val_loss: 1064.7711 - val_mse: 1064.7710 - val_mae: 23.9058\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2871.5613 - mse: 2871.5625 - mae: 30.7422 - val_loss: 1063.9005 - val_mse: 1063.9005 - val_mae: 23.9702\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 340us/step - loss: 2861.3495 - mse: 2861.3491 - mae: 31.1316 - val_loss: 1066.1437 - val_mse: 1066.1437 - val_mae: 23.6786\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2901.6542 - mse: 2901.6543 - mae: 30.8015 - val_loss: 1063.1690 - val_mse: 1063.1691 - val_mae: 24.0101\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 263us/step - loss: 2935.5559 - mse: 2935.5554 - mae: 31.2032 - val_loss: 1062.5500 - val_mse: 1062.5498 - val_mae: 24.0582\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2863.3887 - mse: 2863.3887 - mae: 30.4254 - val_loss: 1063.5431 - val_mse: 1063.5431 - val_mae: 23.8489\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 272us/step - loss: 2784.2039 - mse: 2784.2039 - mae: 30.5261 - val_loss: 1063.1204 - val_mse: 1063.1204 - val_mae: 23.9759\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 0s 248us/step - loss: 2800.7455 - mse: 2800.7454 - mae: 30.2538 - val_loss: 1062.1529 - val_mse: 1062.1530 - val_mae: 24.3646\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2870.3954 - mse: 2870.3950 - mae: 31.0232 - val_loss: 1062.6336 - val_mse: 1062.6337 - val_mae: 23.9616\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2897.4955 - mse: 2897.4961 - mae: 31.2432 - val_loss: 1061.9053 - val_mse: 1061.9052 - val_mae: 24.1844\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2865.0861 - mse: 2865.0864 - mae: 30.4002 - val_loss: 1063.6401 - val_mse: 1063.6400 - val_mae: 23.7225\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2904.5724 - mse: 2904.5718 - mae: 30.7688 - val_loss: 1062.8105 - val_mse: 1062.8105 - val_mae: 23.7837\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2911.2203 - mse: 2911.2207 - mae: 30.8903 - val_loss: 1065.1850 - val_mse: 1065.1849 - val_mae: 23.5532\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2842.0101 - mse: 2842.0098 - mae: 30.6937 - val_loss: 1062.2691 - val_mse: 1062.2692 - val_mae: 23.7551\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2810.7224 - mse: 2810.7214 - mae: 30.2028 - val_loss: 1061.1576 - val_mse: 1061.1576 - val_mae: 23.8292\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 351us/step - loss: 2829.5415 - mse: 2829.5410 - mae: 30.7535 - val_loss: 1058.9355 - val_mse: 1058.9354 - val_mae: 24.2634\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2810.7313 - mse: 2810.7322 - mae: 30.7099 - val_loss: 1059.1624 - val_mse: 1059.1626 - val_mae: 23.8979\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2918.7203 - mse: 2918.7205 - mae: 30.6610 - val_loss: 1059.5157 - val_mse: 1059.5157 - val_mae: 23.8228\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2825.6105 - mse: 2825.6106 - mae: 30.6385 - val_loss: 1058.6003 - val_mse: 1058.6002 - val_mae: 24.5783\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2839.3824 - mse: 2839.3828 - mae: 30.2673 - val_loss: 1058.0903 - val_mse: 1058.0903 - val_mae: 23.9958\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2846.0982 - mse: 2846.0981 - mae: 30.7050 - val_loss: 1057.0276 - val_mse: 1057.0276 - val_mae: 24.1508\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2858.7818 - mse: 2858.7820 - mae: 30.6795 - val_loss: 1057.1225 - val_mse: 1057.1227 - val_mae: 24.0337\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2852.5683 - mse: 2852.5684 - mae: 30.7644 - val_loss: 1056.9071 - val_mse: 1056.9072 - val_mae: 24.0929\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 0s 233us/step - loss: 2819.5741 - mse: 2819.5742 - mae: 30.6009 - val_loss: 1058.4276 - val_mse: 1058.4276 - val_mae: 23.7710\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 254us/step - loss: 2860.2080 - mse: 2860.2068 - mae: 30.6164 - val_loss: 1056.2806 - val_mse: 1056.2806 - val_mae: 24.0313\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2861.1252 - mse: 2861.1252 - mae: 30.8568 - val_loss: 1058.1934 - val_mse: 1058.1932 - val_mae: 23.7463\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2852.3366 - mse: 2852.3364 - mae: 30.5097 - val_loss: 1055.8909 - val_mse: 1055.8907 - val_mae: 24.0225\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2894.2014 - mse: 2894.2012 - mae: 31.1192 - val_loss: 1056.1098 - val_mse: 1056.1097 - val_mae: 23.8387\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2936.9482 - mse: 2936.9482 - mae: 30.8201 - val_loss: 1055.1679 - val_mse: 1055.1680 - val_mae: 23.8845\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2869.4854 - mse: 2869.4861 - mae: 30.7192 - val_loss: 1059.3561 - val_mse: 1059.3561 - val_mae: 23.4438\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2786.9461 - mse: 2786.9458 - mae: 30.2926 - val_loss: 1053.6995 - val_mse: 1053.6996 - val_mae: 23.8644\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2839.4456 - mse: 2839.4468 - mae: 30.6910 - val_loss: 1053.3320 - val_mse: 1053.3320 - val_mae: 24.0182\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2886.8020 - mse: 2886.8018 - mae: 30.6825 - val_loss: 1052.4539 - val_mse: 1052.4540 - val_mae: 24.4091\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2773.8589 - mse: 2773.8582 - mae: 30.4576 - val_loss: 1051.8757 - val_mse: 1051.8756 - val_mae: 24.0448\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2952.7593 - mse: 2952.7598 - mae: 30.4908 - val_loss: 1052.9491 - val_mse: 1052.9489 - val_mae: 23.6115\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2844.6762 - mse: 2844.6760 - mae: 30.7667 - val_loss: 1053.2631 - val_mse: 1053.2633 - val_mae: 23.5873\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2816.6815 - mse: 2816.6819 - mae: 30.3177 - val_loss: 1050.3850 - val_mse: 1050.3850 - val_mae: 23.9952\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2961.9598 - mse: 2961.9585 - mae: 30.7065 - val_loss: 1052.4326 - val_mse: 1052.4326 - val_mae: 23.6380\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2839.7395 - mse: 2839.7395 - mae: 30.7054 - val_loss: 1049.7624 - val_mse: 1049.7625 - val_mae: 23.8651\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 349us/step - loss: 2866.5286 - mse: 2866.5281 - mae: 30.5497 - val_loss: 1048.9349 - val_mse: 1048.9348 - val_mae: 23.7814\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2884.7433 - mse: 2884.7437 - mae: 30.4600 - val_loss: 1048.3842 - val_mse: 1048.3843 - val_mae: 23.8471\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2757.9259 - mse: 2757.9258 - mae: 30.1292 - val_loss: 1046.5652 - val_mse: 1046.5652 - val_mae: 24.1606\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 0s 251us/step - loss: 2837.3733 - mse: 2837.3733 - mae: 29.8971 - val_loss: 1045.7558 - val_mse: 1045.7557 - val_mae: 24.1061\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2864.7653 - mse: 2864.7654 - mae: 30.5504 - val_loss: 1044.9501 - val_mse: 1044.9502 - val_mae: 23.9663\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 372us/step - loss: 2905.5010 - mse: 2905.5017 - mae: 30.5150 - val_loss: 1045.5549 - val_mse: 1045.5548 - val_mae: 23.6706\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2870.3240 - mse: 2870.3228 - mae: 30.0917 - val_loss: 1044.0175 - val_mse: 1044.0176 - val_mae: 23.7962\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2877.2604 - mse: 2877.2612 - mae: 30.9228 - val_loss: 1048.4000 - val_mse: 1048.3999 - val_mae: 23.3747\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2820.8590 - mse: 2820.8577 - mae: 30.2101 - val_loss: 1042.4703 - val_mse: 1042.4702 - val_mae: 24.0221\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 0s 235us/step - loss: 2838.8118 - mse: 2838.8118 - mae: 30.5449 - val_loss: 1044.6137 - val_mse: 1044.6138 - val_mae: 23.5488\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2779.0265 - mse: 2779.0271 - mae: 30.2813 - val_loss: 1041.9681 - val_mse: 1041.9680 - val_mae: 24.0632\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2867.0504 - mse: 2867.0498 - mae: 30.8940 - val_loss: 1041.1398 - val_mse: 1041.1399 - val_mae: 23.8682\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2771.3206 - mse: 2771.3203 - mae: 29.9814 - val_loss: 1039.9386 - val_mse: 1039.9387 - val_mae: 23.9478\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2830.9066 - mse: 2830.9072 - mae: 30.1096 - val_loss: 1040.9449 - val_mse: 1040.9449 - val_mae: 23.7788\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2841.9522 - mse: 2841.9514 - mae: 30.2399 - val_loss: 1040.3538 - val_mse: 1040.3538 - val_mae: 23.9468\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2841.8022 - mse: 2841.8025 - mae: 30.1687 - val_loss: 1041.5567 - val_mse: 1041.5566 - val_mae: 23.6315\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2834.1464 - mse: 2834.1462 - mae: 30.2445 - val_loss: 1043.9915 - val_mse: 1043.9915 - val_mae: 23.3382\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2910.6268 - mse: 2910.6274 - mae: 30.4054 - val_loss: 1038.4299 - val_mse: 1038.4298 - val_mae: 23.7821\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2790.1872 - mse: 2790.1873 - mae: 29.7482 - val_loss: 1037.8383 - val_mse: 1037.8384 - val_mae: 24.2697\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 253us/step - loss: 2809.0443 - mse: 2809.0442 - mae: 30.0794 - val_loss: 1036.6050 - val_mse: 1036.6050 - val_mae: 24.1089\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2460.0569 - mse: 2460.0562 - mae: 29.1071 - val_loss: 1518.6538 - val_mse: 1518.6541 - val_mae: 27.5892\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2528.5362 - mse: 2528.5359 - mae: 30.0439 - val_loss: 1527.9242 - val_mse: 1527.9242 - val_mae: 27.2174\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2489.9833 - mse: 2489.9834 - mae: 28.8129 - val_loss: 1522.2257 - val_mse: 1522.2258 - val_mae: 27.2829\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2534.7174 - mse: 2534.7180 - mae: 29.4014 - val_loss: 1511.5429 - val_mse: 1511.5427 - val_mae: 27.5378\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2550.5686 - mse: 2550.5691 - mae: 29.6404 - val_loss: 1524.7131 - val_mse: 1524.7131 - val_mae: 27.1023\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2499.1696 - mse: 2499.1694 - mae: 29.4893 - val_loss: 1513.3355 - val_mse: 1513.3356 - val_mae: 27.3735\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 269us/step - loss: 2523.3331 - mse: 2523.3340 - mae: 29.4994 - val_loss: 1505.7338 - val_mse: 1505.7338 - val_mae: 27.5866\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 349us/step - loss: 2581.1167 - mse: 2581.1172 - mae: 29.5853 - val_loss: 1523.7804 - val_mse: 1523.7804 - val_mae: 27.0405\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2557.6473 - mse: 2557.6477 - mae: 30.1912 - val_loss: 1510.2172 - val_mse: 1510.2172 - val_mae: 27.3185\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2528.9254 - mse: 2528.9250 - mae: 29.5765 - val_loss: 1514.1558 - val_mse: 1514.1556 - val_mae: 27.1599\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2523.8525 - mse: 2523.8525 - mae: 29.8174 - val_loss: 1508.3729 - val_mse: 1508.3729 - val_mae: 27.2678\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2566.9945 - mse: 2566.9951 - mae: 29.6686 - val_loss: 1505.6150 - val_mse: 1505.6149 - val_mae: 27.3194\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2597.5949 - mse: 2597.5957 - mae: 29.7044 - val_loss: 1508.6748 - val_mse: 1508.6747 - val_mae: 27.1358\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 352us/step - loss: 2558.9597 - mse: 2558.9597 - mae: 29.4722 - val_loss: 1507.4304 - val_mse: 1507.4301 - val_mae: 27.1229\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 373us/step - loss: 2533.6799 - mse: 2533.6802 - mae: 29.1661 - val_loss: 1498.0113 - val_mse: 1498.0114 - val_mae: 27.3638\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2444.4895 - mse: 2444.4897 - mae: 29.1279 - val_loss: 1492.2539 - val_mse: 1492.2539 - val_mae: 27.4608\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2464.8140 - mse: 2464.8145 - mae: 29.2093 - val_loss: 1491.0279 - val_mse: 1491.0280 - val_mae: 27.4147\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2577.8620 - mse: 2577.8621 - mae: 29.9821 - val_loss: 1509.2660 - val_mse: 1509.2659 - val_mae: 26.8797\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2512.9286 - mse: 2512.9277 - mae: 29.4662 - val_loss: 1496.0576 - val_mse: 1496.0575 - val_mae: 27.1989\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2494.3623 - mse: 2494.3618 - mae: 29.6591 - val_loss: 1490.5469 - val_mse: 1490.5470 - val_mae: 27.3215\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2518.0623 - mse: 2518.0627 - mae: 29.0888 - val_loss: 1481.7299 - val_mse: 1481.7297 - val_mae: 27.6519\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2451.7484 - mse: 2451.7483 - mae: 28.9829 - val_loss: 1497.2184 - val_mse: 1497.2184 - val_mae: 27.0742\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2502.1123 - mse: 2502.1128 - mae: 29.1866 - val_loss: 1495.6964 - val_mse: 1495.6964 - val_mae: 27.0731\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2527.8007 - mse: 2527.8015 - mae: 30.0546 - val_loss: 1500.2895 - val_mse: 1500.2896 - val_mae: 26.8840\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2506.3340 - mse: 2506.3340 - mae: 29.2120 - val_loss: 1494.3032 - val_mse: 1494.3030 - val_mae: 26.9976\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 328us/step - loss: 2438.3848 - mse: 2438.3835 - mae: 28.6997 - val_loss: 1497.3646 - val_mse: 1497.3646 - val_mae: 26.8780\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2596.0174 - mse: 2596.0176 - mae: 29.8596 - val_loss: 1502.7121 - val_mse: 1502.7120 - val_mae: 26.7088\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2484.0701 - mse: 2484.0696 - mae: 29.4501 - val_loss: 1482.8363 - val_mse: 1482.8364 - val_mae: 27.1828\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2491.4367 - mse: 2491.4358 - mae: 29.8204 - val_loss: 1493.1239 - val_mse: 1493.1238 - val_mae: 26.8424\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2459.1168 - mse: 2459.1169 - mae: 28.9535 - val_loss: 1478.2857 - val_mse: 1478.2856 - val_mae: 27.3482\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2541.3522 - mse: 2541.3521 - mae: 29.2414 - val_loss: 1486.4991 - val_mse: 1486.4990 - val_mae: 27.1021\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2526.2327 - mse: 2526.2329 - mae: 29.4148 - val_loss: 1489.3912 - val_mse: 1489.3914 - val_mae: 26.9950\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2492.9542 - mse: 2492.9551 - mae: 29.1685 - val_loss: 1487.2083 - val_mse: 1487.2085 - val_mae: 27.0037\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2503.2364 - mse: 2503.2366 - mae: 29.4780 - val_loss: 1483.5250 - val_mse: 1483.5247 - val_mae: 27.0728\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2559.2974 - mse: 2559.2979 - mae: 29.6090 - val_loss: 1483.9112 - val_mse: 1483.9111 - val_mae: 27.0791\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2572.7225 - mse: 2572.7219 - mae: 29.7504 - val_loss: 1497.3973 - val_mse: 1497.3972 - val_mae: 26.7613\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2469.7036 - mse: 2469.7039 - mae: 28.8412 - val_loss: 1477.5052 - val_mse: 1477.5054 - val_mae: 27.3246\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 330us/step - loss: 2500.1459 - mse: 2500.1460 - mae: 29.3452 - val_loss: 1479.2816 - val_mse: 1479.2815 - val_mae: 27.1909\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2561.9837 - mse: 2561.9836 - mae: 29.4639 - val_loss: 1486.2469 - val_mse: 1486.2468 - val_mae: 26.9408\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 343us/step - loss: 2562.5469 - mse: 2562.5479 - mae: 29.4308 - val_loss: 1481.5512 - val_mse: 1481.5514 - val_mae: 27.0136\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2556.5261 - mse: 2556.5266 - mae: 29.4828 - val_loss: 1480.7900 - val_mse: 1480.7900 - val_mae: 27.1265\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2429.7064 - mse: 2429.7068 - mae: 29.0362 - val_loss: 1479.4168 - val_mse: 1479.4169 - val_mae: 27.1143\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2506.2027 - mse: 2506.2019 - mae: 29.3660 - val_loss: 1486.4640 - val_mse: 1486.4640 - val_mae: 26.8744\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2461.3183 - mse: 2461.3184 - mae: 28.9545 - val_loss: 1478.6610 - val_mse: 1478.6611 - val_mae: 27.0716\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2504.0989 - mse: 2504.0991 - mae: 29.2872 - val_loss: 1478.9849 - val_mse: 1478.9851 - val_mae: 27.0792\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2479.5394 - mse: 2479.5396 - mae: 29.1744 - val_loss: 1480.6007 - val_mse: 1480.6007 - val_mae: 26.9988\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2430.8546 - mse: 2430.8547 - mae: 28.9169 - val_loss: 1478.1334 - val_mse: 1478.1333 - val_mae: 27.0175\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2502.0917 - mse: 2502.0916 - mae: 29.3977 - val_loss: 1476.7525 - val_mse: 1476.7524 - val_mae: 27.1130\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2550.1790 - mse: 2550.1790 - mae: 29.3516 - val_loss: 1491.4853 - val_mse: 1491.4854 - val_mae: 26.6998\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 334us/step - loss: 2529.3062 - mse: 2529.3059 - mae: 29.1939 - val_loss: 1485.2074 - val_mse: 1485.2075 - val_mae: 26.7885\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2454.2868 - mse: 2454.2866 - mae: 28.9784 - val_loss: 1477.5825 - val_mse: 1477.5825 - val_mae: 26.9747\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2468.6393 - mse: 2468.6384 - mae: 29.0532 - val_loss: 1487.6227 - val_mse: 1487.6224 - val_mae: 26.6782\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2503.2800 - mse: 2503.2791 - mae: 29.2360 - val_loss: 1471.9931 - val_mse: 1471.9930 - val_mae: 27.0885\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 260us/step - loss: 2538.7470 - mse: 2538.7476 - mae: 29.5833 - val_loss: 1475.0899 - val_mse: 1475.0900 - val_mae: 26.9766\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2464.3363 - mse: 2464.3364 - mae: 28.7224 - val_loss: 1475.2490 - val_mse: 1475.2489 - val_mae: 26.9377\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2498.3731 - mse: 2498.3738 - mae: 29.1734 - val_loss: 1468.1380 - val_mse: 1468.1381 - val_mae: 27.1167\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 336us/step - loss: 2484.2548 - mse: 2484.2537 - mae: 28.7691 - val_loss: 1470.9718 - val_mse: 1470.9719 - val_mae: 27.0311\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2567.8784 - mse: 2567.8789 - mae: 29.5505 - val_loss: 1479.6946 - val_mse: 1479.6946 - val_mae: 26.7993\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 341us/step - loss: 2414.2885 - mse: 2414.2878 - mae: 28.7851 - val_loss: 1475.8846 - val_mse: 1475.8846 - val_mae: 26.8806\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2522.4951 - mse: 2522.4956 - mae: 29.3706 - val_loss: 1481.5033 - val_mse: 1481.5033 - val_mae: 26.7189\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2419.0618 - mse: 2419.0618 - mae: 28.8062 - val_loss: 1477.0693 - val_mse: 1477.0692 - val_mae: 26.8031\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2495.8957 - mse: 2495.8953 - mae: 29.5662 - val_loss: 1474.3032 - val_mse: 1474.3032 - val_mae: 26.8760\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 265us/step - loss: 2464.8555 - mse: 2464.8560 - mae: 29.3017 - val_loss: 1485.8146 - val_mse: 1485.8147 - val_mae: 26.5898\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2499.8037 - mse: 2499.8035 - mae: 28.9902 - val_loss: 1472.9002 - val_mse: 1472.8999 - val_mae: 26.9494\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 276us/step - loss: 2495.1444 - mse: 2495.1453 - mae: 29.1300 - val_loss: 1474.0629 - val_mse: 1474.0627 - val_mae: 26.9263\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2464.3336 - mse: 2464.3333 - mae: 29.0994 - val_loss: 1477.3357 - val_mse: 1477.3358 - val_mae: 26.8081\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2467.5048 - mse: 2467.5051 - mae: 28.8401 - val_loss: 1473.9808 - val_mse: 1473.9807 - val_mae: 26.8310\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 336us/step - loss: 2454.0638 - mse: 2454.0647 - mae: 28.7988 - val_loss: 1476.7793 - val_mse: 1476.7792 - val_mae: 26.7826\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2508.5156 - mse: 2508.5154 - mae: 29.0294 - val_loss: 1479.4398 - val_mse: 1479.4399 - val_mae: 26.7202\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 342us/step - loss: 2516.0259 - mse: 2516.0259 - mae: 29.1506 - val_loss: 1476.3387 - val_mse: 1476.3386 - val_mae: 26.7522\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2462.4027 - mse: 2462.4026 - mae: 28.9510 - val_loss: 1476.8694 - val_mse: 1476.8696 - val_mae: 26.7496\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2549.8341 - mse: 2549.8342 - mae: 29.5147 - val_loss: 1475.2002 - val_mse: 1475.2003 - val_mae: 26.8503\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2524.2831 - mse: 2524.2830 - mae: 29.5091 - val_loss: 1473.5044 - val_mse: 1473.5045 - val_mae: 26.8729\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2443.1788 - mse: 2443.1782 - mae: 28.9713 - val_loss: 1465.8931 - val_mse: 1465.8933 - val_mae: 27.0572\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2467.0544 - mse: 2467.0542 - mae: 29.1078 - val_loss: 1470.3463 - val_mse: 1470.3463 - val_mae: 26.9457\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2480.6548 - mse: 2480.6545 - mae: 29.0884 - val_loss: 1484.7727 - val_mse: 1484.7727 - val_mae: 26.5769\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2461.8469 - mse: 2461.8464 - mae: 28.6988 - val_loss: 1467.3428 - val_mse: 1467.3429 - val_mae: 27.0366\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 327us/step - loss: 2379.3063 - mse: 2379.3071 - mae: 28.6726 - val_loss: 1469.3638 - val_mse: 1469.3638 - val_mae: 26.8720\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2447.9302 - mse: 2447.9302 - mae: 28.8862 - val_loss: 1467.7433 - val_mse: 1467.7429 - val_mae: 26.8798\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2425.7236 - mse: 2425.7239 - mae: 29.1183 - val_loss: 1471.4082 - val_mse: 1471.4084 - val_mae: 26.7346\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2304.5952 - mse: 2304.5945 - mae: 29.1877 - val_loss: 3691.3429 - val_mse: 3691.3428 - val_mae: 23.5869\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 327us/step - loss: 2414.8308 - mse: 2414.8313 - mae: 29.5127 - val_loss: 3690.8591 - val_mse: 3690.8584 - val_mae: 23.7442\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2322.3625 - mse: 2322.3621 - mae: 29.1489 - val_loss: 3690.6060 - val_mse: 3690.6050 - val_mae: 23.7267\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2382.4006 - mse: 2382.4001 - mae: 29.4118 - val_loss: 3691.2353 - val_mse: 3691.2349 - val_mae: 23.8033\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2330.3350 - mse: 2330.3350 - mae: 29.0499 - val_loss: 3693.6082 - val_mse: 3693.6077 - val_mae: 24.0051\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2318.0262 - mse: 2318.0264 - mae: 28.9236 - val_loss: 3697.0610 - val_mse: 3697.0613 - val_mae: 24.0567\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2433.1050 - mse: 2433.1045 - mae: 29.8059 - val_loss: 3697.1466 - val_mse: 3697.1462 - val_mae: 23.8261\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2388.0791 - mse: 2388.0786 - mae: 29.5968 - val_loss: 3698.1597 - val_mse: 3698.1589 - val_mae: 23.9827\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2424.1627 - mse: 2424.1626 - mae: 29.7260 - val_loss: 3698.3118 - val_mse: 3698.3115 - val_mae: 23.8434\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2354.5718 - mse: 2354.5723 - mae: 29.3425 - val_loss: 3700.3155 - val_mse: 3700.3162 - val_mae: 24.0804\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2268.9471 - mse: 2268.9470 - mae: 28.6586 - val_loss: 3697.5324 - val_mse: 3697.5325 - val_mae: 23.3841\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2362.8467 - mse: 2362.8469 - mae: 29.3064 - val_loss: 3699.8951 - val_mse: 3699.8945 - val_mae: 24.2107\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2358.9228 - mse: 2358.9226 - mae: 29.3441 - val_loss: 3698.7098 - val_mse: 3698.7095 - val_mae: 23.9727\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2330.9797 - mse: 2330.9790 - mae: 29.0825 - val_loss: 3697.1809 - val_mse: 3697.1812 - val_mae: 23.8112\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2380.7706 - mse: 2380.7703 - mae: 29.2359 - val_loss: 3699.0903 - val_mse: 3699.0911 - val_mae: 24.3687\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2418.6670 - mse: 2418.6670 - mae: 29.2114 - val_loss: 3695.8378 - val_mse: 3695.8381 - val_mae: 23.9828\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2346.4301 - mse: 2346.4299 - mae: 29.3024 - val_loss: 3696.7467 - val_mse: 3696.7456 - val_mae: 24.1353\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 330us/step - loss: 2295.4804 - mse: 2295.4802 - mae: 29.0227 - val_loss: 3698.6927 - val_mse: 3698.6931 - val_mae: 24.0137\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2364.8938 - mse: 2364.8936 - mae: 29.2982 - val_loss: 3696.5162 - val_mse: 3696.5161 - val_mae: 23.7493\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2409.9377 - mse: 2409.9380 - mae: 29.4780 - val_loss: 3693.8542 - val_mse: 3693.8538 - val_mae: 23.5561\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2342.9832 - mse: 2342.9834 - mae: 29.1266 - val_loss: 3698.1840 - val_mse: 3698.1831 - val_mae: 24.2002\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2284.3359 - mse: 2284.3362 - mae: 28.9748 - val_loss: 3698.8321 - val_mse: 3698.8315 - val_mae: 24.1287\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2341.5483 - mse: 2341.5479 - mae: 29.4884 - val_loss: 3698.1585 - val_mse: 3698.1582 - val_mae: 23.7330\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2385.1618 - mse: 2385.1611 - mae: 29.2526 - val_loss: 3698.7842 - val_mse: 3698.7844 - val_mae: 23.7935\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2338.8914 - mse: 2338.8906 - mae: 29.2945 - val_loss: 3699.2155 - val_mse: 3699.2156 - val_mae: 23.8681\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2427.1432 - mse: 2427.1428 - mae: 29.6903 - val_loss: 3703.0491 - val_mse: 3703.0488 - val_mae: 24.6003\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2318.9303 - mse: 2318.9307 - mae: 28.9262 - val_loss: 3700.2503 - val_mse: 3700.2507 - val_mae: 24.3733\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 346us/step - loss: 2287.2123 - mse: 2287.2124 - mae: 28.7155 - val_loss: 3696.8399 - val_mse: 3696.8401 - val_mae: 23.7379\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2369.6912 - mse: 2369.6904 - mae: 29.4376 - val_loss: 3695.9714 - val_mse: 3695.9702 - val_mae: 23.5137\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2337.2579 - mse: 2337.2583 - mae: 29.3662 - val_loss: 3695.1992 - val_mse: 3695.1992 - val_mae: 23.3251\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2336.0840 - mse: 2336.0845 - mae: 28.8029 - val_loss: 3697.0205 - val_mse: 3697.0200 - val_mae: 23.9573\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2359.1615 - mse: 2359.1611 - mae: 29.3930 - val_loss: 3694.9354 - val_mse: 3694.9348 - val_mae: 23.3159\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2353.4935 - mse: 2353.4934 - mae: 29.2214 - val_loss: 3696.5250 - val_mse: 3696.5254 - val_mae: 23.8948\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2380.1103 - mse: 2380.1091 - mae: 29.2057 - val_loss: 3694.4413 - val_mse: 3694.4412 - val_mae: 23.5703\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2306.5913 - mse: 2306.5913 - mae: 28.5987 - val_loss: 3694.3456 - val_mse: 3694.3457 - val_mae: 23.5942\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2336.4084 - mse: 2336.4082 - mae: 29.2996 - val_loss: 3694.8175 - val_mse: 3694.8174 - val_mae: 23.7306\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2341.0302 - mse: 2341.0298 - mae: 29.2917 - val_loss: 3696.5669 - val_mse: 3696.5669 - val_mae: 23.5314\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2363.7714 - mse: 2363.7715 - mae: 29.0556 - val_loss: 3696.1158 - val_mse: 3696.1165 - val_mae: 23.8143\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2360.3313 - mse: 2360.3313 - mae: 29.1548 - val_loss: 3692.4543 - val_mse: 3692.4534 - val_mae: 23.6722\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2301.4239 - mse: 2301.4236 - mae: 28.7742 - val_loss: 3692.5027 - val_mse: 3692.5027 - val_mae: 23.8064\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2300.0521 - mse: 2300.0530 - mae: 29.0410 - val_loss: 3690.3081 - val_mse: 3690.3083 - val_mae: 23.4745\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2334.2998 - mse: 2334.2991 - mae: 28.8880 - val_loss: 3693.6195 - val_mse: 3693.6191 - val_mae: 24.0397\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2327.8178 - mse: 2327.8181 - mae: 29.0428 - val_loss: 3692.3407 - val_mse: 3692.3411 - val_mae: 23.6905\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2291.9644 - mse: 2291.9646 - mae: 28.8673 - val_loss: 3691.6052 - val_mse: 3691.6045 - val_mae: 23.6629\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2365.3532 - mse: 2365.3535 - mae: 29.4544 - val_loss: 3691.5977 - val_mse: 3691.5981 - val_mae: 23.8053\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2349.6441 - mse: 2349.6443 - mae: 29.2819 - val_loss: 3694.6271 - val_mse: 3694.6279 - val_mae: 24.1006\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2349.6368 - mse: 2349.6370 - mae: 29.3308 - val_loss: 3691.5460 - val_mse: 3691.5466 - val_mae: 23.6357\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 249us/step - loss: 2347.5759 - mse: 2347.5767 - mae: 28.8319 - val_loss: 3693.5240 - val_mse: 3693.5247 - val_mae: 23.8877\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2318.1808 - mse: 2318.1812 - mae: 29.3059 - val_loss: 3691.1893 - val_mse: 3691.1880 - val_mae: 23.7729\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2315.7291 - mse: 2315.7295 - mae: 28.8003 - val_loss: 3692.5241 - val_mse: 3692.5237 - val_mae: 24.0699\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2351.2391 - mse: 2351.2385 - mae: 29.0603 - val_loss: 3692.0274 - val_mse: 3692.0269 - val_mae: 23.9738\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2322.4776 - mse: 2322.4773 - mae: 29.1306 - val_loss: 3691.4532 - val_mse: 3691.4524 - val_mae: 23.7835\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2348.8145 - mse: 2348.8147 - mae: 28.9494 - val_loss: 3693.1472 - val_mse: 3693.1465 - val_mae: 23.9183\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2333.4843 - mse: 2333.4839 - mae: 29.1141 - val_loss: 3692.6981 - val_mse: 3692.6987 - val_mae: 23.6302\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2300.7741 - mse: 2300.7739 - mae: 28.8994 - val_loss: 3695.5594 - val_mse: 3695.5596 - val_mae: 24.1909\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2368.7188 - mse: 2368.7185 - mae: 28.7621 - val_loss: 3694.2380 - val_mse: 3694.2380 - val_mae: 23.5762\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2370.1132 - mse: 2370.1133 - mae: 29.2849 - val_loss: 3693.7754 - val_mse: 3693.7759 - val_mae: 24.0217\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2291.4694 - mse: 2291.4683 - mae: 28.7322 - val_loss: 3695.5785 - val_mse: 3695.5793 - val_mae: 24.1878\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2319.8181 - mse: 2319.8179 - mae: 28.9492 - val_loss: 3692.5805 - val_mse: 3692.5806 - val_mae: 24.0104\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 341us/step - loss: 2322.0916 - mse: 2322.0918 - mae: 28.9114 - val_loss: 3693.6561 - val_mse: 3693.6560 - val_mae: 24.2874\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2333.2881 - mse: 2333.2878 - mae: 29.0101 - val_loss: 3692.9453 - val_mse: 3692.9460 - val_mae: 24.1006\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2324.5371 - mse: 2324.5369 - mae: 28.8867 - val_loss: 3691.2281 - val_mse: 3691.2285 - val_mae: 23.9606\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2272.8581 - mse: 2272.8577 - mae: 29.1663 - val_loss: 3690.8868 - val_mse: 3690.8865 - val_mae: 23.9888\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2338.5487 - mse: 2338.5493 - mae: 29.0208 - val_loss: 3689.5787 - val_mse: 3689.5786 - val_mae: 24.0786\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2330.0745 - mse: 2330.0737 - mae: 28.7323 - val_loss: 3690.1078 - val_mse: 3690.1072 - val_mae: 24.2909\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2279.7862 - mse: 2279.7864 - mae: 28.7991 - val_loss: 3690.0800 - val_mse: 3690.0806 - val_mae: 24.2021\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2308.5715 - mse: 2308.5715 - mae: 28.8424 - val_loss: 3690.0685 - val_mse: 3690.0684 - val_mae: 23.6441\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 238us/step - loss: 2379.5558 - mse: 2379.5559 - mae: 29.5208 - val_loss: 3689.9236 - val_mse: 3689.9241 - val_mae: 23.7468\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2345.4602 - mse: 2345.4602 - mae: 28.8311 - val_loss: 3692.0078 - val_mse: 3692.0071 - val_mae: 24.0721\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 337us/step - loss: 2331.0334 - mse: 2331.0337 - mae: 29.2250 - val_loss: 3691.0661 - val_mse: 3691.0662 - val_mae: 24.1548\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2357.5565 - mse: 2357.5562 - mae: 29.0793 - val_loss: 3691.1777 - val_mse: 3691.1775 - val_mae: 23.8671\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2288.0127 - mse: 2288.0127 - mae: 28.6752 - val_loss: 3691.7939 - val_mse: 3691.7932 - val_mae: 24.3912\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2346.3389 - mse: 2346.3391 - mae: 28.8778 - val_loss: 3689.5307 - val_mse: 3689.5310 - val_mae: 23.7526\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2301.0725 - mse: 2301.0730 - mae: 28.7273 - val_loss: 3688.2715 - val_mse: 3688.2710 - val_mae: 23.7256\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 331us/step - loss: 2270.0477 - mse: 2270.0474 - mae: 29.0765 - val_loss: 3691.6968 - val_mse: 3691.6970 - val_mae: 23.8384\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2323.9020 - mse: 2323.9026 - mae: 29.0657 - val_loss: 3693.7326 - val_mse: 3693.7329 - val_mae: 24.1981\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2287.0019 - mse: 2287.0024 - mae: 28.8906 - val_loss: 3690.7862 - val_mse: 3690.7876 - val_mae: 23.9997\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2308.2577 - mse: 2308.2573 - mae: 28.7332 - val_loss: 3692.1568 - val_mse: 3692.1577 - val_mae: 24.3449\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2317.5524 - mse: 2317.5527 - mae: 28.7932 - val_loss: 3690.3510 - val_mse: 3690.3508 - val_mae: 23.6229\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2380.2486 - mse: 2380.2490 - mae: 28.9982 - val_loss: 3692.2388 - val_mse: 3692.2383 - val_mae: 24.1276\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 336us/step - loss: 2728.5429 - mse: 2728.5420 - mae: 28.6967 - val_loss: 2203.4390 - val_mse: 2203.4390 - val_mae: 26.4660\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2694.8428 - mse: 2694.8423 - mae: 28.3908 - val_loss: 2206.2659 - val_mse: 2206.2659 - val_mae: 26.4444\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2710.2806 - mse: 2710.2810 - mae: 28.5420 - val_loss: 2206.5692 - val_mse: 2206.5693 - val_mae: 26.5610\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2669.9108 - mse: 2669.9106 - mae: 28.0646 - val_loss: 2201.0790 - val_mse: 2201.0791 - val_mae: 26.6226\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2694.8680 - mse: 2694.8679 - mae: 28.1806 - val_loss: 2195.2667 - val_mse: 2195.2668 - val_mae: 26.6014\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2750.0454 - mse: 2750.0444 - mae: 28.2720 - val_loss: 2232.4602 - val_mse: 2232.4602 - val_mae: 26.1493\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2716.7805 - mse: 2716.7805 - mae: 28.3216 - val_loss: 2218.1020 - val_mse: 2218.1023 - val_mae: 26.5774\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2679.6786 - mse: 2679.6780 - mae: 28.4959 - val_loss: 2213.5331 - val_mse: 2213.5330 - val_mae: 26.7561\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2734.1734 - mse: 2734.1736 - mae: 28.4423 - val_loss: 2209.2015 - val_mse: 2209.2017 - val_mae: 26.6368\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2691.5784 - mse: 2691.5793 - mae: 28.0958 - val_loss: 2203.3468 - val_mse: 2203.3472 - val_mae: 26.6491\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 246us/step - loss: 2724.6442 - mse: 2724.6443 - mae: 28.5754 - val_loss: 2205.5593 - val_mse: 2205.5593 - val_mae: 26.8109\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 325us/step - loss: 2685.6418 - mse: 2685.6416 - mae: 28.6355 - val_loss: 2211.1199 - val_mse: 2211.1196 - val_mae: 26.5539\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2669.6748 - mse: 2669.6755 - mae: 28.2474 - val_loss: 2210.2475 - val_mse: 2210.2473 - val_mae: 26.7592\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2676.0331 - mse: 2676.0322 - mae: 28.2749 - val_loss: 2210.1236 - val_mse: 2210.1233 - val_mae: 26.8553\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2670.5302 - mse: 2670.5305 - mae: 28.4428 - val_loss: 2233.7364 - val_mse: 2233.7363 - val_mae: 26.6480\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2668.2336 - mse: 2668.2329 - mae: 28.1748 - val_loss: 2225.0632 - val_mse: 2225.0632 - val_mae: 26.5061\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2692.7452 - mse: 2692.7446 - mae: 28.2461 - val_loss: 2240.3999 - val_mse: 2240.4001 - val_mae: 26.4725\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2658.8308 - mse: 2658.8303 - mae: 28.4104 - val_loss: 2233.5414 - val_mse: 2233.5415 - val_mae: 26.4626\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 336us/step - loss: 2742.1000 - mse: 2742.0994 - mae: 28.5222 - val_loss: 2222.0770 - val_mse: 2222.0774 - val_mae: 26.5921\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2655.4141 - mse: 2655.4150 - mae: 28.1478 - val_loss: 2221.3716 - val_mse: 2221.3721 - val_mae: 26.7024\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2716.1950 - mse: 2716.1951 - mae: 28.4417 - val_loss: 2234.2932 - val_mse: 2234.2932 - val_mae: 26.6829\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 341us/step - loss: 2658.5437 - mse: 2658.5439 - mae: 28.1984 - val_loss: 2244.1136 - val_mse: 2244.1138 - val_mae: 26.3060\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2662.0970 - mse: 2662.0964 - mae: 27.8274 - val_loss: 2223.1694 - val_mse: 2223.1697 - val_mae: 26.7114\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2679.5420 - mse: 2679.5422 - mae: 28.2773 - val_loss: 2223.6567 - val_mse: 2223.6567 - val_mae: 26.4579\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2675.2394 - mse: 2675.2393 - mae: 28.2938 - val_loss: 2217.6267 - val_mse: 2217.6267 - val_mae: 26.6996\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2638.5317 - mse: 2638.5312 - mae: 28.2091 - val_loss: 2206.4076 - val_mse: 2206.4072 - val_mae: 26.9868\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2661.0503 - mse: 2661.0505 - mae: 28.2343 - val_loss: 2219.2251 - val_mse: 2219.2253 - val_mae: 26.6154\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2683.1215 - mse: 2683.1216 - mae: 28.4250 - val_loss: 2205.2322 - val_mse: 2205.2322 - val_mae: 26.7333\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2689.6215 - mse: 2689.6218 - mae: 28.5565 - val_loss: 2214.8997 - val_mse: 2214.8994 - val_mae: 26.4955\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 335us/step - loss: 2697.1513 - mse: 2697.1519 - mae: 28.2957 - val_loss: 2215.7514 - val_mse: 2215.7515 - val_mae: 26.6511\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2698.9660 - mse: 2698.9658 - mae: 28.3434 - val_loss: 2212.8171 - val_mse: 2212.8171 - val_mae: 26.7135\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2702.5258 - mse: 2702.5264 - mae: 28.6366 - val_loss: 2228.0575 - val_mse: 2228.0576 - val_mae: 26.2567\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 263us/step - loss: 2673.3938 - mse: 2673.3938 - mae: 28.3471 - val_loss: 2215.0025 - val_mse: 2215.0022 - val_mae: 26.6471\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 335us/step - loss: 2712.8164 - mse: 2712.8167 - mae: 28.4967 - val_loss: 2215.4789 - val_mse: 2215.4792 - val_mae: 26.6211\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2717.6506 - mse: 2717.6509 - mae: 28.3942 - val_loss: 2215.3627 - val_mse: 2215.3628 - val_mae: 26.7316\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2685.6030 - mse: 2685.6040 - mae: 28.5125 - val_loss: 2217.9055 - val_mse: 2217.9053 - val_mae: 26.5845\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2722.2186 - mse: 2722.2192 - mae: 28.6119 - val_loss: 2217.4178 - val_mse: 2217.4175 - val_mae: 26.8057\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2699.5213 - mse: 2699.5210 - mae: 28.0200 - val_loss: 2232.9781 - val_mse: 2232.9780 - val_mae: 26.3315\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2687.7741 - mse: 2687.7737 - mae: 28.1428 - val_loss: 2233.2896 - val_mse: 2233.2898 - val_mae: 26.2457\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2654.6239 - mse: 2654.6248 - mae: 28.2438 - val_loss: 2217.1689 - val_mse: 2217.1692 - val_mae: 26.8540\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2677.0058 - mse: 2677.0059 - mae: 28.4002 - val_loss: 2219.1689 - val_mse: 2219.1689 - val_mae: 26.7452\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2678.8261 - mse: 2678.8264 - mae: 28.1444 - val_loss: 2228.6673 - val_mse: 2228.6675 - val_mae: 26.5670\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2634.6717 - mse: 2634.6711 - mae: 28.1796 - val_loss: 2210.5817 - val_mse: 2210.5820 - val_mae: 26.9383\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2680.6004 - mse: 2680.6006 - mae: 28.3461 - val_loss: 2223.2958 - val_mse: 2223.2957 - val_mae: 26.6158\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2703.4466 - mse: 2703.4465 - mae: 28.1921 - val_loss: 2230.4707 - val_mse: 2230.4707 - val_mae: 26.4807\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2672.9535 - mse: 2672.9536 - mae: 28.0779 - val_loss: 2225.9895 - val_mse: 2225.9895 - val_mae: 26.9441\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2690.6427 - mse: 2690.6431 - mae: 28.4159 - val_loss: 2235.4674 - val_mse: 2235.4673 - val_mae: 26.7179\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 259us/step - loss: 2641.5741 - mse: 2641.5750 - mae: 28.1154 - val_loss: 2222.1621 - val_mse: 2222.1619 - val_mae: 26.7092\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2700.8456 - mse: 2700.8450 - mae: 28.6954 - val_loss: 2234.7334 - val_mse: 2234.7336 - val_mae: 26.3483\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2716.0127 - mse: 2716.0127 - mae: 28.3455 - val_loss: 2220.9014 - val_mse: 2220.9014 - val_mae: 26.7197\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2652.9815 - mse: 2652.9819 - mae: 28.0139 - val_loss: 2221.2699 - val_mse: 2221.2700 - val_mae: 26.9172\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2661.3971 - mse: 2661.3962 - mae: 28.3508 - val_loss: 2217.9884 - val_mse: 2217.9885 - val_mae: 26.8262\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2627.9136 - mse: 2627.9131 - mae: 27.8861 - val_loss: 2207.1776 - val_mse: 2207.1775 - val_mae: 26.9961\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2721.2137 - mse: 2721.2126 - mae: 28.3379 - val_loss: 2215.9373 - val_mse: 2215.9373 - val_mae: 26.8196\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2703.5099 - mse: 2703.5095 - mae: 28.5400 - val_loss: 2216.9429 - val_mse: 2216.9431 - val_mae: 26.8639\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 341us/step - loss: 2623.6089 - mse: 2623.6086 - mae: 27.9346 - val_loss: 2219.9070 - val_mse: 2219.9072 - val_mae: 26.7836\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2698.4960 - mse: 2698.4963 - mae: 28.3334 - val_loss: 2203.7958 - val_mse: 2203.7957 - val_mae: 27.1348\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2655.5668 - mse: 2655.5662 - mae: 28.1488 - val_loss: 2224.9895 - val_mse: 2224.9895 - val_mae: 26.4784\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2672.2419 - mse: 2672.2419 - mae: 27.9565 - val_loss: 2228.7115 - val_mse: 2228.7117 - val_mae: 26.4040\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - ETA: 0s - loss: 2673.9548 - mse: 2673.9546 - mae: 27.99 - 1s 345us/step - loss: 2637.3471 - mse: 2637.3472 - mae: 27.8921 - val_loss: 2220.7088 - val_mse: 2220.7090 - val_mae: 26.4762\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2700.2787 - mse: 2700.2795 - mae: 28.5015 - val_loss: 2231.4246 - val_mse: 2231.4241 - val_mae: 26.3421\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2671.8062 - mse: 2671.8064 - mae: 28.4497 - val_loss: 2239.6463 - val_mse: 2239.6462 - val_mae: 26.3693\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2682.3511 - mse: 2682.3506 - mae: 28.1206 - val_loss: 2226.1802 - val_mse: 2226.1804 - val_mae: 26.6878\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2711.7460 - mse: 2711.7461 - mae: 28.3198 - val_loss: 2226.2458 - val_mse: 2226.2461 - val_mae: 26.6662\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2679.7530 - mse: 2679.7527 - mae: 28.3611 - val_loss: 2230.2425 - val_mse: 2230.2422 - val_mae: 26.5793\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2688.2916 - mse: 2688.2913 - mae: 28.0029 - val_loss: 2237.6973 - val_mse: 2237.6975 - val_mae: 26.5842\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2672.1740 - mse: 2672.1746 - mae: 28.0290 - val_loss: 2237.8415 - val_mse: 2237.8411 - val_mae: 26.5096\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2666.7236 - mse: 2666.7236 - mae: 28.0794 - val_loss: 2239.1433 - val_mse: 2239.1431 - val_mae: 26.2933\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2664.9670 - mse: 2664.9663 - mae: 28.1008 - val_loss: 2228.9062 - val_mse: 2228.9065 - val_mae: 26.4770\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2662.3738 - mse: 2662.3738 - mae: 27.8717 - val_loss: 2230.5920 - val_mse: 2230.5923 - val_mae: 26.4575\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2669.1754 - mse: 2669.1750 - mae: 28.0569 - val_loss: 2224.8604 - val_mse: 2224.8604 - val_mae: 26.5519\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2695.0594 - mse: 2695.0593 - mae: 27.9574 - val_loss: 2225.2267 - val_mse: 2225.2266 - val_mae: 26.5763\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2661.0489 - mse: 2661.0488 - mae: 28.1335 - val_loss: 2217.5342 - val_mse: 2217.5342 - val_mae: 27.0854\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2690.2992 - mse: 2690.2998 - mae: 28.2898 - val_loss: 2226.6170 - val_mse: 2226.6167 - val_mae: 26.7497\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2706.8942 - mse: 2706.8938 - mae: 28.0968 - val_loss: 2233.7743 - val_mse: 2233.7744 - val_mae: 26.5833\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2662.5754 - mse: 2662.5750 - mae: 28.3686 - val_loss: 2233.8433 - val_mse: 2233.8430 - val_mae: 26.6273\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 331us/step - loss: 2634.6011 - mse: 2634.6011 - mae: 28.1667 - val_loss: 2226.8307 - val_mse: 2226.8311 - val_mae: 26.7156\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2644.0005 - mse: 2644.0002 - mae: 28.2430 - val_loss: 2221.7719 - val_mse: 2221.7717 - val_mae: 26.9963\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 267us/step - loss: 2642.4441 - mse: 2642.4438 - mae: 27.7939 - val_loss: 2231.2330 - val_mse: 2231.2332 - val_mae: 26.8687\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2668.1797 - mse: 2668.1802 - mae: 28.1576 - val_loss: 2241.7938 - val_mse: 2241.7939 - val_mae: 26.6160\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13302.2767 - mse: 13302.2773 - mae: 109.7853 - val_loss: 34557.9629 - val_mse: 34557.9648 - val_mae: 132.5029\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 13043.7491 - mse: 13043.7471 - mae: 108.6067 - val_loss: 34105.1886 - val_mse: 34105.1914 - val_mae: 130.7920\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 12330.0946 - mse: 12330.0947 - mae: 105.2724 - val_loss: 32776.2728 - val_mse: 32776.2734 - val_mae: 125.6358\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 10367.3184 - mse: 10367.3184 - mae: 95.4906 - val_loss: 29366.4745 - val_mse: 29366.4746 - val_mae: 111.3098\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 289us/step - loss: 6365.6167 - mse: 6365.6177 - mae: 70.3259 - val_loss: 22399.5058 - val_mse: 22399.5059 - val_mae: 73.7348\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2938.9639 - mse: 2938.9641 - mae: 39.9277 - val_loss: 18306.7439 - val_mse: 18306.7422 - val_mae: 43.1048\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2996.0871 - mse: 2996.0874 - mae: 40.1551 - val_loss: 19069.2224 - val_mse: 19069.2227 - val_mae: 48.1016\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 2971.8900 - mse: 2971.8901 - mae: 39.3825 - val_loss: 19012.8518 - val_mse: 19012.8516 - val_mae: 47.6681\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 281us/step - loss: 2561.8448 - mse: 2561.8450 - mae: 38.1774 - val_loss: 18720.7812 - val_mse: 18720.7812 - val_mae: 45.5938\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 2545.2857 - mse: 2545.2854 - mae: 35.9100 - val_loss: 18497.8710 - val_mse: 18497.8691 - val_mae: 44.0380\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2446.1394 - mse: 2446.1392 - mae: 35.6678 - val_loss: 18625.1004 - val_mse: 18625.0996 - val_mae: 44.8739\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 339us/step - loss: 2826.2701 - mse: 2826.2703 - mae: 38.0667 - val_loss: 18569.2672 - val_mse: 18569.2656 - val_mae: 44.4742\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 271us/step - loss: 2480.8531 - mse: 2480.8533 - mae: 35.6551 - val_loss: 18328.9114 - val_mse: 18328.9141 - val_mae: 42.9978\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 347us/step - loss: 2579.9095 - mse: 2579.9092 - mae: 37.1052 - val_loss: 18519.6186 - val_mse: 18519.6172 - val_mae: 44.0968\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 289us/step - loss: 2498.4070 - mse: 2498.4070 - mae: 36.8726 - val_loss: 18467.9135 - val_mse: 18467.9121 - val_mae: 43.7366\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 2452.5872 - mse: 2452.5872 - mae: 36.2915 - val_loss: 18312.0454 - val_mse: 18312.0488 - val_mae: 42.8306\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2612.2418 - mse: 2612.2417 - mae: 38.0752 - val_loss: 18621.1226 - val_mse: 18621.1191 - val_mae: 44.7172\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 331us/step - loss: 2514.0920 - mse: 2514.0920 - mae: 35.8388 - val_loss: 18395.4476 - val_mse: 18395.4473 - val_mae: 43.2034\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 347us/step - loss: 2464.3800 - mse: 2464.3799 - mae: 36.0506 - val_loss: 18594.9872 - val_mse: 18594.9863 - val_mae: 44.4786\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 376us/step - loss: 2715.1713 - mse: 2715.1716 - mae: 37.5287 - val_loss: 18437.9007 - val_mse: 18437.9004 - val_mae: 43.3986\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 376us/step - loss: 2608.7402 - mse: 2608.7405 - mae: 36.4789 - val_loss: 18577.9688 - val_mse: 18577.9668 - val_mae: 44.3032\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 355us/step - loss: 2495.1033 - mse: 2495.1033 - mae: 34.9245 - val_loss: 18588.1666 - val_mse: 18588.1660 - val_mae: 44.3506\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2456.2713 - mse: 2456.2712 - mae: 34.6831 - val_loss: 18483.2177 - val_mse: 18483.2188 - val_mae: 43.6101\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2360.2399 - mse: 2360.2397 - mae: 35.7595 - val_loss: 18521.5222 - val_mse: 18521.5195 - val_mae: 43.8375\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2331.5934 - mse: 2331.5938 - mae: 35.2297 - val_loss: 18427.5968 - val_mse: 18427.5977 - val_mae: 43.1792\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2546.0364 - mse: 2546.0361 - mae: 35.8794 - val_loss: 18641.2830 - val_mse: 18641.2832 - val_mae: 44.6236\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2610.7248 - mse: 2610.7249 - mae: 36.7012 - val_loss: 18306.4698 - val_mse: 18306.4707 - val_mae: 42.3871\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2405.3010 - mse: 2405.3010 - mae: 35.2181 - val_loss: 18334.7090 - val_mse: 18334.7109 - val_mae: 42.4964\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 368us/step - loss: 2282.9471 - mse: 2282.9470 - mae: 34.1687 - val_loss: 18298.0309 - val_mse: 18298.0293 - val_mae: 42.2790\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2420.0382 - mse: 2420.0383 - mae: 35.4824 - val_loss: 18143.4349 - val_mse: 18143.4336 - val_mae: 41.4572\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2165.1091 - mse: 2165.1089 - mae: 33.6145 - val_loss: 18217.6259 - val_mse: 18217.6270 - val_mae: 41.7882\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 236us/step - loss: 2183.9787 - mse: 2183.9788 - mae: 34.4526 - val_loss: 18062.8821 - val_mse: 18062.8828 - val_mae: 41.0788\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 236us/step - loss: 2398.3269 - mse: 2398.3267 - mae: 34.9510 - val_loss: 18259.6848 - val_mse: 18259.6855 - val_mae: 41.9389\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2204.4963 - mse: 2204.4961 - mae: 33.5689 - val_loss: 18030.8708 - val_mse: 18030.8711 - val_mae: 40.8972\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 2260.7392 - mse: 2260.7395 - mae: 33.7778 - val_loss: 18194.3530 - val_mse: 18194.3535 - val_mae: 41.5342\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2216.5286 - mse: 2216.5286 - mae: 34.4067 - val_loss: 18139.1268 - val_mse: 18139.1270 - val_mae: 41.2238\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 370us/step - loss: 2277.9149 - mse: 2277.9150 - mae: 33.6821 - val_loss: 18204.1143 - val_mse: 18204.1133 - val_mae: 41.5256\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2300.3048 - mse: 2300.3049 - mae: 34.3683 - val_loss: 18312.5201 - val_mse: 18312.5195 - val_mae: 42.0777\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 2065.5479 - mse: 2065.5479 - mae: 31.9868 - val_loss: 17979.3959 - val_mse: 17979.3965 - val_mae: 40.5699\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 362us/step - loss: 2347.3195 - mse: 2347.3196 - mae: 34.4358 - val_loss: 18402.9560 - val_mse: 18402.9570 - val_mae: 42.6167\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2130.2810 - mse: 2130.2812 - mae: 33.7612 - val_loss: 18290.4592 - val_mse: 18290.4570 - val_mae: 41.8755\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2188.3579 - mse: 2188.3579 - mae: 33.2565 - val_loss: 18160.9814 - val_mse: 18160.9824 - val_mae: 41.1402\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 260us/step - loss: 2259.8759 - mse: 2259.8762 - mae: 34.0208 - val_loss: 18172.8785 - val_mse: 18172.8809 - val_mae: 41.1743\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2454.6315 - mse: 2454.6313 - mae: 35.3140 - val_loss: 18336.0796 - val_mse: 18336.0801 - val_mae: 42.0766\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2194.3108 - mse: 2194.3108 - mae: 33.2020 - val_loss: 18164.4959 - val_mse: 18164.4961 - val_mae: 41.0678\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 231us/step - loss: 2119.2429 - mse: 2119.2429 - mae: 32.6555 - val_loss: 18304.1746 - val_mse: 18304.1758 - val_mae: 41.8304\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2177.5125 - mse: 2177.5125 - mae: 32.6703 - val_loss: 18137.9869 - val_mse: 18137.9863 - val_mae: 40.8846\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 2106.5067 - mse: 2106.5066 - mae: 32.3001 - val_loss: 18317.1670 - val_mse: 18317.1660 - val_mae: 41.8616\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 369us/step - loss: 2268.9432 - mse: 2268.9434 - mae: 33.7757 - val_loss: 18266.7547 - val_mse: 18266.7559 - val_mae: 41.5187\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 400us/step - loss: 2181.5846 - mse: 2181.5842 - mae: 33.3064 - val_loss: 18219.0734 - val_mse: 18219.0742 - val_mae: 41.2255\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 2241.6747 - mse: 2241.6746 - mae: 33.4141 - val_loss: 18142.7761 - val_mse: 18142.7734 - val_mae: 40.7840\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2219.5049 - mse: 2219.5049 - mae: 33.2678 - val_loss: 18202.2894 - val_mse: 18202.2891 - val_mae: 41.0741\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 1959.6715 - mse: 1959.6716 - mae: 31.4637 - val_loss: 18123.9023 - val_mse: 18123.9023 - val_mae: 40.6367\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 2237.4677 - mse: 2237.4678 - mae: 33.4979 - val_loss: 18350.4623 - val_mse: 18350.4629 - val_mae: 41.9307\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 327us/step - loss: 2230.0658 - mse: 2230.0657 - mae: 33.4155 - val_loss: 18161.5502 - val_mse: 18161.5508 - val_mae: 40.7644\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 2164.3492 - mse: 2164.3494 - mae: 31.4088 - val_loss: 18130.1079 - val_mse: 18130.1055 - val_mae: 40.5633\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 329us/step - loss: 2116.8079 - mse: 2116.8079 - mae: 31.4920 - val_loss: 18089.7192 - val_mse: 18089.7207 - val_mae: 40.3415\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 343us/step - loss: 2207.9835 - mse: 2207.9832 - mae: 33.5695 - val_loss: 18138.3027 - val_mse: 18138.3027 - val_mae: 40.5593\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 447us/step - loss: 2079.7229 - mse: 2079.7227 - mae: 31.8842 - val_loss: 18036.2517 - val_mse: 18036.2500 - val_mae: 40.0605\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2092.1614 - mse: 2092.1614 - mae: 31.9069 - val_loss: 18149.9357 - val_mse: 18149.9336 - val_mae: 40.5758\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 359us/step - loss: 2140.0014 - mse: 2140.0012 - mae: 33.2767 - val_loss: 17885.0722 - val_mse: 17885.0723 - val_mae: 39.5775\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 2154.1257 - mse: 2154.1257 - mae: 31.6607 - val_loss: 18153.1268 - val_mse: 18153.1270 - val_mae: 40.5367\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 359us/step - loss: 2092.6359 - mse: 2092.6360 - mae: 31.8760 - val_loss: 18206.3708 - val_mse: 18206.3691 - val_mae: 40.7920\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 371us/step - loss: 2161.6834 - mse: 2161.6833 - mae: 30.9830 - val_loss: 18088.6270 - val_mse: 18088.6289 - val_mae: 40.1398\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 1988.2193 - mse: 1988.2194 - mae: 31.0664 - val_loss: 17995.7057 - val_mse: 17995.7031 - val_mae: 39.7097\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 325us/step - loss: 2139.2834 - mse: 2139.2834 - mae: 32.0958 - val_loss: 17999.3824 - val_mse: 17999.3828 - val_mae: 39.6931\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2082.5529 - mse: 2082.5530 - mae: 32.0626 - val_loss: 18036.4064 - val_mse: 18036.4062 - val_mae: 39.8104\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 325us/step - loss: 2166.2146 - mse: 2166.2148 - mae: 32.5659 - val_loss: 18155.1752 - val_mse: 18155.1758 - val_mae: 40.3765\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2078.4592 - mse: 2078.4590 - mae: 31.4142 - val_loss: 17846.6187 - val_mse: 17846.6172 - val_mae: 39.2055\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2005.5086 - mse: 2005.5085 - mae: 31.7015 - val_loss: 17970.4855 - val_mse: 17970.4863 - val_mae: 39.4703\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 1938.7438 - mse: 1938.7439 - mae: 30.9713 - val_loss: 18008.1664 - val_mse: 18008.1680 - val_mae: 39.5751\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 1917.4345 - mse: 1917.4343 - mae: 30.5068 - val_loss: 17855.9886 - val_mse: 17855.9902 - val_mae: 39.1169\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 290us/step - loss: 2150.6953 - mse: 2150.6953 - mae: 32.0600 - val_loss: 18050.5738 - val_mse: 18050.5742 - val_mae: 39.6939\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 226us/step - loss: 2135.0350 - mse: 2135.0349 - mae: 31.9213 - val_loss: 17905.6073 - val_mse: 17905.6055 - val_mae: 39.1288\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 2103.4870 - mse: 2103.4873 - mae: 32.3801 - val_loss: 18077.9151 - val_mse: 18077.9141 - val_mae: 39.7719\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 2063.7055 - mse: 2063.7053 - mae: 31.1186 - val_loss: 17920.0632 - val_mse: 17920.0645 - val_mae: 39.0916\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2049.3960 - mse: 2049.3962 - mae: 30.9396 - val_loss: 17931.8160 - val_mse: 17931.8145 - val_mae: 39.0944\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 1803.8141 - mse: 1803.8142 - mae: 29.9872 - val_loss: 18033.5128 - val_mse: 18033.5117 - val_mae: 39.4610\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 1958.0777 - mse: 1958.0776 - mae: 30.9348 - val_loss: 17708.1036 - val_mse: 17708.1016 - val_mae: 38.5808\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 2051.8535 - mse: 2051.8533 - mae: 31.7085 - val_loss: 17930.9701 - val_mse: 17930.9707 - val_mae: 39.0013\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4276.3562 - mse: 4276.3564 - mae: 35.6610 - val_loss: 2198.2166 - val_mse: 2198.2166 - val_mae: 31.9522\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 248us/step - loss: 4612.7282 - mse: 4612.7280 - mae: 37.8049 - val_loss: 2266.6641 - val_mse: 2266.6643 - val_mae: 32.1410\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4441.0913 - mse: 4441.0913 - mae: 35.4023 - val_loss: 2254.3217 - val_mse: 2254.3218 - val_mae: 32.0945\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4091.8599 - mse: 4091.8601 - mae: 35.6180 - val_loss: 2253.4685 - val_mse: 2253.4685 - val_mae: 32.0830\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4413.3704 - mse: 4413.3696 - mae: 36.1044 - val_loss: 2290.3916 - val_mse: 2290.3918 - val_mae: 32.1787\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4200.4340 - mse: 4200.4341 - mae: 34.9496 - val_loss: 2259.7092 - val_mse: 2259.7092 - val_mae: 32.0873\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4199.7778 - mse: 4199.7778 - mae: 35.0135 - val_loss: 2231.9624 - val_mse: 2231.9622 - val_mae: 32.0036\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 268us/step - loss: 4205.4736 - mse: 4205.4736 - mae: 35.3498 - val_loss: 2237.1139 - val_mse: 2237.1138 - val_mae: 32.0139\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 333us/step - loss: 4294.4588 - mse: 4294.4590 - mae: 34.3409 - val_loss: 2298.5912 - val_mse: 2298.5913 - val_mae: 32.1777\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4327.1487 - mse: 4327.1489 - mae: 35.4364 - val_loss: 2278.0934 - val_mse: 2278.0935 - val_mae: 32.1176\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 4209.7132 - mse: 4209.7134 - mae: 35.4698 - val_loss: 2215.4195 - val_mse: 2215.4194 - val_mae: 31.9420\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 265us/step - loss: 4305.9590 - mse: 4305.9590 - mae: 36.0926 - val_loss: 2348.0002 - val_mse: 2348.0000 - val_mae: 32.2856\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 4249.6413 - mse: 4249.6421 - mae: 35.4335 - val_loss: 2218.0512 - val_mse: 2218.0513 - val_mae: 31.9392\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 4403.8063 - mse: 4403.8066 - mae: 34.7040 - val_loss: 2224.9193 - val_mse: 2224.9192 - val_mae: 31.9526\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 374us/step - loss: 4326.4735 - mse: 4326.4731 - mae: 35.6687 - val_loss: 2288.4207 - val_mse: 2288.4209 - val_mae: 32.1129\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4126.4236 - mse: 4126.4233 - mae: 34.7595 - val_loss: 2282.3771 - val_mse: 2282.3772 - val_mae: 32.0932\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4050.3630 - mse: 4050.3628 - mae: 35.2135 - val_loss: 2203.6666 - val_mse: 2203.6665 - val_mae: 31.8823\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4250.3230 - mse: 4250.3232 - mae: 35.3172 - val_loss: 2281.4333 - val_mse: 2281.4336 - val_mae: 32.0806\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 384us/step - loss: 4243.4322 - mse: 4243.4316 - mae: 34.4415 - val_loss: 2305.0967 - val_mse: 2305.0964 - val_mae: 32.1329\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 235us/step - loss: 4287.4741 - mse: 4287.4751 - mae: 34.9531 - val_loss: 2251.2295 - val_mse: 2251.2295 - val_mae: 31.9961\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 242us/step - loss: 4379.0024 - mse: 4379.0024 - mae: 36.5021 - val_loss: 2318.9185 - val_mse: 2318.9185 - val_mae: 32.1611\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4260.3475 - mse: 4260.3477 - mae: 35.1600 - val_loss: 2266.8895 - val_mse: 2266.8896 - val_mae: 32.0321\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 247us/step - loss: 4290.6276 - mse: 4290.6274 - mae: 35.1035 - val_loss: 2263.4417 - val_mse: 2263.4417 - val_mae: 32.0179\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 4127.2444 - mse: 4127.2441 - mae: 34.2830 - val_loss: 2175.9995 - val_mse: 2175.9995 - val_mae: 31.7826\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4164.1277 - mse: 4164.1279 - mae: 35.2354 - val_loss: 2268.8054 - val_mse: 2268.8054 - val_mae: 32.0209\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4215.0205 - mse: 4215.0205 - mae: 34.7525 - val_loss: 2297.1096 - val_mse: 2297.1099 - val_mae: 32.0869\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 4163.2500 - mse: 4163.2495 - mae: 34.6432 - val_loss: 2309.8351 - val_mse: 2309.8350 - val_mae: 32.1157\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 362us/step - loss: 4173.2836 - mse: 4173.2837 - mae: 35.0114 - val_loss: 2266.8931 - val_mse: 2266.8933 - val_mae: 32.0050\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 328us/step - loss: 3991.4263 - mse: 3991.4263 - mae: 33.5157 - val_loss: 2182.9049 - val_mse: 2182.9048 - val_mae: 31.7799\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4194.2777 - mse: 4194.2773 - mae: 35.2107 - val_loss: 2260.8966 - val_mse: 2260.8967 - val_mae: 31.9820\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 258us/step - loss: 4269.5243 - mse: 4269.5239 - mae: 34.1399 - val_loss: 2246.5909 - val_mse: 2246.5911 - val_mae: 31.9409\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 374us/step - loss: 4208.7097 - mse: 4208.7100 - mae: 34.3838 - val_loss: 2316.3328 - val_mse: 2316.3328 - val_mae: 32.1130\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4329.6633 - mse: 4329.6636 - mae: 35.3205 - val_loss: 2331.7260 - val_mse: 2331.7261 - val_mae: 32.1487\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 3925.6593 - mse: 3925.6592 - mae: 33.2929 - val_loss: 2248.5717 - val_mse: 2248.5718 - val_mae: 31.9359\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4268.5419 - mse: 4268.5415 - mae: 34.0660 - val_loss: 2288.4045 - val_mse: 2288.4045 - val_mae: 32.0334\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4235.1284 - mse: 4235.1289 - mae: 35.1811 - val_loss: 2268.8954 - val_mse: 2268.8955 - val_mae: 31.9760\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 344us/step - loss: 4011.3659 - mse: 4011.3662 - mae: 33.3502 - val_loss: 2283.2507 - val_mse: 2283.2510 - val_mae: 32.0092\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4095.2792 - mse: 4095.2788 - mae: 34.2588 - val_loss: 2268.4058 - val_mse: 2268.4058 - val_mae: 31.9659\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 280us/step - loss: 3934.1893 - mse: 3934.1892 - mae: 33.6066 - val_loss: 2253.5931 - val_mse: 2253.5930 - val_mae: 31.9253\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4208.8749 - mse: 4208.8750 - mae: 34.6321 - val_loss: 2322.0848 - val_mse: 2322.0847 - val_mae: 32.0988\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 3830.4828 - mse: 3830.4824 - mae: 33.0515 - val_loss: 2232.5446 - val_mse: 2232.5447 - val_mae: 31.8632\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4115.8107 - mse: 4115.8110 - mae: 34.5522 - val_loss: 2278.0641 - val_mse: 2278.0640 - val_mae: 31.9780\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4178.8819 - mse: 4178.8818 - mae: 33.5550 - val_loss: 2252.8852 - val_mse: 2252.8853 - val_mae: 31.9071\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4106.6840 - mse: 4106.6831 - mae: 33.7429 - val_loss: 2315.3325 - val_mse: 2315.3323 - val_mae: 32.0674\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 3936.1385 - mse: 3936.1384 - mae: 32.6106 - val_loss: 2288.5802 - val_mse: 2288.5803 - val_mae: 31.9971\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4131.0270 - mse: 4131.0264 - mae: 33.6818 - val_loss: 2273.6654 - val_mse: 2273.6653 - val_mae: 31.9533\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 285us/step - loss: 4134.0049 - mse: 4134.0049 - mae: 34.7629 - val_loss: 2287.5075 - val_mse: 2287.5078 - val_mae: 31.9855\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4020.4152 - mse: 4020.4155 - mae: 33.0253 - val_loss: 2304.0767 - val_mse: 2304.0767 - val_mae: 32.0239\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4151.8283 - mse: 4151.8281 - mae: 33.9968 - val_loss: 2245.2838 - val_mse: 2245.2839 - val_mae: 31.8651\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 353us/step - loss: 4212.7732 - mse: 4212.7734 - mae: 34.0609 - val_loss: 2261.5680 - val_mse: 2261.5679 - val_mae: 31.9065\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4229.9050 - mse: 4229.9048 - mae: 34.2438 - val_loss: 2336.7515 - val_mse: 2336.7515 - val_mae: 32.0999\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4086.5523 - mse: 4086.5525 - mae: 33.2860 - val_loss: 2245.2934 - val_mse: 2245.2932 - val_mae: 31.8519\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 276us/step - loss: 4106.1807 - mse: 4106.1807 - mae: 34.3278 - val_loss: 2227.1205 - val_mse: 2227.1206 - val_mae: 31.7997\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 3991.3977 - mse: 3991.3975 - mae: 33.9965 - val_loss: 2281.2866 - val_mse: 2281.2869 - val_mae: 31.9406\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 279us/step - loss: 4232.3848 - mse: 4232.3848 - mae: 34.6462 - val_loss: 2357.8110 - val_mse: 2357.8110 - val_mae: 32.1381\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4067.6859 - mse: 4067.6863 - mae: 32.9135 - val_loss: 2247.2393 - val_mse: 2247.2393 - val_mae: 31.8381\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4093.2249 - mse: 4093.2253 - mae: 33.8209 - val_loss: 2278.6011 - val_mse: 2278.6011 - val_mae: 31.9179\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4148.5524 - mse: 4148.5527 - mae: 33.2089 - val_loss: 2246.6837 - val_mse: 2246.6836 - val_mae: 31.8289\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4121.7049 - mse: 4121.7051 - mae: 33.4450 - val_loss: 2266.4374 - val_mse: 2266.4375 - val_mae: 31.8798\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 355us/step - loss: 4186.2147 - mse: 4186.2144 - mae: 34.4285 - val_loss: 2302.4715 - val_mse: 2302.4714 - val_mae: 31.9725\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4013.7795 - mse: 4013.7793 - mae: 33.3761 - val_loss: 2268.7919 - val_mse: 2268.7920 - val_mae: 31.8788\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 337us/step - loss: 3939.3867 - mse: 3939.3865 - mae: 32.9499 - val_loss: 2280.6859 - val_mse: 2280.6860 - val_mae: 31.9090\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 378us/step - loss: 4035.1543 - mse: 4035.1545 - mae: 33.0075 - val_loss: 2234.7366 - val_mse: 2234.7366 - val_mae: 31.7801\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 3993.8146 - mse: 3993.8147 - mae: 34.1752 - val_loss: 2250.9630 - val_mse: 2250.9629 - val_mae: 31.8212\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 272us/step - loss: 4161.6399 - mse: 4161.6401 - mae: 32.8322 - val_loss: 2287.6301 - val_mse: 2287.6299 - val_mae: 31.9159\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 4034.2420 - mse: 4034.2419 - mae: 33.5669 - val_loss: 2261.3064 - val_mse: 2261.3059 - val_mae: 31.8418\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 3996.0480 - mse: 3996.0476 - mae: 33.6574 - val_loss: 2292.6750 - val_mse: 2292.6750 - val_mae: 31.9218\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 3925.6815 - mse: 3925.6816 - mae: 33.5378 - val_loss: 2270.4617 - val_mse: 2270.4619 - val_mae: 31.8580\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 4091.9416 - mse: 4091.9412 - mae: 33.4869 - val_loss: 2295.1666 - val_mse: 2295.1667 - val_mae: 31.9218\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 301us/step - loss: 3867.4690 - mse: 3867.4685 - mae: 32.0924 - val_loss: 2254.5886 - val_mse: 2254.5884 - val_mae: 31.8088\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4229.1434 - mse: 4229.1436 - mae: 34.6976 - val_loss: 2373.3929 - val_mse: 2373.3931 - val_mae: 32.1333\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4001.5360 - mse: 4001.5354 - mae: 32.9925 - val_loss: 2274.6920 - val_mse: 2274.6919 - val_mae: 31.8573\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 349us/step - loss: 4172.4173 - mse: 4172.4175 - mae: 33.7854 - val_loss: 2319.8079 - val_mse: 2319.8081 - val_mae: 31.9791\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 362us/step - loss: 3982.1610 - mse: 3982.1602 - mae: 32.4747 - val_loss: 2249.1886 - val_mse: 2249.1885 - val_mae: 31.7835\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 365us/step - loss: 4114.3832 - mse: 4114.3828 - mae: 33.4856 - val_loss: 2228.4303 - val_mse: 2228.4302 - val_mae: 31.7238\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 247us/step - loss: 4065.7261 - mse: 4065.7261 - mae: 34.6028 - val_loss: 2304.4545 - val_mse: 2304.4546 - val_mae: 31.9345\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 4136.0875 - mse: 4136.0879 - mae: 33.2659 - val_loss: 2277.8691 - val_mse: 2277.8691 - val_mae: 31.8564\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 339us/step - loss: 4203.3951 - mse: 4203.3950 - mae: 34.1093 - val_loss: 2308.9947 - val_mse: 2308.9946 - val_mae: 31.9388\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 381us/step - loss: 3997.4518 - mse: 3997.4521 - mae: 32.7615 - val_loss: 2325.4003 - val_mse: 2325.4001 - val_mae: 31.9795\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 295us/step - loss: 3920.3968 - mse: 3920.3960 - mae: 32.1213 - val_loss: 2291.8782 - val_mse: 2291.8782 - val_mae: 31.8870\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3490.4526 - mse: 3490.4524 - mae: 33.6686 - val_loss: 1449.0642 - val_mse: 1449.0642 - val_mae: 24.9393\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3394.8919 - mse: 3394.8921 - mae: 33.8596 - val_loss: 1448.0254 - val_mse: 1448.0255 - val_mae: 25.7281\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 266us/step - loss: 3348.0048 - mse: 3348.0046 - mae: 33.2765 - val_loss: 1447.7901 - val_mse: 1447.7902 - val_mae: 25.1228\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 246us/step - loss: 3342.7214 - mse: 3342.7212 - mae: 32.6272 - val_loss: 1447.3306 - val_mse: 1447.3307 - val_mae: 25.3287\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3411.4336 - mse: 3411.4341 - mae: 32.6761 - val_loss: 1448.1554 - val_mse: 1448.1553 - val_mae: 25.0487\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3399.4489 - mse: 3399.4495 - mae: 32.5817 - val_loss: 1447.8403 - val_mse: 1447.8402 - val_mae: 25.6574\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 337us/step - loss: 3379.9565 - mse: 3379.9565 - mae: 32.9737 - val_loss: 1449.3427 - val_mse: 1449.3425 - val_mae: 24.8408\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3350.2859 - mse: 3350.2859 - mae: 32.2831 - val_loss: 1446.8178 - val_mse: 1446.8176 - val_mae: 25.5685\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 277us/step - loss: 3458.5840 - mse: 3458.5833 - mae: 32.8733 - val_loss: 1448.4470 - val_mse: 1448.4470 - val_mae: 24.8930\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 341us/step - loss: 3303.9722 - mse: 3303.9722 - mae: 32.0291 - val_loss: 1446.4167 - val_mse: 1446.4166 - val_mae: 25.3060\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3298.5241 - mse: 3298.5232 - mae: 32.0469 - val_loss: 1446.2893 - val_mse: 1446.2894 - val_mae: 25.4024\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3434.2864 - mse: 3434.2859 - mae: 32.7328 - val_loss: 1446.9358 - val_mse: 1446.9357 - val_mae: 25.0256\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3440.8463 - mse: 3440.8464 - mae: 33.0167 - val_loss: 1446.2105 - val_mse: 1446.2104 - val_mae: 25.1619\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3430.2393 - mse: 3430.2385 - mae: 33.2403 - val_loss: 1446.1997 - val_mse: 1446.1997 - val_mae: 25.2835\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 266us/step - loss: 3355.2263 - mse: 3355.2268 - mae: 32.2167 - val_loss: 1452.8248 - val_mse: 1452.8248 - val_mae: 26.5122\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3306.4706 - mse: 3306.4702 - mae: 31.6071 - val_loss: 1446.7254 - val_mse: 1446.7255 - val_mae: 25.4975\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3406.6898 - mse: 3406.6902 - mae: 32.4276 - val_loss: 1447.7193 - val_mse: 1447.7194 - val_mae: 25.8453\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3274.9206 - mse: 3274.9207 - mae: 32.0905 - val_loss: 1446.6896 - val_mse: 1446.6897 - val_mae: 25.3103\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 266us/step - loss: 3413.9753 - mse: 3413.9756 - mae: 32.8569 - val_loss: 1446.7862 - val_mse: 1446.7864 - val_mae: 25.3563\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3282.2383 - mse: 3282.2388 - mae: 31.9731 - val_loss: 1446.9286 - val_mse: 1446.9285 - val_mae: 25.4842\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3322.0862 - mse: 3322.0862 - mae: 31.8578 - val_loss: 1449.8638 - val_mse: 1449.8638 - val_mae: 26.1676\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3287.2866 - mse: 3287.2864 - mae: 31.7641 - val_loss: 1446.9889 - val_mse: 1446.9890 - val_mae: 25.4146\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3405.5079 - mse: 3405.5078 - mae: 32.6227 - val_loss: 1449.4011 - val_mse: 1449.4011 - val_mae: 26.0946\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3337.0170 - mse: 3337.0173 - mae: 32.0182 - val_loss: 1447.4178 - val_mse: 1447.4176 - val_mae: 25.6641\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3362.8239 - mse: 3362.8240 - mae: 32.4930 - val_loss: 1447.4521 - val_mse: 1447.4521 - val_mae: 25.6183\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3305.4150 - mse: 3305.4160 - mae: 31.8407 - val_loss: 1449.1413 - val_mse: 1449.1414 - val_mae: 25.9012\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3418.6918 - mse: 3418.6907 - mae: 32.6620 - val_loss: 1448.5661 - val_mse: 1448.5660 - val_mae: 25.1971\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3373.2050 - mse: 3373.2046 - mae: 32.3085 - val_loss: 1449.1987 - val_mse: 1449.1987 - val_mae: 25.8027\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 367us/step - loss: 3369.7777 - mse: 3369.7778 - mae: 32.7828 - val_loss: 1449.2851 - val_mse: 1449.2853 - val_mae: 25.2891\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3185.8683 - mse: 3185.8689 - mae: 30.8050 - val_loss: 1455.5130 - val_mse: 1455.5128 - val_mae: 26.6123\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3304.7745 - mse: 3304.7739 - mae: 32.8177 - val_loss: 1450.0151 - val_mse: 1450.0153 - val_mae: 25.0573\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 335us/step - loss: 3154.7118 - mse: 3154.7124 - mae: 31.2387 - val_loss: 1449.2597 - val_mse: 1449.2596 - val_mae: 25.7304\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3296.9851 - mse: 3296.9849 - mae: 30.8365 - val_loss: 1449.5219 - val_mse: 1449.5219 - val_mae: 25.7191\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3243.7813 - mse: 3243.7805 - mae: 31.3202 - val_loss: 1451.6598 - val_mse: 1451.6598 - val_mae: 26.1331\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3346.9884 - mse: 3346.9873 - mae: 32.4638 - val_loss: 1449.8395 - val_mse: 1449.8396 - val_mae: 25.4972\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3365.3361 - mse: 3365.3364 - mae: 33.0022 - val_loss: 1449.8265 - val_mse: 1449.8267 - val_mae: 25.4707\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 282us/step - loss: 3318.8031 - mse: 3318.8030 - mae: 31.7983 - val_loss: 1450.4653 - val_mse: 1450.4655 - val_mae: 25.8923\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 294us/step - loss: 3341.2573 - mse: 3341.2571 - mae: 32.3045 - val_loss: 1450.0148 - val_mse: 1450.0146 - val_mae: 25.6286\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3415.2317 - mse: 3415.2322 - mae: 32.8421 - val_loss: 1449.8252 - val_mse: 1449.8252 - val_mae: 25.7475\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3238.6082 - mse: 3238.6079 - mae: 31.6618 - val_loss: 1449.8391 - val_mse: 1449.8391 - val_mae: 25.8277\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3320.1649 - mse: 3320.1655 - mae: 31.6776 - val_loss: 1450.3778 - val_mse: 1450.3781 - val_mae: 25.9565\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 336us/step - loss: 3266.0692 - mse: 3266.0696 - mae: 31.4823 - val_loss: 1449.5505 - val_mse: 1449.5507 - val_mae: 25.5928\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 364us/step - loss: 3297.0953 - mse: 3297.0952 - mae: 31.4436 - val_loss: 1449.6205 - val_mse: 1449.6206 - val_mae: 25.2398\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3416.3416 - mse: 3416.3416 - mae: 32.8476 - val_loss: 1449.6613 - val_mse: 1449.6613 - val_mae: 25.3566\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 258us/step - loss: 3309.4430 - mse: 3309.4431 - mae: 31.6372 - val_loss: 1450.2703 - val_mse: 1450.2704 - val_mae: 25.8121\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3313.8665 - mse: 3313.8672 - mae: 31.4242 - val_loss: 1450.0409 - val_mse: 1450.0409 - val_mae: 25.4692\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 278us/step - loss: 3296.8298 - mse: 3296.8284 - mae: 32.3136 - val_loss: 1450.2913 - val_mse: 1450.2915 - val_mae: 25.5361\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3425.7327 - mse: 3425.7327 - mae: 32.7984 - val_loss: 1450.4892 - val_mse: 1450.4893 - val_mae: 25.7535\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 284us/step - loss: 3200.9230 - mse: 3200.9224 - mae: 31.8278 - val_loss: 1450.1438 - val_mse: 1450.1437 - val_mae: 25.7144\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3250.7922 - mse: 3250.7920 - mae: 31.8792 - val_loss: 1451.1769 - val_mse: 1451.1770 - val_mae: 25.9371\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3326.7781 - mse: 3326.7783 - mae: 32.0014 - val_loss: 1450.9835 - val_mse: 1450.9836 - val_mae: 25.8230\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3283.8346 - mse: 3283.8342 - mae: 32.3675 - val_loss: 1451.7129 - val_mse: 1451.7130 - val_mae: 25.1547\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3373.6676 - mse: 3373.6675 - mae: 32.4061 - val_loss: 1450.9921 - val_mse: 1450.9919 - val_mae: 25.3942\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3246.1155 - mse: 3246.1155 - mae: 31.2834 - val_loss: 1453.3285 - val_mse: 1453.3285 - val_mae: 26.1085\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3296.5494 - mse: 3296.5491 - mae: 31.9820 - val_loss: 1451.6346 - val_mse: 1451.6345 - val_mae: 25.3682\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3217.0089 - mse: 3217.0078 - mae: 31.6447 - val_loss: 1451.6467 - val_mse: 1451.6466 - val_mae: 25.3743\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3326.7454 - mse: 3326.7456 - mae: 31.7235 - val_loss: 1452.0740 - val_mse: 1452.0740 - val_mae: 25.8460\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3283.8944 - mse: 3283.8940 - mae: 32.3489 - val_loss: 1451.8193 - val_mse: 1451.8193 - val_mae: 25.2672\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3326.5310 - mse: 3326.5300 - mae: 32.6367 - val_loss: 1452.1838 - val_mse: 1452.1838 - val_mae: 25.9755\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3294.8714 - mse: 3294.8716 - mae: 31.8149 - val_loss: 1451.7740 - val_mse: 1451.7739 - val_mae: 25.6273\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3294.0140 - mse: 3294.0144 - mae: 31.8839 - val_loss: 1452.1682 - val_mse: 1452.1681 - val_mae: 25.8984\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3220.8646 - mse: 3220.8650 - mae: 30.9284 - val_loss: 1452.4049 - val_mse: 1452.4048 - val_mae: 25.6322\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3187.0935 - mse: 3187.0940 - mae: 31.3562 - val_loss: 1451.9830 - val_mse: 1451.9829 - val_mae: 25.5579\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 294us/step - loss: 3390.3532 - mse: 3390.3533 - mae: 32.8132 - val_loss: 1452.1548 - val_mse: 1452.1545 - val_mae: 25.3696\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 331us/step - loss: 3257.7867 - mse: 3257.7864 - mae: 32.0253 - val_loss: 1453.6212 - val_mse: 1453.6212 - val_mae: 26.0398\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3284.7817 - mse: 3284.7812 - mae: 32.4647 - val_loss: 1452.2253 - val_mse: 1452.2251 - val_mae: 25.6045\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 274us/step - loss: 3307.6565 - mse: 3307.6567 - mae: 32.3349 - val_loss: 1452.1527 - val_mse: 1452.1526 - val_mae: 25.5670\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3238.7123 - mse: 3238.7109 - mae: 31.9491 - val_loss: 1451.8914 - val_mse: 1451.8914 - val_mae: 25.6511\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 267us/step - loss: 3156.6653 - mse: 3156.6653 - mae: 31.4442 - val_loss: 1452.6515 - val_mse: 1452.6516 - val_mae: 26.0393\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 390us/step - loss: 3290.2788 - mse: 3290.2786 - mae: 31.9536 - val_loss: 1451.0765 - val_mse: 1451.0764 - val_mae: 25.5450\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3349.8780 - mse: 3349.8787 - mae: 31.5365 - val_loss: 1451.6215 - val_mse: 1451.6213 - val_mae: 25.8965\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 338us/step - loss: 3208.7681 - mse: 3208.7683 - mae: 31.1307 - val_loss: 1455.2805 - val_mse: 1455.2806 - val_mae: 26.4817\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3241.2564 - mse: 3241.2571 - mae: 31.4905 - val_loss: 1451.5722 - val_mse: 1451.5720 - val_mae: 25.6411\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3218.4513 - mse: 3218.4512 - mae: 30.8000 - val_loss: 1452.5033 - val_mse: 1452.5033 - val_mae: 26.0069\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3248.9074 - mse: 3248.9077 - mae: 31.6994 - val_loss: 1452.2051 - val_mse: 1452.2048 - val_mae: 25.9447\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3315.7365 - mse: 3315.7366 - mae: 32.2493 - val_loss: 1450.6739 - val_mse: 1450.6741 - val_mae: 25.5733\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3233.7766 - mse: 3233.7766 - mae: 31.0335 - val_loss: 1451.7567 - val_mse: 1451.7568 - val_mae: 26.0192\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3158.1563 - mse: 3158.1565 - mae: 31.4464 - val_loss: 1451.5577 - val_mse: 1451.5576 - val_mae: 25.3017\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 353us/step - loss: 3255.2686 - mse: 3255.2683 - mae: 31.5639 - val_loss: 1451.9049 - val_mse: 1451.9049 - val_mae: 25.8493\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3308.0846 - mse: 3308.0857 - mae: 32.3606 - val_loss: 1450.9854 - val_mse: 1450.9855 - val_mae: 25.6555\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2924.7937 - mse: 2924.7935 - mae: 31.8889 - val_loss: 1056.7866 - val_mse: 1056.7865 - val_mae: 23.7354\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2901.8333 - mse: 2901.8333 - mae: 31.4372 - val_loss: 1060.1724 - val_mse: 1060.1722 - val_mae: 23.3975\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2950.2303 - mse: 2950.2312 - mae: 30.6046 - val_loss: 1057.6399 - val_mse: 1057.6399 - val_mae: 23.6212\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 316us/step - loss: 2890.9278 - mse: 2890.9275 - mae: 31.3575 - val_loss: 1057.0779 - val_mse: 1057.0780 - val_mae: 23.6486\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 344us/step - loss: 2926.2294 - mse: 2926.2290 - mae: 30.7420 - val_loss: 1058.9564 - val_mse: 1058.9564 - val_mae: 23.4128\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2922.9021 - mse: 2922.9021 - mae: 31.2228 - val_loss: 1057.9826 - val_mse: 1057.9825 - val_mae: 23.3814\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2931.7803 - mse: 2931.7803 - mae: 30.7755 - val_loss: 1054.7896 - val_mse: 1054.7894 - val_mae: 23.5989\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 0s 250us/step - loss: 2940.4244 - mse: 2940.4238 - mae: 31.4189 - val_loss: 1061.7525 - val_mse: 1061.7526 - val_mae: 23.1069\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2884.8825 - mse: 2884.8831 - mae: 30.8846 - val_loss: 1055.3072 - val_mse: 1055.3071 - val_mae: 23.4367\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2918.3324 - mse: 2918.3328 - mae: 30.4852 - val_loss: 1054.5675 - val_mse: 1054.5675 - val_mae: 23.4017\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2935.1191 - mse: 2935.1191 - mae: 30.9051 - val_loss: 1056.0772 - val_mse: 1056.0771 - val_mae: 23.2859\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 255us/step - loss: 2881.7641 - mse: 2881.7642 - mae: 31.2571 - val_loss: 1058.6354 - val_mse: 1058.6354 - val_mae: 23.2569\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2880.3145 - mse: 2880.3145 - mae: 30.5405 - val_loss: 1052.5687 - val_mse: 1052.5686 - val_mae: 23.7235\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2864.5004 - mse: 2864.5007 - mae: 30.7036 - val_loss: 1051.9742 - val_mse: 1051.9742 - val_mae: 23.8162\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 277us/step - loss: 2917.5375 - mse: 2917.5371 - mae: 31.0522 - val_loss: 1055.3665 - val_mse: 1055.3665 - val_mae: 23.4600\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2994.8459 - mse: 2994.8464 - mae: 31.3243 - val_loss: 1054.2086 - val_mse: 1054.2085 - val_mae: 23.5138\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2905.7880 - mse: 2905.7878 - mae: 30.8169 - val_loss: 1053.7777 - val_mse: 1053.7777 - val_mae: 23.5228\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2926.1527 - mse: 2926.1533 - mae: 31.6983 - val_loss: 1052.7157 - val_mse: 1052.7157 - val_mae: 23.6329\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2862.4073 - mse: 2862.4070 - mae: 30.5245 - val_loss: 1051.9946 - val_mse: 1051.9946 - val_mae: 23.6910\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2905.0130 - mse: 2905.0129 - mae: 30.8114 - val_loss: 1053.4084 - val_mse: 1053.4084 - val_mae: 23.4593\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2863.0185 - mse: 2863.0181 - mae: 30.4182 - val_loss: 1056.0333 - val_mse: 1056.0332 - val_mae: 23.3422\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2877.1575 - mse: 2877.1572 - mae: 31.0521 - val_loss: 1051.8596 - val_mse: 1051.8597 - val_mae: 23.6180\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2843.1613 - mse: 2843.1614 - mae: 30.6812 - val_loss: 1051.0916 - val_mse: 1051.0919 - val_mae: 23.6613\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2842.9200 - mse: 2842.9202 - mae: 30.6023 - val_loss: 1050.7606 - val_mse: 1050.7606 - val_mae: 23.7078\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2834.9415 - mse: 2834.9412 - mae: 30.8067 - val_loss: 1050.4509 - val_mse: 1050.4508 - val_mae: 23.5915\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2924.9067 - mse: 2924.9075 - mae: 30.9456 - val_loss: 1050.2393 - val_mse: 1050.2393 - val_mae: 23.6747\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2910.2980 - mse: 2910.2986 - mae: 30.9006 - val_loss: 1051.2313 - val_mse: 1051.2313 - val_mae: 23.6509\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2874.0467 - mse: 2874.0459 - mae: 30.5996 - val_loss: 1050.3071 - val_mse: 1050.3070 - val_mae: 23.6758\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2867.2371 - mse: 2867.2371 - mae: 30.4408 - val_loss: 1049.0346 - val_mse: 1049.0344 - val_mae: 23.7957\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2963.1481 - mse: 2963.1484 - mae: 30.8892 - val_loss: 1051.4188 - val_mse: 1051.4188 - val_mae: 23.5483\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2895.1375 - mse: 2895.1375 - mae: 30.5340 - val_loss: 1052.0207 - val_mse: 1052.0206 - val_mae: 23.5513\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2981.4675 - mse: 2981.4670 - mae: 31.5376 - val_loss: 1056.3135 - val_mse: 1056.3135 - val_mae: 23.2711\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2914.6253 - mse: 2914.6250 - mae: 30.9251 - val_loss: 1051.3683 - val_mse: 1051.3684 - val_mae: 23.5895\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2881.0768 - mse: 2881.0769 - mae: 30.2243 - val_loss: 1055.3945 - val_mse: 1055.3944 - val_mae: 23.3351\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2871.0855 - mse: 2871.0854 - mae: 30.2899 - val_loss: 1052.0438 - val_mse: 1052.0438 - val_mae: 23.4792\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2803.9631 - mse: 2803.9631 - mae: 30.4832 - val_loss: 1048.3970 - val_mse: 1048.3971 - val_mae: 24.0723\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2909.9695 - mse: 2909.9700 - mae: 31.1726 - val_loss: 1049.8721 - val_mse: 1049.8719 - val_mae: 23.6346\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 255us/step - loss: 2855.4597 - mse: 2855.4595 - mae: 30.6732 - val_loss: 1049.9724 - val_mse: 1049.9724 - val_mae: 23.6877\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2906.1551 - mse: 2906.1550 - mae: 30.9145 - val_loss: 1050.5357 - val_mse: 1050.5356 - val_mae: 23.6072\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2859.4105 - mse: 2859.4111 - mae: 30.7801 - val_loss: 1049.2393 - val_mse: 1049.2394 - val_mae: 23.8325\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2823.6397 - mse: 2823.6411 - mae: 30.1990 - val_loss: 1050.9558 - val_mse: 1050.9557 - val_mae: 23.6131\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2864.3002 - mse: 2864.2998 - mae: 30.4648 - val_loss: 1049.8916 - val_mse: 1049.8915 - val_mae: 23.6742\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2852.7505 - mse: 2852.7500 - mae: 30.4466 - val_loss: 1048.7966 - val_mse: 1048.7966 - val_mae: 23.9066\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2868.0931 - mse: 2868.0920 - mae: 30.6783 - val_loss: 1049.3228 - val_mse: 1049.3229 - val_mae: 23.8035\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 274us/step - loss: 2822.4134 - mse: 2822.4141 - mae: 30.4466 - val_loss: 1049.5430 - val_mse: 1049.5428 - val_mae: 23.7771\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2868.3579 - mse: 2868.3582 - mae: 30.4687 - val_loss: 1050.5522 - val_mse: 1050.5522 - val_mae: 23.6518\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2851.4230 - mse: 2851.4233 - mae: 30.7495 - val_loss: 1049.3174 - val_mse: 1049.3175 - val_mae: 23.8715\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2862.0058 - mse: 2862.0059 - mae: 30.8465 - val_loss: 1049.2577 - val_mse: 1049.2578 - val_mae: 23.7299\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2883.5027 - mse: 2883.5027 - mae: 30.5625 - val_loss: 1049.0683 - val_mse: 1049.0682 - val_mae: 23.8861\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2875.2707 - mse: 2875.2708 - mae: 30.3208 - val_loss: 1048.6336 - val_mse: 1048.6337 - val_mae: 24.1400\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2790.2358 - mse: 2790.2354 - mae: 30.2666 - val_loss: 1048.2788 - val_mse: 1048.2788 - val_mae: 24.0666\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 379us/step - loss: 2860.4836 - mse: 2860.4844 - mae: 30.5593 - val_loss: 1047.8538 - val_mse: 1047.8539 - val_mae: 23.8073\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2831.4761 - mse: 2831.4751 - mae: 30.3190 - val_loss: 1049.2633 - val_mse: 1049.2632 - val_mae: 23.5607\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 340us/step - loss: 2846.1297 - mse: 2846.1299 - mae: 30.3551 - val_loss: 1048.8796 - val_mse: 1048.8798 - val_mae: 23.6199\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 338us/step - loss: 2876.5699 - mse: 2876.5693 - mae: 30.5816 - val_loss: 1047.9855 - val_mse: 1047.9855 - val_mae: 23.9933\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2825.5905 - mse: 2825.5906 - mae: 30.4499 - val_loss: 1048.2289 - val_mse: 1048.2289 - val_mae: 24.2015\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2837.0578 - mse: 2837.0583 - mae: 30.8692 - val_loss: 1047.9982 - val_mse: 1047.9983 - val_mae: 23.7206\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2853.3906 - mse: 2853.3906 - mae: 31.0142 - val_loss: 1048.9266 - val_mse: 1048.9265 - val_mae: 23.6847\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 336us/step - loss: 2862.4408 - mse: 2862.4412 - mae: 30.1630 - val_loss: 1047.7238 - val_mse: 1047.7239 - val_mae: 23.9409\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2742.8007 - mse: 2742.8013 - mae: 30.0006 - val_loss: 1048.1837 - val_mse: 1048.1837 - val_mae: 23.7874\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2837.7166 - mse: 2837.7161 - mae: 30.3011 - val_loss: 1047.1197 - val_mse: 1047.1198 - val_mae: 23.9207\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 327us/step - loss: 2932.9320 - mse: 2932.9316 - mae: 30.8260 - val_loss: 1048.1944 - val_mse: 1048.1943 - val_mae: 23.6576\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2908.3402 - mse: 2908.3403 - mae: 30.5934 - val_loss: 1048.1990 - val_mse: 1048.1990 - val_mae: 23.5750\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2862.7381 - mse: 2862.7380 - mae: 30.4796 - val_loss: 1046.8620 - val_mse: 1046.8619 - val_mae: 23.7419\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 350us/step - loss: 2865.7669 - mse: 2865.7681 - mae: 30.3763 - val_loss: 1048.6546 - val_mse: 1048.6547 - val_mae: 23.5538\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2870.2188 - mse: 2870.2190 - mae: 30.3848 - val_loss: 1048.6336 - val_mse: 1048.6335 - val_mae: 23.6790\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2793.1867 - mse: 2793.1865 - mae: 30.5106 - val_loss: 1048.3102 - val_mse: 1048.3103 - val_mae: 23.8728\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2832.4944 - mse: 2832.4934 - mae: 30.4576 - val_loss: 1047.7004 - val_mse: 1047.7004 - val_mae: 23.9192\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2958.2994 - mse: 2958.3003 - mae: 31.0165 - val_loss: 1046.9689 - val_mse: 1046.9689 - val_mae: 23.9046\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 358us/step - loss: 2861.7714 - mse: 2861.7715 - mae: 30.6337 - val_loss: 1045.8617 - val_mse: 1045.8618 - val_mae: 23.7617\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2824.7673 - mse: 2824.7676 - mae: 30.1771 - val_loss: 1044.9730 - val_mse: 1044.9729 - val_mae: 23.7937\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 330us/step - loss: 2811.4170 - mse: 2811.4167 - mae: 30.0393 - val_loss: 1044.0690 - val_mse: 1044.0690 - val_mae: 23.8149\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2843.7085 - mse: 2843.7085 - mae: 30.5928 - val_loss: 1044.3038 - val_mse: 1044.3037 - val_mae: 24.3536\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2788.6898 - mse: 2788.6895 - mae: 30.4370 - val_loss: 1042.1926 - val_mse: 1042.1926 - val_mae: 23.9508\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2835.9877 - mse: 2835.9871 - mae: 30.5871 - val_loss: 1044.6523 - val_mse: 1044.6523 - val_mae: 23.4681\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2856.7621 - mse: 2856.7620 - mae: 29.8538 - val_loss: 1042.4996 - val_mse: 1042.4996 - val_mae: 23.8947\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2833.7341 - mse: 2833.7334 - mae: 30.2925 - val_loss: 1041.8667 - val_mse: 1041.8667 - val_mae: 23.9261\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 0s 245us/step - loss: 2787.5777 - mse: 2787.5779 - mae: 30.1066 - val_loss: 1041.8395 - val_mse: 1041.8396 - val_mae: 23.8471\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 251us/step - loss: 2929.9159 - mse: 2929.9165 - mae: 30.9670 - val_loss: 1042.3748 - val_mse: 1042.3748 - val_mae: 23.6866\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2829.6749 - mse: 2829.6750 - mae: 30.2513 - val_loss: 1041.7492 - val_mse: 1041.7491 - val_mae: 24.0247\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2591.1771 - mse: 2591.1772 - mae: 30.1075 - val_loss: 1533.1640 - val_mse: 1533.1642 - val_mae: 27.9719\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2521.8802 - mse: 2521.8801 - mae: 29.4999 - val_loss: 1530.3633 - val_mse: 1530.3632 - val_mae: 28.1079\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2544.9779 - mse: 2544.9780 - mae: 30.0809 - val_loss: 1528.4819 - val_mse: 1528.4817 - val_mae: 28.2028\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 285us/step - loss: 2556.9550 - mse: 2556.9543 - mae: 29.9569 - val_loss: 1536.3150 - val_mse: 1536.3148 - val_mae: 27.7259\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2564.8754 - mse: 2564.8748 - mae: 29.8139 - val_loss: 1535.5684 - val_mse: 1535.5686 - val_mae: 27.7641\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2540.1979 - mse: 2540.1975 - mae: 29.6034 - val_loss: 1530.6528 - val_mse: 1530.6528 - val_mae: 28.0089\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2595.7604 - mse: 2595.7610 - mae: 29.8273 - val_loss: 1526.3950 - val_mse: 1526.3949 - val_mae: 28.2969\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2516.3150 - mse: 2516.3147 - mae: 29.3588 - val_loss: 1529.6038 - val_mse: 1529.6039 - val_mae: 28.0344\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 324us/step - loss: 2541.3813 - mse: 2541.3813 - mae: 29.4132 - val_loss: 1530.9535 - val_mse: 1530.9534 - val_mae: 27.9021\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2464.4067 - mse: 2464.4067 - mae: 29.3612 - val_loss: 1530.0081 - val_mse: 1530.0082 - val_mae: 27.9013\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2484.8365 - mse: 2484.8364 - mae: 29.0643 - val_loss: 1527.3521 - val_mse: 1527.3521 - val_mae: 28.0332\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2568.6555 - mse: 2568.6560 - mae: 29.7987 - val_loss: 1525.1828 - val_mse: 1525.1825 - val_mae: 28.1895\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 328us/step - loss: 2531.5993 - mse: 2531.5989 - mae: 29.9726 - val_loss: 1532.2663 - val_mse: 1532.2660 - val_mae: 27.7552\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 357us/step - loss: 2477.8478 - mse: 2477.8477 - mae: 29.5480 - val_loss: 1526.9521 - val_mse: 1526.9519 - val_mae: 28.0467\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2538.2576 - mse: 2538.2578 - mae: 29.4048 - val_loss: 1526.0930 - val_mse: 1526.0929 - val_mae: 28.0943\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2517.3363 - mse: 2517.3364 - mae: 29.5255 - val_loss: 1533.6292 - val_mse: 1533.6293 - val_mae: 27.6807\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2504.3929 - mse: 2504.3933 - mae: 29.3685 - val_loss: 1528.6163 - val_mse: 1528.6162 - val_mae: 27.9343\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2407.3023 - mse: 2407.3022 - mae: 29.0499 - val_loss: 1528.0749 - val_mse: 1528.0746 - val_mae: 27.9372\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2512.9706 - mse: 2512.9707 - mae: 29.2892 - val_loss: 1530.1727 - val_mse: 1530.1726 - val_mae: 27.8265\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 327us/step - loss: 2532.3510 - mse: 2532.3506 - mae: 29.6242 - val_loss: 1524.6742 - val_mse: 1524.6743 - val_mae: 28.1719\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2578.5492 - mse: 2578.5498 - mae: 29.9065 - val_loss: 1529.0857 - val_mse: 1529.0857 - val_mae: 27.8755\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 268us/step - loss: 2498.6205 - mse: 2498.6196 - mae: 29.3764 - val_loss: 1533.9091 - val_mse: 1533.9091 - val_mae: 27.6429\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2477.7041 - mse: 2477.7046 - mae: 29.1778 - val_loss: 1529.5682 - val_mse: 1529.5684 - val_mae: 27.8077\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2562.5890 - mse: 2562.5891 - mae: 29.5361 - val_loss: 1527.0618 - val_mse: 1527.0619 - val_mae: 27.9740\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2441.4869 - mse: 2441.4868 - mae: 29.1698 - val_loss: 1525.1050 - val_mse: 1525.1050 - val_mae: 28.1233\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2531.9880 - mse: 2531.9875 - mae: 29.1618 - val_loss: 1526.3672 - val_mse: 1526.3671 - val_mae: 27.9991\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2536.4466 - mse: 2536.4463 - mae: 29.6037 - val_loss: 1533.2573 - val_mse: 1533.2572 - val_mae: 27.6266\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2484.2719 - mse: 2484.2725 - mae: 29.3626 - val_loss: 1526.3232 - val_mse: 1526.3234 - val_mae: 27.9494\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2534.5223 - mse: 2534.5229 - mae: 29.4244 - val_loss: 1531.0572 - val_mse: 1531.0573 - val_mae: 27.6694\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2537.2977 - mse: 2537.2974 - mae: 29.5292 - val_loss: 1531.1493 - val_mse: 1531.1492 - val_mae: 27.6519\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 230us/step - loss: 2538.7671 - mse: 2538.7671 - mae: 29.6124 - val_loss: 1528.5391 - val_mse: 1528.5392 - val_mae: 27.7519\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 268us/step - loss: 2582.3000 - mse: 2582.3003 - mae: 29.5194 - val_loss: 1531.2937 - val_mse: 1531.2935 - val_mae: 27.6074\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2513.0938 - mse: 2513.0940 - mae: 29.5558 - val_loss: 1522.3004 - val_mse: 1522.3003 - val_mae: 28.0850\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2496.3152 - mse: 2496.3149 - mae: 29.4998 - val_loss: 1529.3329 - val_mse: 1529.3330 - val_mae: 27.6523\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2528.7027 - mse: 2528.7029 - mae: 29.5705 - val_loss: 1533.8607 - val_mse: 1533.8606 - val_mae: 27.4697\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 330us/step - loss: 2505.9625 - mse: 2505.9631 - mae: 29.5003 - val_loss: 1526.8553 - val_mse: 1526.8553 - val_mae: 27.7655\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 266us/step - loss: 2495.4488 - mse: 2495.4490 - mae: 29.3037 - val_loss: 1523.5981 - val_mse: 1523.5981 - val_mae: 27.9398\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2471.8075 - mse: 2471.8079 - mae: 29.2380 - val_loss: 1523.7648 - val_mse: 1523.7649 - val_mae: 27.9041\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 266us/step - loss: 2495.9201 - mse: 2495.9192 - mae: 29.7481 - val_loss: 1521.8216 - val_mse: 1521.8217 - val_mae: 28.0037\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 232us/step - loss: 2440.1542 - mse: 2440.1538 - mae: 29.0185 - val_loss: 1521.5874 - val_mse: 1521.5875 - val_mae: 27.9866\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2504.3413 - mse: 2504.3413 - mae: 29.6280 - val_loss: 1521.4117 - val_mse: 1521.4117 - val_mae: 27.9468\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2533.2975 - mse: 2533.2969 - mae: 29.1382 - val_loss: 1520.8882 - val_mse: 1520.8883 - val_mae: 27.9779\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2518.9421 - mse: 2518.9409 - mae: 29.2850 - val_loss: 1520.8684 - val_mse: 1520.8683 - val_mae: 27.9776\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 343us/step - loss: 2515.0891 - mse: 2515.0896 - mae: 29.1451 - val_loss: 1524.0420 - val_mse: 1524.0420 - val_mae: 27.7990\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 247us/step - loss: 2514.4125 - mse: 2514.4126 - mae: 29.3066 - val_loss: 1521.0154 - val_mse: 1521.0153 - val_mae: 27.9528\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2487.5925 - mse: 2487.5918 - mae: 29.3556 - val_loss: 1522.5977 - val_mse: 1522.5977 - val_mae: 27.8692\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2511.5186 - mse: 2511.5188 - mae: 29.2523 - val_loss: 1523.7913 - val_mse: 1523.7914 - val_mae: 27.8038\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2482.6914 - mse: 2482.6914 - mae: 29.1087 - val_loss: 1518.6434 - val_mse: 1518.6436 - val_mae: 28.1028\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2459.1833 - mse: 2459.1826 - mae: 28.9205 - val_loss: 1523.2958 - val_mse: 1523.2955 - val_mae: 27.7769\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2463.2450 - mse: 2463.2444 - mae: 28.8327 - val_loss: 1516.9639 - val_mse: 1516.9639 - val_mae: 28.1775\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2517.0542 - mse: 2517.0542 - mae: 29.6146 - val_loss: 1522.4660 - val_mse: 1522.4662 - val_mae: 27.7988\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2498.0711 - mse: 2498.0708 - mae: 29.4429 - val_loss: 1521.1821 - val_mse: 1521.1819 - val_mae: 27.8517\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2523.4794 - mse: 2523.4795 - mae: 29.3718 - val_loss: 1522.6078 - val_mse: 1522.6078 - val_mae: 27.7814\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2547.0289 - mse: 2547.0283 - mae: 29.2742 - val_loss: 1519.1810 - val_mse: 1519.1810 - val_mae: 27.9561\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2495.3755 - mse: 2495.3752 - mae: 28.9341 - val_loss: 1514.9086 - val_mse: 1514.9084 - val_mae: 28.2955\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2495.8772 - mse: 2495.8770 - mae: 29.4333 - val_loss: 1521.7947 - val_mse: 1521.7948 - val_mae: 27.7827\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2487.3802 - mse: 2487.3801 - mae: 29.1242 - val_loss: 1522.0461 - val_mse: 1522.0463 - val_mae: 27.7430\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2543.0558 - mse: 2543.0552 - mae: 29.5524 - val_loss: 1525.2658 - val_mse: 1525.2659 - val_mae: 27.5729\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2506.5478 - mse: 2506.5481 - mae: 29.0840 - val_loss: 1519.1063 - val_mse: 1519.1061 - val_mae: 27.8637\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 332us/step - loss: 2513.5490 - mse: 2513.5491 - mae: 29.1874 - val_loss: 1523.0099 - val_mse: 1523.0096 - val_mae: 27.6343\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 264us/step - loss: 2520.7690 - mse: 2520.7686 - mae: 29.0212 - val_loss: 1518.9483 - val_mse: 1518.9484 - val_mae: 27.8520\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 266us/step - loss: 2439.9619 - mse: 2439.9619 - mae: 28.7032 - val_loss: 1519.9012 - val_mse: 1519.9011 - val_mae: 27.8006\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2489.8189 - mse: 2489.8184 - mae: 29.3999 - val_loss: 1518.3516 - val_mse: 1518.3517 - val_mae: 27.8901\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2479.7966 - mse: 2479.7974 - mae: 28.7977 - val_loss: 1517.4545 - val_mse: 1517.4546 - val_mae: 27.9079\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2532.5057 - mse: 2532.5061 - mae: 29.4351 - val_loss: 1519.8681 - val_mse: 1519.8682 - val_mae: 27.7346\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2499.4997 - mse: 2499.4985 - mae: 28.9509 - val_loss: 1516.2522 - val_mse: 1516.2522 - val_mae: 27.9131\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 250us/step - loss: 2514.1523 - mse: 2514.1526 - mae: 29.0252 - val_loss: 1514.1375 - val_mse: 1514.1375 - val_mae: 28.0181\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2470.0937 - mse: 2470.0933 - mae: 29.0418 - val_loss: 1523.3315 - val_mse: 1523.3313 - val_mae: 27.4973\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2582.6163 - mse: 2582.6169 - mae: 29.3143 - val_loss: 1514.7617 - val_mse: 1514.7616 - val_mae: 27.9073\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2455.5751 - mse: 2455.5752 - mae: 29.0186 - val_loss: 1517.3909 - val_mse: 1517.3910 - val_mae: 27.6773\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 336us/step - loss: 2491.9939 - mse: 2491.9929 - mae: 28.8389 - val_loss: 1512.8373 - val_mse: 1512.8373 - val_mae: 27.9321\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2434.7099 - mse: 2434.7100 - mae: 28.9732 - val_loss: 1513.5502 - val_mse: 1513.5502 - val_mae: 27.8282\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 327us/step - loss: 2471.9871 - mse: 2471.9871 - mae: 29.2407 - val_loss: 1509.6423 - val_mse: 1509.6423 - val_mae: 28.0837\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2478.4896 - mse: 2478.4895 - mae: 28.8075 - val_loss: 1511.5204 - val_mse: 1511.5203 - val_mae: 27.9390\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2486.2246 - mse: 2486.2234 - mae: 28.6093 - val_loss: 1513.3507 - val_mse: 1513.3506 - val_mae: 27.8288\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2485.9218 - mse: 2485.9214 - mae: 29.1305 - val_loss: 1517.5280 - val_mse: 1517.5281 - val_mae: 27.5961\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2441.8462 - mse: 2441.8462 - mae: 29.0273 - val_loss: 1514.7011 - val_mse: 1514.7012 - val_mae: 27.7076\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2472.9491 - mse: 2472.9482 - mae: 29.2559 - val_loss: 1514.0319 - val_mse: 1514.0320 - val_mae: 27.7121\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2429.5110 - mse: 2429.5120 - mae: 28.7827 - val_loss: 1512.5338 - val_mse: 1512.5338 - val_mae: 27.8191\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2468.6590 - mse: 2468.6589 - mae: 28.8910 - val_loss: 1512.4838 - val_mse: 1512.4839 - val_mae: 27.8093\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 275us/step - loss: 2393.6955 - mse: 2393.6965 - mae: 29.7301 - val_loss: 3706.2903 - val_mse: 3706.2908 - val_mae: 23.0556\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2408.0665 - mse: 2408.0662 - mae: 29.6798 - val_loss: 3702.1568 - val_mse: 3702.1570 - val_mae: 23.4882\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 229us/step - loss: 2395.8771 - mse: 2395.8770 - mae: 29.5016 - val_loss: 3701.2646 - val_mse: 3701.2644 - val_mae: 23.7609\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2329.0743 - mse: 2329.0735 - mae: 29.2275 - val_loss: 3701.6740 - val_mse: 3701.6733 - val_mae: 23.8635\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2367.1473 - mse: 2367.1482 - mae: 29.3987 - val_loss: 3699.8726 - val_mse: 3699.8730 - val_mae: 24.0161\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2381.3133 - mse: 2381.3135 - mae: 29.7329 - val_loss: 3700.8474 - val_mse: 3700.8472 - val_mae: 23.7074\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2362.6135 - mse: 2362.6133 - mae: 29.2483 - val_loss: 3702.8552 - val_mse: 3702.8547 - val_mae: 23.4812\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2396.0496 - mse: 2396.0493 - mae: 29.3414 - val_loss: 3702.3863 - val_mse: 3702.3870 - val_mae: 23.7991\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2309.8014 - mse: 2309.8020 - mae: 29.1700 - val_loss: 3702.4691 - val_mse: 3702.4690 - val_mae: 23.3396\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2383.7838 - mse: 2383.7837 - mae: 29.4427 - val_loss: 3700.0078 - val_mse: 3700.0078 - val_mae: 23.8508\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2318.8610 - mse: 2318.8613 - mae: 29.1013 - val_loss: 3700.8461 - val_mse: 3700.8464 - val_mae: 23.5890\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2366.9597 - mse: 2366.9587 - mae: 29.1034 - val_loss: 3700.5303 - val_mse: 3700.5300 - val_mae: 23.6677\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2401.5988 - mse: 2401.5991 - mae: 29.7120 - val_loss: 3702.6443 - val_mse: 3702.6453 - val_mae: 23.6015\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2266.9128 - mse: 2266.9126 - mae: 29.1293 - val_loss: 3702.6818 - val_mse: 3702.6824 - val_mae: 23.7848\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 259us/step - loss: 2371.7013 - mse: 2371.7007 - mae: 29.3928 - val_loss: 3702.1059 - val_mse: 3702.1055 - val_mae: 24.1352\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2366.7178 - mse: 2366.7178 - mae: 29.6250 - val_loss: 3701.1779 - val_mse: 3701.1785 - val_mae: 23.7464\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 328us/step - loss: 2369.1256 - mse: 2369.1260 - mae: 29.5772 - val_loss: 3699.9701 - val_mse: 3699.9695 - val_mae: 23.8358\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 325us/step - loss: 2376.5465 - mse: 2376.5459 - mae: 29.4982 - val_loss: 3701.3419 - val_mse: 3701.3413 - val_mae: 23.5899\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2422.6748 - mse: 2422.6746 - mae: 29.7264 - val_loss: 3700.9955 - val_mse: 3700.9961 - val_mae: 23.2870\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2375.7109 - mse: 2375.7114 - mae: 29.3757 - val_loss: 3700.0976 - val_mse: 3700.0981 - val_mae: 23.5819\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2348.6104 - mse: 2348.6108 - mae: 29.2312 - val_loss: 3701.1524 - val_mse: 3701.1526 - val_mae: 23.4327\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 346us/step - loss: 2347.2895 - mse: 2347.2896 - mae: 29.2985 - val_loss: 3699.9533 - val_mse: 3699.9531 - val_mae: 23.7621\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2363.3325 - mse: 2363.3330 - mae: 29.5470 - val_loss: 3703.0809 - val_mse: 3703.0811 - val_mae: 23.2213\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2383.0437 - mse: 2383.0435 - mae: 29.7407 - val_loss: 3701.3877 - val_mse: 3701.3887 - val_mae: 23.8049\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2410.5899 - mse: 2410.5903 - mae: 29.7434 - val_loss: 3700.2579 - val_mse: 3700.2578 - val_mae: 23.4319\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 330us/step - loss: 2303.2103 - mse: 2303.2102 - mae: 28.6232 - val_loss: 3699.8220 - val_mse: 3699.8215 - val_mae: 24.1002\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2426.7112 - mse: 2426.7104 - mae: 29.7311 - val_loss: 3698.3779 - val_mse: 3698.3782 - val_mae: 23.5077\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2323.8959 - mse: 2323.8953 - mae: 29.0491 - val_loss: 3698.1366 - val_mse: 3698.1365 - val_mae: 23.5643\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 345us/step - loss: 2382.8299 - mse: 2382.8296 - mae: 29.5673 - val_loss: 3697.7066 - val_mse: 3697.7073 - val_mae: 23.6926\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2353.3247 - mse: 2353.3247 - mae: 29.1826 - val_loss: 3699.4856 - val_mse: 3699.4858 - val_mae: 23.5189\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2376.3880 - mse: 2376.3879 - mae: 29.4854 - val_loss: 3698.3807 - val_mse: 3698.3809 - val_mae: 24.1070\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2350.7369 - mse: 2350.7368 - mae: 29.3339 - val_loss: 3698.3850 - val_mse: 3698.3855 - val_mae: 23.7968\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 280us/step - loss: 2364.2327 - mse: 2364.2329 - mae: 29.2412 - val_loss: 3696.7661 - val_mse: 3696.7664 - val_mae: 23.7575\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2368.1587 - mse: 2368.1587 - mae: 29.3896 - val_loss: 3696.9699 - val_mse: 3696.9709 - val_mae: 23.6471\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2360.2394 - mse: 2360.2397 - mae: 29.0785 - val_loss: 3697.4710 - val_mse: 3697.4705 - val_mae: 23.5571\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2365.8898 - mse: 2365.8892 - mae: 29.3106 - val_loss: 3695.1227 - val_mse: 3695.1221 - val_mae: 23.7099\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2341.5953 - mse: 2341.5955 - mae: 29.3662 - val_loss: 3694.3329 - val_mse: 3694.3323 - val_mae: 23.9257\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2393.2430 - mse: 2393.2424 - mae: 29.8026 - val_loss: 3694.4851 - val_mse: 3694.4849 - val_mae: 23.8161\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2357.1757 - mse: 2357.1755 - mae: 29.3593 - val_loss: 3695.6479 - val_mse: 3695.6477 - val_mae: 23.9412\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2391.7217 - mse: 2391.7219 - mae: 29.7894 - val_loss: 3696.4452 - val_mse: 3696.4443 - val_mae: 23.6848\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 330us/step - loss: 2339.7537 - mse: 2339.7534 - mae: 29.2552 - val_loss: 3697.2091 - val_mse: 3697.2087 - val_mae: 23.2856\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 340us/step - loss: 2359.3130 - mse: 2359.3130 - mae: 28.8329 - val_loss: 3695.9392 - val_mse: 3695.9397 - val_mae: 24.0513\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 246us/step - loss: 2412.7131 - mse: 2412.7131 - mae: 29.7888 - val_loss: 3697.1963 - val_mse: 3697.1970 - val_mae: 23.5282\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2340.9633 - mse: 2340.9631 - mae: 29.3103 - val_loss: 3697.8384 - val_mse: 3697.8386 - val_mae: 23.8367\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2405.2927 - mse: 2405.2927 - mae: 29.5843 - val_loss: 3697.0963 - val_mse: 3697.0964 - val_mae: 23.7598\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2390.7353 - mse: 2390.7354 - mae: 29.3576 - val_loss: 3696.2689 - val_mse: 3696.2688 - val_mae: 23.5176\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2348.0862 - mse: 2348.0864 - mae: 29.1277 - val_loss: 3696.1277 - val_mse: 3696.1265 - val_mae: 23.7816\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2329.5645 - mse: 2329.5637 - mae: 29.2966 - val_loss: 3695.5496 - val_mse: 3695.5498 - val_mae: 23.6967\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 351us/step - loss: 2384.2910 - mse: 2384.2910 - mae: 29.4668 - val_loss: 3693.7725 - val_mse: 3693.7720 - val_mae: 23.4953\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 262us/step - loss: 2353.6460 - mse: 2353.6462 - mae: 29.0973 - val_loss: 3692.9673 - val_mse: 3692.9673 - val_mae: 23.8652\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 257us/step - loss: 2345.7618 - mse: 2345.7620 - mae: 29.0589 - val_loss: 3694.4913 - val_mse: 3694.4917 - val_mae: 23.4793\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2340.9318 - mse: 2340.9324 - mae: 29.0576 - val_loss: 3692.9671 - val_mse: 3692.9675 - val_mae: 23.4718\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2376.3171 - mse: 2376.3167 - mae: 29.3367 - val_loss: 3691.5354 - val_mse: 3691.5352 - val_mae: 23.7491\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2334.7101 - mse: 2334.7100 - mae: 28.8480 - val_loss: 3691.9052 - val_mse: 3691.9045 - val_mae: 23.8035\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 332us/step - loss: 2372.5052 - mse: 2372.5061 - mae: 29.4919 - val_loss: 3693.1261 - val_mse: 3693.1255 - val_mae: 23.6592\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 342us/step - loss: 2373.1065 - mse: 2373.1072 - mae: 29.6341 - val_loss: 3695.0560 - val_mse: 3695.0559 - val_mae: 23.4051\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2367.6584 - mse: 2367.6587 - mae: 29.3578 - val_loss: 3694.8229 - val_mse: 3694.8223 - val_mae: 23.8237\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2284.2282 - mse: 2284.2283 - mae: 28.8607 - val_loss: 3693.3064 - val_mse: 3693.3059 - val_mae: 24.0841\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 339us/step - loss: 2235.1062 - mse: 2235.1072 - mae: 28.4980 - val_loss: 3691.9744 - val_mse: 3691.9739 - val_mae: 23.9744\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2365.9896 - mse: 2365.9897 - mae: 29.3314 - val_loss: 3692.9306 - val_mse: 3692.9304 - val_mae: 23.7776\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2329.3387 - mse: 2329.3384 - mae: 29.1313 - val_loss: 3695.3523 - val_mse: 3695.3521 - val_mae: 24.0492\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 329us/step - loss: 2292.0684 - mse: 2292.0681 - mae: 29.0390 - val_loss: 3692.7819 - val_mse: 3692.7825 - val_mae: 23.7479\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2386.5179 - mse: 2386.5188 - mae: 29.2715 - val_loss: 3695.3963 - val_mse: 3695.3955 - val_mae: 23.3915\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2374.7697 - mse: 2374.7693 - mae: 29.3575 - val_loss: 3693.2332 - val_mse: 3693.2339 - val_mae: 23.6555\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2338.7957 - mse: 2338.7966 - mae: 29.1779 - val_loss: 3694.4289 - val_mse: 3694.4290 - val_mae: 24.0746\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 263us/step - loss: 2332.3502 - mse: 2332.3501 - mae: 29.0757 - val_loss: 3692.9753 - val_mse: 3692.9753 - val_mae: 23.2567\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 337us/step - loss: 2268.3876 - mse: 2268.3882 - mae: 28.7996 - val_loss: 3693.4211 - val_mse: 3693.4209 - val_mae: 24.2463\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2329.4390 - mse: 2329.4392 - mae: 29.3054 - val_loss: 3693.4453 - val_mse: 3693.4441 - val_mae: 23.7049\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2369.4024 - mse: 2369.4023 - mae: 29.3157 - val_loss: 3693.3875 - val_mse: 3693.3875 - val_mae: 23.7510\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2311.9272 - mse: 2311.9272 - mae: 28.9242 - val_loss: 3695.5457 - val_mse: 3695.5452 - val_mae: 23.6878\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2318.8490 - mse: 2318.8486 - mae: 29.0430 - val_loss: 3697.8067 - val_mse: 3697.8062 - val_mae: 23.3716\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2319.0601 - mse: 2319.0603 - mae: 29.0345 - val_loss: 3695.5441 - val_mse: 3695.5442 - val_mae: 23.3013\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2332.2561 - mse: 2332.2561 - mae: 29.0381 - val_loss: 3694.4464 - val_mse: 3694.4460 - val_mae: 23.6452\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2319.8400 - mse: 2319.8403 - mae: 29.0821 - val_loss: 3694.5013 - val_mse: 3694.5012 - val_mae: 23.9309\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2325.0112 - mse: 2325.0110 - mae: 29.1913 - val_loss: 3694.5870 - val_mse: 3694.5874 - val_mae: 23.5015\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2257.7148 - mse: 2257.7146 - mae: 28.7357 - val_loss: 3694.1859 - val_mse: 3694.1853 - val_mae: 23.5798\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2344.0799 - mse: 2344.0796 - mae: 29.1058 - val_loss: 3694.2745 - val_mse: 3694.2754 - val_mae: 23.4823\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2335.5004 - mse: 2335.5000 - mae: 28.7054 - val_loss: 3695.2172 - val_mse: 3695.2175 - val_mae: 23.8526\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2294.2108 - mse: 2294.2102 - mae: 29.1732 - val_loss: 3696.0906 - val_mse: 3696.0911 - val_mae: 23.5138\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2311.4853 - mse: 2311.4861 - mae: 29.1066 - val_loss: 3695.2143 - val_mse: 3695.2146 - val_mae: 23.5268\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2721.5339 - mse: 2721.5347 - mae: 28.6691 - val_loss: 2219.1035 - val_mse: 2219.1033 - val_mae: 26.6337\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2672.9369 - mse: 2672.9373 - mae: 28.2442 - val_loss: 2215.2200 - val_mse: 2215.2197 - val_mae: 26.8169\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2689.2535 - mse: 2689.2537 - mae: 28.5004 - val_loss: 2214.5524 - val_mse: 2214.5522 - val_mae: 26.7925\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2711.1380 - mse: 2711.1375 - mae: 28.3703 - val_loss: 2221.6288 - val_mse: 2221.6287 - val_mae: 26.6430\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2703.7131 - mse: 2703.7129 - mae: 28.2427 - val_loss: 2222.7767 - val_mse: 2222.7766 - val_mae: 26.6963\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2686.5039 - mse: 2686.5039 - mae: 28.4652 - val_loss: 2217.6978 - val_mse: 2217.6978 - val_mae: 26.9083\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2715.1455 - mse: 2715.1453 - mae: 28.5217 - val_loss: 2233.6359 - val_mse: 2233.6362 - val_mae: 26.6423\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2701.1626 - mse: 2701.1621 - mae: 28.6564 - val_loss: 2217.3971 - val_mse: 2217.3975 - val_mae: 26.8664\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 248us/step - loss: 2692.2752 - mse: 2692.2749 - mae: 28.5165 - val_loss: 2225.1265 - val_mse: 2225.1267 - val_mae: 26.6211\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2704.9375 - mse: 2704.9360 - mae: 28.3399 - val_loss: 2229.7630 - val_mse: 2229.7629 - val_mae: 26.6530\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2729.9346 - mse: 2729.9346 - mae: 28.5174 - val_loss: 2235.8926 - val_mse: 2235.8926 - val_mae: 26.5007\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2671.2418 - mse: 2671.2422 - mae: 28.2133 - val_loss: 2227.1324 - val_mse: 2227.1323 - val_mae: 26.8849\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2713.6424 - mse: 2713.6431 - mae: 28.5714 - val_loss: 2233.4869 - val_mse: 2233.4871 - val_mae: 26.7708\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2728.9750 - mse: 2728.9749 - mae: 28.5635 - val_loss: 2254.6205 - val_mse: 2254.6204 - val_mae: 26.3367\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2737.9243 - mse: 2737.9236 - mae: 28.3628 - val_loss: 2248.9728 - val_mse: 2248.9727 - val_mae: 26.6578\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2710.5619 - mse: 2710.5620 - mae: 28.3433 - val_loss: 2249.6489 - val_mse: 2249.6492 - val_mae: 26.8012\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2641.9407 - mse: 2641.9399 - mae: 27.8094 - val_loss: 2243.8691 - val_mse: 2243.8691 - val_mae: 26.9923\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2700.6616 - mse: 2700.6616 - mae: 28.4269 - val_loss: 2259.3325 - val_mse: 2259.3325 - val_mae: 26.4363\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 265us/step - loss: 2742.8868 - mse: 2742.8875 - mae: 28.4889 - val_loss: 2248.4005 - val_mse: 2248.4004 - val_mae: 26.7983\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 265us/step - loss: 2699.4699 - mse: 2699.4707 - mae: 28.4266 - val_loss: 2229.5288 - val_mse: 2229.5288 - val_mae: 27.0604\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 269us/step - loss: 2713.2665 - mse: 2713.2678 - mae: 28.2957 - val_loss: 2240.9895 - val_mse: 2240.9895 - val_mae: 26.6825\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2709.9046 - mse: 2709.9038 - mae: 28.3836 - val_loss: 2243.1642 - val_mse: 2243.1643 - val_mae: 26.7348\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2682.4573 - mse: 2682.4568 - mae: 27.9831 - val_loss: 2227.6965 - val_mse: 2227.6970 - val_mae: 27.0378\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2734.4651 - mse: 2734.4651 - mae: 28.5121 - val_loss: 2232.8130 - val_mse: 2232.8127 - val_mae: 26.4856\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 256us/step - loss: 2731.0721 - mse: 2731.0720 - mae: 28.3837 - val_loss: 2236.5915 - val_mse: 2236.5916 - val_mae: 26.5572\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 268us/step - loss: 2696.7980 - mse: 2696.7971 - mae: 28.4174 - val_loss: 2224.8862 - val_mse: 2224.8865 - val_mae: 27.0961\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2719.9232 - mse: 2719.9243 - mae: 28.7956 - val_loss: 2235.2396 - val_mse: 2235.2397 - val_mae: 26.7992\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2683.5641 - mse: 2683.5645 - mae: 28.5499 - val_loss: 2235.5201 - val_mse: 2235.5200 - val_mae: 26.7908\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2681.8178 - mse: 2681.8171 - mae: 28.1697 - val_loss: 2252.1720 - val_mse: 2252.1724 - val_mae: 26.6069\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2696.1321 - mse: 2696.1313 - mae: 28.0808 - val_loss: 2258.9330 - val_mse: 2258.9331 - val_mae: 26.6972\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 338us/step - loss: 2716.5614 - mse: 2716.5613 - mae: 28.4500 - val_loss: 2248.2619 - val_mse: 2248.2620 - val_mae: 26.9518\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2696.5722 - mse: 2696.5718 - mae: 28.3563 - val_loss: 2246.5147 - val_mse: 2246.5149 - val_mae: 26.8686\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2739.5508 - mse: 2739.5510 - mae: 28.4902 - val_loss: 2252.3924 - val_mse: 2252.3921 - val_mae: 26.6350\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 248us/step - loss: 2702.6668 - mse: 2702.6677 - mae: 28.4108 - val_loss: 2233.7122 - val_mse: 2233.7119 - val_mae: 27.0998\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2654.4508 - mse: 2654.4507 - mae: 28.3260 - val_loss: 2243.1327 - val_mse: 2243.1328 - val_mae: 26.5986\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2658.7105 - mse: 2658.7109 - mae: 28.1847 - val_loss: 2243.8018 - val_mse: 2243.8020 - val_mae: 26.6947\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2680.2373 - mse: 2680.2361 - mae: 28.1345 - val_loss: 2236.8913 - val_mse: 2236.8918 - val_mae: 26.8888\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2673.9007 - mse: 2673.9014 - mae: 28.2577 - val_loss: 2237.8489 - val_mse: 2237.8484 - val_mae: 26.9146\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2724.5873 - mse: 2724.5869 - mae: 28.7562 - val_loss: 2238.7395 - val_mse: 2238.7393 - val_mae: 26.5706\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2697.0973 - mse: 2697.0972 - mae: 28.5271 - val_loss: 2233.0968 - val_mse: 2233.0967 - val_mae: 26.9058\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2722.8598 - mse: 2722.8594 - mae: 28.6474 - val_loss: 2242.1383 - val_mse: 2242.1384 - val_mae: 26.4319\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2698.1558 - mse: 2698.1570 - mae: 28.1279 - val_loss: 2239.4208 - val_mse: 2239.4209 - val_mae: 26.9047\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2697.7661 - mse: 2697.7659 - mae: 28.4556 - val_loss: 2239.4922 - val_mse: 2239.4924 - val_mae: 26.7572\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2704.9686 - mse: 2704.9690 - mae: 28.1480 - val_loss: 2237.3603 - val_mse: 2237.3604 - val_mae: 26.8014\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2738.7335 - mse: 2738.7329 - mae: 28.5975 - val_loss: 2231.7150 - val_mse: 2231.7146 - val_mae: 26.7141\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 338us/step - loss: 2669.5745 - mse: 2669.5745 - mae: 28.1940 - val_loss: 2223.3837 - val_mse: 2223.3838 - val_mae: 27.0808\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2675.3519 - mse: 2675.3518 - mae: 28.1823 - val_loss: 2226.0729 - val_mse: 2226.0730 - val_mae: 26.9376\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 276us/step - loss: 2732.9238 - mse: 2732.9226 - mae: 28.5962 - val_loss: 2243.4122 - val_mse: 2243.4119 - val_mae: 26.7066\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2706.6902 - mse: 2706.6907 - mae: 28.1279 - val_loss: 2237.2017 - val_mse: 2237.2017 - val_mae: 26.8545\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2714.6188 - mse: 2714.6189 - mae: 28.4549 - val_loss: 2236.5828 - val_mse: 2236.5828 - val_mae: 27.0057\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2681.5935 - mse: 2681.5923 - mae: 28.2893 - val_loss: 2249.2305 - val_mse: 2249.2305 - val_mae: 26.7763\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 366us/step - loss: 2658.4952 - mse: 2658.4949 - mae: 28.2498 - val_loss: 2253.3344 - val_mse: 2253.3342 - val_mae: 26.7897\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 339us/step - loss: 2720.4808 - mse: 2720.4802 - mae: 28.5706 - val_loss: 2248.2509 - val_mse: 2248.2512 - val_mae: 27.0057\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2682.5175 - mse: 2682.5176 - mae: 28.2389 - val_loss: 2241.2610 - val_mse: 2241.2605 - val_mae: 27.0425\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2659.8008 - mse: 2659.8008 - mae: 27.9411 - val_loss: 2254.2520 - val_mse: 2254.2520 - val_mae: 27.0204\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2654.4872 - mse: 2654.4873 - mae: 28.5571 - val_loss: 2253.0926 - val_mse: 2253.0928 - val_mae: 26.9241\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2716.7246 - mse: 2716.7239 - mae: 28.7020 - val_loss: 2254.9604 - val_mse: 2254.9600 - val_mae: 26.7546\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2706.8125 - mse: 2706.8123 - mae: 28.3548 - val_loss: 2247.4294 - val_mse: 2247.4297 - val_mae: 26.8207\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2687.8707 - mse: 2687.8701 - mae: 28.5381 - val_loss: 2248.1130 - val_mse: 2248.1130 - val_mae: 26.8918\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2693.5660 - mse: 2693.5649 - mae: 28.3112 - val_loss: 2244.9727 - val_mse: 2244.9727 - val_mae: 27.0144\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2656.0180 - mse: 2656.0173 - mae: 28.3662 - val_loss: 2264.5121 - val_mse: 2264.5120 - val_mae: 26.6666\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2722.1501 - mse: 2722.1501 - mae: 28.5401 - val_loss: 2262.8797 - val_mse: 2262.8801 - val_mae: 26.8068\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2668.8741 - mse: 2668.8743 - mae: 28.4414 - val_loss: 2251.0746 - val_mse: 2251.0745 - val_mae: 26.8311\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2701.9935 - mse: 2701.9924 - mae: 28.2343 - val_loss: 2244.6908 - val_mse: 2244.6909 - val_mae: 27.0590\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2679.5108 - mse: 2679.5120 - mae: 28.2232 - val_loss: 2261.1547 - val_mse: 2261.1548 - val_mae: 26.7298\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2679.8140 - mse: 2679.8132 - mae: 28.1935 - val_loss: 2263.1652 - val_mse: 2263.1648 - val_mae: 26.8821\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2703.8907 - mse: 2703.8901 - mae: 28.3059 - val_loss: 2257.7427 - val_mse: 2257.7429 - val_mae: 26.8700\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 260us/step - loss: 2677.0235 - mse: 2677.0234 - mae: 28.0878 - val_loss: 2264.7741 - val_mse: 2264.7737 - val_mae: 26.8622\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 261us/step - loss: 2721.2141 - mse: 2721.2141 - mae: 28.4107 - val_loss: 2273.5817 - val_mse: 2273.5818 - val_mae: 26.9148\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2651.9961 - mse: 2651.9968 - mae: 27.9093 - val_loss: 2270.5531 - val_mse: 2270.5535 - val_mae: 26.9462\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2671.6757 - mse: 2671.6755 - mae: 28.4163 - val_loss: 2258.1605 - val_mse: 2258.1606 - val_mae: 27.4220\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2698.1733 - mse: 2698.1736 - mae: 28.3824 - val_loss: 2261.3133 - val_mse: 2261.3132 - val_mae: 27.1053\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2615.1099 - mse: 2615.1091 - mae: 28.1707 - val_loss: 2265.6332 - val_mse: 2265.6331 - val_mae: 27.0185\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2691.1331 - mse: 2691.1340 - mae: 28.3250 - val_loss: 2256.3383 - val_mse: 2256.3381 - val_mae: 27.0840\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2656.0736 - mse: 2656.0737 - mae: 27.9385 - val_loss: 2249.9053 - val_mse: 2249.9053 - val_mae: 27.3018\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2653.8860 - mse: 2653.8865 - mae: 28.0214 - val_loss: 2260.4697 - val_mse: 2260.4697 - val_mae: 27.1947\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2645.2977 - mse: 2645.2988 - mae: 28.1807 - val_loss: 2249.5029 - val_mse: 2249.5029 - val_mae: 27.3420\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2673.3773 - mse: 2673.3770 - mae: 28.1183 - val_loss: 2256.7997 - val_mse: 2256.7998 - val_mae: 27.0204\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 342us/step - loss: 2663.1929 - mse: 2663.1936 - mae: 28.1188 - val_loss: 2259.7254 - val_mse: 2259.7256 - val_mae: 27.0745\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2682.0350 - mse: 2682.0344 - mae: 28.0778 - val_loss: 2259.4008 - val_mse: 2259.4009 - val_mae: 27.0813\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13329.8273 - mse: 13329.8271 - mae: 109.9080 - val_loss: 34618.7980 - val_mse: 34618.7969 - val_mae: 132.7346\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 13172.2557 - mse: 13172.2559 - mae: 109.1968 - val_loss: 34296.7039 - val_mse: 34296.7031 - val_mae: 131.5378\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 233us/step - loss: 12674.4698 - mse: 12674.4697 - mae: 106.9023 - val_loss: 33321.4535 - val_mse: 33321.4531 - val_mae: 127.8525\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 259us/step - loss: 11347.9554 - mse: 11347.9551 - mae: 100.5153 - val_loss: 30692.8311 - val_mse: 30692.8320 - val_mae: 117.3523\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 8382.2059 - mse: 8382.2051 - mae: 83.5117 - val_loss: 24803.8661 - val_mse: 24803.8652 - val_mae: 89.5235\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 4195.1881 - mse: 4195.1880 - mae: 51.1739 - val_loss: 18070.2518 - val_mse: 18070.2520 - val_mae: 41.9330\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 356us/step - loss: 2589.4130 - mse: 2589.4131 - mae: 36.8050 - val_loss: 17373.3376 - val_mse: 17373.3379 - val_mae: 37.4055\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 2777.3411 - mse: 2777.3413 - mae: 37.7624 - val_loss: 17589.3531 - val_mse: 17589.3516 - val_mae: 38.5120\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 280us/step - loss: 2844.2017 - mse: 2844.2017 - mae: 38.3223 - val_loss: 17441.1777 - val_mse: 17441.1777 - val_mae: 37.6487\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2494.5114 - mse: 2494.5112 - mae: 36.1600 - val_loss: 17365.8175 - val_mse: 17365.8184 - val_mae: 37.2470\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2531.0919 - mse: 2531.0918 - mae: 36.3083 - val_loss: 17499.1041 - val_mse: 17499.1035 - val_mae: 37.8727\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2409.0763 - mse: 2409.0762 - mae: 36.4992 - val_loss: 17568.3055 - val_mse: 17568.3066 - val_mae: 38.1984\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2571.0756 - mse: 2571.0754 - mae: 35.6869 - val_loss: 17622.9413 - val_mse: 17622.9395 - val_mae: 38.4320\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2658.0727 - mse: 2658.0728 - mae: 37.3416 - val_loss: 17534.6457 - val_mse: 17534.6465 - val_mae: 37.8905\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2508.8744 - mse: 2508.8745 - mae: 36.0980 - val_loss: 17350.2284 - val_mse: 17350.2285 - val_mae: 36.9820\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 288us/step - loss: 2549.8234 - mse: 2549.8235 - mae: 35.5889 - val_loss: 17431.2746 - val_mse: 17431.2754 - val_mae: 37.2234\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 369us/step - loss: 2381.5377 - mse: 2381.5378 - mae: 34.3864 - val_loss: 17484.5695 - val_mse: 17484.5703 - val_mae: 37.4506\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 391us/step - loss: 2581.3331 - mse: 2581.3330 - mae: 36.5095 - val_loss: 17610.3223 - val_mse: 17610.3223 - val_mae: 38.1130\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2675.4490 - mse: 2675.4487 - mae: 36.2445 - val_loss: 17646.4402 - val_mse: 17646.4395 - val_mae: 38.2927\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 385us/step - loss: 2182.8881 - mse: 2182.8879 - mae: 34.2999 - val_loss: 17334.2265 - val_mse: 17334.2266 - val_mae: 36.8128\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 336us/step - loss: 2451.0178 - mse: 2451.0178 - mae: 36.0407 - val_loss: 17435.3592 - val_mse: 17435.3574 - val_mae: 37.0706\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 273us/step - loss: 2206.5397 - mse: 2206.5398 - mae: 34.1295 - val_loss: 17384.7519 - val_mse: 17384.7520 - val_mae: 36.8798\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 353us/step - loss: 2457.8297 - mse: 2457.8301 - mae: 35.2870 - val_loss: 17488.6579 - val_mse: 17488.6562 - val_mae: 37.2517\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2248.5141 - mse: 2248.5139 - mae: 33.7631 - val_loss: 17256.3575 - val_mse: 17256.3555 - val_mae: 36.6356\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 2159.6646 - mse: 2159.6648 - mae: 33.0275 - val_loss: 17328.7152 - val_mse: 17328.7168 - val_mae: 36.7120\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 2207.1457 - mse: 2207.1455 - mae: 33.3102 - val_loss: 17415.4672 - val_mse: 17415.4688 - val_mae: 36.8699\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 406us/step - loss: 2360.6459 - mse: 2360.6462 - mae: 34.0785 - val_loss: 17424.6933 - val_mse: 17424.6934 - val_mae: 36.8660\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 257us/step - loss: 2372.2935 - mse: 2372.2932 - mae: 34.3064 - val_loss: 17306.4733 - val_mse: 17306.4746 - val_mae: 36.6135\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2453.9500 - mse: 2453.9502 - mae: 35.8789 - val_loss: 17603.1034 - val_mse: 17603.1035 - val_mae: 37.6067\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 391us/step - loss: 2217.6460 - mse: 2217.6458 - mae: 33.3997 - val_loss: 17453.2930 - val_mse: 17453.2949 - val_mae: 36.8688\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2377.7812 - mse: 2377.7812 - mae: 34.4850 - val_loss: 17509.8094 - val_mse: 17509.8105 - val_mae: 37.0543\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2024.9392 - mse: 2024.9390 - mae: 32.0083 - val_loss: 17437.4082 - val_mse: 17437.4102 - val_mae: 36.7937\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 339us/step - loss: 2268.5655 - mse: 2268.5654 - mae: 32.8768 - val_loss: 17433.4183 - val_mse: 17433.4180 - val_mae: 36.7593\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2317.6010 - mse: 2317.6011 - mae: 34.2082 - val_loss: 17465.7527 - val_mse: 17465.7520 - val_mae: 36.8208\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 287us/step - loss: 2137.1774 - mse: 2137.1775 - mae: 33.1789 - val_loss: 17577.9345 - val_mse: 17577.9355 - val_mae: 37.2574\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2299.5173 - mse: 2299.5173 - mae: 34.0134 - val_loss: 17379.2684 - val_mse: 17379.2676 - val_mae: 36.6096\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 343us/step - loss: 2285.0391 - mse: 2285.0388 - mae: 32.1851 - val_loss: 17411.7293 - val_mse: 17411.7305 - val_mae: 36.6346\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 2351.2875 - mse: 2351.2876 - mae: 33.07 - 0s 273us/step - loss: 2239.0930 - mse: 2239.0930 - mae: 32.1668 - val_loss: 17279.9953 - val_mse: 17279.9961 - val_mae: 36.4696\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 334us/step - loss: 2262.8191 - mse: 2262.8191 - mae: 33.2594 - val_loss: 17697.5517 - val_mse: 17697.5508 - val_mae: 37.7688\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 237us/step - loss: 2133.9091 - mse: 2133.9094 - mae: 31.9267 - val_loss: 17242.3397 - val_mse: 17242.3398 - val_mae: 36.4101\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 2308.5219 - mse: 2308.5217 - mae: 33.3039 - val_loss: 17487.1945 - val_mse: 17487.1953 - val_mae: 36.7274\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2189.9760 - mse: 2189.9761 - mae: 31.7866 - val_loss: 17366.8526 - val_mse: 17366.8516 - val_mae: 36.5165\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 2191.0291 - mse: 2191.0295 - mae: 32.6400 - val_loss: 17368.1555 - val_mse: 17368.1562 - val_mae: 36.5155\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2134.7771 - mse: 2134.7773 - mae: 32.3740 - val_loss: 17504.5398 - val_mse: 17504.5391 - val_mae: 36.7406\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 344us/step - loss: 2291.1199 - mse: 2291.1196 - mae: 33.6415 - val_loss: 17593.5570 - val_mse: 17593.5566 - val_mae: 37.0366\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 394us/step - loss: 2151.3117 - mse: 2151.3120 - mae: 32.0910 - val_loss: 17409.1043 - val_mse: 17409.1035 - val_mae: 36.5436\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 377us/step - loss: 2108.6773 - mse: 2108.6775 - mae: 31.9673 - val_loss: 17499.7955 - val_mse: 17499.7969 - val_mae: 36.6915\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 2047.8094 - mse: 2047.8094 - mae: 31.7573 - val_loss: 17569.2690 - val_mse: 17569.2676 - val_mae: 36.8518\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 387us/step - loss: 2081.5151 - mse: 2081.5151 - mae: 31.8464 - val_loss: 17379.1250 - val_mse: 17379.1230 - val_mae: 36.4470\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 288us/step - loss: 1891.6658 - mse: 1891.6659 - mae: 31.1871 - val_loss: 17102.6321 - val_mse: 17102.6328 - val_mae: 36.3224\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 243us/step - loss: 2175.3572 - mse: 2175.3572 - mae: 33.3984 - val_loss: 17635.8470 - val_mse: 17635.8477 - val_mae: 37.0001\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 389us/step - loss: 2101.8240 - mse: 2101.8240 - mae: 32.0629 - val_loss: 17487.8430 - val_mse: 17487.8438 - val_mae: 36.5407\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 397us/step - loss: 2107.7611 - mse: 2107.7615 - mae: 32.5758 - val_loss: 17594.1882 - val_mse: 17594.1875 - val_mae: 36.7893\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 386us/step - loss: 2310.3246 - mse: 2310.3247 - mae: 33.9480 - val_loss: 17531.8892 - val_mse: 17531.8887 - val_mae: 36.6129\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 325us/step - loss: 1982.3781 - mse: 1982.3781 - mae: 30.9431 - val_loss: 17340.8918 - val_mse: 17340.8926 - val_mae: 36.3356\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 271us/step - loss: 1887.7825 - mse: 1887.7825 - mae: 31.6814 - val_loss: 17346.5465 - val_mse: 17346.5469 - val_mae: 36.3375\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 213us/step - loss: 2017.2929 - mse: 2017.2927 - mae: 31.3269 - val_loss: 17432.4082 - val_mse: 17432.4082 - val_mae: 36.4163\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 1991.3860 - mse: 1991.3857 - mae: 31.5080 - val_loss: 17387.0808 - val_mse: 17387.0801 - val_mae: 36.3600\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 334us/step - loss: 2197.8887 - mse: 2197.8887 - mae: 32.3660 - val_loss: 17606.4292 - val_mse: 17606.4297 - val_mae: 36.7315\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 340us/step - loss: 1952.0305 - mse: 1952.0304 - mae: 29.8027 - val_loss: 17316.0276 - val_mse: 17316.0273 - val_mae: 36.2812\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 254us/step - loss: 1960.1210 - mse: 1960.1212 - mae: 30.4700 - val_loss: 17510.3760 - val_mse: 17510.3730 - val_mae: 36.5126\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 254us/step - loss: 1984.2877 - mse: 1984.2877 - mae: 30.9608 - val_loss: 17381.1688 - val_mse: 17381.1680 - val_mae: 36.3501\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 2084.5294 - mse: 2084.5291 - mae: 31.6971 - val_loss: 17315.3101 - val_mse: 17315.3105 - val_mae: 36.2699\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 329us/step - loss: 2105.7597 - mse: 2105.7598 - mae: 32.3113 - val_loss: 17632.8458 - val_mse: 17632.8457 - val_mae: 36.7564\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 255us/step - loss: 2106.1485 - mse: 2106.1484 - mae: 31.9112 - val_loss: 17449.4071 - val_mse: 17449.4082 - val_mae: 36.3726\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2148.3729 - mse: 2148.3730 - mae: 31.5972 - val_loss: 17460.5689 - val_mse: 17460.5684 - val_mae: 36.3779\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 273us/step - loss: 2128.3495 - mse: 2128.3496 - mae: 31.7534 - val_loss: 17555.6150 - val_mse: 17555.6152 - val_mae: 36.5311\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 245us/step - loss: 1994.1131 - mse: 1994.1132 - mae: 30.7516 - val_loss: 17330.1542 - val_mse: 17330.1543 - val_mae: 36.2530\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 1825.4601 - mse: 1825.4603 - mae: 29.8342 - val_loss: 17354.3988 - val_mse: 17354.3984 - val_mae: 36.2695\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 247us/step - loss: 1812.4618 - mse: 1812.4619 - mae: 30.1892 - val_loss: 17233.3715 - val_mse: 17233.3711 - val_mae: 36.2453\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 244us/step - loss: 1804.1350 - mse: 1804.1349 - mae: 29.7560 - val_loss: 17505.4104 - val_mse: 17505.4102 - val_mae: 36.4201\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 240us/step - loss: 2090.6072 - mse: 2090.6072 - mae: 31.2827 - val_loss: 17518.9008 - val_mse: 17518.9004 - val_mae: 36.4266\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 283us/step - loss: 1777.8553 - mse: 1777.8555 - mae: 29.2959 - val_loss: 17441.1157 - val_mse: 17441.1152 - val_mae: 36.3217\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 396us/step - loss: 1967.6195 - mse: 1967.6195 - mae: 30.6132 - val_loss: 17343.1519 - val_mse: 17343.1523 - val_mae: 36.2194\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 1915.5326 - mse: 1915.5327 - mae: 29.8465 - val_loss: 17307.9553 - val_mse: 17307.9551 - val_mae: 36.2120\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 1852.1565 - mse: 1852.1566 - mae: 29.8214 - val_loss: 17301.3405 - val_mse: 17301.3398 - val_mae: 36.2200\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 280us/step - loss: 1657.0741 - mse: 1657.0741 - mae: 28.1960 - val_loss: 17350.7590 - val_mse: 17350.7598 - val_mae: 36.2284\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 276us/step - loss: 1897.7356 - mse: 1897.7355 - mae: 30.2842 - val_loss: 17508.7607 - val_mse: 17508.7598 - val_mae: 36.3930\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 1861.1649 - mse: 1861.1649 - mae: 28.7268 - val_loss: 17294.9065 - val_mse: 17294.9082 - val_mae: 36.2301\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 290us/step - loss: 2110.0583 - mse: 2110.0581 - mae: 31.9026 - val_loss: 17554.5497 - val_mse: 17554.5508 - val_mae: 36.4463\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 279us/step - loss: 4227.1544 - mse: 4227.1538 - mae: 34.8045 - val_loss: 2240.1690 - val_mse: 2240.1689 - val_mae: 30.7058\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4342.5378 - mse: 4342.5376 - mae: 34.8086 - val_loss: 2293.7058 - val_mse: 2293.7056 - val_mae: 30.9386\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4224.9519 - mse: 4224.9517 - mae: 35.6930 - val_loss: 2222.3361 - val_mse: 2222.3359 - val_mae: 30.5870\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 4369.1059 - mse: 4369.1060 - mae: 35.6007 - val_loss: 2327.4975 - val_mse: 2327.4976 - val_mae: 31.0844\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 354us/step - loss: 4260.7500 - mse: 4260.7505 - mae: 34.8064 - val_loss: 2340.0107 - val_mse: 2340.0107 - val_mae: 31.1406\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 348us/step - loss: 4253.0995 - mse: 4253.0996 - mae: 34.8676 - val_loss: 2346.7369 - val_mse: 2346.7371 - val_mae: 31.1694\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 358us/step - loss: 4099.6211 - mse: 4099.6211 - mae: 34.5704 - val_loss: 2343.6391 - val_mse: 2343.6392 - val_mae: 31.1547\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4347.9932 - mse: 4347.9941 - mae: 35.0358 - val_loss: 2292.7843 - val_mse: 2292.7844 - val_mae: 30.9073\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 333us/step - loss: 4132.0788 - mse: 4132.0786 - mae: 34.8136 - val_loss: 2262.4384 - val_mse: 2262.4382 - val_mae: 30.7641\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 340us/step - loss: 4000.2275 - mse: 4000.2273 - mae: 33.6994 - val_loss: 2271.9464 - val_mse: 2271.9463 - val_mae: 30.8122\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4058.9028 - mse: 4058.9026 - mae: 34.4676 - val_loss: 2268.9299 - val_mse: 2268.9302 - val_mae: 30.7924\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 280us/step - loss: 4010.4301 - mse: 4010.4312 - mae: 34.1418 - val_loss: 2302.4022 - val_mse: 2302.4021 - val_mae: 30.9423\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 4203.1377 - mse: 4203.1377 - mae: 35.0474 - val_loss: 2295.7349 - val_mse: 2295.7351 - val_mae: 30.9091\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4224.6671 - mse: 4224.6670 - mae: 34.6341 - val_loss: 2322.9972 - val_mse: 2322.9971 - val_mae: 31.0356\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 236us/step - loss: 4164.9417 - mse: 4164.9419 - mae: 34.6215 - val_loss: 2310.9303 - val_mse: 2310.9304 - val_mae: 30.9804\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 235us/step - loss: 4198.9919 - mse: 4198.9927 - mae: 34.8527 - val_loss: 2250.0626 - val_mse: 2250.0625 - val_mae: 30.6915\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4287.8028 - mse: 4287.8022 - mae: 34.6697 - val_loss: 2345.2700 - val_mse: 2345.2700 - val_mae: 31.1372\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 350us/step - loss: 4135.0946 - mse: 4135.0942 - mae: 33.8988 - val_loss: 2300.5623 - val_mse: 2300.5623 - val_mae: 30.9173\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 233us/step - loss: 4140.5117 - mse: 4140.5122 - mae: 34.0680 - val_loss: 2265.1593 - val_mse: 2265.1594 - val_mae: 30.7488\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4387.0827 - mse: 4387.0825 - mae: 35.9483 - val_loss: 2363.7803 - val_mse: 2363.7803 - val_mae: 31.2189\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4143.9881 - mse: 4143.9888 - mae: 33.5531 - val_loss: 2321.0306 - val_mse: 2321.0308 - val_mae: 31.0102\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 229us/step - loss: 4155.3853 - mse: 4155.3848 - mae: 34.5129 - val_loss: 2275.8147 - val_mse: 2275.8147 - val_mae: 30.8045\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 236us/step - loss: 4108.2190 - mse: 4108.2192 - mae: 34.1290 - val_loss: 2258.5952 - val_mse: 2258.5950 - val_mae: 30.7245\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4166.9195 - mse: 4166.9199 - mae: 33.9258 - val_loss: 2273.9245 - val_mse: 2273.9246 - val_mae: 30.7929\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 333us/step - loss: 4121.7877 - mse: 4121.7886 - mae: 34.9520 - val_loss: 2292.0356 - val_mse: 2292.0354 - val_mae: 30.8784\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4167.9217 - mse: 4167.9209 - mae: 35.8338 - val_loss: 2325.0265 - val_mse: 2325.0266 - val_mae: 31.0332\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4107.1375 - mse: 4107.1382 - mae: 34.4726 - val_loss: 2302.0439 - val_mse: 2302.0439 - val_mae: 30.9206\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 350us/step - loss: 4175.5717 - mse: 4175.5718 - mae: 34.2815 - val_loss: 2321.4172 - val_mse: 2321.4170 - val_mae: 31.0082\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 4297.0744 - mse: 4297.0737 - mae: 34.9774 - val_loss: 2322.5079 - val_mse: 2322.5081 - val_mae: 31.0113\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4160.4031 - mse: 4160.4028 - mae: 33.1586 - val_loss: 2308.9226 - val_mse: 2308.9229 - val_mae: 30.9472\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 344us/step - loss: 4292.8745 - mse: 4292.8740 - mae: 35.0415 - val_loss: 2346.5879 - val_mse: 2346.5879 - val_mae: 31.1276\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 225us/step - loss: 4058.6384 - mse: 4058.6389 - mae: 33.5730 - val_loss: 2261.4092 - val_mse: 2261.4092 - val_mae: 30.7325\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 214us/step - loss: 4021.7137 - mse: 4021.7136 - mae: 34.2523 - val_loss: 2270.0990 - val_mse: 2270.0986 - val_mae: 30.7642\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4356.4101 - mse: 4356.4097 - mae: 34.9586 - val_loss: 2374.6070 - val_mse: 2374.6072 - val_mae: 31.2585\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 295us/step - loss: 4101.8411 - mse: 4101.8403 - mae: 33.8485 - val_loss: 2247.3292 - val_mse: 2247.3293 - val_mae: 30.6514\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 228us/step - loss: 4205.8556 - mse: 4205.8550 - mae: 34.8763 - val_loss: 2325.7356 - val_mse: 2325.7356 - val_mae: 31.0091\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 229us/step - loss: 3903.6767 - mse: 3903.6768 - mae: 33.0031 - val_loss: 2302.8283 - val_mse: 2302.8284 - val_mae: 30.9002\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 233us/step - loss: 4036.0523 - mse: 4036.0530 - mae: 34.2029 - val_loss: 2301.1015 - val_mse: 2301.1013 - val_mae: 30.8965\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 249us/step - loss: 4175.5734 - mse: 4175.5728 - mae: 34.7277 - val_loss: 2333.4314 - val_mse: 2333.4312 - val_mae: 31.0488\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 371us/step - loss: 4144.9957 - mse: 4144.9961 - mae: 34.1292 - val_loss: 2335.0981 - val_mse: 2335.0984 - val_mae: 31.0541\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4014.0621 - mse: 4014.0623 - mae: 33.7645 - val_loss: 2311.6834 - val_mse: 2311.6836 - val_mae: 30.9414\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 339us/step - loss: 3982.7066 - mse: 3982.7063 - mae: 32.7767 - val_loss: 2310.9823 - val_mse: 2310.9824 - val_mae: 30.9397\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4100.2866 - mse: 4100.2861 - mae: 33.8610 - val_loss: 2318.6944 - val_mse: 2318.6946 - val_mae: 30.9805\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4064.4109 - mse: 4064.4114 - mae: 33.8195 - val_loss: 2359.9676 - val_mse: 2359.9678 - val_mae: 31.1759\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4199.6761 - mse: 4199.6758 - mae: 34.1763 - val_loss: 2338.7287 - val_mse: 2338.7288 - val_mae: 31.0686\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4152.6223 - mse: 4152.6221 - mae: 34.3112 - val_loss: 2369.1570 - val_mse: 2369.1567 - val_mae: 31.2196\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 295us/step - loss: 4150.4520 - mse: 4150.4512 - mae: 33.8840 - val_loss: 2376.2305 - val_mse: 2376.2305 - val_mae: 31.2597\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 3974.6664 - mse: 3974.6665 - mae: 33.5062 - val_loss: 2341.4523 - val_mse: 2341.4521 - val_mae: 31.0877\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 348us/step - loss: 3950.1183 - mse: 3950.1184 - mae: 33.0886 - val_loss: 2359.2903 - val_mse: 2359.2900 - val_mae: 31.1744\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 378us/step - loss: 3926.3373 - mse: 3926.3372 - mae: 33.2094 - val_loss: 2272.3431 - val_mse: 2272.3433 - val_mae: 30.7708\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 343us/step - loss: 4149.4829 - mse: 4149.4829 - mae: 33.4323 - val_loss: 2326.0705 - val_mse: 2326.0703 - val_mae: 31.0033\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 402us/step - loss: 4152.9023 - mse: 4152.9033 - mae: 33.5517 - val_loss: 2317.0090 - val_mse: 2317.0090 - val_mae: 30.9607\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 406us/step - loss: 4032.7408 - mse: 4032.7410 - mae: 33.8226 - val_loss: 2297.0205 - val_mse: 2297.0205 - val_mae: 30.8736\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 346us/step - loss: 3880.3363 - mse: 3880.3367 - mae: 33.3958 - val_loss: 2321.2737 - val_mse: 2321.2737 - val_mae: 30.9880\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 342us/step - loss: 4061.0334 - mse: 4061.0334 - mae: 33.4686 - val_loss: 2289.3136 - val_mse: 2289.3137 - val_mae: 30.8463\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4220.7211 - mse: 4220.7212 - mae: 33.9369 - val_loss: 2351.1807 - val_mse: 2351.1804 - val_mae: 31.1341\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 3934.4190 - mse: 3934.4194 - mae: 33.3920 - val_loss: 2316.1717 - val_mse: 2316.1716 - val_mae: 30.9740\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4027.5738 - mse: 4027.5737 - mae: 34.2354 - val_loss: 2378.8585 - val_mse: 2378.8584 - val_mae: 31.2703\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4044.9611 - mse: 4044.9614 - mae: 33.0271 - val_loss: 2349.6681 - val_mse: 2349.6682 - val_mae: 31.1289\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4042.7382 - mse: 4042.7380 - mae: 32.7304 - val_loss: 2260.9109 - val_mse: 2260.9106 - val_mae: 30.7221\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4103.3741 - mse: 4103.3740 - mae: 33.9888 - val_loss: 2338.1980 - val_mse: 2338.1982 - val_mae: 31.0777\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4120.5511 - mse: 4120.5513 - mae: 34.2454 - val_loss: 2348.5622 - val_mse: 2348.5623 - val_mae: 31.1328\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 3945.5304 - mse: 3945.5300 - mae: 32.6680 - val_loss: 2277.1686 - val_mse: 2277.1685 - val_mae: 30.8105\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 301us/step - loss: 3990.2540 - mse: 3990.2539 - mae: 33.3253 - val_loss: 2330.0371 - val_mse: 2330.0371 - val_mae: 31.0518\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 241us/step - loss: 4071.1753 - mse: 4071.1755 - mae: 33.5471 - val_loss: 2317.8579 - val_mse: 2317.8577 - val_mae: 30.9965\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4050.6953 - mse: 4050.6948 - mae: 33.0109 - val_loss: 2376.0936 - val_mse: 2376.0933 - val_mae: 31.2662\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 3914.2706 - mse: 3914.2703 - mae: 32.1479 - val_loss: 2298.9585 - val_mse: 2298.9590 - val_mae: 30.9030\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 245us/step - loss: 3973.7287 - mse: 3973.7285 - mae: 34.0620 - val_loss: 2328.6743 - val_mse: 2328.6743 - val_mae: 31.0339\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 262us/step - loss: 3819.1784 - mse: 3819.1777 - mae: 33.0612 - val_loss: 2262.4660 - val_mse: 2262.4661 - val_mae: 30.7264\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 337us/step - loss: 4192.0694 - mse: 4192.0688 - mae: 34.0569 - val_loss: 2320.6465 - val_mse: 2320.6465 - val_mae: 30.9932\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 389us/step - loss: 4031.0652 - mse: 4031.0652 - mae: 33.1596 - val_loss: 2326.5101 - val_mse: 2326.5100 - val_mae: 31.0218\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 340us/step - loss: 4016.4839 - mse: 4016.4839 - mae: 33.5293 - val_loss: 2343.6644 - val_mse: 2343.6646 - val_mae: 31.1024\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4092.6191 - mse: 4092.6189 - mae: 33.7754 - val_loss: 2321.9921 - val_mse: 2321.9922 - val_mae: 31.0064\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 336us/step - loss: 4029.9482 - mse: 4029.9480 - mae: 32.7818 - val_loss: 2301.6787 - val_mse: 2301.6787 - val_mae: 30.9189\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4008.1030 - mse: 4008.1033 - mae: 33.9178 - val_loss: 2330.7108 - val_mse: 2330.7107 - val_mae: 31.0496\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 3961.4444 - mse: 3961.4438 - mae: 33.1439 - val_loss: 2325.1593 - val_mse: 2325.1594 - val_mae: 31.0293\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 333us/step - loss: 4102.0376 - mse: 4102.0376 - mae: 33.6131 - val_loss: 2312.6050 - val_mse: 2312.6050 - val_mae: 30.9737\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 374us/step - loss: 4006.9044 - mse: 4006.9043 - mae: 32.9430 - val_loss: 2305.4165 - val_mse: 2305.4165 - val_mae: 30.9378\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 333us/step - loss: 3962.8666 - mse: 3962.8667 - mae: 33.4645 - val_loss: 2268.4975 - val_mse: 2268.4978 - val_mae: 30.7685\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4185.5823 - mse: 4185.5820 - mae: 33.2519 - val_loss: 2374.8632 - val_mse: 2374.8633 - val_mae: 31.2617\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 262us/step - loss: 3409.3023 - mse: 3409.3020 - mae: 32.9496 - val_loss: 1447.1619 - val_mse: 1447.1620 - val_mae: 24.6283\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3404.0623 - mse: 3404.0615 - mae: 33.4768 - val_loss: 1453.3331 - val_mse: 1453.3330 - val_mae: 24.3878\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3325.4936 - mse: 3325.4934 - mae: 32.1680 - val_loss: 1438.2631 - val_mse: 1438.2631 - val_mae: 25.1485\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3372.3371 - mse: 3372.3364 - mae: 33.3548 - val_loss: 1447.5345 - val_mse: 1447.5345 - val_mae: 24.6059\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3502.4290 - mse: 3502.4294 - mae: 33.1309 - val_loss: 1444.4244 - val_mse: 1444.4243 - val_mae: 24.7201\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3255.5001 - mse: 3255.5000 - mae: 32.2950 - val_loss: 1437.6290 - val_mse: 1437.6292 - val_mae: 25.1705\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3487.8051 - mse: 3487.8047 - mae: 33.0117 - val_loss: 1449.7806 - val_mse: 1449.7808 - val_mae: 24.5109\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3313.4103 - mse: 3313.4106 - mae: 32.9083 - val_loss: 1451.3973 - val_mse: 1451.3972 - val_mae: 24.4784\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3368.6506 - mse: 3368.6501 - mae: 32.3923 - val_loss: 1441.2924 - val_mse: 1441.2925 - val_mae: 24.9620\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 270us/step - loss: 3265.8101 - mse: 3265.8101 - mae: 31.8653 - val_loss: 1437.3362 - val_mse: 1437.3364 - val_mae: 25.3843\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 369us/step - loss: 3449.2109 - mse: 3449.2114 - mae: 33.2336 - val_loss: 1447.1275 - val_mse: 1447.1276 - val_mae: 24.6472\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 346us/step - loss: 3472.5456 - mse: 3472.5459 - mae: 31.9632 - val_loss: 1441.3466 - val_mse: 1441.3468 - val_mae: 24.9718\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3386.5300 - mse: 3386.5298 - mae: 32.6837 - val_loss: 1445.8263 - val_mse: 1445.8264 - val_mae: 24.7853\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 335us/step - loss: 3216.6045 - mse: 3216.6047 - mae: 32.0133 - val_loss: 1442.9326 - val_mse: 1442.9326 - val_mae: 24.9754\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3377.0177 - mse: 3377.0166 - mae: 32.7168 - val_loss: 1450.3851 - val_mse: 1450.3850 - val_mae: 24.6065\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3340.2581 - mse: 3340.2578 - mae: 32.4485 - val_loss: 1444.7080 - val_mse: 1444.7080 - val_mae: 24.8847\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3280.4933 - mse: 3280.4939 - mae: 32.0618 - val_loss: 1444.3572 - val_mse: 1444.3573 - val_mae: 24.9116\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3382.5447 - mse: 3382.5444 - mae: 32.3755 - val_loss: 1442.3749 - val_mse: 1442.3750 - val_mae: 25.0390\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 345us/step - loss: 3446.8161 - mse: 3446.8167 - mae: 32.9826 - val_loss: 1447.7654 - val_mse: 1447.7654 - val_mae: 24.7437\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3401.1583 - mse: 3401.1584 - mae: 32.2292 - val_loss: 1438.8694 - val_mse: 1438.8693 - val_mae: 25.6778\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 264us/step - loss: 3389.3845 - mse: 3389.3840 - mae: 32.9583 - val_loss: 1447.9444 - val_mse: 1447.9443 - val_mae: 24.7489\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3281.3701 - mse: 3281.3699 - mae: 32.1550 - val_loss: 1450.2665 - val_mse: 1450.2666 - val_mae: 24.6213\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3261.8420 - mse: 3261.8420 - mae: 31.8740 - val_loss: 1442.9445 - val_mse: 1442.9445 - val_mae: 25.0484\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 347us/step - loss: 3430.7361 - mse: 3430.7366 - mae: 32.7047 - val_loss: 1439.8540 - val_mse: 1439.8538 - val_mae: 25.5343\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3350.1615 - mse: 3350.1611 - mae: 32.1768 - val_loss: 1441.8730 - val_mse: 1441.8730 - val_mae: 25.2043\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3429.0997 - mse: 3429.0994 - mae: 32.5979 - val_loss: 1444.2536 - val_mse: 1444.2537 - val_mae: 25.0192\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3320.9288 - mse: 3320.9280 - mae: 32.4616 - val_loss: 1443.0328 - val_mse: 1443.0327 - val_mae: 25.1540\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 342us/step - loss: 3315.0466 - mse: 3315.0464 - mae: 31.9491 - val_loss: 1445.5940 - val_mse: 1445.5941 - val_mae: 24.9855\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 378us/step - loss: 3331.5319 - mse: 3331.5315 - mae: 32.5447 - val_loss: 1443.0422 - val_mse: 1443.0422 - val_mae: 25.1911\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3292.5131 - mse: 3292.5144 - mae: 31.7427 - val_loss: 1445.5531 - val_mse: 1445.5531 - val_mae: 25.0337\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 264us/step - loss: 3288.8409 - mse: 3288.8411 - mae: 32.1967 - val_loss: 1450.6701 - val_mse: 1450.6703 - val_mae: 24.7660\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3323.2753 - mse: 3323.2759 - mae: 32.1407 - val_loss: 1445.7925 - val_mse: 1445.7925 - val_mae: 25.0283\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3350.0805 - mse: 3350.0793 - mae: 32.0979 - val_loss: 1444.6444 - val_mse: 1444.6445 - val_mae: 25.1564\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3292.0457 - mse: 3292.0459 - mae: 32.1686 - val_loss: 1443.2286 - val_mse: 1443.2284 - val_mae: 25.3179\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 255us/step - loss: 3317.9302 - mse: 3317.9297 - mae: 32.2076 - val_loss: 1443.8996 - val_mse: 1443.8995 - val_mae: 25.1919\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3323.3174 - mse: 3323.3174 - mae: 32.6304 - val_loss: 1443.7223 - val_mse: 1443.7223 - val_mae: 25.2207\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3388.2688 - mse: 3388.2683 - mae: 32.2919 - val_loss: 1445.0119 - val_mse: 1445.0121 - val_mae: 25.0953\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 355us/step - loss: 3248.6752 - mse: 3248.6753 - mae: 32.4243 - val_loss: 1446.7583 - val_mse: 1446.7582 - val_mae: 24.9778\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 368us/step - loss: 3340.0495 - mse: 3340.0491 - mae: 31.6843 - val_loss: 1445.1314 - val_mse: 1445.1313 - val_mae: 25.1526\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 354us/step - loss: 3328.0992 - mse: 3328.0989 - mae: 32.2782 - val_loss: 1448.5896 - val_mse: 1448.5894 - val_mae: 24.9087\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3346.4589 - mse: 3346.4592 - mae: 32.1916 - val_loss: 1443.8867 - val_mse: 1443.8867 - val_mae: 25.3319\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3394.4130 - mse: 3394.4131 - mae: 33.0165 - val_loss: 1461.3408 - val_mse: 1461.3409 - val_mae: 24.4035\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3175.5395 - mse: 3175.5400 - mae: 30.8280 - val_loss: 1445.0012 - val_mse: 1445.0013 - val_mae: 25.2319\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 259us/step - loss: 3253.6521 - mse: 3253.6521 - mae: 31.8601 - val_loss: 1450.6540 - val_mse: 1450.6541 - val_mae: 24.8073\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 270us/step - loss: 3288.0793 - mse: 3288.0786 - mae: 32.2065 - val_loss: 1447.8212 - val_mse: 1447.8215 - val_mae: 24.9620\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3231.3095 - mse: 3231.3098 - mae: 31.0707 - val_loss: 1445.9148 - val_mse: 1445.9148 - val_mae: 25.0848\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 263us/step - loss: 3308.0949 - mse: 3308.0947 - mae: 32.0166 - val_loss: 1446.1845 - val_mse: 1446.1847 - val_mae: 25.0730\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3304.4612 - mse: 3304.4612 - mae: 31.6098 - val_loss: 1448.0054 - val_mse: 1448.0055 - val_mae: 24.9532\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 250us/step - loss: 3330.6958 - mse: 3330.6946 - mae: 32.4999 - val_loss: 1444.9038 - val_mse: 1444.9039 - val_mae: 25.2282\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3335.9819 - mse: 3335.9819 - mae: 31.8400 - val_loss: 1446.6024 - val_mse: 1446.6023 - val_mae: 25.0747\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3264.6762 - mse: 3264.6755 - mae: 32.0157 - val_loss: 1452.9002 - val_mse: 1452.9003 - val_mae: 24.7061\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3252.4400 - mse: 3252.4409 - mae: 31.0562 - val_loss: 1445.1750 - val_mse: 1445.1749 - val_mae: 25.2313\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3190.1138 - mse: 3190.1145 - mae: 31.4411 - val_loss: 1450.5789 - val_mse: 1450.5789 - val_mae: 24.8095\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3412.7294 - mse: 3412.7300 - mae: 32.3987 - val_loss: 1447.7261 - val_mse: 1447.7263 - val_mae: 25.0005\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3249.9379 - mse: 3249.9375 - mae: 31.8682 - val_loss: 1446.7827 - val_mse: 1446.7830 - val_mae: 25.0543\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3212.6407 - mse: 3212.6401 - mae: 31.5733 - val_loss: 1444.9835 - val_mse: 1444.9836 - val_mae: 25.2423\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 361us/step - loss: 3270.7591 - mse: 3270.7583 - mae: 31.7509 - val_loss: 1449.2874 - val_mse: 1449.2872 - val_mae: 24.9141\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3297.3358 - mse: 3297.3364 - mae: 32.2016 - val_loss: 1445.8619 - val_mse: 1445.8619 - val_mae: 25.1742\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 337us/step - loss: 3346.7226 - mse: 3346.7231 - mae: 32.0166 - val_loss: 1452.1048 - val_mse: 1452.1047 - val_mae: 24.7722\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3310.0525 - mse: 3310.0544 - mae: 31.4651 - val_loss: 1447.6202 - val_mse: 1447.6204 - val_mae: 25.0544\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3321.6212 - mse: 3321.6218 - mae: 32.4281 - val_loss: 1444.7022 - val_mse: 1444.7020 - val_mae: 25.4675\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3312.7722 - mse: 3312.7725 - mae: 32.1332 - val_loss: 1451.1459 - val_mse: 1451.1459 - val_mae: 24.8836\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3231.6482 - mse: 3231.6482 - mae: 31.1013 - val_loss: 1446.3228 - val_mse: 1446.3228 - val_mae: 25.2785\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3359.0691 - mse: 3359.0693 - mae: 31.9061 - val_loss: 1445.3040 - val_mse: 1445.3042 - val_mae: 25.4585\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 251us/step - loss: 3217.9859 - mse: 3217.9863 - mae: 31.1515 - val_loss: 1446.7160 - val_mse: 1446.7159 - val_mae: 25.2372\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 272us/step - loss: 3295.5525 - mse: 3295.5522 - mae: 32.2220 - val_loss: 1449.1095 - val_mse: 1449.1096 - val_mae: 25.0369\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3315.0659 - mse: 3315.0657 - mae: 32.0932 - val_loss: 1448.2998 - val_mse: 1448.2998 - val_mae: 25.0689\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3247.4937 - mse: 3247.4934 - mae: 31.3623 - val_loss: 1446.8277 - val_mse: 1446.8278 - val_mae: 25.1834\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3283.0459 - mse: 3283.0461 - mae: 31.2784 - val_loss: 1446.2612 - val_mse: 1446.2612 - val_mae: 25.2753\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 219us/step - loss: 3291.1748 - mse: 3291.1753 - mae: 31.6384 - val_loss: 1445.3426 - val_mse: 1445.3424 - val_mae: 25.9823\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 217us/step - loss: 3345.5973 - mse: 3345.5984 - mae: 31.8416 - val_loss: 1451.1264 - val_mse: 1451.1262 - val_mae: 24.8377\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 294us/step - loss: 3147.5531 - mse: 3147.5532 - mae: 31.7478 - val_loss: 1444.1548 - val_mse: 1444.1549 - val_mae: 25.5551\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3249.4465 - mse: 3249.4468 - mae: 31.8117 - val_loss: 1450.2293 - val_mse: 1450.2291 - val_mae: 24.8407\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 318us/step - loss: 3206.4771 - mse: 3206.4768 - mae: 30.9664 - val_loss: 1444.5046 - val_mse: 1444.5045 - val_mae: 25.3842\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3261.6445 - mse: 3261.6453 - mae: 32.1675 - val_loss: 1445.9789 - val_mse: 1445.9788 - val_mae: 25.2638\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 325us/step - loss: 3123.0108 - mse: 3123.0103 - mae: 30.7993 - val_loss: 1444.7486 - val_mse: 1444.7485 - val_mae: 25.4840\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3234.5274 - mse: 3234.5281 - mae: 31.3787 - val_loss: 1448.0989 - val_mse: 1448.0989 - val_mae: 25.0984\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3302.8902 - mse: 3302.8899 - mae: 31.8347 - val_loss: 1445.8618 - val_mse: 1445.8616 - val_mae: 25.4453\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 362us/step - loss: 3266.3113 - mse: 3266.3123 - mae: 31.5741 - val_loss: 1448.4866 - val_mse: 1448.4866 - val_mae: 25.1029\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 389us/step - loss: 3225.6991 - mse: 3225.6987 - mae: 31.4815 - val_loss: 1445.9578 - val_mse: 1445.9579 - val_mae: 25.4458\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 252us/step - loss: 2921.5862 - mse: 2921.5859 - mae: 30.7345 - val_loss: 1071.2724 - val_mse: 1071.2723 - val_mae: 23.6463\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2863.3595 - mse: 2863.3594 - mae: 30.9602 - val_loss: 1077.5634 - val_mse: 1077.5634 - val_mae: 23.2543\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2944.3899 - mse: 2944.3904 - mae: 31.1185 - val_loss: 1074.7908 - val_mse: 1074.7908 - val_mae: 23.2991\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2817.0831 - mse: 2817.0830 - mae: 30.6288 - val_loss: 1072.7223 - val_mse: 1072.7223 - val_mae: 23.2929\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 0s 223us/step - loss: 2919.5164 - mse: 2919.5164 - mae: 31.0203 - val_loss: 1073.1529 - val_mse: 1073.1531 - val_mae: 23.2231\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 275us/step - loss: 2886.8169 - mse: 2886.8164 - mae: 30.9930 - val_loss: 1069.8963 - val_mse: 1069.8964 - val_mae: 23.3147\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2797.5343 - mse: 2797.5349 - mae: 30.7475 - val_loss: 1071.3725 - val_mse: 1071.3724 - val_mae: 23.1891\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2884.5535 - mse: 2884.5535 - mae: 31.3867 - val_loss: 1065.5378 - val_mse: 1065.5378 - val_mae: 23.4247\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2905.3263 - mse: 2905.3262 - mae: 31.2447 - val_loss: 1064.8902 - val_mse: 1064.8903 - val_mae: 23.3488\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 277us/step - loss: 2876.4898 - mse: 2876.4893 - mae: 30.6756 - val_loss: 1071.2182 - val_mse: 1071.2180 - val_mae: 23.0800\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2836.0063 - mse: 2836.0066 - mae: 31.0681 - val_loss: 1065.4109 - val_mse: 1065.4108 - val_mae: 23.2365\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2803.5169 - mse: 2803.5178 - mae: 30.6562 - val_loss: 1062.5268 - val_mse: 1062.5267 - val_mae: 23.3221\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 0s 242us/step - loss: 2908.7962 - mse: 2908.7964 - mae: 30.6376 - val_loss: 1061.7496 - val_mse: 1061.7495 - val_mae: 23.3163\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2836.7607 - mse: 2836.7612 - mae: 30.4681 - val_loss: 1060.5224 - val_mse: 1060.5225 - val_mae: 23.3821\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2886.8517 - mse: 2886.8518 - mae: 31.1156 - val_loss: 1062.6633 - val_mse: 1062.6633 - val_mae: 23.2538\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2864.6198 - mse: 2864.6199 - mae: 30.9718 - val_loss: 1060.2074 - val_mse: 1060.2074 - val_mae: 23.3812\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 0s 219us/step - loss: 2904.5866 - mse: 2904.5862 - mae: 30.6085 - val_loss: 1062.7236 - val_mse: 1062.7236 - val_mae: 23.2061\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 0s 246us/step - loss: 2924.6544 - mse: 2924.6550 - mae: 30.9486 - val_loss: 1060.0117 - val_mse: 1060.0116 - val_mae: 23.3016\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 271us/step - loss: 2831.9210 - mse: 2831.9209 - mae: 30.8320 - val_loss: 1063.9292 - val_mse: 1063.9293 - val_mae: 23.0921\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2857.4612 - mse: 2857.4614 - mae: 30.7479 - val_loss: 1058.7753 - val_mse: 1058.7753 - val_mae: 23.2825\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 319us/step - loss: 2882.2139 - mse: 2882.2141 - mae: 30.8471 - val_loss: 1058.3997 - val_mse: 1058.3998 - val_mae: 23.2825\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 379us/step - loss: 2847.9898 - mse: 2847.9900 - mae: 30.4874 - val_loss: 1060.4554 - val_mse: 1060.4554 - val_mae: 23.1537\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2851.4243 - mse: 2851.4238 - mae: 30.5971 - val_loss: 1054.0454 - val_mse: 1054.0455 - val_mae: 23.5324\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 252us/step - loss: 2833.3820 - mse: 2833.3811 - mae: 30.7433 - val_loss: 1054.1908 - val_mse: 1054.1906 - val_mae: 23.4693\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2972.1786 - mse: 2972.1785 - mae: 31.5532 - val_loss: 1061.9331 - val_mse: 1061.9331 - val_mae: 23.0343\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 261us/step - loss: 2786.0758 - mse: 2786.0757 - mae: 30.3406 - val_loss: 1052.2891 - val_mse: 1052.2891 - val_mae: 23.5619\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2749.9830 - mse: 2749.9836 - mae: 30.0119 - val_loss: 1050.7745 - val_mse: 1050.7744 - val_mae: 23.7552\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2820.7122 - mse: 2820.7131 - mae: 30.7845 - val_loss: 1054.1843 - val_mse: 1054.1843 - val_mae: 23.4203\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 333us/step - loss: 2781.5931 - mse: 2781.5933 - mae: 30.6772 - val_loss: 1051.4588 - val_mse: 1051.4586 - val_mae: 23.6101\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 258us/step - loss: 2842.3820 - mse: 2842.3813 - mae: 30.7882 - val_loss: 1053.4619 - val_mse: 1053.4620 - val_mae: 23.3708\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2787.5838 - mse: 2787.5823 - mae: 29.7972 - val_loss: 1051.3684 - val_mse: 1051.3684 - val_mae: 23.4920\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2905.4220 - mse: 2905.4214 - mae: 30.9546 - val_loss: 1051.5349 - val_mse: 1051.5348 - val_mae: 23.4321\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2861.1472 - mse: 2861.1460 - mae: 30.7624 - val_loss: 1048.1835 - val_mse: 1048.1835 - val_mae: 23.6957\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2927.0427 - mse: 2927.0420 - mae: 30.9746 - val_loss: 1057.8246 - val_mse: 1057.8248 - val_mae: 23.0194\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 340us/step - loss: 2837.3352 - mse: 2837.3342 - mae: 30.0771 - val_loss: 1049.1975 - val_mse: 1049.1975 - val_mae: 23.4639\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2805.3852 - mse: 2805.3857 - mae: 30.3372 - val_loss: 1047.8975 - val_mse: 1047.8976 - val_mae: 23.4500\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2861.3746 - mse: 2861.3748 - mae: 30.4731 - val_loss: 1044.9106 - val_mse: 1044.9105 - val_mae: 23.7162\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 358us/step - loss: 2816.5707 - mse: 2816.5706 - mae: 30.6371 - val_loss: 1050.2656 - val_mse: 1050.2654 - val_mae: 23.2028\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2910.5099 - mse: 2910.5093 - mae: 30.6135 - val_loss: 1044.2476 - val_mse: 1044.2476 - val_mae: 23.6645\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2886.0734 - mse: 2886.0725 - mae: 30.3991 - val_loss: 1047.2336 - val_mse: 1047.2336 - val_mae: 23.3489\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 269us/step - loss: 2875.7507 - mse: 2875.7512 - mae: 30.7269 - val_loss: 1048.2648 - val_mse: 1048.2648 - val_mae: 23.2713\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 316us/step - loss: 2844.7578 - mse: 2844.7581 - mae: 30.8493 - val_loss: 1051.6973 - val_mse: 1051.6973 - val_mae: 23.0499\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2857.9914 - mse: 2857.9912 - mae: 30.3205 - val_loss: 1052.4768 - val_mse: 1052.4768 - val_mae: 22.9899\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2867.9669 - mse: 2867.9666 - mae: 30.7632 - val_loss: 1046.4040 - val_mse: 1046.4041 - val_mae: 23.3139\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 265us/step - loss: 2857.6075 - mse: 2857.6072 - mae: 30.4069 - val_loss: 1048.2116 - val_mse: 1048.2113 - val_mae: 23.1805\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 0s 241us/step - loss: 2888.2363 - mse: 2888.2361 - mae: 30.3795 - val_loss: 1045.8650 - val_mse: 1045.8649 - val_mae: 23.2726\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2826.7479 - mse: 2826.7473 - mae: 30.3765 - val_loss: 1048.2581 - val_mse: 1048.2581 - val_mae: 23.1644\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2845.2187 - mse: 2845.2180 - mae: 30.2520 - val_loss: 1047.8372 - val_mse: 1047.8373 - val_mae: 23.1423\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2852.6284 - mse: 2852.6277 - mae: 30.2516 - val_loss: 1046.9602 - val_mse: 1046.9602 - val_mae: 23.1794\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 0s 246us/step - loss: 2823.5884 - mse: 2823.5891 - mae: 30.7323 - val_loss: 1044.2956 - val_mse: 1044.2957 - val_mae: 23.3252\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 0s 246us/step - loss: 2829.8709 - mse: 2829.8721 - mae: 29.7343 - val_loss: 1039.8686 - val_mse: 1039.8685 - val_mae: 23.8126\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 268us/step - loss: 2825.3160 - mse: 2825.3152 - mae: 30.2322 - val_loss: 1040.4513 - val_mse: 1040.4513 - val_mae: 23.5750\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 270us/step - loss: 2840.1423 - mse: 2840.1414 - mae: 30.4537 - val_loss: 1039.1612 - val_mse: 1039.1613 - val_mae: 23.6966\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2817.6215 - mse: 2817.6213 - mae: 30.5540 - val_loss: 1041.5585 - val_mse: 1041.5585 - val_mae: 23.3722\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2800.9307 - mse: 2800.9302 - mae: 30.0700 - val_loss: 1040.3732 - val_mse: 1040.3732 - val_mae: 23.3940\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2771.5496 - mse: 2771.5493 - mae: 30.1459 - val_loss: 1038.2679 - val_mse: 1038.2677 - val_mae: 23.5356\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2850.2446 - mse: 2850.2444 - mae: 30.2140 - val_loss: 1036.1792 - val_mse: 1036.1792 - val_mae: 23.8027\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2800.7804 - mse: 2800.7800 - mae: 30.2651 - val_loss: 1039.8317 - val_mse: 1039.8317 - val_mae: 23.3001\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2766.7540 - mse: 2766.7539 - mae: 30.0188 - val_loss: 1037.6774 - val_mse: 1037.6774 - val_mae: 23.4295\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2840.7531 - mse: 2840.7524 - mae: 30.4168 - val_loss: 1036.7159 - val_mse: 1036.7158 - val_mae: 23.4535\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2762.5320 - mse: 2762.5315 - mae: 30.4477 - val_loss: 1035.5724 - val_mse: 1035.5724 - val_mae: 23.5212\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2885.8515 - mse: 2885.8516 - mae: 31.0570 - val_loss: 1035.6443 - val_mse: 1035.6444 - val_mae: 23.4378\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 263us/step - loss: 2794.6017 - mse: 2794.6013 - mae: 30.1224 - val_loss: 1035.7212 - val_mse: 1035.7211 - val_mae: 23.4653\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2744.4668 - mse: 2744.4666 - mae: 29.3133 - val_loss: 1036.0958 - val_mse: 1036.0958 - val_mae: 23.4501\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 0s 245us/step - loss: 2821.9798 - mse: 2821.9795 - mae: 30.6903 - val_loss: 1036.2622 - val_mse: 1036.2622 - val_mae: 23.3902\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2844.6296 - mse: 2844.6299 - mae: 30.8627 - val_loss: 1038.6903 - val_mse: 1038.6901 - val_mae: 23.1668\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2797.5642 - mse: 2797.5640 - mae: 30.1654 - val_loss: 1037.8319 - val_mse: 1037.8319 - val_mae: 23.2401\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2800.0229 - mse: 2800.0232 - mae: 29.5635 - val_loss: 1034.1234 - val_mse: 1034.1233 - val_mae: 23.6055\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 270us/step - loss: 2799.6523 - mse: 2799.6528 - mae: 29.9198 - val_loss: 1035.5222 - val_mse: 1035.5223 - val_mae: 23.4272\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 316us/step - loss: 2861.1265 - mse: 2861.1272 - mae: 30.4552 - val_loss: 1038.6768 - val_mse: 1038.6768 - val_mae: 23.1596\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 272us/step - loss: 2820.4706 - mse: 2820.4712 - mae: 30.2395 - val_loss: 1034.8590 - val_mse: 1034.8590 - val_mae: 23.4235\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 266us/step - loss: 2786.5738 - mse: 2786.5742 - mae: 30.1907 - val_loss: 1033.1615 - val_mse: 1033.1616 - val_mae: 23.5661\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 260us/step - loss: 2806.6050 - mse: 2806.6050 - mae: 29.5851 - val_loss: 1039.4823 - val_mse: 1039.4824 - val_mae: 23.0860\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2849.7624 - mse: 2849.7620 - mae: 30.6000 - val_loss: 1036.3405 - val_mse: 1036.3403 - val_mae: 23.2116\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2776.7108 - mse: 2776.7107 - mae: 29.9395 - val_loss: 1036.3425 - val_mse: 1036.3424 - val_mae: 23.1869\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 330us/step - loss: 2834.9395 - mse: 2834.9397 - mae: 30.4306 - val_loss: 1035.2727 - val_mse: 1035.2726 - val_mae: 23.2682\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2760.5800 - mse: 2760.5803 - mae: 29.7979 - val_loss: 1031.5492 - val_mse: 1031.5492 - val_mae: 23.5773\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2854.3066 - mse: 2854.3076 - mae: 30.4142 - val_loss: 1033.2391 - val_mse: 1033.2390 - val_mae: 23.3073\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2765.9267 - mse: 2765.9265 - mae: 29.7399 - val_loss: 1031.0383 - val_mse: 1031.0383 - val_mae: 23.5255\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 345us/step - loss: 2830.4303 - mse: 2830.4294 - mae: 30.3236 - val_loss: 1034.8169 - val_mse: 1034.8168 - val_mae: 23.1759\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2442.5715 - mse: 2442.5706 - mae: 29.2376 - val_loss: 1489.5152 - val_mse: 1489.5151 - val_mae: 26.7571\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 275us/step - loss: 2557.1346 - mse: 2557.1350 - mae: 29.7689 - val_loss: 1496.9327 - val_mse: 1496.9327 - val_mae: 26.5528\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 245us/step - loss: 2510.5055 - mse: 2510.5054 - mae: 29.1998 - val_loss: 1473.6626 - val_mse: 1473.6626 - val_mae: 27.1099\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 280us/step - loss: 2496.7923 - mse: 2496.7932 - mae: 29.1869 - val_loss: 1483.3662 - val_mse: 1483.3663 - val_mae: 26.7845\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2515.9992 - mse: 2515.9993 - mae: 29.6693 - val_loss: 1475.1529 - val_mse: 1475.1528 - val_mae: 26.9310\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2459.5111 - mse: 2459.5115 - mae: 29.2291 - val_loss: 1473.7720 - val_mse: 1473.7722 - val_mae: 26.9499\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 280us/step - loss: 2531.9897 - mse: 2531.9890 - mae: 30.0185 - val_loss: 1488.1794 - val_mse: 1488.1797 - val_mae: 26.5592\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2480.4657 - mse: 2480.4656 - mae: 29.2623 - val_loss: 1469.9823 - val_mse: 1469.9823 - val_mae: 26.9700\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 260us/step - loss: 2557.2016 - mse: 2557.2007 - mae: 29.6988 - val_loss: 1481.0357 - val_mse: 1481.0358 - val_mae: 26.6443\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 259us/step - loss: 2514.6062 - mse: 2514.6067 - mae: 29.0213 - val_loss: 1478.4174 - val_mse: 1478.4174 - val_mae: 26.6767\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2517.6658 - mse: 2517.6653 - mae: 29.4497 - val_loss: 1487.6593 - val_mse: 1487.6593 - val_mae: 26.4437\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2542.9478 - mse: 2542.9478 - mae: 29.2427 - val_loss: 1488.0955 - val_mse: 1488.0957 - val_mae: 26.4347\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2560.3810 - mse: 2560.3813 - mae: 30.0201 - val_loss: 1476.2334 - val_mse: 1476.2334 - val_mae: 26.6629\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 327us/step - loss: 2479.2412 - mse: 2479.2405 - mae: 29.6912 - val_loss: 1470.6912 - val_mse: 1470.6913 - val_mae: 26.7742\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2519.7847 - mse: 2519.7844 - mae: 29.8706 - val_loss: 1474.7673 - val_mse: 1474.7672 - val_mae: 26.6721\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2502.5078 - mse: 2502.5073 - mae: 29.3846 - val_loss: 1466.7500 - val_mse: 1466.7502 - val_mae: 26.8484\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2475.1140 - mse: 2475.1143 - mae: 28.4811 - val_loss: 1476.1984 - val_mse: 1476.1986 - val_mae: 26.5818\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2500.3497 - mse: 2500.3501 - mae: 29.5186 - val_loss: 1483.6220 - val_mse: 1483.6221 - val_mae: 26.4171\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 221us/step - loss: 2515.2697 - mse: 2515.2693 - mae: 29.3581 - val_loss: 1472.5953 - val_mse: 1472.5955 - val_mae: 26.6078\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2444.7672 - mse: 2444.7678 - mae: 28.7412 - val_loss: 1469.3051 - val_mse: 1469.3051 - val_mae: 26.7103\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 275us/step - loss: 2480.8886 - mse: 2480.8894 - mae: 29.2496 - val_loss: 1465.5632 - val_mse: 1465.5634 - val_mae: 26.7651\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2518.8508 - mse: 2518.8511 - mae: 29.1945 - val_loss: 1468.8202 - val_mse: 1468.8203 - val_mae: 26.6735\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2476.9751 - mse: 2476.9746 - mae: 29.2477 - val_loss: 1468.3164 - val_mse: 1468.3164 - val_mae: 26.6489\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2519.2361 - mse: 2519.2363 - mae: 29.7524 - val_loss: 1470.4192 - val_mse: 1470.4192 - val_mae: 26.6082\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 271us/step - loss: 2470.9653 - mse: 2470.9656 - mae: 28.7591 - val_loss: 1462.7076 - val_mse: 1462.7076 - val_mae: 26.8113\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2459.3918 - mse: 2459.3916 - mae: 29.0768 - val_loss: 1463.8613 - val_mse: 1463.8615 - val_mae: 26.7376\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2425.7052 - mse: 2425.7051 - mae: 29.3208 - val_loss: 1475.3035 - val_mse: 1475.3036 - val_mae: 26.4315\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2503.2370 - mse: 2503.2373 - mae: 29.6602 - val_loss: 1469.1823 - val_mse: 1469.1825 - val_mae: 26.5702\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 261us/step - loss: 2508.2553 - mse: 2508.2551 - mae: 29.3370 - val_loss: 1466.4521 - val_mse: 1466.4521 - val_mae: 26.6295\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 261us/step - loss: 2500.5099 - mse: 2500.5095 - mae: 29.2489 - val_loss: 1465.9860 - val_mse: 1465.9861 - val_mae: 26.6229\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 249us/step - loss: 2484.7877 - mse: 2484.7878 - mae: 29.3065 - val_loss: 1466.1470 - val_mse: 1466.1470 - val_mae: 26.5860\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2436.1206 - mse: 2436.1204 - mae: 29.1360 - val_loss: 1471.1553 - val_mse: 1471.1553 - val_mae: 26.4315\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 251us/step - loss: 2486.4874 - mse: 2486.4873 - mae: 29.1299 - val_loss: 1464.4245 - val_mse: 1464.4246 - val_mae: 26.5758\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 352us/step - loss: 2540.3223 - mse: 2540.3237 - mae: 29.2413 - val_loss: 1478.8218 - val_mse: 1478.8217 - val_mae: 26.2588\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2473.7930 - mse: 2473.7932 - mae: 29.3166 - val_loss: 1465.3990 - val_mse: 1465.3992 - val_mae: 26.5937\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 336us/step - loss: 2455.8465 - mse: 2455.8477 - mae: 28.9730 - val_loss: 1472.6813 - val_mse: 1472.6813 - val_mae: 26.3898\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2497.8219 - mse: 2497.8223 - mae: 28.9662 - val_loss: 1462.3373 - val_mse: 1462.3373 - val_mae: 26.6405\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2480.1187 - mse: 2480.1194 - mae: 28.9512 - val_loss: 1468.8047 - val_mse: 1468.8047 - val_mae: 26.4772\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2379.4113 - mse: 2379.4114 - mae: 28.8647 - val_loss: 1468.0038 - val_mse: 1468.0038 - val_mae: 26.5007\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2519.2069 - mse: 2519.2065 - mae: 29.3824 - val_loss: 1468.6125 - val_mse: 1468.6124 - val_mae: 26.5127\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 270us/step - loss: 2474.4636 - mse: 2474.4631 - mae: 29.0969 - val_loss: 1464.3421 - val_mse: 1464.3420 - val_mae: 26.5796\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 271us/step - loss: 2489.6203 - mse: 2489.6206 - mae: 29.2002 - val_loss: 1469.7177 - val_mse: 1469.7179 - val_mae: 26.4128\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2514.1319 - mse: 2514.1316 - mae: 29.5689 - val_loss: 1471.6476 - val_mse: 1471.6477 - val_mae: 26.3746\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2518.0821 - mse: 2518.0815 - mae: 29.5684 - val_loss: 1469.5737 - val_mse: 1469.5737 - val_mae: 26.4520\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2494.6331 - mse: 2494.6335 - mae: 29.3314 - val_loss: 1460.0808 - val_mse: 1460.0808 - val_mae: 26.6358\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2486.8544 - mse: 2486.8542 - mae: 29.1411 - val_loss: 1462.3460 - val_mse: 1462.3458 - val_mae: 26.5571\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 274us/step - loss: 2437.3536 - mse: 2437.3538 - mae: 29.0854 - val_loss: 1460.6903 - val_mse: 1460.6903 - val_mae: 26.5879\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2536.8581 - mse: 2536.8577 - mae: 29.5854 - val_loss: 1463.9848 - val_mse: 1463.9847 - val_mae: 26.5151\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2448.6039 - mse: 2448.6042 - mae: 28.7015 - val_loss: 1463.6704 - val_mse: 1463.6703 - val_mae: 26.5044\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 351us/step - loss: 2453.0654 - mse: 2453.0654 - mae: 28.4788 - val_loss: 1467.2355 - val_mse: 1467.2355 - val_mae: 26.4279\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2472.0209 - mse: 2472.0208 - mae: 29.1758 - val_loss: 1461.0958 - val_mse: 1461.0958 - val_mae: 26.5478\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2520.4865 - mse: 2520.4873 - mae: 29.5126 - val_loss: 1467.6830 - val_mse: 1467.6830 - val_mae: 26.3520\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2562.4944 - mse: 2562.4944 - mae: 29.8500 - val_loss: 1455.1316 - val_mse: 1455.1317 - val_mae: 26.6231\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 246us/step - loss: 2438.1020 - mse: 2438.1028 - mae: 28.9512 - val_loss: 1457.3322 - val_mse: 1457.3320 - val_mae: 26.5583\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2498.9598 - mse: 2498.9597 - mae: 29.2840 - val_loss: 1469.0864 - val_mse: 1469.0864 - val_mae: 26.2964\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2450.6379 - mse: 2450.6387 - mae: 28.7418 - val_loss: 1462.8189 - val_mse: 1462.8188 - val_mae: 26.4548\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2393.6449 - mse: 2393.6448 - mae: 28.8667 - val_loss: 1464.6419 - val_mse: 1464.6418 - val_mae: 26.3932\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2403.6443 - mse: 2403.6445 - mae: 28.7396 - val_loss: 1466.1351 - val_mse: 1466.1350 - val_mae: 26.3294\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2402.1220 - mse: 2402.1218 - mae: 28.6397 - val_loss: 1460.9621 - val_mse: 1460.9623 - val_mae: 26.4136\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 280us/step - loss: 2489.1319 - mse: 2489.1306 - mae: 29.5681 - val_loss: 1466.4644 - val_mse: 1466.4642 - val_mae: 26.2759\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 272us/step - loss: 2494.3122 - mse: 2494.3120 - mae: 28.9134 - val_loss: 1455.7737 - val_mse: 1455.7736 - val_mae: 26.5461\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2467.5334 - mse: 2467.5334 - mae: 28.9911 - val_loss: 1462.7719 - val_mse: 1462.7717 - val_mae: 26.3230\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2424.6138 - mse: 2424.6138 - mae: 28.6625 - val_loss: 1458.5218 - val_mse: 1458.5217 - val_mae: 26.3928\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2392.2631 - mse: 2392.2632 - mae: 28.2363 - val_loss: 1456.7686 - val_mse: 1456.7687 - val_mae: 26.4124\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2389.9419 - mse: 2389.9417 - mae: 28.5401 - val_loss: 1457.9656 - val_mse: 1457.9656 - val_mae: 26.3857\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2456.4369 - mse: 2456.4377 - mae: 29.0313 - val_loss: 1460.0812 - val_mse: 1460.0811 - val_mae: 26.3050\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2486.5193 - mse: 2486.5198 - mae: 29.2502 - val_loss: 1459.9869 - val_mse: 1459.9868 - val_mae: 26.2957\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2405.4496 - mse: 2405.4497 - mae: 28.7067 - val_loss: 1458.8110 - val_mse: 1458.8108 - val_mae: 26.2626\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2489.5693 - mse: 2489.5701 - mae: 28.8367 - val_loss: 1453.8343 - val_mse: 1453.8342 - val_mae: 26.3874\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2469.9024 - mse: 2469.9026 - mae: 28.8652 - val_loss: 1453.3925 - val_mse: 1453.3927 - val_mae: 26.3762\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2466.7208 - mse: 2466.7209 - mae: 28.9290 - val_loss: 1449.1562 - val_mse: 1449.1562 - val_mae: 26.4508\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2480.7427 - mse: 2480.7432 - mae: 28.7622 - val_loss: 1450.8553 - val_mse: 1450.8553 - val_mae: 26.3711\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 279us/step - loss: 2452.4624 - mse: 2452.4629 - mae: 28.8604 - val_loss: 1455.3781 - val_mse: 1455.3783 - val_mae: 26.2411\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2438.7112 - mse: 2438.7114 - mae: 28.8369 - val_loss: 1451.5091 - val_mse: 1451.5092 - val_mae: 26.3451\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2434.6358 - mse: 2434.6362 - mae: 29.1700 - val_loss: 1459.4523 - val_mse: 1459.4520 - val_mae: 26.1611\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2441.9148 - mse: 2441.9146 - mae: 28.7086 - val_loss: 1456.2826 - val_mse: 1456.2823 - val_mae: 26.2425\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2477.4645 - mse: 2477.4644 - mae: 29.1080 - val_loss: 1458.5493 - val_mse: 1458.5493 - val_mae: 26.1329\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 259us/step - loss: 2497.9793 - mse: 2497.9800 - mae: 28.5686 - val_loss: 1454.9040 - val_mse: 1454.9041 - val_mae: 26.1966\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2372.4778 - mse: 2372.4780 - mae: 28.6542 - val_loss: 1446.8876 - val_mse: 1446.8876 - val_mae: 26.3400\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 340us/step - loss: 2504.0203 - mse: 2504.0215 - mae: 29.1388 - val_loss: 1464.1452 - val_mse: 1464.1454 - val_mae: 25.9684\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - ETA: 0s - loss: 2376.9655 - mse: 2376.9656 - mae: 29.08 - 1s 302us/step - loss: 2387.9369 - mse: 2387.9368 - mae: 29.2320 - val_loss: 3657.7735 - val_mse: 3657.7732 - val_mae: 23.4090\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2363.6449 - mse: 2363.6443 - mae: 29.4864 - val_loss: 3659.3478 - val_mse: 3659.3477 - val_mae: 23.4949\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2388.8907 - mse: 2388.8901 - mae: 29.4799 - val_loss: 3660.5457 - val_mse: 3660.5452 - val_mae: 23.6746\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2319.9499 - mse: 2319.9497 - mae: 28.8274 - val_loss: 3662.4922 - val_mse: 3662.4927 - val_mae: 24.0152\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 330us/step - loss: 2381.3859 - mse: 2381.3853 - mae: 29.2199 - val_loss: 3663.2198 - val_mse: 3663.2200 - val_mae: 23.9185\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2375.6032 - mse: 2375.6028 - mae: 29.3788 - val_loss: 3662.4338 - val_mse: 3662.4338 - val_mae: 23.7982\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2352.2220 - mse: 2352.2227 - mae: 29.1982 - val_loss: 3663.4867 - val_mse: 3663.4863 - val_mae: 23.6379\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2357.7333 - mse: 2357.7339 - mae: 28.9158 - val_loss: 3664.2811 - val_mse: 3664.2817 - val_mae: 23.9358\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2299.2844 - mse: 2299.2852 - mae: 29.0446 - val_loss: 3662.5124 - val_mse: 3662.5117 - val_mae: 23.1162\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 252us/step - loss: 2320.5858 - mse: 2320.5852 - mae: 29.0263 - val_loss: 3662.7110 - val_mse: 3662.7112 - val_mae: 23.1647\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2314.6600 - mse: 2314.6604 - mae: 29.1519 - val_loss: 3663.2084 - val_mse: 3663.2095 - val_mae: 23.5290\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2283.2800 - mse: 2283.2800 - mae: 28.8578 - val_loss: 3662.5191 - val_mse: 3662.5183 - val_mae: 23.2586\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 257us/step - loss: 2375.8244 - mse: 2375.8250 - mae: 29.1692 - val_loss: 3662.7920 - val_mse: 3662.7925 - val_mae: 23.4985\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2381.2027 - mse: 2381.2029 - mae: 29.4561 - val_loss: 3661.8071 - val_mse: 3661.8071 - val_mae: 23.3626\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2337.3719 - mse: 2337.3716 - mae: 29.2466 - val_loss: 3663.1078 - val_mse: 3663.1074 - val_mae: 23.4513\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2246.1237 - mse: 2246.1245 - mae: 28.7664 - val_loss: 3662.1871 - val_mse: 3662.1868 - val_mae: 23.2514\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2334.0706 - mse: 2334.0703 - mae: 29.1082 - val_loss: 3663.0521 - val_mse: 3663.0518 - val_mae: 23.4652\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2341.0602 - mse: 2341.0605 - mae: 28.8676 - val_loss: 3662.0882 - val_mse: 3662.0876 - val_mae: 23.3030\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2321.3051 - mse: 2321.3044 - mae: 29.1889 - val_loss: 3662.3923 - val_mse: 3662.3926 - val_mae: 23.4553\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 329us/step - loss: 2308.5025 - mse: 2308.5034 - mae: 29.2105 - val_loss: 3661.9524 - val_mse: 3661.9531 - val_mae: 22.7768\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2342.5830 - mse: 2342.5830 - mae: 28.9790 - val_loss: 3661.5179 - val_mse: 3661.5181 - val_mae: 23.5197\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2331.3220 - mse: 2331.3225 - mae: 28.9961 - val_loss: 3659.6979 - val_mse: 3659.6982 - val_mae: 23.1892\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2283.2868 - mse: 2283.2869 - mae: 28.9657 - val_loss: 3664.0149 - val_mse: 3664.0144 - val_mae: 24.1088\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2305.7022 - mse: 2305.7024 - mae: 28.7756 - val_loss: 3661.6426 - val_mse: 3661.6433 - val_mae: 23.7236\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2354.4408 - mse: 2354.4395 - mae: 29.4784 - val_loss: 3659.7301 - val_mse: 3659.7300 - val_mae: 23.4346\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2332.4606 - mse: 2332.4607 - mae: 28.9456 - val_loss: 3659.9899 - val_mse: 3659.9900 - val_mae: 23.2978\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2350.8350 - mse: 2350.8352 - mae: 29.0969 - val_loss: 3659.7195 - val_mse: 3659.7185 - val_mae: 23.2732\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2342.2739 - mse: 2342.2737 - mae: 29.2679 - val_loss: 3659.5616 - val_mse: 3659.5615 - val_mae: 23.3689\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 329us/step - loss: 2358.9236 - mse: 2358.9236 - mae: 29.2948 - val_loss: 3657.4712 - val_mse: 3657.4705 - val_mae: 23.2726\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 340us/step - loss: 2310.4057 - mse: 2310.4055 - mae: 29.0200 - val_loss: 3659.6577 - val_mse: 3659.6577 - val_mae: 23.6133\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2306.4259 - mse: 2306.4253 - mae: 28.7348 - val_loss: 3661.7936 - val_mse: 3661.7935 - val_mae: 23.2832\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2330.1118 - mse: 2330.1125 - mae: 28.8870 - val_loss: 3663.2797 - val_mse: 3663.2800 - val_mae: 23.8181\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 350us/step - loss: 2362.8569 - mse: 2362.8572 - mae: 29.0262 - val_loss: 3660.6259 - val_mse: 3660.6257 - val_mae: 23.4060\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 340us/step - loss: 2277.7586 - mse: 2277.7581 - mae: 28.6360 - val_loss: 3660.5148 - val_mse: 3660.5144 - val_mae: 23.5983\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2330.4132 - mse: 2330.4124 - mae: 28.5499 - val_loss: 3658.2074 - val_mse: 3658.2075 - val_mae: 23.4591\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2324.6392 - mse: 2324.6396 - mae: 29.0070 - val_loss: 3658.5227 - val_mse: 3658.5232 - val_mae: 23.5869\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2272.6040 - mse: 2272.6047 - mae: 28.4071 - val_loss: 3662.3281 - val_mse: 3662.3286 - val_mae: 24.1091\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2334.3953 - mse: 2334.3948 - mae: 28.8260 - val_loss: 3660.0115 - val_mse: 3660.0115 - val_mae: 23.4214\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2259.0649 - mse: 2259.0647 - mae: 28.5354 - val_loss: 3662.6852 - val_mse: 3662.6848 - val_mae: 23.4934\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2324.3708 - mse: 2324.3711 - mae: 28.7174 - val_loss: 3664.0017 - val_mse: 3664.0020 - val_mae: 23.7195\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2277.4084 - mse: 2277.4082 - mae: 28.5114 - val_loss: 3664.0128 - val_mse: 3664.0117 - val_mae: 23.8295\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2353.3815 - mse: 2353.3806 - mae: 29.0649 - val_loss: 3661.5165 - val_mse: 3661.5161 - val_mae: 23.3238\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2374.2628 - mse: 2374.2632 - mae: 28.8084 - val_loss: 3659.5661 - val_mse: 3659.5659 - val_mae: 23.1819\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 280us/step - loss: 2313.7442 - mse: 2313.7446 - mae: 28.8220 - val_loss: 3660.7270 - val_mse: 3660.7275 - val_mae: 23.5124\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 240us/step - loss: 2321.3473 - mse: 2321.3481 - mae: 28.7823 - val_loss: 3660.8492 - val_mse: 3660.8499 - val_mae: 23.1514\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2267.5426 - mse: 2267.5425 - mae: 28.8328 - val_loss: 3660.4865 - val_mse: 3660.4871 - val_mae: 23.1585\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2305.6590 - mse: 2305.6587 - mae: 28.7709 - val_loss: 3661.4786 - val_mse: 3661.4780 - val_mae: 23.8799\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2304.0188 - mse: 2304.0181 - mae: 28.8433 - val_loss: 3662.9745 - val_mse: 3662.9739 - val_mae: 23.5296\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2319.3040 - mse: 2319.3040 - mae: 28.6822 - val_loss: 3662.5960 - val_mse: 3662.5959 - val_mae: 23.6648\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 339us/step - loss: 2301.0795 - mse: 2301.0796 - mae: 28.7368 - val_loss: 3662.6089 - val_mse: 3662.6084 - val_mae: 23.5765\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2357.7955 - mse: 2357.7954 - mae: 29.2260 - val_loss: 3659.7524 - val_mse: 3659.7537 - val_mae: 23.2921\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2301.7505 - mse: 2301.7502 - mae: 28.9548 - val_loss: 3659.9201 - val_mse: 3659.9204 - val_mae: 23.6129\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2333.1657 - mse: 2333.1650 - mae: 28.8236 - val_loss: 3660.6064 - val_mse: 3660.6064 - val_mae: 23.3830\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2274.6261 - mse: 2274.6267 - mae: 28.5622 - val_loss: 3659.8304 - val_mse: 3659.8306 - val_mae: 23.6904\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2272.0953 - mse: 2272.0955 - mae: 28.4900 - val_loss: 3659.6568 - val_mse: 3659.6577 - val_mae: 23.8239\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2257.9168 - mse: 2257.9172 - mae: 28.5968 - val_loss: 3660.9187 - val_mse: 3660.9197 - val_mae: 23.7996\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2307.7214 - mse: 2307.7209 - mae: 28.9747 - val_loss: 3659.1801 - val_mse: 3659.1790 - val_mae: 23.4788\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2289.2409 - mse: 2289.2407 - mae: 28.6779 - val_loss: 3661.1008 - val_mse: 3661.1018 - val_mae: 23.6169\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 341us/step - loss: 2367.3479 - mse: 2367.3479 - mae: 29.1498 - val_loss: 3661.4827 - val_mse: 3661.4822 - val_mae: 23.6588\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2328.7432 - mse: 2328.7437 - mae: 28.9559 - val_loss: 3657.8930 - val_mse: 3657.8933 - val_mae: 23.5732\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2321.5288 - mse: 2321.5293 - mae: 28.5526 - val_loss: 3662.7756 - val_mse: 3662.7754 - val_mae: 24.1974\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2311.6341 - mse: 2311.6338 - mae: 29.1163 - val_loss: 3660.6037 - val_mse: 3660.6035 - val_mae: 23.8620\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2267.6087 - mse: 2267.6079 - mae: 28.6405 - val_loss: 3660.1409 - val_mse: 3660.1409 - val_mae: 23.7219\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2297.5697 - mse: 2297.5696 - mae: 29.0037 - val_loss: 3659.2248 - val_mse: 3659.2258 - val_mae: 23.3328\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2326.5458 - mse: 2326.5464 - mae: 28.8190 - val_loss: 3658.1266 - val_mse: 3658.1262 - val_mae: 23.7320\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2265.8705 - mse: 2265.8713 - mae: 28.7545 - val_loss: 3658.9298 - val_mse: 3658.9299 - val_mae: 23.4744\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2254.8922 - mse: 2254.8923 - mae: 28.5557 - val_loss: 3658.5887 - val_mse: 3658.5891 - val_mae: 23.4703\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2273.9726 - mse: 2273.9724 - mae: 28.5713 - val_loss: 3658.8585 - val_mse: 3658.8594 - val_mae: 23.6819\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2238.6526 - mse: 2238.6523 - mae: 28.4294 - val_loss: 3658.1902 - val_mse: 3658.1904 - val_mae: 23.7561\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2289.0905 - mse: 2289.0903 - mae: 28.7701 - val_loss: 3659.4592 - val_mse: 3659.4587 - val_mae: 23.6401\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2308.1762 - mse: 2308.1758 - mae: 28.4464 - val_loss: 3658.8688 - val_mse: 3658.8691 - val_mae: 23.9487\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 346us/step - loss: 2304.8846 - mse: 2304.8840 - mae: 28.9278 - val_loss: 3660.8800 - val_mse: 3660.8806 - val_mae: 23.7709\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2252.3643 - mse: 2252.3640 - mae: 28.6915 - val_loss: 3660.0202 - val_mse: 3660.0195 - val_mae: 23.6709\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 370us/step - loss: 2333.5563 - mse: 2333.5562 - mae: 28.7979 - val_loss: 3660.4820 - val_mse: 3660.4829 - val_mae: 23.7637\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2304.1393 - mse: 2304.1396 - mae: 29.0406 - val_loss: 3658.0614 - val_mse: 3658.0615 - val_mae: 23.4964\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 349us/step - loss: 2279.1796 - mse: 2279.1802 - mae: 28.3096 - val_loss: 3659.1379 - val_mse: 3659.1379 - val_mae: 23.7817\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2303.8296 - mse: 2303.8306 - mae: 28.9978 - val_loss: 3657.0134 - val_mse: 3657.0137 - val_mae: 23.4671\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2318.0968 - mse: 2318.0964 - mae: 29.0119 - val_loss: 3656.9651 - val_mse: 3656.9663 - val_mae: 23.7705\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2285.1557 - mse: 2285.1550 - mae: 28.4376 - val_loss: 3657.8927 - val_mse: 3657.8928 - val_mae: 24.1069\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 269us/step - loss: 2354.6659 - mse: 2354.6660 - mae: 29.1985 - val_loss: 3658.0519 - val_mse: 3658.0522 - val_mae: 23.3472\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2686.9687 - mse: 2686.9690 - mae: 28.3851 - val_loss: 2132.9492 - val_mse: 2132.9490 - val_mae: 25.9424\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 331us/step - loss: 2670.6656 - mse: 2670.6665 - mae: 28.1317 - val_loss: 2130.4289 - val_mse: 2130.4287 - val_mae: 26.2102\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2654.2864 - mse: 2654.2864 - mae: 28.3526 - val_loss: 2137.0257 - val_mse: 2137.0256 - val_mae: 26.0563\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2646.3050 - mse: 2646.3057 - mae: 28.4602 - val_loss: 2143.9344 - val_mse: 2143.9343 - val_mae: 25.9716\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2710.9699 - mse: 2710.9692 - mae: 28.4099 - val_loss: 2143.1494 - val_mse: 2143.1499 - val_mae: 26.0944\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2672.7081 - mse: 2672.7083 - mae: 28.3433 - val_loss: 2144.6618 - val_mse: 2144.6616 - val_mae: 25.9213\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 270us/step - loss: 2671.5567 - mse: 2671.5569 - mae: 28.0335 - val_loss: 2134.9003 - val_mse: 2134.9006 - val_mae: 26.2768\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2729.4282 - mse: 2729.4302 - mae: 28.0163 - val_loss: 2139.5293 - val_mse: 2139.5295 - val_mae: 26.2751\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2663.6547 - mse: 2663.6548 - mae: 28.5377 - val_loss: 2145.7374 - val_mse: 2145.7380 - val_mae: 26.1612\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2671.9037 - mse: 2671.9031 - mae: 28.1552 - val_loss: 2140.6799 - val_mse: 2140.6799 - val_mae: 26.4445\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 342us/step - loss: 2643.2001 - mse: 2643.2012 - mae: 28.1292 - val_loss: 2154.7011 - val_mse: 2154.7012 - val_mae: 25.9604\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2651.4976 - mse: 2651.4978 - mae: 27.9146 - val_loss: 2160.2667 - val_mse: 2160.2668 - val_mae: 26.0552\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 359us/step - loss: 2710.8538 - mse: 2710.8542 - mae: 28.0991 - val_loss: 2161.5039 - val_mse: 2161.5042 - val_mae: 25.9672\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 345us/step - loss: 2641.5598 - mse: 2641.5603 - mae: 27.7923 - val_loss: 2155.4460 - val_mse: 2155.4463 - val_mae: 25.8724\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2632.7540 - mse: 2632.7546 - mae: 27.7375 - val_loss: 2139.8340 - val_mse: 2139.8337 - val_mae: 26.4620\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 338us/step - loss: 2694.4641 - mse: 2694.4639 - mae: 28.0649 - val_loss: 2157.0563 - val_mse: 2157.0564 - val_mae: 26.0711\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2636.7881 - mse: 2636.7874 - mae: 28.0463 - val_loss: 2151.8303 - val_mse: 2151.8301 - val_mae: 26.4175\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2728.6310 - mse: 2728.6318 - mae: 28.4886 - val_loss: 2165.8688 - val_mse: 2165.8689 - val_mae: 25.7230\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2667.8774 - mse: 2667.8774 - mae: 27.8732 - val_loss: 2145.0493 - val_mse: 2145.0496 - val_mae: 26.6098\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2643.9681 - mse: 2643.9678 - mae: 28.2037 - val_loss: 2165.6502 - val_mse: 2165.6504 - val_mae: 26.3209\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2675.2072 - mse: 2675.2075 - mae: 28.0865 - val_loss: 2172.4991 - val_mse: 2172.4990 - val_mae: 26.1986\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2658.4490 - mse: 2658.4487 - mae: 28.0058 - val_loss: 2162.0171 - val_mse: 2162.0168 - val_mae: 26.4952\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 351us/step - loss: 2624.4555 - mse: 2624.4546 - mae: 27.7643 - val_loss: 2180.5039 - val_mse: 2180.5042 - val_mae: 25.9484\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2704.1454 - mse: 2704.1453 - mae: 28.1255 - val_loss: 2180.4650 - val_mse: 2180.4653 - val_mae: 26.1259\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2632.5960 - mse: 2632.5957 - mae: 27.8096 - val_loss: 2172.2190 - val_mse: 2172.2190 - val_mae: 26.4416\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 269us/step - loss: 2672.5130 - mse: 2672.5129 - mae: 28.0657 - val_loss: 2177.0336 - val_mse: 2177.0337 - val_mae: 26.1814\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 330us/step - loss: 2647.8783 - mse: 2647.8784 - mae: 28.0433 - val_loss: 2175.8017 - val_mse: 2175.8018 - val_mae: 26.1866\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2693.2246 - mse: 2693.2241 - mae: 28.2379 - val_loss: 2176.4784 - val_mse: 2176.4785 - val_mae: 26.1471\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2663.6137 - mse: 2663.6138 - mae: 28.0340 - val_loss: 2170.4910 - val_mse: 2170.4907 - val_mae: 26.2181\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2643.5437 - mse: 2643.5435 - mae: 28.0280 - val_loss: 2173.2842 - val_mse: 2173.2842 - val_mae: 26.1605\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2685.8722 - mse: 2685.8726 - mae: 28.0131 - val_loss: 2168.0256 - val_mse: 2168.0256 - val_mae: 26.2069\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2680.1598 - mse: 2680.1599 - mae: 27.9598 - val_loss: 2172.4315 - val_mse: 2172.4319 - val_mae: 26.1991\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 253us/step - loss: 2715.1214 - mse: 2715.1211 - mae: 28.0262 - val_loss: 2174.7413 - val_mse: 2174.7415 - val_mae: 26.0843\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2641.2779 - mse: 2641.2778 - mae: 27.6414 - val_loss: 2162.9662 - val_mse: 2162.9658 - val_mae: 26.2571\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2662.5057 - mse: 2662.5054 - mae: 28.0717 - val_loss: 2151.3387 - val_mse: 2151.3389 - val_mae: 26.8286\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2668.1002 - mse: 2668.1011 - mae: 28.0423 - val_loss: 2151.0126 - val_mse: 2151.0125 - val_mae: 26.3599\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2666.5356 - mse: 2666.5352 - mae: 28.0519 - val_loss: 2160.1564 - val_mse: 2160.1565 - val_mae: 26.2384\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2613.8394 - mse: 2613.8391 - mae: 27.9064 - val_loss: 2156.5686 - val_mse: 2156.5686 - val_mae: 25.9905\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2637.8115 - mse: 2637.8105 - mae: 27.9095 - val_loss: 2155.9462 - val_mse: 2155.9463 - val_mae: 26.1791\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 348us/step - loss: 2702.4270 - mse: 2702.4270 - mae: 28.5686 - val_loss: 2158.8291 - val_mse: 2158.8291 - val_mae: 26.1612\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2679.5540 - mse: 2679.5537 - mae: 28.0937 - val_loss: 2163.7168 - val_mse: 2163.7170 - val_mae: 26.3881\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2672.5608 - mse: 2672.5615 - mae: 28.1285 - val_loss: 2163.4350 - val_mse: 2163.4348 - val_mae: 26.2608\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2639.5466 - mse: 2639.5464 - mae: 27.8930 - val_loss: 2165.4551 - val_mse: 2165.4553 - val_mae: 26.0220\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 349us/step - loss: 2693.7633 - mse: 2693.7637 - mae: 27.9579 - val_loss: 2160.2186 - val_mse: 2160.2183 - val_mae: 26.3976\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2664.5052 - mse: 2664.5046 - mae: 28.1257 - val_loss: 2173.0906 - val_mse: 2173.0903 - val_mae: 25.8746\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2665.8995 - mse: 2665.8997 - mae: 28.1941 - val_loss: 2180.1859 - val_mse: 2180.1860 - val_mae: 25.9241\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2658.1934 - mse: 2658.1941 - mae: 27.9781 - val_loss: 2165.9494 - val_mse: 2165.9492 - val_mae: 26.3686\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2658.1850 - mse: 2658.1843 - mae: 28.1724 - val_loss: 2156.6722 - val_mse: 2156.6721 - val_mae: 26.2586\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 355us/step - loss: 2663.2037 - mse: 2663.2041 - mae: 27.5764 - val_loss: 2165.7065 - val_mse: 2165.7068 - val_mae: 26.1310\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2718.4612 - mse: 2718.4619 - mae: 28.4684 - val_loss: 2171.6361 - val_mse: 2171.6365 - val_mae: 26.1501\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 349us/step - loss: 2627.1716 - mse: 2627.1716 - mae: 27.8542 - val_loss: 2156.6122 - val_mse: 2156.6123 - val_mae: 26.4561\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2675.2498 - mse: 2675.2505 - mae: 28.1377 - val_loss: 2162.5505 - val_mse: 2162.5503 - val_mae: 26.5040\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2586.1503 - mse: 2586.1504 - mae: 27.9819 - val_loss: 2155.1567 - val_mse: 2155.1570 - val_mae: 26.5342\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2650.3547 - mse: 2650.3552 - mae: 27.9707 - val_loss: 2166.2803 - val_mse: 2166.2800 - val_mae: 26.3187\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2688.0599 - mse: 2688.0603 - mae: 28.1020 - val_loss: 2176.0540 - val_mse: 2176.0537 - val_mae: 25.9331\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2667.3161 - mse: 2667.3169 - mae: 27.8039 - val_loss: 2164.9827 - val_mse: 2164.9829 - val_mae: 26.0933\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 340us/step - loss: 2654.2322 - mse: 2654.2334 - mae: 28.0985 - val_loss: 2167.8100 - val_mse: 2167.8098 - val_mae: 26.1473\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 329us/step - loss: 2596.6970 - mse: 2596.6970 - mae: 27.8159 - val_loss: 2165.9430 - val_mse: 2165.9431 - val_mae: 26.0780\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2639.3652 - mse: 2639.3662 - mae: 27.9654 - val_loss: 2166.9043 - val_mse: 2166.9043 - val_mae: 26.1988\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2618.3352 - mse: 2618.3350 - mae: 27.9384 - val_loss: 2158.3665 - val_mse: 2158.3669 - val_mae: 26.1753\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2703.4266 - mse: 2703.4268 - mae: 28.0764 - val_loss: 2163.0060 - val_mse: 2163.0061 - val_mae: 26.5312\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 333us/step - loss: 2629.1875 - mse: 2629.1868 - mae: 27.8360 - val_loss: 2165.8820 - val_mse: 2165.8821 - val_mae: 26.2179\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2588.0371 - mse: 2588.0366 - mae: 27.5584 - val_loss: 2150.1390 - val_mse: 2150.1392 - val_mae: 26.4331\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2622.0614 - mse: 2622.0618 - mae: 28.0550 - val_loss: 2163.7688 - val_mse: 2163.7690 - val_mae: 26.2564\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2681.3945 - mse: 2681.3948 - mae: 27.9834 - val_loss: 2170.4596 - val_mse: 2170.4595 - val_mae: 26.2111\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2615.1803 - mse: 2615.1804 - mae: 27.8741 - val_loss: 2169.8664 - val_mse: 2169.8665 - val_mae: 26.3383\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2626.0667 - mse: 2626.0671 - mae: 27.8454 - val_loss: 2168.5864 - val_mse: 2168.5862 - val_mae: 26.3834\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 358us/step - loss: 2596.4541 - mse: 2596.4529 - mae: 27.8477 - val_loss: 2171.0031 - val_mse: 2171.0029 - val_mae: 26.2315\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 333us/step - loss: 2666.7790 - mse: 2666.7795 - mae: 28.1712 - val_loss: 2170.5981 - val_mse: 2170.5981 - val_mae: 26.4756\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 347us/step - loss: 2665.0556 - mse: 2665.0554 - mae: 28.1463 - val_loss: 2175.0341 - val_mse: 2175.0342 - val_mae: 26.0529\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2608.4440 - mse: 2608.4431 - mae: 27.6090 - val_loss: 2163.7576 - val_mse: 2163.7578 - val_mae: 26.4266\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2623.7718 - mse: 2623.7715 - mae: 27.7956 - val_loss: 2166.8372 - val_mse: 2166.8372 - val_mae: 26.4563\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2622.8196 - mse: 2622.8196 - mae: 28.3838 - val_loss: 2169.4590 - val_mse: 2169.4592 - val_mae: 26.1226\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2610.6362 - mse: 2610.6350 - mae: 27.5993 - val_loss: 2161.7878 - val_mse: 2161.7881 - val_mae: 26.6215\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2636.2914 - mse: 2636.2917 - mae: 27.9996 - val_loss: 2170.7873 - val_mse: 2170.7874 - val_mae: 26.4712\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2674.3792 - mse: 2674.3782 - mae: 27.7987 - val_loss: 2181.9967 - val_mse: 2181.9966 - val_mae: 26.0965\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2625.9299 - mse: 2625.9297 - mae: 27.9907 - val_loss: 2164.5005 - val_mse: 2164.5007 - val_mae: 26.8740\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 272us/step - loss: 2655.3513 - mse: 2655.3506 - mae: 28.2263 - val_loss: 2175.5005 - val_mse: 2175.5007 - val_mae: 25.9755\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2627.6127 - mse: 2627.6123 - mae: 27.5835 - val_loss: 2155.8538 - val_mse: 2155.8540 - val_mae: 26.2586\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2660.9227 - mse: 2660.9231 - mae: 28.1717 - val_loss: 2157.9821 - val_mse: 2157.9819 - val_mae: 26.4043\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13284.4544 - mse: 13284.4541 - mae: 109.7098 - val_loss: 34512.6530 - val_mse: 34512.6562 - val_mae: 132.3377\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 271us/step - loss: 12971.7029 - mse: 12971.7031 - mae: 108.2818 - val_loss: 33893.2557 - val_mse: 33893.2539 - val_mae: 130.0062\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 367us/step - loss: 12048.2284 - mse: 12048.2275 - mae: 103.9042 - val_loss: 31998.4090 - val_mse: 31998.4082 - val_mae: 122.6045\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 248us/step - loss: 9657.0774 - mse: 9657.0781 - mae: 91.2772 - val_loss: 27251.7496 - val_mse: 27251.7500 - val_mae: 101.7425\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 5161.3492 - mse: 5161.3496 - mae: 59.2395 - val_loss: 19637.4934 - val_mse: 19637.4941 - val_mae: 53.4705\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 257us/step - loss: 2578.4692 - mse: 2578.4692 - mae: 37.3945 - val_loss: 17599.2089 - val_mse: 17599.2070 - val_mae: 35.1245\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 261us/step - loss: 2684.3195 - mse: 2684.3193 - mae: 37.6440 - val_loss: 18073.1058 - val_mse: 18073.1055 - val_mae: 37.8908\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2415.9626 - mse: 2415.9626 - mae: 35.1880 - val_loss: 17644.3341 - val_mse: 17644.3340 - val_mae: 35.2659\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2724.9831 - mse: 2724.9829 - mae: 37.9750 - val_loss: 17952.6429 - val_mse: 17952.6426 - val_mae: 36.9800\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 251us/step - loss: 2613.1209 - mse: 2613.1211 - mae: 36.5164 - val_loss: 17969.1054 - val_mse: 17969.1035 - val_mae: 37.0622\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 259us/step - loss: 2696.7209 - mse: 2696.7207 - mae: 38.3302 - val_loss: 18290.9147 - val_mse: 18290.9141 - val_mae: 39.6959\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 281us/step - loss: 2580.8667 - mse: 2580.8667 - mae: 36.3758 - val_loss: 18230.0536 - val_mse: 18230.0547 - val_mae: 39.1005\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 362us/step - loss: 2760.0913 - mse: 2760.0916 - mae: 37.3550 - val_loss: 17930.5753 - val_mse: 17930.5762 - val_mae: 36.7144\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2563.9285 - mse: 2563.9285 - mae: 36.4700 - val_loss: 17893.4430 - val_mse: 17893.4414 - val_mae: 36.4621\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 359us/step - loss: 2588.3592 - mse: 2588.3591 - mae: 36.7385 - val_loss: 17924.5365 - val_mse: 17924.5352 - val_mae: 36.6270\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 363us/step - loss: 2402.1684 - mse: 2402.1685 - mae: 34.4294 - val_loss: 17748.4233 - val_mse: 17748.4238 - val_mae: 35.6384\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2435.9943 - mse: 2435.9944 - mae: 35.3972 - val_loss: 17859.0009 - val_mse: 17859.0000 - val_mae: 36.2019\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2358.3083 - mse: 2358.3088 - mae: 34.4536 - val_loss: 17880.1533 - val_mse: 17880.1543 - val_mae: 36.3094\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 2477.0493 - mse: 2477.0496 - mae: 35.0727 - val_loss: 17849.1126 - val_mse: 17849.1113 - val_mae: 36.1214\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 246us/step - loss: 2465.5300 - mse: 2465.5300 - mae: 35.6361 - val_loss: 17783.7709 - val_mse: 17783.7695 - val_mae: 35.7694\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2587.6758 - mse: 2587.6760 - mae: 36.0124 - val_loss: 18105.7001 - val_mse: 18105.6973 - val_mae: 37.7769\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 2178.5964 - mse: 2178.5962 - mae: 33.6873 - val_loss: 17698.3706 - val_mse: 17698.3711 - val_mae: 35.4006\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2802.7319 - mse: 2802.7319 - mae: 37.0835 - val_loss: 18158.7005 - val_mse: 18158.6992 - val_mae: 38.1821\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2324.9606 - mse: 2324.9604 - mae: 33.7769 - val_loss: 17816.8474 - val_mse: 17816.8457 - val_mae: 35.8996\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2531.8032 - mse: 2531.8032 - mae: 36.0804 - val_loss: 18068.0898 - val_mse: 18068.0898 - val_mae: 37.4060\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 328us/step - loss: 2309.3340 - mse: 2309.3337 - mae: 32.9056 - val_loss: 17794.4893 - val_mse: 17794.4902 - val_mae: 35.7773\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 338us/step - loss: 2401.9714 - mse: 2401.9714 - mae: 33.5412 - val_loss: 17917.6465 - val_mse: 17917.6484 - val_mae: 36.4343\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 376us/step - loss: 2297.0662 - mse: 2297.0662 - mae: 33.0213 - val_loss: 17910.1856 - val_mse: 17910.1855 - val_mae: 36.3832\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2275.5112 - mse: 2275.5112 - mae: 34.0789 - val_loss: 17828.5882 - val_mse: 17828.5879 - val_mae: 35.9371\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 362us/step - loss: 2509.2745 - mse: 2509.2744 - mae: 34.8604 - val_loss: 17885.4131 - val_mse: 17885.4141 - val_mae: 36.2332\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 355us/step - loss: 2319.9746 - mse: 2319.9746 - mae: 33.3162 - val_loss: 17899.8533 - val_mse: 17899.8535 - val_mae: 36.3045\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 2311.6119 - mse: 2311.6118 - mae: 33.9498 - val_loss: 17793.8911 - val_mse: 17793.8945 - val_mae: 35.7531\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 350us/step - loss: 2342.4858 - mse: 2342.4858 - mae: 34.2936 - val_loss: 17955.8143 - val_mse: 17955.8145 - val_mae: 36.5952\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 2282.3103 - mse: 2282.3103 - mae: 32.3779 - val_loss: 17547.7698 - val_mse: 17547.7695 - val_mae: 34.9689\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 316us/step - loss: 2192.2147 - mse: 2192.2148 - mae: 32.3615 - val_loss: 17813.9846 - val_mse: 17813.9844 - val_mae: 35.8298\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 2069.9756 - mse: 2069.9756 - mae: 32.0126 - val_loss: 17855.7282 - val_mse: 17855.7285 - val_mae: 36.0268\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 371us/step - loss: 2239.9472 - mse: 2239.9475 - mae: 33.3307 - val_loss: 17786.7627 - val_mse: 17786.7637 - val_mae: 35.6918\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 287us/step - loss: 2188.5033 - mse: 2188.5032 - mae: 33.2657 - val_loss: 17864.0090 - val_mse: 17864.0078 - val_mae: 36.0612\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 2166.8678 - mse: 2166.8677 - mae: 32.8452 - val_loss: 17860.5891 - val_mse: 17860.5898 - val_mae: 36.0358\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 2252.4355 - mse: 2252.4355 - mae: 33.0376 - val_loss: 17957.0354 - val_mse: 17957.0352 - val_mae: 36.5591\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2231.1741 - mse: 2231.1741 - mae: 32.9520 - val_loss: 17664.1411 - val_mse: 17664.1406 - val_mae: 35.2610\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2283.4992 - mse: 2283.4993 - mae: 33.3382 - val_loss: 17733.2585 - val_mse: 17733.2578 - val_mae: 35.4647\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2198.2878 - mse: 2198.2881 - mae: 31.8576 - val_loss: 17681.0449 - val_mse: 17681.0449 - val_mae: 35.3147\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 2075.4263 - mse: 2075.4263 - mae: 31.9976 - val_loss: 17761.1639 - val_mse: 17761.1641 - val_mae: 35.5544\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 2225.5950 - mse: 2225.5950 - mae: 33.5868 - val_loss: 17910.4079 - val_mse: 17910.4062 - val_mae: 36.2583\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 329us/step - loss: 2352.0514 - mse: 2352.0515 - mae: 33.9986 - val_loss: 18083.4004 - val_mse: 18083.4004 - val_mae: 37.3509\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 273us/step - loss: 2160.9400 - mse: 2160.9402 - mae: 31.8070 - val_loss: 17702.0551 - val_mse: 17702.0566 - val_mae: 35.3915\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 1898.9362 - mse: 1898.9363 - mae: 30.6516 - val_loss: 17664.2451 - val_mse: 17664.2441 - val_mae: 35.3077\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2088.1537 - mse: 2088.1538 - mae: 31.7320 - val_loss: 17724.4713 - val_mse: 17724.4727 - val_mae: 35.4567\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2172.6051 - mse: 2172.6047 - mae: 32.3837 - val_loss: 17921.9648 - val_mse: 17921.9648 - val_mae: 36.2981\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 223us/step - loss: 2155.3842 - mse: 2155.3843 - mae: 31.6672 - val_loss: 17639.1980 - val_mse: 17639.1973 - val_mae: 35.2877\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 267us/step - loss: 2262.5267 - mse: 2262.5266 - mae: 32.4788 - val_loss: 17791.9506 - val_mse: 17791.9492 - val_mae: 35.6765\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 2101.8535 - mse: 2101.8535 - mae: 31.3998 - val_loss: 17742.1136 - val_mse: 17742.1113 - val_mae: 35.5288\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 335us/step - loss: 1860.4509 - mse: 1860.4509 - mae: 30.7661 - val_loss: 17695.0441 - val_mse: 17695.0469 - val_mae: 35.4353\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2055.7818 - mse: 2055.7817 - mae: 31.0177 - val_loss: 17787.5528 - val_mse: 17787.5527 - val_mae: 35.6668\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 2025.6617 - mse: 2025.6616 - mae: 30.6334 - val_loss: 17805.2821 - val_mse: 17805.2812 - val_mae: 35.7317\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 1981.9515 - mse: 1981.9515 - mae: 31.7144 - val_loss: 17719.1340 - val_mse: 17719.1348 - val_mae: 35.5123\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2132.1180 - mse: 2132.1179 - mae: 31.9522 - val_loss: 17630.1293 - val_mse: 17630.1309 - val_mae: 35.3499\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2062.8958 - mse: 2062.8960 - mae: 30.5785 - val_loss: 17664.1235 - val_mse: 17664.1230 - val_mae: 35.4169\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 1817.5927 - mse: 1817.5930 - mae: 28.6663 - val_loss: 17478.6300 - val_mse: 17478.6309 - val_mae: 35.4128\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 2231.3626 - mse: 2231.3625 - mae: 31.9669 - val_loss: 17840.0391 - val_mse: 17840.0391 - val_mae: 35.8836\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2077.4936 - mse: 2077.4937 - mae: 31.2751 - val_loss: 17766.3518 - val_mse: 17766.3516 - val_mae: 35.6642\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 2298.6156 - mse: 2298.6155 - mae: 33.0320 - val_loss: 17785.4717 - val_mse: 17785.4727 - val_mae: 35.7244\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 345us/step - loss: 2019.4962 - mse: 2019.4961 - mae: 31.3826 - val_loss: 17847.4935 - val_mse: 17847.4941 - val_mae: 35.9331\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 289us/step - loss: 1945.1978 - mse: 1945.1979 - mae: 30.0700 - val_loss: 17472.4370 - val_mse: 17472.4355 - val_mae: 35.5679\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 277us/step - loss: 2103.3219 - mse: 2103.3223 - mae: 30.7681 - val_loss: 17804.5759 - val_mse: 17804.5742 - val_mae: 35.8026\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 1886.1758 - mse: 1886.1758 - mae: 29.5522 - val_loss: 17673.8613 - val_mse: 17673.8594 - val_mae: 35.5172\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2030.8398 - mse: 2030.8397 - mae: 31.3871 - val_loss: 17692.1654 - val_mse: 17692.1660 - val_mae: 35.5603\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 1908.6704 - mse: 1908.6703 - mae: 30.8004 - val_loss: 17670.1112 - val_mse: 17670.1113 - val_mae: 35.5266\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 1929.5566 - mse: 1929.5565 - mae: 29.2019 - val_loss: 17684.2936 - val_mse: 17684.2949 - val_mae: 35.5593\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2020.3232 - mse: 2020.3234 - mae: 30.9122 - val_loss: 17669.9099 - val_mse: 17669.9102 - val_mae: 35.5474\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 316us/step - loss: 1847.0235 - mse: 1847.0238 - mae: 29.1388 - val_loss: 17650.6998 - val_mse: 17650.6973 - val_mae: 35.5550\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 276us/step - loss: 1929.3178 - mse: 1929.3179 - mae: 30.2043 - val_loss: 17677.4238 - val_mse: 17677.4219 - val_mae: 35.5834\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 2064.0484 - mse: 2064.0483 - mae: 31.4723 - val_loss: 17749.7392 - val_mse: 17749.7383 - val_mae: 35.7243\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 1986.2916 - mse: 1986.2916 - mae: 30.6624 - val_loss: 17770.9173 - val_mse: 17770.9180 - val_mae: 35.7814\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 1919.2800 - mse: 1919.2798 - mae: 29.6587 - val_loss: 17580.9259 - val_mse: 17580.9258 - val_mae: 35.6828\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 1927.2456 - mse: 1927.2457 - mae: 30.4441 - val_loss: 17688.4937 - val_mse: 17688.4941 - val_mae: 35.6708\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 1868.0550 - mse: 1868.0551 - mae: 30.4920 - val_loss: 17873.5803 - val_mse: 17873.5820 - val_mae: 36.1053\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 367us/step - loss: 1871.4721 - mse: 1871.4720 - mae: 29.3942 - val_loss: 17621.0963 - val_mse: 17621.0977 - val_mae: 35.7433\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 369us/step - loss: 2003.1063 - mse: 2003.1062 - mae: 30.5774 - val_loss: 17751.8966 - val_mse: 17751.8965 - val_mae: 35.7918\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4153.4015 - mse: 4153.4014 - mae: 34.5498 - val_loss: 2182.6997 - val_mse: 2182.6997 - val_mae: 31.5846\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 332us/step - loss: 4307.4125 - mse: 4307.4126 - mae: 34.9363 - val_loss: 2157.8829 - val_mse: 2157.8828 - val_mae: 31.5237\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4238.4158 - mse: 4238.4160 - mae: 35.3652 - val_loss: 2272.8718 - val_mse: 2272.8721 - val_mae: 31.8318\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 252us/step - loss: 4197.8268 - mse: 4197.8267 - mae: 34.1403 - val_loss: 2223.7597 - val_mse: 2223.7595 - val_mae: 31.6939\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4298.6553 - mse: 4298.6543 - mae: 35.2993 - val_loss: 2239.9674 - val_mse: 2239.9673 - val_mae: 31.7379\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 251us/step - loss: 4205.3201 - mse: 4205.3203 - mae: 34.7731 - val_loss: 2302.7277 - val_mse: 2302.7275 - val_mae: 31.9197\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 263us/step - loss: 4046.3655 - mse: 4046.3660 - mae: 34.5240 - val_loss: 2294.3754 - val_mse: 2294.3752 - val_mae: 31.8915\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4255.5919 - mse: 4255.5923 - mae: 34.2637 - val_loss: 2228.0181 - val_mse: 2228.0183 - val_mae: 31.7053\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4094.8634 - mse: 4094.8635 - mae: 34.5418 - val_loss: 2235.2883 - val_mse: 2235.2883 - val_mae: 31.7236\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4218.6562 - mse: 4218.6558 - mae: 34.5505 - val_loss: 2358.6302 - val_mse: 2358.6304 - val_mae: 32.1085\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 336us/step - loss: 4213.4298 - mse: 4213.4297 - mae: 34.1380 - val_loss: 2236.0853 - val_mse: 2236.0852 - val_mae: 31.7257\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4160.3232 - mse: 4160.3228 - mae: 34.7202 - val_loss: 2216.3148 - val_mse: 2216.3149 - val_mae: 31.6708\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 352us/step - loss: 4316.0403 - mse: 4316.0400 - mae: 34.7225 - val_loss: 2265.5131 - val_mse: 2265.5132 - val_mae: 31.8002\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 302us/step - loss: 4135.8806 - mse: 4135.8809 - mae: 33.9389 - val_loss: 2208.4034 - val_mse: 2208.4033 - val_mae: 31.6481\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 276us/step - loss: 4208.0404 - mse: 4208.0405 - mae: 34.4034 - val_loss: 2200.2830 - val_mse: 2200.2832 - val_mae: 31.6239\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4199.9509 - mse: 4199.9497 - mae: 34.4569 - val_loss: 2315.2943 - val_mse: 2315.2939 - val_mae: 31.9469\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 247us/step - loss: 4119.8750 - mse: 4119.8750 - mae: 34.2792 - val_loss: 2257.4105 - val_mse: 2257.4104 - val_mae: 31.7758\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 256us/step - loss: 3857.6195 - mse: 3857.6201 - mae: 32.6045 - val_loss: 2196.2962 - val_mse: 2196.2964 - val_mae: 31.6095\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 301us/step - loss: 4080.7737 - mse: 4080.7727 - mae: 34.0209 - val_loss: 2291.8431 - val_mse: 2291.8433 - val_mae: 31.8690\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4182.2741 - mse: 4182.2744 - mae: 33.7452 - val_loss: 2271.0161 - val_mse: 2271.0161 - val_mae: 31.8098\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 253us/step - loss: 4148.6010 - mse: 4148.6011 - mae: 33.1406 - val_loss: 2231.9700 - val_mse: 2231.9697 - val_mae: 31.7082\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 329us/step - loss: 4183.0217 - mse: 4183.0220 - mae: 34.3180 - val_loss: 2211.5238 - val_mse: 2211.5239 - val_mae: 31.6510\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 376us/step - loss: 4138.6382 - mse: 4138.6382 - mae: 34.1107 - val_loss: 2278.9867 - val_mse: 2278.9868 - val_mae: 31.8331\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 4134.6189 - mse: 4134.6191 - mae: 33.8556 - val_loss: 2244.6909 - val_mse: 2244.6909 - val_mae: 31.7403\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4110.2024 - mse: 4110.2021 - mae: 33.7269 - val_loss: 2256.8552 - val_mse: 2256.8552 - val_mae: 31.7711\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4186.3222 - mse: 4186.3218 - mae: 34.2639 - val_loss: 2242.6604 - val_mse: 2242.6604 - val_mae: 31.7347\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 248us/step - loss: 4266.8821 - mse: 4266.8828 - mae: 34.8102 - val_loss: 2260.5524 - val_mse: 2260.5522 - val_mae: 31.7822\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 335us/step - loss: 4205.6569 - mse: 4205.6567 - mae: 33.1823 - val_loss: 2289.7751 - val_mse: 2289.7754 - val_mae: 31.8619\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4055.1892 - mse: 4055.1890 - mae: 33.6052 - val_loss: 2263.0932 - val_mse: 2263.0930 - val_mae: 31.7888\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 248us/step - loss: 4030.6547 - mse: 4030.6543 - mae: 33.6271 - val_loss: 2222.0836 - val_mse: 2222.0835 - val_mae: 31.6783\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 226us/step - loss: 3676.6845 - mse: 3676.6841 - mae: 33.3495 - val_loss: 2242.4646 - val_mse: 2242.4646 - val_mae: 31.7322\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 346us/step - loss: 4071.6279 - mse: 4071.6284 - mae: 34.4497 - val_loss: 2267.3374 - val_mse: 2267.3369 - val_mae: 31.7975\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 358us/step - loss: 4036.6109 - mse: 4036.6111 - mae: 33.2223 - val_loss: 2291.9878 - val_mse: 2291.9878 - val_mae: 31.8633\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 338us/step - loss: 3940.1296 - mse: 3940.1296 - mae: 33.2523 - val_loss: 2280.5688 - val_mse: 2280.5688 - val_mae: 31.8312\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4086.2643 - mse: 4086.2642 - mae: 33.8240 - val_loss: 2282.3128 - val_mse: 2282.3127 - val_mae: 31.8361\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4068.0510 - mse: 4068.0515 - mae: 33.8562 - val_loss: 2314.8043 - val_mse: 2314.8042 - val_mae: 31.9264\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 394us/step - loss: 4062.6385 - mse: 4062.6379 - mae: 34.1701 - val_loss: 2268.5284 - val_mse: 2268.5286 - val_mae: 31.7992\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 328us/step - loss: 3886.4553 - mse: 3886.4553 - mae: 33.5923 - val_loss: 2251.9403 - val_mse: 2251.9402 - val_mae: 31.7513\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 271us/step - loss: 4022.5709 - mse: 4022.5708 - mae: 33.8842 - val_loss: 2276.3384 - val_mse: 2276.3389 - val_mae: 31.8197\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 3945.2779 - mse: 3945.2773 - mae: 32.8411 - val_loss: 2253.2208 - val_mse: 2253.2207 - val_mae: 31.7541\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 323us/step - loss: 3814.2549 - mse: 3814.2549 - mae: 33.4537 - val_loss: 2215.0343 - val_mse: 2215.0344 - val_mae: 31.6490\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4075.8835 - mse: 4075.8831 - mae: 32.7506 - val_loss: 2260.9511 - val_mse: 2260.9512 - val_mae: 31.7752\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4248.8200 - mse: 4248.8198 - mae: 34.5772 - val_loss: 2277.2464 - val_mse: 2277.2466 - val_mae: 31.8193\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 251us/step - loss: 4119.4793 - mse: 4119.4795 - mae: 34.3178 - val_loss: 2306.4470 - val_mse: 2306.4470 - val_mae: 31.8974\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 347us/step - loss: 4007.8106 - mse: 4007.8108 - mae: 33.8000 - val_loss: 2275.2563 - val_mse: 2275.2566 - val_mae: 31.8113\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 336us/step - loss: 4209.0491 - mse: 4209.0488 - mae: 33.6636 - val_loss: 2281.0683 - val_mse: 2281.0681 - val_mae: 31.8276\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4054.9405 - mse: 4054.9409 - mae: 32.3572 - val_loss: 2258.7460 - val_mse: 2258.7461 - val_mae: 31.7644\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4101.2386 - mse: 4101.2383 - mae: 33.4991 - val_loss: 2256.7318 - val_mse: 2256.7319 - val_mae: 31.7549\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 4049.8279 - mse: 4049.8276 - mae: 33.6793 - val_loss: 2259.9986 - val_mse: 2259.9988 - val_mae: 31.7624\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 391us/step - loss: 4148.3328 - mse: 4148.3325 - mae: 33.6133 - val_loss: 2294.5128 - val_mse: 2294.5125 - val_mae: 31.8580\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 3978.5227 - mse: 3978.5215 - mae: 33.0710 - val_loss: 2247.8747 - val_mse: 2247.8745 - val_mae: 31.7265\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 377us/step - loss: 3959.1017 - mse: 3959.1016 - mae: 32.8857 - val_loss: 2239.2011 - val_mse: 2239.2009 - val_mae: 31.7019\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4136.7386 - mse: 4136.7388 - mae: 33.4797 - val_loss: 2251.3774 - val_mse: 2251.3772 - val_mae: 31.7362\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 4135.5191 - mse: 4135.5190 - mae: 33.8213 - val_loss: 2346.5839 - val_mse: 2346.5842 - val_mae: 32.0005\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4009.3326 - mse: 4009.3330 - mae: 33.0530 - val_loss: 2247.4868 - val_mse: 2247.4868 - val_mae: 31.7220\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4300.4040 - mse: 4300.4033 - mae: 34.4193 - val_loss: 2377.9787 - val_mse: 2377.9790 - val_mae: 32.0959\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4004.1432 - mse: 4004.1428 - mae: 32.9673 - val_loss: 2253.0251 - val_mse: 2253.0251 - val_mae: 31.7390\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 363us/step - loss: 4039.7934 - mse: 4039.7935 - mae: 32.7435 - val_loss: 2233.9181 - val_mse: 2233.9182 - val_mae: 31.6829\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 258us/step - loss: 4052.7096 - mse: 4052.7092 - mae: 32.3676 - val_loss: 2255.1450 - val_mse: 2255.1450 - val_mae: 31.7434\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 252us/step - loss: 4199.5936 - mse: 4199.5938 - mae: 34.1718 - val_loss: 2279.6693 - val_mse: 2279.6692 - val_mae: 31.8121\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 257us/step - loss: 4141.1869 - mse: 4141.1875 - mae: 32.4073 - val_loss: 2272.0156 - val_mse: 2272.0159 - val_mae: 31.7897\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 3938.2673 - mse: 3938.2671 - mae: 32.0791 - val_loss: 2241.9544 - val_mse: 2241.9543 - val_mae: 31.7026\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4008.9527 - mse: 4008.9524 - mae: 33.8134 - val_loss: 2261.7939 - val_mse: 2261.7939 - val_mae: 31.7595\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4062.4157 - mse: 4062.4150 - mae: 33.5243 - val_loss: 2277.0651 - val_mse: 2277.0652 - val_mae: 31.8027\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 259us/step - loss: 3859.8867 - mse: 3859.8870 - mae: 32.8515 - val_loss: 2307.5551 - val_mse: 2307.5552 - val_mae: 31.8850\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4068.7011 - mse: 4068.7007 - mae: 33.2294 - val_loss: 2273.9472 - val_mse: 2273.9473 - val_mae: 31.7910\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4036.1466 - mse: 4036.1467 - mae: 33.0582 - val_loss: 2258.1409 - val_mse: 2258.1411 - val_mae: 31.7431\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4048.7065 - mse: 4048.7065 - mae: 33.7370 - val_loss: 2294.5284 - val_mse: 2294.5286 - val_mae: 31.8477\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4073.4536 - mse: 4073.4539 - mae: 33.8218 - val_loss: 2260.7729 - val_mse: 2260.7729 - val_mae: 31.7526\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 245us/step - loss: 3991.5876 - mse: 3991.5872 - mae: 33.3090 - val_loss: 2242.6189 - val_mse: 2242.6189 - val_mae: 31.6963\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 254us/step - loss: 4069.7843 - mse: 4069.7847 - mae: 33.7440 - val_loss: 2332.4596 - val_mse: 2332.4595 - val_mae: 31.9468\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 370us/step - loss: 3849.2580 - mse: 3849.2581 - mae: 31.9483 - val_loss: 2274.7383 - val_mse: 2274.7383 - val_mae: 31.7882\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4045.7810 - mse: 4045.7805 - mae: 33.6631 - val_loss: 2336.4623 - val_mse: 2336.4622 - val_mae: 31.9578\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4112.4307 - mse: 4112.4307 - mae: 34.1237 - val_loss: 2325.8218 - val_mse: 2325.8218 - val_mae: 31.9280\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 3977.8166 - mse: 3977.8162 - mae: 32.9301 - val_loss: 2285.1346 - val_mse: 2285.1345 - val_mae: 31.8181\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 297us/step - loss: 4097.8059 - mse: 4097.8062 - mae: 32.6415 - val_loss: 2285.4155 - val_mse: 2285.4155 - val_mae: 31.8199\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 351us/step - loss: 4022.7461 - mse: 4022.7468 - mae: 33.8138 - val_loss: 2267.6041 - val_mse: 2267.6040 - val_mae: 31.7694\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 4011.7499 - mse: 4011.7500 - mae: 32.5396 - val_loss: 2298.0570 - val_mse: 2298.0569 - val_mae: 31.8585\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 362us/step - loss: 4025.1397 - mse: 4025.1399 - mae: 33.7683 - val_loss: 2302.3848 - val_mse: 2302.3848 - val_mae: 31.8694\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 345us/step - loss: 3984.8732 - mse: 3984.8733 - mae: 32.2730 - val_loss: 2261.7882 - val_mse: 2261.7883 - val_mae: 31.7525\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 254us/step - loss: 3449.0298 - mse: 3449.0291 - mae: 33.2928 - val_loss: 1452.8022 - val_mse: 1452.8021 - val_mae: 25.8107\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 256us/step - loss: 3359.8970 - mse: 3359.8970 - mae: 32.7855 - val_loss: 1453.5939 - val_mse: 1453.5941 - val_mae: 25.7891\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3364.8440 - mse: 3364.8447 - mae: 33.0473 - val_loss: 1454.2318 - val_mse: 1454.2318 - val_mae: 25.2655\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 267us/step - loss: 3376.0632 - mse: 3376.0635 - mae: 32.1031 - val_loss: 1455.3157 - val_mse: 1455.3157 - val_mae: 25.8742\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3548.6398 - mse: 3548.6409 - mae: 33.9041 - val_loss: 1456.0471 - val_mse: 1456.0472 - val_mae: 25.9082\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3384.3772 - mse: 3384.3767 - mae: 32.6712 - val_loss: 1456.0094 - val_mse: 1456.0094 - val_mae: 25.4405\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3392.6746 - mse: 3392.6743 - mae: 32.2338 - val_loss: 1456.8495 - val_mse: 1456.8496 - val_mae: 25.4221\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3372.3909 - mse: 3372.3906 - mae: 32.6189 - val_loss: 1457.7736 - val_mse: 1457.7736 - val_mae: 25.3992\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3332.2307 - mse: 3332.2314 - mae: 31.7148 - val_loss: 1458.8199 - val_mse: 1458.8198 - val_mae: 25.8373\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3414.5591 - mse: 3414.5591 - mae: 32.4904 - val_loss: 1459.9439 - val_mse: 1459.9438 - val_mae: 25.9579\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3359.5565 - mse: 3359.5566 - mae: 32.5629 - val_loss: 1459.8173 - val_mse: 1459.8174 - val_mae: 25.4299\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3367.8692 - mse: 3367.8689 - mae: 32.5449 - val_loss: 1461.7813 - val_mse: 1461.7812 - val_mae: 25.1149\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3219.0199 - mse: 3219.0200 - mae: 31.8950 - val_loss: 1462.0405 - val_mse: 1462.0406 - val_mae: 26.0423\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 332us/step - loss: 3405.2624 - mse: 3405.2625 - mae: 32.8249 - val_loss: 1461.9885 - val_mse: 1461.9883 - val_mae: 25.3086\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 372us/step - loss: 3285.8866 - mse: 3285.8860 - mae: 31.8817 - val_loss: 1462.6495 - val_mse: 1462.6495 - val_mae: 25.9414\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 328us/step - loss: 3323.6493 - mse: 3323.6494 - mae: 32.1068 - val_loss: 1463.6860 - val_mse: 1463.6860 - val_mae: 25.1202\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3379.6628 - mse: 3379.6626 - mae: 32.6089 - val_loss: 1462.6482 - val_mse: 1462.6482 - val_mae: 25.5609\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 346us/step - loss: 3358.9387 - mse: 3358.9385 - mae: 32.7377 - val_loss: 1462.5674 - val_mse: 1462.5674 - val_mae: 25.7793\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3365.8361 - mse: 3365.8367 - mae: 32.1285 - val_loss: 1463.1038 - val_mse: 1463.1038 - val_mae: 25.7819\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3329.2198 - mse: 3329.2200 - mae: 32.4489 - val_loss: 1464.0361 - val_mse: 1464.0363 - val_mae: 25.9134\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 335us/step - loss: 3263.3168 - mse: 3263.3167 - mae: 31.7857 - val_loss: 1463.2499 - val_mse: 1463.2500 - val_mae: 25.5599\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 353us/step - loss: 3297.9553 - mse: 3297.9551 - mae: 32.0798 - val_loss: 1466.0926 - val_mse: 1466.0925 - val_mae: 26.2878\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3338.8768 - mse: 3338.8777 - mae: 32.4426 - val_loss: 1465.4179 - val_mse: 1465.4180 - val_mae: 25.1664\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 339us/step - loss: 3324.5963 - mse: 3324.5957 - mae: 32.8332 - val_loss: 1464.7591 - val_mse: 1464.7590 - val_mae: 25.9194\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3372.0701 - mse: 3372.0693 - mae: 32.2198 - val_loss: 1464.1910 - val_mse: 1464.1910 - val_mae: 25.6563\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3365.9867 - mse: 3365.9863 - mae: 32.4406 - val_loss: 1464.6479 - val_mse: 1464.6477 - val_mae: 25.6566\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3372.5334 - mse: 3372.5334 - mae: 32.5990 - val_loss: 1464.3145 - val_mse: 1464.3145 - val_mae: 25.4758\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3350.6748 - mse: 3350.6755 - mae: 32.9308 - val_loss: 1465.2425 - val_mse: 1465.2424 - val_mae: 25.8975\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 271us/step - loss: 3256.3184 - mse: 3256.3188 - mae: 31.3725 - val_loss: 1465.3568 - val_mse: 1465.3569 - val_mae: 25.8488\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3280.6774 - mse: 3280.6772 - mae: 32.3601 - val_loss: 1465.6869 - val_mse: 1465.6869 - val_mae: 25.9146\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 336us/step - loss: 3323.1179 - mse: 3323.1194 - mae: 32.9160 - val_loss: 1465.9056 - val_mse: 1465.9056 - val_mae: 25.3675\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 349us/step - loss: 3268.5118 - mse: 3268.5120 - mae: 32.3038 - val_loss: 1465.9574 - val_mse: 1465.9573 - val_mae: 25.4706\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 336us/step - loss: 3261.7810 - mse: 3261.7822 - mae: 31.7629 - val_loss: 1466.5792 - val_mse: 1466.5791 - val_mae: 25.3725\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3298.8765 - mse: 3298.8762 - mae: 31.8383 - val_loss: 1467.1190 - val_mse: 1467.1191 - val_mae: 25.3320\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3331.3570 - mse: 3331.3569 - mae: 32.0497 - val_loss: 1467.5763 - val_mse: 1467.5762 - val_mae: 25.9108\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 263us/step - loss: 3264.8294 - mse: 3264.8293 - mae: 31.2533 - val_loss: 1469.5867 - val_mse: 1469.5868 - val_mae: 26.2421\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3340.7143 - mse: 3340.7141 - mae: 32.1097 - val_loss: 1467.8307 - val_mse: 1467.8303 - val_mae: 25.7054\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3181.8231 - mse: 3181.8225 - mae: 31.6463 - val_loss: 1468.8855 - val_mse: 1468.8855 - val_mae: 26.0123\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3272.3116 - mse: 3272.3123 - mae: 31.5631 - val_loss: 1471.0839 - val_mse: 1471.0839 - val_mae: 26.3177\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3266.7980 - mse: 3266.7983 - mae: 31.5487 - val_loss: 1471.3320 - val_mse: 1471.3320 - val_mae: 26.3124\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3258.1802 - mse: 3258.1797 - mae: 31.9256 - val_loss: 1470.1134 - val_mse: 1470.1135 - val_mae: 26.0676\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3240.3745 - mse: 3240.3745 - mae: 31.5151 - val_loss: 1470.8110 - val_mse: 1470.8108 - val_mae: 26.2052\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3410.3384 - mse: 3410.3391 - mae: 32.8518 - val_loss: 1469.1117 - val_mse: 1469.1118 - val_mae: 25.7980\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3243.2224 - mse: 3243.2224 - mae: 32.3594 - val_loss: 1469.6718 - val_mse: 1469.6718 - val_mae: 25.8719\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3335.9824 - mse: 3335.9822 - mae: 32.2851 - val_loss: 1469.5686 - val_mse: 1469.5687 - val_mae: 25.7359\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3180.1561 - mse: 3180.1567 - mae: 31.8434 - val_loss: 1470.5994 - val_mse: 1470.5995 - val_mae: 25.3091\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 325us/step - loss: 3308.4632 - mse: 3308.4629 - mae: 31.8652 - val_loss: 1470.6078 - val_mse: 1470.6078 - val_mae: 25.6887\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 337us/step - loss: 3204.4107 - mse: 3204.4114 - mae: 31.6329 - val_loss: 1471.3164 - val_mse: 1471.3163 - val_mae: 25.9244\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3278.0290 - mse: 3278.0300 - mae: 32.0963 - val_loss: 1470.8678 - val_mse: 1470.8677 - val_mae: 25.7593\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 253us/step - loss: 3242.4953 - mse: 3242.4956 - mae: 31.3042 - val_loss: 1471.2062 - val_mse: 1471.2062 - val_mae: 25.5509\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3263.3090 - mse: 3263.3093 - mae: 31.4773 - val_loss: 1471.6640 - val_mse: 1471.6641 - val_mae: 25.7714\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3344.7750 - mse: 3344.7747 - mae: 32.1099 - val_loss: 1471.4339 - val_mse: 1471.4342 - val_mae: 25.5243\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3311.3149 - mse: 3311.3140 - mae: 31.9852 - val_loss: 1471.7595 - val_mse: 1471.7595 - val_mae: 25.9964\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3316.4768 - mse: 3316.4780 - mae: 31.6762 - val_loss: 1472.8437 - val_mse: 1472.8436 - val_mae: 26.1041\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3267.4507 - mse: 3267.4500 - mae: 31.0872 - val_loss: 1474.0698 - val_mse: 1474.0698 - val_mae: 26.3056\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 325us/step - loss: 3215.3067 - mse: 3215.3059 - mae: 31.6116 - val_loss: 1474.8357 - val_mse: 1474.8357 - val_mae: 26.3907\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 350us/step - loss: 3170.8115 - mse: 3170.8113 - mae: 31.8313 - val_loss: 1471.8984 - val_mse: 1471.8984 - val_mae: 25.7565\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 273us/step - loss: 3280.8130 - mse: 3280.8127 - mae: 31.3049 - val_loss: 1474.7536 - val_mse: 1474.7537 - val_mae: 26.3690\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3412.6427 - mse: 3412.6421 - mae: 32.5371 - val_loss: 1472.8912 - val_mse: 1472.8912 - val_mae: 25.4061\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 338us/step - loss: 3298.4236 - mse: 3298.4238 - mae: 31.9482 - val_loss: 1472.5716 - val_mse: 1472.5717 - val_mae: 25.7680\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 345us/step - loss: 3207.8123 - mse: 3207.8123 - mae: 31.8004 - val_loss: 1473.0066 - val_mse: 1473.0067 - val_mae: 25.5327\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3288.5389 - mse: 3288.5386 - mae: 31.5284 - val_loss: 1474.6099 - val_mse: 1474.6100 - val_mae: 26.1806\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3234.8938 - mse: 3234.8931 - mae: 31.4694 - val_loss: 1474.0758 - val_mse: 1474.0759 - val_mae: 26.0547\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 358us/step - loss: 3279.2570 - mse: 3279.2568 - mae: 31.5974 - val_loss: 1473.0777 - val_mse: 1473.0779 - val_mae: 25.7619\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3173.5731 - mse: 3173.5730 - mae: 31.4507 - val_loss: 1473.1192 - val_mse: 1473.1193 - val_mae: 25.4400\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 332us/step - loss: 3254.6012 - mse: 3254.6006 - mae: 31.5402 - val_loss: 1474.3642 - val_mse: 1474.3643 - val_mae: 26.2367\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3302.4920 - mse: 3302.4922 - mae: 32.0915 - val_loss: 1473.2915 - val_mse: 1473.2916 - val_mae: 25.9678\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3171.4065 - mse: 3171.4062 - mae: 31.4338 - val_loss: 1473.4110 - val_mse: 1473.4111 - val_mae: 25.9300\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3213.4514 - mse: 3213.4514 - mae: 31.6520 - val_loss: 1474.3352 - val_mse: 1474.3351 - val_mae: 26.1656\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3234.6449 - mse: 3234.6448 - mae: 31.5243 - val_loss: 1473.5010 - val_mse: 1473.5010 - val_mae: 25.9202\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3349.5740 - mse: 3349.5732 - mae: 32.34 - 0s 317us/step - loss: 3314.1558 - mse: 3314.1548 - mae: 32.3078 - val_loss: 1473.8688 - val_mse: 1473.8688 - val_mae: 25.9603\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3285.7081 - mse: 3285.7083 - mae: 31.4121 - val_loss: 1474.3288 - val_mse: 1474.3289 - val_mae: 26.0533\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3180.2521 - mse: 3180.2529 - mae: 31.4393 - val_loss: 1474.6350 - val_mse: 1474.6349 - val_mae: 26.1946\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3365.2985 - mse: 3365.2986 - mae: 32.3437 - val_loss: 1473.3600 - val_mse: 1473.3600 - val_mae: 25.9777\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3328.2531 - mse: 3328.2524 - mae: 31.7238 - val_loss: 1472.9204 - val_mse: 1472.9204 - val_mae: 25.8365\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 253us/step - loss: 3312.2742 - mse: 3312.2742 - mae: 31.6231 - val_loss: 1473.9585 - val_mse: 1473.9584 - val_mae: 26.1365\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3390.7097 - mse: 3390.7087 - mae: 32.3958 - val_loss: 1472.7775 - val_mse: 1472.7775 - val_mae: 25.7688\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 282us/step - loss: 3208.9673 - mse: 3208.9668 - mae: 31.2316 - val_loss: 1473.5320 - val_mse: 1473.5319 - val_mae: 25.9504\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3195.5170 - mse: 3195.5178 - mae: 31.6478 - val_loss: 1474.3306 - val_mse: 1474.3307 - val_mae: 26.1031\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 274us/step - loss: 3263.5976 - mse: 3263.5974 - mae: 31.8648 - val_loss: 1476.6385 - val_mse: 1476.6385 - val_mae: 26.4290\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2913.3426 - mse: 2913.3423 - mae: 31.1414 - val_loss: 1082.1759 - val_mse: 1082.1759 - val_mae: 24.7338\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2856.8054 - mse: 2856.8052 - mae: 30.8174 - val_loss: 1081.2946 - val_mse: 1081.2946 - val_mae: 24.4680\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2910.9485 - mse: 2910.9490 - mae: 31.0604 - val_loss: 1081.2285 - val_mse: 1081.2285 - val_mae: 24.9167\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2894.8389 - mse: 2894.8391 - mae: 31.4158 - val_loss: 1080.2092 - val_mse: 1080.2092 - val_mae: 24.6733\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 334us/step - loss: 2922.1550 - mse: 2922.1548 - mae: 31.1588 - val_loss: 1079.6277 - val_mse: 1079.6278 - val_mae: 24.6072\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 358us/step - loss: 2876.7020 - mse: 2876.7024 - mae: 30.9865 - val_loss: 1079.8334 - val_mse: 1079.8333 - val_mae: 24.2854\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2910.6005 - mse: 2910.5996 - mae: 31.4102 - val_loss: 1082.0150 - val_mse: 1082.0150 - val_mae: 24.0013\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 0s 248us/step - loss: 2939.6687 - mse: 2939.6694 - mae: 30.6581 - val_loss: 1078.7337 - val_mse: 1078.7338 - val_mae: 24.2202\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2920.2443 - mse: 2920.2446 - mae: 30.8917 - val_loss: 1077.6595 - val_mse: 1077.6595 - val_mae: 24.3718\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 270us/step - loss: 2955.9929 - mse: 2955.9932 - mae: 31.5217 - val_loss: 1081.3279 - val_mse: 1081.3280 - val_mae: 23.9160\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2810.9720 - mse: 2810.9722 - mae: 30.4553 - val_loss: 1076.4895 - val_mse: 1076.4895 - val_mae: 24.4113\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2835.1762 - mse: 2835.1765 - mae: 30.2058 - val_loss: 1075.5608 - val_mse: 1075.5609 - val_mae: 24.4739\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2921.5384 - mse: 2921.5386 - mae: 30.9097 - val_loss: 1075.3472 - val_mse: 1075.3472 - val_mae: 24.6082\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 0s 234us/step - loss: 2874.1355 - mse: 2874.1360 - mae: 30.5828 - val_loss: 1075.3036 - val_mse: 1075.3036 - val_mae: 24.3184\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 263us/step - loss: 2979.6396 - mse: 2979.6401 - mae: 31.0463 - val_loss: 1073.9957 - val_mse: 1073.9956 - val_mae: 24.3601\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 346us/step - loss: 2868.2442 - mse: 2868.2444 - mae: 30.9773 - val_loss: 1073.2928 - val_mse: 1073.2928 - val_mae: 24.5766\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2884.2534 - mse: 2884.2537 - mae: 30.6156 - val_loss: 1074.5861 - val_mse: 1074.5859 - val_mae: 24.1938\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2818.3272 - mse: 2818.3276 - mae: 31.0576 - val_loss: 1074.1820 - val_mse: 1074.1820 - val_mae: 24.6882\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2876.3983 - mse: 2876.3984 - mae: 30.8472 - val_loss: 1073.7147 - val_mse: 1073.7147 - val_mae: 24.6110\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 350us/step - loss: 2894.1972 - mse: 2894.1975 - mae: 30.8287 - val_loss: 1074.8913 - val_mse: 1074.8912 - val_mae: 24.2109\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2911.1433 - mse: 2911.1436 - mae: 31.0055 - val_loss: 1073.4282 - val_mse: 1073.4283 - val_mae: 24.3435\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 274us/step - loss: 2946.0169 - mse: 2946.0171 - mae: 31.0150 - val_loss: 1073.4889 - val_mse: 1073.4890 - val_mae: 24.5466\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 277us/step - loss: 2974.0084 - mse: 2974.0078 - mae: 31.6675 - val_loss: 1073.2652 - val_mse: 1073.2650 - val_mae: 24.5530\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2893.9112 - mse: 2893.9109 - mae: 30.8299 - val_loss: 1072.9998 - val_mse: 1072.9996 - val_mae: 24.4573\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2898.7171 - mse: 2898.7170 - mae: 30.9419 - val_loss: 1072.0133 - val_mse: 1072.0133 - val_mae: 24.5172\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2845.2433 - mse: 2845.2424 - mae: 30.1461 - val_loss: 1072.1760 - val_mse: 1072.1761 - val_mae: 24.7135\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2878.7277 - mse: 2878.7275 - mae: 31.0445 - val_loss: 1071.7441 - val_mse: 1071.7443 - val_mae: 24.3206\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 338us/step - loss: 2769.6383 - mse: 2769.6387 - mae: 30.3608 - val_loss: 1071.2340 - val_mse: 1071.2340 - val_mae: 24.7443\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2939.1292 - mse: 2939.1292 - mae: 31.2956 - val_loss: 1070.4446 - val_mse: 1070.4445 - val_mae: 24.1605\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 339us/step - loss: 2859.2498 - mse: 2859.2495 - mae: 30.7963 - val_loss: 1069.5832 - val_mse: 1069.5831 - val_mae: 24.2164\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2830.9714 - mse: 2830.9717 - mae: 30.7306 - val_loss: 1068.5463 - val_mse: 1068.5463 - val_mae: 24.5030\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2927.1381 - mse: 2927.1375 - mae: 30.8944 - val_loss: 1068.5603 - val_mse: 1068.5603 - val_mae: 24.2621\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2958.5322 - mse: 2958.5315 - mae: 30.9583 - val_loss: 1070.6779 - val_mse: 1070.6779 - val_mae: 23.9576\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2911.9469 - mse: 2911.9470 - mae: 30.7109 - val_loss: 1069.6192 - val_mse: 1069.6191 - val_mae: 24.0152\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2861.7128 - mse: 2861.7124 - mae: 30.0827 - val_loss: 1067.4730 - val_mse: 1067.4729 - val_mae: 24.4070\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2890.9687 - mse: 2890.9675 - mae: 30.5733 - val_loss: 1067.0539 - val_mse: 1067.0541 - val_mae: 24.5051\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2897.6508 - mse: 2897.6511 - mae: 30.5575 - val_loss: 1069.2016 - val_mse: 1069.2015 - val_mae: 23.9354\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2907.2564 - mse: 2907.2556 - mae: 30.3778 - val_loss: 1066.9164 - val_mse: 1066.9164 - val_mae: 24.1883\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 271us/step - loss: 2965.6538 - mse: 2965.6541 - mae: 30.9866 - val_loss: 1067.9845 - val_mse: 1067.9844 - val_mae: 23.9969\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2846.7397 - mse: 2846.7400 - mae: 30.0933 - val_loss: 1066.5773 - val_mse: 1066.5771 - val_mae: 24.7423\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2877.0272 - mse: 2877.0276 - mae: 30.4237 - val_loss: 1065.3832 - val_mse: 1065.3832 - val_mae: 24.3744\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 337us/step - loss: 2928.3271 - mse: 2928.3271 - mae: 30.6805 - val_loss: 1065.1926 - val_mse: 1065.1926 - val_mae: 24.3945\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 341us/step - loss: 2895.1127 - mse: 2895.1125 - mae: 30.8744 - val_loss: 1065.0568 - val_mse: 1065.0570 - val_mae: 24.5789\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2869.2531 - mse: 2869.2534 - mae: 30.6253 - val_loss: 1065.0282 - val_mse: 1065.0282 - val_mae: 24.1678\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2937.1079 - mse: 2937.1077 - mae: 30.7242 - val_loss: 1063.9036 - val_mse: 1063.9036 - val_mae: 24.5319\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 330us/step - loss: 2875.5455 - mse: 2875.5457 - mae: 30.7834 - val_loss: 1064.9534 - val_mse: 1064.9534 - val_mae: 24.0699\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 330us/step - loss: 2894.9000 - mse: 2894.9001 - mae: 30.5691 - val_loss: 1064.7557 - val_mse: 1064.7557 - val_mae: 24.0769\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 366us/step - loss: 2861.8132 - mse: 2861.8135 - mae: 30.2048 - val_loss: 1063.6205 - val_mse: 1063.6205 - val_mae: 24.6605\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 269us/step - loss: 2870.5669 - mse: 2870.5676 - mae: 30.4590 - val_loss: 1061.6918 - val_mse: 1061.6918 - val_mae: 24.4138\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2942.1756 - mse: 2942.1755 - mae: 30.9885 - val_loss: 1061.5431 - val_mse: 1061.5428 - val_mae: 24.3306\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2896.0819 - mse: 2896.0813 - mae: 30.5450 - val_loss: 1062.7503 - val_mse: 1062.7505 - val_mae: 24.0336\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 342us/step - loss: 2851.3969 - mse: 2851.3975 - mae: 30.5004 - val_loss: 1061.3514 - val_mse: 1061.3514 - val_mae: 24.2362\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2827.8126 - mse: 2827.8118 - mae: 30.3631 - val_loss: 1061.2025 - val_mse: 1061.2025 - val_mae: 24.6462\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 332us/step - loss: 2801.1334 - mse: 2801.1340 - mae: 30.2910 - val_loss: 1060.2971 - val_mse: 1060.2972 - val_mae: 24.5710\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2836.1249 - mse: 2836.1250 - mae: 30.4108 - val_loss: 1060.0061 - val_mse: 1060.0059 - val_mae: 24.4384\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2848.5306 - mse: 2848.5305 - mae: 30.3926 - val_loss: 1059.5543 - val_mse: 1059.5542 - val_mae: 24.2671\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2855.4971 - mse: 2855.4968 - mae: 30.0637 - val_loss: 1058.4871 - val_mse: 1058.4869 - val_mae: 24.4044\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2924.7091 - mse: 2924.7087 - mae: 30.6505 - val_loss: 1057.6609 - val_mse: 1057.6609 - val_mae: 24.3449\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2868.5021 - mse: 2868.5024 - mae: 30.3337 - val_loss: 1058.1435 - val_mse: 1058.1434 - val_mae: 24.0644\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 367us/step - loss: 2919.0122 - mse: 2919.0110 - mae: 31.1694 - val_loss: 1060.9106 - val_mse: 1060.9104 - val_mae: 23.7243\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2849.7838 - mse: 2849.7839 - mae: 30.1116 - val_loss: 1056.4033 - val_mse: 1056.4033 - val_mae: 24.4156\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2851.3067 - mse: 2851.3057 - mae: 30.4014 - val_loss: 1056.7029 - val_mse: 1056.7029 - val_mae: 24.3483\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 319us/step - loss: 2764.4087 - mse: 2764.4094 - mae: 29.7533 - val_loss: 1056.1584 - val_mse: 1056.1583 - val_mae: 24.4380\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2812.2257 - mse: 2812.2263 - mae: 30.4797 - val_loss: 1056.5397 - val_mse: 1056.5398 - val_mae: 24.6759\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2751.7718 - mse: 2751.7727 - mae: 29.9656 - val_loss: 1055.4215 - val_mse: 1055.4215 - val_mae: 24.4442\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2859.7036 - mse: 2859.7048 - mae: 30.3112 - val_loss: 1055.2404 - val_mse: 1055.2405 - val_mae: 24.4068\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 345us/step - loss: 2823.6312 - mse: 2823.6304 - mae: 30.0820 - val_loss: 1054.5496 - val_mse: 1054.5496 - val_mae: 24.6551\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 389us/step - loss: 2863.9293 - mse: 2863.9294 - mae: 30.4339 - val_loss: 1056.4468 - val_mse: 1056.4469 - val_mae: 24.9226\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2801.3011 - mse: 2801.3020 - mae: 30.1068 - val_loss: 1052.2600 - val_mse: 1052.2599 - val_mae: 24.1650\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2792.1280 - mse: 2792.1282 - mae: 30.3507 - val_loss: 1053.3598 - val_mse: 1053.3597 - val_mae: 23.8298\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2874.1214 - mse: 2874.1213 - mae: 30.8500 - val_loss: 1052.5130 - val_mse: 1052.5132 - val_mae: 23.9434\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2796.3413 - mse: 2796.3416 - mae: 30.1657 - val_loss: 1051.1130 - val_mse: 1051.1132 - val_mae: 24.2219\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2875.4495 - mse: 2875.4497 - mae: 30.7147 - val_loss: 1051.0192 - val_mse: 1051.0192 - val_mae: 24.2840\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2805.4224 - mse: 2805.4224 - mae: 30.2984 - val_loss: 1050.4244 - val_mse: 1050.4244 - val_mae: 24.4030\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2831.0758 - mse: 2831.0764 - mae: 30.0534 - val_loss: 1050.0633 - val_mse: 1050.0634 - val_mae: 24.4075\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 334us/step - loss: 2866.6753 - mse: 2866.6746 - mae: 30.8332 - val_loss: 1050.3095 - val_mse: 1050.3094 - val_mae: 24.3161\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2826.9712 - mse: 2826.9714 - mae: 30.5046 - val_loss: 1048.9468 - val_mse: 1048.9468 - val_mae: 24.2345\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2761.2969 - mse: 2761.2971 - mae: 30.2330 - val_loss: 1048.8745 - val_mse: 1048.8745 - val_mae: 24.3946\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - ETA: 0s - loss: 2918.6238 - mse: 2918.6238 - mae: 30.09 - 1s 343us/step - loss: 2876.0371 - mse: 2876.0369 - mae: 30.0427 - val_loss: 1050.3121 - val_mse: 1050.3120 - val_mae: 23.8215\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2805.9857 - mse: 2805.9856 - mae: 30.2551 - val_loss: 1048.8398 - val_mse: 1048.8400 - val_mae: 24.6226\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2525.3195 - mse: 2525.3184 - mae: 29.8036 - val_loss: 1541.0743 - val_mse: 1541.0742 - val_mae: 27.2749\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2505.8808 - mse: 2505.8801 - mae: 29.6531 - val_loss: 1537.6148 - val_mse: 1537.6147 - val_mae: 27.2964\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2521.2877 - mse: 2521.2869 - mae: 29.4186 - val_loss: 1525.2770 - val_mse: 1525.2770 - val_mae: 27.6064\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2526.9266 - mse: 2526.9265 - mae: 29.7901 - val_loss: 1534.3163 - val_mse: 1534.3163 - val_mae: 27.3153\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2543.1855 - mse: 2543.1853 - mae: 29.8793 - val_loss: 1539.1114 - val_mse: 1539.1113 - val_mae: 27.1150\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2528.3170 - mse: 2528.3169 - mae: 29.8038 - val_loss: 1528.6540 - val_mse: 1528.6541 - val_mae: 27.2966\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2565.0601 - mse: 2565.0603 - mae: 30.0128 - val_loss: 1526.3520 - val_mse: 1526.3519 - val_mae: 27.3397\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 342us/step - loss: 2602.4585 - mse: 2602.4585 - mae: 29.9615 - val_loss: 1536.4398 - val_mse: 1536.4398 - val_mae: 27.0636\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2530.0901 - mse: 2530.0894 - mae: 29.6467 - val_loss: 1523.0317 - val_mse: 1523.0319 - val_mae: 27.3864\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2538.3439 - mse: 2538.3445 - mae: 29.9848 - val_loss: 1527.0729 - val_mse: 1527.0729 - val_mae: 27.2868\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 347us/step - loss: 2564.2132 - mse: 2564.2134 - mae: 30.1704 - val_loss: 1527.3799 - val_mse: 1527.3799 - val_mae: 27.2799\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2466.8405 - mse: 2466.8406 - mae: 29.6760 - val_loss: 1524.5759 - val_mse: 1524.5759 - val_mae: 27.3026\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2535.9098 - mse: 2535.9092 - mae: 29.8345 - val_loss: 1518.2466 - val_mse: 1518.2467 - val_mae: 27.4838\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2556.3204 - mse: 2556.3210 - mae: 29.9308 - val_loss: 1513.3456 - val_mse: 1513.3456 - val_mae: 27.5221\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 330us/step - loss: 2581.7860 - mse: 2581.7856 - mae: 30.0935 - val_loss: 1518.4566 - val_mse: 1518.4564 - val_mae: 27.3843\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2470.7953 - mse: 2470.7947 - mae: 29.5930 - val_loss: 1523.5861 - val_mse: 1523.5861 - val_mae: 27.2693\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2518.0092 - mse: 2518.0083 - mae: 29.6945 - val_loss: 1515.1034 - val_mse: 1515.1036 - val_mae: 27.5546\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2553.2747 - mse: 2553.2742 - mae: 29.4719 - val_loss: 1515.4996 - val_mse: 1515.4996 - val_mae: 27.4617\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2484.1524 - mse: 2484.1519 - mae: 29.4078 - val_loss: 1516.8234 - val_mse: 1516.8235 - val_mae: 27.4135\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 347us/step - loss: 2468.0957 - mse: 2468.0955 - mae: 28.9035 - val_loss: 1515.5590 - val_mse: 1515.5591 - val_mae: 27.3716\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2452.1974 - mse: 2452.1975 - mae: 29.3164 - val_loss: 1506.6485 - val_mse: 1506.6484 - val_mae: 27.5391\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 341us/step - loss: 2454.8670 - mse: 2454.8667 - mae: 29.2495 - val_loss: 1499.7993 - val_mse: 1499.7993 - val_mae: 27.8083\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2518.6440 - mse: 2518.6443 - mae: 29.7918 - val_loss: 1505.8863 - val_mse: 1505.8866 - val_mae: 27.4917\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2511.1449 - mse: 2511.1448 - mae: 29.0622 - val_loss: 1501.3212 - val_mse: 1501.3210 - val_mae: 27.6292\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2468.6540 - mse: 2468.6538 - mae: 29.3357 - val_loss: 1506.4129 - val_mse: 1506.4130 - val_mae: 27.4416\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2476.4823 - mse: 2476.4819 - mae: 29.1396 - val_loss: 1503.3341 - val_mse: 1503.3340 - val_mae: 27.5257\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2522.1903 - mse: 2522.1895 - mae: 29.4419 - val_loss: 1502.0410 - val_mse: 1502.0411 - val_mae: 27.5896\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2503.5543 - mse: 2503.5544 - mae: 29.4604 - val_loss: 1503.7391 - val_mse: 1503.7391 - val_mae: 27.4990\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 272us/step - loss: 2543.1382 - mse: 2543.1394 - mae: 29.4567 - val_loss: 1502.4242 - val_mse: 1502.4243 - val_mae: 27.4623\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 275us/step - loss: 2508.9391 - mse: 2508.9382 - mae: 29.1823 - val_loss: 1496.8685 - val_mse: 1496.8685 - val_mae: 27.6832\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2491.5898 - mse: 2491.5896 - mae: 29.3724 - val_loss: 1498.7546 - val_mse: 1498.7548 - val_mae: 27.4838\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 359us/step - loss: 2407.4729 - mse: 2407.4734 - mae: 28.8762 - val_loss: 1494.5199 - val_mse: 1494.5199 - val_mae: 27.5524\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2479.4835 - mse: 2479.4832 - mae: 29.6610 - val_loss: 1494.8657 - val_mse: 1494.8657 - val_mae: 27.4766\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2540.4520 - mse: 2540.4521 - mae: 29.3359 - val_loss: 1503.9545 - val_mse: 1503.9547 - val_mae: 27.2018\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2437.2933 - mse: 2437.2927 - mae: 28.9083 - val_loss: 1499.2682 - val_mse: 1499.2681 - val_mae: 27.3342\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2537.0662 - mse: 2537.0662 - mae: 29.3212 - val_loss: 1495.0904 - val_mse: 1495.0906 - val_mae: 27.5127\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2474.9864 - mse: 2474.9871 - mae: 29.5541 - val_loss: 1503.5655 - val_mse: 1503.5656 - val_mae: 27.1868\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2517.0028 - mse: 2517.0027 - mae: 29.1292 - val_loss: 1494.5129 - val_mse: 1494.5128 - val_mae: 27.3817\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2488.2227 - mse: 2488.2224 - mae: 29.3149 - val_loss: 1494.7260 - val_mse: 1494.7258 - val_mae: 27.4415\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 269us/step - loss: 2517.8074 - mse: 2517.8079 - mae: 29.3906 - val_loss: 1504.9784 - val_mse: 1504.9784 - val_mae: 27.1959\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2573.7731 - mse: 2573.7732 - mae: 29.8840 - val_loss: 1492.9962 - val_mse: 1492.9962 - val_mae: 27.5040\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2484.0097 - mse: 2484.0090 - mae: 29.1544 - val_loss: 1495.1394 - val_mse: 1495.1393 - val_mae: 27.4488\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2499.1045 - mse: 2499.1047 - mae: 29.4634 - val_loss: 1502.0460 - val_mse: 1502.0460 - val_mae: 27.2032\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2456.9797 - mse: 2456.9795 - mae: 28.9018 - val_loss: 1485.4059 - val_mse: 1485.4060 - val_mae: 27.7915\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2471.9788 - mse: 2471.9790 - mae: 28.9063 - val_loss: 1483.7058 - val_mse: 1483.7058 - val_mae: 27.9010\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2461.4511 - mse: 2461.4507 - mae: 28.9185 - val_loss: 1481.5259 - val_mse: 1481.5258 - val_mae: 27.8904\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2527.0713 - mse: 2527.0718 - mae: 29.1683 - val_loss: 1479.7059 - val_mse: 1479.7061 - val_mae: 27.9676\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 324us/step - loss: 2575.4027 - mse: 2575.4023 - mae: 29.7563 - val_loss: 1489.9490 - val_mse: 1489.9491 - val_mae: 27.4720\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2468.2908 - mse: 2468.2908 - mae: 29.2659 - val_loss: 1490.8859 - val_mse: 1490.8861 - val_mae: 27.3780\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 363us/step - loss: 2451.6091 - mse: 2451.6091 - mae: 28.8627 - val_loss: 1485.8648 - val_mse: 1485.8647 - val_mae: 27.5560\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2482.1501 - mse: 2482.1501 - mae: 29.3651 - val_loss: 1485.5516 - val_mse: 1485.5515 - val_mae: 27.5744\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2510.9779 - mse: 2510.9775 - mae: 29.1328 - val_loss: 1486.0620 - val_mse: 1486.0618 - val_mae: 27.5846\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2507.0578 - mse: 2507.0583 - mae: 28.9619 - val_loss: 1484.1738 - val_mse: 1484.1737 - val_mae: 27.6283\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2527.4657 - mse: 2527.4656 - mae: 29.4618 - val_loss: 1487.5477 - val_mse: 1487.5476 - val_mae: 27.4585\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2452.5766 - mse: 2452.5762 - mae: 28.9706 - val_loss: 1484.8220 - val_mse: 1484.8221 - val_mae: 27.5992\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2459.9684 - mse: 2459.9685 - mae: 28.8677 - val_loss: 1490.8195 - val_mse: 1490.8193 - val_mae: 27.3731\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 244us/step - loss: 2497.9629 - mse: 2497.9631 - mae: 28.8836 - val_loss: 1482.2492 - val_mse: 1482.2493 - val_mae: 27.6591\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2463.6341 - mse: 2463.6338 - mae: 29.2579 - val_loss: 1482.8152 - val_mse: 1482.8153 - val_mae: 27.5495\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2497.1591 - mse: 2497.1597 - mae: 29.1850 - val_loss: 1475.6910 - val_mse: 1475.6910 - val_mae: 28.0524\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2377.4172 - mse: 2377.4177 - mae: 28.8779 - val_loss: 1482.5772 - val_mse: 1482.5771 - val_mae: 27.6034\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2465.9010 - mse: 2465.9004 - mae: 29.5315 - val_loss: 1481.6756 - val_mse: 1481.6758 - val_mae: 27.5581\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2449.7903 - mse: 2449.7900 - mae: 28.9481 - val_loss: 1481.7897 - val_mse: 1481.7898 - val_mae: 27.5028\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 267us/step - loss: 2435.6997 - mse: 2435.7002 - mae: 28.8240 - val_loss: 1477.3031 - val_mse: 1477.3031 - val_mae: 27.7324\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2485.3041 - mse: 2485.3044 - mae: 29.4910 - val_loss: 1480.6375 - val_mse: 1480.6373 - val_mae: 27.5175\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2454.9920 - mse: 2454.9915 - mae: 28.6856 - val_loss: 1473.3009 - val_mse: 1473.3010 - val_mae: 27.8412\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2501.2533 - mse: 2501.2539 - mae: 29.1313 - val_loss: 1475.1906 - val_mse: 1475.1908 - val_mae: 27.7878\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2462.3699 - mse: 2462.3701 - mae: 29.0697 - val_loss: 1477.9432 - val_mse: 1477.9431 - val_mae: 27.5948\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2361.4087 - mse: 2361.4084 - mae: 28.6014 - val_loss: 1476.8024 - val_mse: 1476.8024 - val_mae: 27.6143\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2459.2966 - mse: 2459.2964 - mae: 29.3187 - val_loss: 1479.0784 - val_mse: 1479.0785 - val_mae: 27.4667\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 359us/step - loss: 2472.1096 - mse: 2472.1096 - mae: 28.8805 - val_loss: 1476.4058 - val_mse: 1476.4056 - val_mae: 27.6662\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2503.7464 - mse: 2503.7449 - mae: 29.2533 - val_loss: 1473.1990 - val_mse: 1473.1990 - val_mae: 27.8161\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2529.4826 - mse: 2529.4827 - mae: 29.0608 - val_loss: 1475.1814 - val_mse: 1475.1815 - val_mae: 27.8493\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2504.4976 - mse: 2504.4971 - mae: 29.4286 - val_loss: 1478.7056 - val_mse: 1478.7057 - val_mae: 27.6676\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2421.9599 - mse: 2421.9600 - mae: 28.7905 - val_loss: 1477.2009 - val_mse: 1477.2009 - val_mae: 27.5808\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2499.4389 - mse: 2499.4385 - mae: 29.0007 - val_loss: 1467.8430 - val_mse: 1467.8428 - val_mae: 28.0195\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2501.5625 - mse: 2501.5627 - mae: 28.9909 - val_loss: 1474.5950 - val_mse: 1474.5948 - val_mae: 27.6715\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2481.5489 - mse: 2481.5496 - mae: 28.8191 - val_loss: 1476.7878 - val_mse: 1476.7880 - val_mae: 27.5742\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2446.1427 - mse: 2446.1431 - mae: 28.9152 - val_loss: 1482.2003 - val_mse: 1482.2004 - val_mae: 27.3945\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 269us/step - loss: 2461.5755 - mse: 2461.5752 - mae: 29.0108 - val_loss: 1471.6545 - val_mse: 1471.6543 - val_mae: 27.9127\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2442.6933 - mse: 2442.6936 - mae: 28.7827 - val_loss: 1477.3301 - val_mse: 1477.3301 - val_mae: 27.6340\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2360.7176 - mse: 2360.7180 - mae: 29.3576 - val_loss: 3689.0751 - val_mse: 3689.0750 - val_mae: 24.1804\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2391.4192 - mse: 2391.4189 - mae: 29.6504 - val_loss: 3690.6083 - val_mse: 3690.6077 - val_mae: 24.3820\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2399.4742 - mse: 2399.4739 - mae: 29.6949 - val_loss: 3694.0921 - val_mse: 3694.0920 - val_mae: 24.8066\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 339us/step - loss: 2425.3301 - mse: 2425.3303 - mae: 29.4967 - val_loss: 3688.1870 - val_mse: 3688.1877 - val_mae: 24.0746\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2329.4534 - mse: 2329.4529 - mae: 29.3747 - val_loss: 3692.4242 - val_mse: 3692.4241 - val_mae: 24.7015\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2350.6415 - mse: 2350.6426 - mae: 29.4597 - val_loss: 3689.2900 - val_mse: 3689.2896 - val_mae: 24.1734\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2364.4883 - mse: 2364.4880 - mae: 29.2014 - val_loss: 3690.0861 - val_mse: 3690.0857 - val_mae: 24.2710\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2283.2798 - mse: 2283.2800 - mae: 28.8951 - val_loss: 3690.3496 - val_mse: 3690.3501 - val_mae: 24.3533\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2340.9780 - mse: 2340.9783 - mae: 29.2477 - val_loss: 3689.7617 - val_mse: 3689.7610 - val_mae: 24.1140\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2357.8269 - mse: 2357.8279 - mae: 29.4663 - val_loss: 3692.1939 - val_mse: 3692.1941 - val_mae: 24.4041\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2359.2444 - mse: 2359.2449 - mae: 29.1025 - val_loss: 3691.6422 - val_mse: 3691.6416 - val_mae: 24.3013\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2291.7308 - mse: 2291.7312 - mae: 28.9687 - val_loss: 3691.1979 - val_mse: 3691.1968 - val_mae: 24.3091\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 329us/step - loss: 2372.8741 - mse: 2372.8738 - mae: 29.5775 - val_loss: 3690.0183 - val_mse: 3690.0188 - val_mae: 24.1734\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 257us/step - loss: 2360.6435 - mse: 2360.6438 - mae: 29.1452 - val_loss: 3692.8710 - val_mse: 3692.8708 - val_mae: 24.5570\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2367.7061 - mse: 2367.7056 - mae: 29.5000 - val_loss: 3692.1394 - val_mse: 3692.1399 - val_mae: 24.4271\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2337.1122 - mse: 2337.1125 - mae: 28.8548 - val_loss: 3694.4148 - val_mse: 3694.4150 - val_mae: 24.6278\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2364.7064 - mse: 2364.7065 - mae: 29.3865 - val_loss: 3692.2037 - val_mse: 3692.2041 - val_mae: 24.2855\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2359.4356 - mse: 2359.4358 - mae: 29.2481 - val_loss: 3690.4723 - val_mse: 3690.4714 - val_mae: 24.0455\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2322.8820 - mse: 2322.8818 - mae: 29.3738 - val_loss: 3692.0507 - val_mse: 3692.0505 - val_mae: 24.2623\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2353.0223 - mse: 2353.0220 - mae: 29.2642 - val_loss: 3692.9728 - val_mse: 3692.9731 - val_mae: 24.4877\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2319.0126 - mse: 2319.0129 - mae: 29.1605 - val_loss: 3689.8528 - val_mse: 3689.8528 - val_mae: 23.9436\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 341us/step - loss: 2376.3720 - mse: 2376.3726 - mae: 29.4562 - val_loss: 3690.1593 - val_mse: 3690.1597 - val_mae: 24.0406\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2321.9799 - mse: 2321.9790 - mae: 29.1489 - val_loss: 3690.6163 - val_mse: 3690.6162 - val_mae: 23.9723\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2319.7032 - mse: 2319.7029 - mae: 28.8917 - val_loss: 3696.5605 - val_mse: 3696.5598 - val_mae: 24.8178\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2349.7184 - mse: 2349.7183 - mae: 29.2739 - val_loss: 3695.1704 - val_mse: 3695.1704 - val_mae: 24.8471\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 376us/step - loss: 2358.5973 - mse: 2358.5972 - mae: 29.2883 - val_loss: 3688.6306 - val_mse: 3688.6309 - val_mae: 23.8970\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2385.0408 - mse: 2385.0408 - mae: 29.2666 - val_loss: 3689.2918 - val_mse: 3689.2917 - val_mae: 23.9788\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2386.7274 - mse: 2386.7273 - mae: 29.2909 - val_loss: 3690.3899 - val_mse: 3690.3892 - val_mae: 24.0854\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2348.1571 - mse: 2348.1572 - mae: 28.9846 - val_loss: 3689.9218 - val_mse: 3689.9221 - val_mae: 24.1031\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2374.9458 - mse: 2374.9456 - mae: 29.2998 - val_loss: 3689.4813 - val_mse: 3689.4817 - val_mae: 24.0864\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2350.4953 - mse: 2350.4946 - mae: 29.2414 - val_loss: 3697.1769 - val_mse: 3697.1775 - val_mae: 25.0125\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 345us/step - loss: 2356.2120 - mse: 2356.2119 - mae: 28.9650 - val_loss: 3692.1438 - val_mse: 3692.1428 - val_mae: 24.4237\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2399.3021 - mse: 2399.3025 - mae: 29.3878 - val_loss: 3689.3801 - val_mse: 3689.3801 - val_mae: 23.9146\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2362.9040 - mse: 2362.9045 - mae: 29.2253 - val_loss: 3689.9975 - val_mse: 3689.9973 - val_mae: 24.1599\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 337us/step - loss: 2281.7207 - mse: 2281.7212 - mae: 28.7452 - val_loss: 3693.2600 - val_mse: 3693.2598 - val_mae: 24.5383\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2352.5763 - mse: 2352.5769 - mae: 29.2766 - val_loss: 3691.4725 - val_mse: 3691.4729 - val_mae: 24.2429\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2296.9479 - mse: 2296.9480 - mae: 29.0100 - val_loss: 3689.7733 - val_mse: 3689.7737 - val_mae: 23.9770\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 355us/step - loss: 2348.5066 - mse: 2348.5068 - mae: 29.1008 - val_loss: 3692.3914 - val_mse: 3692.3918 - val_mae: 24.5345\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2369.3947 - mse: 2369.3950 - mae: 29.2693 - val_loss: 3694.2274 - val_mse: 3694.2273 - val_mae: 24.6649\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 373us/step - loss: 2370.4012 - mse: 2370.4019 - mae: 29.0910 - val_loss: 3691.6827 - val_mse: 3691.6824 - val_mae: 24.2766\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 346us/step - loss: 2345.2559 - mse: 2345.2556 - mae: 29.1339 - val_loss: 3692.8061 - val_mse: 3692.8066 - val_mae: 24.4313\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2345.4668 - mse: 2345.4678 - mae: 29.1959 - val_loss: 3691.4336 - val_mse: 3691.4338 - val_mae: 24.4021\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 338us/step - loss: 2366.0149 - mse: 2366.0146 - mae: 29.1893 - val_loss: 3690.2069 - val_mse: 3690.2058 - val_mae: 24.2602\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 325us/step - loss: 2343.2303 - mse: 2343.2295 - mae: 29.1556 - val_loss: 3693.1702 - val_mse: 3693.1704 - val_mae: 24.6329\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2326.8880 - mse: 2326.8872 - mae: 29.1130 - val_loss: 3692.6608 - val_mse: 3692.6604 - val_mae: 24.6926\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 346us/step - loss: 2377.0728 - mse: 2377.0732 - mae: 29.4152 - val_loss: 3691.0009 - val_mse: 3691.0017 - val_mae: 24.1935\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2363.1575 - mse: 2363.1572 - mae: 29.3840 - val_loss: 3689.5263 - val_mse: 3689.5261 - val_mae: 24.0228\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2335.3222 - mse: 2335.3220 - mae: 29.1974 - val_loss: 3691.1056 - val_mse: 3691.1057 - val_mae: 24.4654\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2298.0905 - mse: 2298.0901 - mae: 29.0145 - val_loss: 3691.5504 - val_mse: 3691.5508 - val_mae: 24.2648\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2296.9994 - mse: 2296.9995 - mae: 28.6951 - val_loss: 3692.7004 - val_mse: 3692.6997 - val_mae: 24.3721\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 330us/step - loss: 2315.7603 - mse: 2315.7605 - mae: 28.7570 - val_loss: 3696.0830 - val_mse: 3696.0830 - val_mae: 24.7150\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2381.0054 - mse: 2381.0051 - mae: 29.4388 - val_loss: 3695.1568 - val_mse: 3695.1570 - val_mae: 24.8150\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2334.9264 - mse: 2334.9263 - mae: 29.2606 - val_loss: 3688.8660 - val_mse: 3688.8655 - val_mae: 24.1885\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2355.8911 - mse: 2355.8906 - mae: 29.1280 - val_loss: 3691.9489 - val_mse: 3691.9495 - val_mae: 24.8007\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2245.3521 - mse: 2245.3523 - mae: 28.7655 - val_loss: 3694.1400 - val_mse: 3694.1396 - val_mae: 24.8349\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2319.9721 - mse: 2319.9719 - mae: 28.8898 - val_loss: 3691.8371 - val_mse: 3691.8381 - val_mae: 24.5670\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2340.4605 - mse: 2340.4607 - mae: 29.1235 - val_loss: 3692.4672 - val_mse: 3692.4675 - val_mae: 24.6819\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2334.5482 - mse: 2334.5479 - mae: 28.9768 - val_loss: 3697.7587 - val_mse: 3697.7583 - val_mae: 25.1874\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2281.1713 - mse: 2281.1711 - mae: 28.9590 - val_loss: 3692.6016 - val_mse: 3692.6018 - val_mae: 24.7064\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 334us/step - loss: 2324.1406 - mse: 2324.1406 - mae: 29.0877 - val_loss: 3693.0241 - val_mse: 3693.0242 - val_mae: 24.7346\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2353.9737 - mse: 2353.9731 - mae: 29.3444 - val_loss: 3688.5703 - val_mse: 3688.5703 - val_mae: 24.2722\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2307.6132 - mse: 2307.6130 - mae: 28.7113 - val_loss: 3688.0209 - val_mse: 3688.0215 - val_mae: 24.2836\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2308.6728 - mse: 2308.6731 - mae: 29.0508 - val_loss: 3691.3947 - val_mse: 3691.3945 - val_mae: 24.6632\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 256us/step - loss: 2327.2923 - mse: 2327.2922 - mae: 28.6571 - val_loss: 3697.8964 - val_mse: 3697.8965 - val_mae: 25.3996\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2417.4801 - mse: 2417.4802 - mae: 29.8615 - val_loss: 3690.6831 - val_mse: 3690.6833 - val_mae: 24.8571\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2303.8266 - mse: 2303.8257 - mae: 29.1255 - val_loss: 3686.6726 - val_mse: 3686.6726 - val_mae: 24.1793\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2332.7188 - mse: 2332.7190 - mae: 29.0099 - val_loss: 3692.6646 - val_mse: 3692.6636 - val_mae: 24.8144\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 353us/step - loss: 2299.9684 - mse: 2299.9673 - mae: 28.8911 - val_loss: 3691.3421 - val_mse: 3691.3420 - val_mae: 24.5302\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2328.7064 - mse: 2328.7065 - mae: 29.0670 - val_loss: 3692.3263 - val_mse: 3692.3264 - val_mae: 24.6904\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2258.9852 - mse: 2258.9856 - mae: 28.9662 - val_loss: 3693.9849 - val_mse: 3693.9849 - val_mae: 24.8803\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2335.8226 - mse: 2335.8232 - mae: 29.1146 - val_loss: 3692.2742 - val_mse: 3692.2747 - val_mae: 24.6691\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2321.8205 - mse: 2321.8203 - mae: 28.7194 - val_loss: 3692.7345 - val_mse: 3692.7358 - val_mae: 24.8496\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2312.0276 - mse: 2312.0281 - mae: 28.8611 - val_loss: 3691.8852 - val_mse: 3691.8860 - val_mae: 24.6867\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2292.5941 - mse: 2292.5940 - mae: 28.8666 - val_loss: 3688.1828 - val_mse: 3688.1831 - val_mae: 24.3409\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 325us/step - loss: 2307.1736 - mse: 2307.1736 - mae: 29.0019 - val_loss: 3690.4574 - val_mse: 3690.4565 - val_mae: 24.5522\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2272.3371 - mse: 2272.3364 - mae: 28.8700 - val_loss: 3688.8530 - val_mse: 3688.8535 - val_mae: 24.2603\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2377.3614 - mse: 2377.3608 - mae: 29.1149 - val_loss: 3692.2503 - val_mse: 3692.2507 - val_mae: 24.7205\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2267.3431 - mse: 2267.3433 - mae: 28.8003 - val_loss: 3695.9476 - val_mse: 3695.9478 - val_mae: 24.9946\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2294.8860 - mse: 2294.8860 - mae: 29.0619 - val_loss: 3695.0918 - val_mse: 3695.0920 - val_mae: 24.8632\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 327us/step - loss: 2341.8277 - mse: 2341.8271 - mae: 29.3212 - val_loss: 3688.8078 - val_mse: 3688.8079 - val_mae: 24.1346\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2657.7902 - mse: 2657.7903 - mae: 28.3924 - val_loss: 2193.2994 - val_mse: 2193.2996 - val_mae: 27.4885\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2712.3070 - mse: 2712.3074 - mae: 28.6733 - val_loss: 2212.2203 - val_mse: 2212.2205 - val_mae: 26.5555\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2710.8232 - mse: 2710.8232 - mae: 28.5460 - val_loss: 2204.3308 - val_mse: 2204.3311 - val_mae: 27.0087\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 353us/step - loss: 2742.5934 - mse: 2742.5935 - mae: 28.3253 - val_loss: 2202.6916 - val_mse: 2202.6914 - val_mae: 27.6108\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2650.0138 - mse: 2650.0137 - mae: 28.1943 - val_loss: 2211.2990 - val_mse: 2211.2991 - val_mae: 26.9419\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2719.0159 - mse: 2719.0151 - mae: 28.5218 - val_loss: 2213.8093 - val_mse: 2213.8091 - val_mae: 27.3426\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2720.9740 - mse: 2720.9741 - mae: 28.8033 - val_loss: 2232.1329 - val_mse: 2232.1331 - val_mae: 26.8736\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2701.7227 - mse: 2701.7214 - mae: 28.4011 - val_loss: 2230.7268 - val_mse: 2230.7268 - val_mae: 26.9197\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 274us/step - loss: 2672.5092 - mse: 2672.5090 - mae: 28.0500 - val_loss: 2219.0618 - val_mse: 2219.0615 - val_mae: 27.3615\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2724.5545 - mse: 2724.5535 - mae: 28.4256 - val_loss: 2219.0567 - val_mse: 2219.0569 - val_mae: 27.1128\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2743.8609 - mse: 2743.8606 - mae: 28.7725 - val_loss: 2222.1716 - val_mse: 2222.1716 - val_mae: 26.9045\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2712.4937 - mse: 2712.4941 - mae: 28.4011 - val_loss: 2229.8161 - val_mse: 2229.8154 - val_mae: 26.8590\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2713.4090 - mse: 2713.4092 - mae: 28.4866 - val_loss: 2221.8873 - val_mse: 2221.8877 - val_mae: 27.4773\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2740.1849 - mse: 2740.1851 - mae: 28.5157 - val_loss: 2234.8160 - val_mse: 2234.8162 - val_mae: 27.0939\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2709.8163 - mse: 2709.8164 - mae: 28.4737 - val_loss: 2247.7257 - val_mse: 2247.7258 - val_mae: 26.9518\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2731.7521 - mse: 2731.7517 - mae: 28.5084 - val_loss: 2235.5667 - val_mse: 2235.5664 - val_mae: 27.2731\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2765.4053 - mse: 2765.4048 - mae: 28.5808 - val_loss: 2247.6669 - val_mse: 2247.6667 - val_mae: 27.1485\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2686.0511 - mse: 2686.0510 - mae: 28.8582 - val_loss: 2235.7314 - val_mse: 2235.7317 - val_mae: 27.2965\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2710.8965 - mse: 2710.8962 - mae: 28.5167 - val_loss: 2234.0936 - val_mse: 2234.0938 - val_mae: 27.3019\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2733.2157 - mse: 2733.2170 - mae: 28.6260 - val_loss: 2231.9637 - val_mse: 2231.9636 - val_mae: 27.1345\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2723.1469 - mse: 2723.1467 - mae: 28.5915 - val_loss: 2236.1365 - val_mse: 2236.1367 - val_mae: 27.1992\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2679.7095 - mse: 2679.7087 - mae: 28.2210 - val_loss: 2230.2862 - val_mse: 2230.2864 - val_mae: 27.1689\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2661.3685 - mse: 2661.3677 - mae: 28.3539 - val_loss: 2222.9316 - val_mse: 2222.9314 - val_mae: 27.3041\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2744.3657 - mse: 2744.3667 - mae: 28.6656 - val_loss: 2233.8440 - val_mse: 2233.8440 - val_mae: 27.3958\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 340us/step - loss: 2702.8462 - mse: 2702.8467 - mae: 28.4477 - val_loss: 2240.5384 - val_mse: 2240.5383 - val_mae: 26.9840\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 350us/step - loss: 2667.5735 - mse: 2667.5732 - mae: 28.0769 - val_loss: 2227.3713 - val_mse: 2227.3708 - val_mae: 27.4661\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2721.9965 - mse: 2721.9966 - mae: 28.6067 - val_loss: 2233.1816 - val_mse: 2233.1812 - val_mae: 27.5367\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2657.4095 - mse: 2657.4087 - mae: 28.2945 - val_loss: 2236.0043 - val_mse: 2236.0042 - val_mae: 27.4183\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2699.0186 - mse: 2699.0190 - mae: 28.4982 - val_loss: 2232.3043 - val_mse: 2232.3042 - val_mae: 27.0861\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2691.6701 - mse: 2691.6697 - mae: 28.4657 - val_loss: 2229.3647 - val_mse: 2229.3647 - val_mae: 27.1863\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2709.0251 - mse: 2709.0251 - mae: 28.2850 - val_loss: 2232.2222 - val_mse: 2232.2222 - val_mae: 26.9339\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2751.7754 - mse: 2751.7759 - mae: 28.5407 - val_loss: 2252.9996 - val_mse: 2252.9998 - val_mae: 26.5553\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2603.9132 - mse: 2603.9131 - mae: 28.1060 - val_loss: 2231.0968 - val_mse: 2231.0972 - val_mae: 27.2939\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2648.4835 - mse: 2648.4827 - mae: 28.2858 - val_loss: 2228.1683 - val_mse: 2228.1682 - val_mae: 27.1384\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2733.4583 - mse: 2733.4595 - mae: 28.1932 - val_loss: 2221.6840 - val_mse: 2221.6838 - val_mae: 27.0381\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2675.8999 - mse: 2675.9006 - mae: 28.2576 - val_loss: 2213.3366 - val_mse: 2213.3364 - val_mae: 27.3342\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2669.8318 - mse: 2669.8315 - mae: 28.2932 - val_loss: 2223.5185 - val_mse: 2223.5186 - val_mae: 27.3827\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2688.2283 - mse: 2688.2280 - mae: 28.2446 - val_loss: 2231.2781 - val_mse: 2231.2781 - val_mae: 27.3421\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2658.8737 - mse: 2658.8735 - mae: 28.2603 - val_loss: 2229.8513 - val_mse: 2229.8513 - val_mae: 27.2981\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2666.7307 - mse: 2666.7310 - mae: 28.0791 - val_loss: 2231.1708 - val_mse: 2231.1704 - val_mae: 27.4987\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2737.5729 - mse: 2737.5735 - mae: 28.4535 - val_loss: 2245.6945 - val_mse: 2245.6946 - val_mae: 27.1320\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2688.2320 - mse: 2688.2319 - mae: 28.0045 - val_loss: 2245.9225 - val_mse: 2245.9229 - val_mae: 27.1064\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 344us/step - loss: 2716.8360 - mse: 2716.8367 - mae: 28.8271 - val_loss: 2242.0815 - val_mse: 2242.0813 - val_mae: 27.4057\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2698.4095 - mse: 2698.4092 - mae: 28.2258 - val_loss: 2239.0547 - val_mse: 2239.0547 - val_mae: 27.4383\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2706.9327 - mse: 2706.9326 - mae: 28.4963 - val_loss: 2235.3216 - val_mse: 2235.3213 - val_mae: 27.2959\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 330us/step - loss: 2699.6469 - mse: 2699.6460 - mae: 28.5202 - val_loss: 2225.9815 - val_mse: 2225.9814 - val_mae: 27.3438\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2740.3602 - mse: 2740.3604 - mae: 28.5232 - val_loss: 2243.8583 - val_mse: 2243.8582 - val_mae: 26.9373\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2657.3862 - mse: 2657.3860 - mae: 28.2825 - val_loss: 2237.3532 - val_mse: 2237.3535 - val_mae: 27.0303\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2689.0170 - mse: 2689.0173 - mae: 28.2752 - val_loss: 2243.4612 - val_mse: 2243.4612 - val_mae: 27.0234\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2702.1245 - mse: 2702.1248 - mae: 28.3793 - val_loss: 2241.5360 - val_mse: 2241.5359 - val_mae: 27.1756\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2697.9928 - mse: 2697.9922 - mae: 28.1116 - val_loss: 2251.8848 - val_mse: 2251.8850 - val_mae: 27.0159\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2656.3046 - mse: 2656.3044 - mae: 28.1806 - val_loss: 2235.1260 - val_mse: 2235.1260 - val_mae: 27.3914\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2656.6501 - mse: 2656.6509 - mae: 28.1133 - val_loss: 2239.9890 - val_mse: 2239.9888 - val_mae: 26.9459\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2691.7508 - mse: 2691.7507 - mae: 28.0564 - val_loss: 2248.4617 - val_mse: 2248.4619 - val_mae: 26.9783\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2704.5903 - mse: 2704.5906 - mae: 28.4629 - val_loss: 2231.6048 - val_mse: 2231.6050 - val_mae: 27.4016\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2724.8770 - mse: 2724.8770 - mae: 28.7473 - val_loss: 2230.9409 - val_mse: 2230.9407 - val_mae: 27.3289\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2611.5105 - mse: 2611.5110 - mae: 28.1043 - val_loss: 2229.8667 - val_mse: 2229.8669 - val_mae: 27.5042\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2645.9847 - mse: 2645.9854 - mae: 28.2692 - val_loss: 2232.0812 - val_mse: 2232.0808 - val_mae: 27.8337\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2651.3404 - mse: 2651.3406 - mae: 28.1834 - val_loss: 2238.7070 - val_mse: 2238.7065 - val_mae: 27.5940\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2675.4214 - mse: 2675.4207 - mae: 28.0950 - val_loss: 2243.2515 - val_mse: 2243.2515 - val_mae: 27.8865\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2726.6238 - mse: 2726.6238 - mae: 28.3690 - val_loss: 2250.0294 - val_mse: 2250.0295 - val_mae: 27.2605\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2661.8171 - mse: 2661.8169 - mae: 27.8809 - val_loss: 2243.4446 - val_mse: 2243.4451 - val_mae: 27.3907\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2668.7632 - mse: 2668.7637 - mae: 28.0130 - val_loss: 2243.2800 - val_mse: 2243.2803 - val_mae: 27.3875\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 336us/step - loss: 2656.7419 - mse: 2656.7424 - mae: 28.5877 - val_loss: 2243.7698 - val_mse: 2243.7695 - val_mae: 27.1991\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 363us/step - loss: 2671.9351 - mse: 2671.9348 - mae: 28.3330 - val_loss: 2236.6471 - val_mse: 2236.6470 - val_mae: 27.2744\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2655.0938 - mse: 2655.0942 - mae: 27.9984 - val_loss: 2223.5861 - val_mse: 2223.5862 - val_mae: 27.6248\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 266us/step - loss: 2663.8950 - mse: 2663.8955 - mae: 27.9777 - val_loss: 2236.9839 - val_mse: 2236.9841 - val_mae: 27.3308\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2661.9901 - mse: 2661.9897 - mae: 27.9448 - val_loss: 2236.6414 - val_mse: 2236.6411 - val_mae: 27.3479\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2711.2510 - mse: 2711.2502 - mae: 28.3997 - val_loss: 2229.5989 - val_mse: 2229.5994 - val_mae: 27.6840\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2673.9951 - mse: 2673.9954 - mae: 27.7087 - val_loss: 2229.9499 - val_mse: 2229.9502 - val_mae: 27.4167\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2657.2902 - mse: 2657.2903 - mae: 27.9279 - val_loss: 2240.4544 - val_mse: 2240.4543 - val_mae: 27.2341\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2710.2939 - mse: 2710.2942 - mae: 28.3641 - val_loss: 2243.7754 - val_mse: 2243.7759 - val_mae: 27.1112\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2688.1629 - mse: 2688.1626 - mae: 28.0311 - val_loss: 2237.2886 - val_mse: 2237.2883 - val_mae: 27.5316\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2690.0709 - mse: 2690.0718 - mae: 28.2810 - val_loss: 2243.9318 - val_mse: 2243.9319 - val_mae: 27.4548\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2656.1440 - mse: 2656.1438 - mae: 28.0224 - val_loss: 2231.8884 - val_mse: 2231.8882 - val_mae: 27.7642\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2654.6356 - mse: 2654.6345 - mae: 28.1408 - val_loss: 2244.5416 - val_mse: 2244.5417 - val_mae: 26.9675\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2647.3807 - mse: 2647.3809 - mae: 27.9758 - val_loss: 2239.7584 - val_mse: 2239.7583 - val_mae: 27.1489\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2684.6482 - mse: 2684.6484 - mae: 28.2691 - val_loss: 2239.1498 - val_mse: 2239.1501 - val_mae: 27.2112\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2688.0366 - mse: 2688.0361 - mae: 28.2590 - val_loss: 2228.5201 - val_mse: 2228.5200 - val_mae: 27.6042\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2666.6641 - mse: 2666.6648 - mae: 27.7975 - val_loss: 2224.8393 - val_mse: 2224.8391 - val_mae: 27.3825\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13314.2910 - mse: 13314.2910 - mae: 109.8367 - val_loss: 34576.4277 - val_mse: 34576.4297 - val_mae: 132.5712\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 13127.4170 - mse: 13127.4170 - mae: 108.9863 - val_loss: 34212.0701 - val_mse: 34212.0703 - val_mae: 131.1946\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 12622.1770 - mse: 12622.1768 - mae: 106.6757 - val_loss: 33218.5240 - val_mse: 33218.5234 - val_mae: 127.3669\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 11357.9131 - mse: 11357.9121 - mae: 100.5199 - val_loss: 30764.0375 - val_mse: 30764.0371 - val_mae: 117.3797\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 232us/step - loss: 8746.0764 - mse: 8746.0771 - mae: 86.2139 - val_loss: 25526.0920 - val_mse: 25526.0898 - val_mae: 92.5269\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 284us/step - loss: 4577.4735 - mse: 4577.4731 - mae: 55.6905 - val_loss: 18895.9409 - val_mse: 18895.9395 - val_mae: 44.3794\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2684.8842 - mse: 2684.8843 - mae: 38.1908 - val_loss: 17385.3109 - val_mse: 17385.3105 - val_mae: 36.5825\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2589.1627 - mse: 2589.1628 - mae: 36.7329 - val_loss: 17808.7818 - val_mse: 17808.7793 - val_mae: 36.4433\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 342us/step - loss: 2820.5611 - mse: 2820.5610 - mae: 38.2776 - val_loss: 17621.2540 - val_mse: 17621.2539 - val_mae: 36.3062\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 257us/step - loss: 2751.0406 - mse: 2751.0408 - mae: 37.8583 - val_loss: 17737.7559 - val_mse: 17737.7559 - val_mae: 36.2884\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 259us/step - loss: 2707.5031 - mse: 2707.5029 - mae: 36.7258 - val_loss: 17788.7758 - val_mse: 17788.7754 - val_mae: 36.2942\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 256us/step - loss: 2679.2130 - mse: 2679.2131 - mae: 37.3028 - val_loss: 17819.9814 - val_mse: 17819.9824 - val_mae: 36.2915\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2570.2116 - mse: 2570.2114 - mae: 35.7255 - val_loss: 17780.2497 - val_mse: 17780.2480 - val_mae: 36.2051\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2718.1278 - mse: 2718.1277 - mae: 37.4247 - val_loss: 17858.6037 - val_mse: 17858.6016 - val_mae: 36.2791\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 333us/step - loss: 2806.3148 - mse: 2806.3147 - mae: 37.9728 - val_loss: 17920.6927 - val_mse: 17920.6934 - val_mae: 36.3749\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 2638.0407 - mse: 2638.0405 - mae: 37.8735 - val_loss: 17632.2184 - val_mse: 17632.2188 - val_mae: 36.0006\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 272us/step - loss: 2691.8372 - mse: 2691.8372 - mae: 36.7559 - val_loss: 17718.4078 - val_mse: 17718.4082 - val_mae: 36.0246\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2753.6550 - mse: 2753.6550 - mae: 35.9350 - val_loss: 17806.5468 - val_mse: 17806.5488 - val_mae: 36.1149\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 351us/step - loss: 2667.5985 - mse: 2667.5984 - mae: 37.3378 - val_loss: 17938.4089 - val_mse: 17938.4082 - val_mae: 36.3640\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2304.6125 - mse: 2304.6123 - mae: 34.4907 - val_loss: 17658.2190 - val_mse: 17658.2207 - val_mae: 35.9310\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2295.8179 - mse: 2295.8179 - mae: 34.4166 - val_loss: 17644.0951 - val_mse: 17644.0938 - val_mae: 35.8953\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2558.9879 - mse: 2558.9878 - mae: 35.4509 - val_loss: 17569.2619 - val_mse: 17569.2617 - val_mae: 35.9273\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2286.2318 - mse: 2286.2322 - mae: 34.7677 - val_loss: 17476.6441 - val_mse: 17476.6445 - val_mae: 36.0156\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 345us/step - loss: 2295.9901 - mse: 2295.9900 - mae: 33.2818 - val_loss: 17554.6043 - val_mse: 17554.6016 - val_mae: 35.9307\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 341us/step - loss: 2350.5237 - mse: 2350.5237 - mae: 33.6893 - val_loss: 17846.6451 - val_mse: 17846.6445 - val_mae: 36.1008\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 334us/step - loss: 2373.9569 - mse: 2373.9570 - mae: 34.0474 - val_loss: 17802.9191 - val_mse: 17802.9180 - val_mae: 36.0000\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 327us/step - loss: 2257.2043 - mse: 2257.2043 - mae: 32.4595 - val_loss: 17619.2260 - val_mse: 17619.2266 - val_mae: 35.8199\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2483.3806 - mse: 2483.3809 - mae: 35.4503 - val_loss: 17688.0330 - val_mse: 17688.0293 - val_mae: 35.8139\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2435.9160 - mse: 2435.9158 - mae: 34.6703 - val_loss: 17767.7867 - val_mse: 17767.7852 - val_mae: 35.9020\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2316.0929 - mse: 2316.0928 - mae: 35.2195 - val_loss: 17655.9894 - val_mse: 17655.9902 - val_mae: 35.7717\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2271.9495 - mse: 2271.9495 - mae: 33.6912 - val_loss: 17726.6999 - val_mse: 17726.6992 - val_mae: 35.7876\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2431.6361 - mse: 2431.6357 - mae: 34.7646 - val_loss: 17853.4854 - val_mse: 17853.4844 - val_mae: 36.0143\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 329us/step - loss: 2114.2123 - mse: 2114.2122 - mae: 31.1054 - val_loss: 17617.1433 - val_mse: 17617.1445 - val_mae: 35.7285\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2188.7591 - mse: 2188.7588 - mae: 32.6555 - val_loss: 17668.2048 - val_mse: 17668.2031 - val_mae: 35.7115\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2254.9727 - mse: 2254.9724 - mae: 34.0601 - val_loss: 17641.3943 - val_mse: 17641.3945 - val_mae: 35.6814\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 360us/step - loss: 2434.8436 - mse: 2434.8435 - mae: 34.9182 - val_loss: 17946.6676 - val_mse: 17946.6680 - val_mae: 36.2545\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 266us/step - loss: 2165.9394 - mse: 2165.9395 - mae: 32.4129 - val_loss: 17539.3696 - val_mse: 17539.3711 - val_mae: 35.7571\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 2342.0385 - mse: 2342.0386 - mae: 33.8214 - val_loss: 17589.3349 - val_mse: 17589.3359 - val_mae: 35.6830\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 382us/step - loss: 2450.3375 - mse: 2450.3374 - mae: 34.7899 - val_loss: 17824.0677 - val_mse: 17824.0684 - val_mae: 35.8855\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 528us/step - loss: 2215.6519 - mse: 2215.6521 - mae: 32.6485 - val_loss: 17805.5537 - val_mse: 17805.5547 - val_mae: 35.8475\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 365us/step - loss: 2177.9810 - mse: 2177.9810 - mae: 32.0581 - val_loss: 17590.7180 - val_mse: 17590.7188 - val_mae: 35.7026\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2212.6857 - mse: 2212.6860 - mae: 32.5548 - val_loss: 17636.2594 - val_mse: 17636.2598 - val_mae: 35.6447\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2024.4962 - mse: 2024.4963 - mae: 31.0438 - val_loss: 17678.5644 - val_mse: 17678.5645 - val_mae: 35.6540\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 333us/step - loss: 2261.4769 - mse: 2261.4768 - mae: 32.3473 - val_loss: 17613.8111 - val_mse: 17613.8105 - val_mae: 35.6722\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 2216.5232 - mse: 2216.5229 - mae: 33.2654 - val_loss: 17602.4425 - val_mse: 17602.4395 - val_mae: 35.6847\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 2240.2635 - mse: 2240.2634 - mae: 33.1929 - val_loss: 17776.2651 - val_mse: 17776.2637 - val_mae: 35.7931\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2214.8137 - mse: 2214.8135 - mae: 32.6545 - val_loss: 17798.9223 - val_mse: 17798.9219 - val_mae: 35.8395\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 271us/step - loss: 2282.7692 - mse: 2282.7693 - mae: 33.0996 - val_loss: 17631.9674 - val_mse: 17631.9688 - val_mae: 35.6925\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 2203.6820 - mse: 2203.6819 - mae: 32.7025 - val_loss: 17591.0390 - val_mse: 17591.0371 - val_mae: 35.7542\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 365us/step - loss: 1972.5417 - mse: 1972.5417 - mae: 31.4521 - val_loss: 17610.7412 - val_mse: 17610.7422 - val_mae: 35.7529\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 2013.3685 - mse: 2013.3684 - mae: 30.9254 - val_loss: 17609.3746 - val_mse: 17609.3730 - val_mae: 35.7662\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 259us/step - loss: 2129.8715 - mse: 2129.8716 - mae: 31.6358 - val_loss: 17591.4697 - val_mse: 17591.4727 - val_mae: 35.7956\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 264us/step - loss: 2078.9697 - mse: 2078.9697 - mae: 31.5042 - val_loss: 17811.5536 - val_mse: 17811.5547 - val_mae: 35.9074\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2097.4214 - mse: 2097.4211 - mae: 31.8651 - val_loss: 17654.3247 - val_mse: 17654.3242 - val_mae: 35.7506\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2035.9441 - mse: 2035.9438 - mae: 30.6420 - val_loss: 17676.6555 - val_mse: 17676.6543 - val_mae: 35.7360\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 265us/step - loss: 1849.5364 - mse: 1849.5366 - mae: 29.4665 - val_loss: 17519.4133 - val_mse: 17519.4141 - val_mae: 35.9340\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 365us/step - loss: 2216.0343 - mse: 2216.0342 - mae: 32.5907 - val_loss: 17733.8563 - val_mse: 17733.8555 - val_mae: 35.7827\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2142.2106 - mse: 2142.2104 - mae: 31.5955 - val_loss: 17669.8926 - val_mse: 17669.8926 - val_mae: 35.7690\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2022.7509 - mse: 2022.7507 - mae: 31.4863 - val_loss: 17514.9482 - val_mse: 17514.9492 - val_mae: 35.9628\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2076.2800 - mse: 2076.2800 - mae: 31.6596 - val_loss: 17677.6233 - val_mse: 17677.6230 - val_mae: 35.7726\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 1952.6803 - mse: 1952.6804 - mae: 31.2269 - val_loss: 17585.3286 - val_mse: 17585.3281 - val_mae: 35.8591\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 263us/step - loss: 2184.0341 - mse: 2184.0342 - mae: 32.2922 - val_loss: 17787.1527 - val_mse: 17787.1523 - val_mae: 35.8836\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 2016.8481 - mse: 2016.8484 - mae: 31.1314 - val_loss: 17765.0498 - val_mse: 17765.0508 - val_mae: 35.8506\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 249us/step - loss: 1870.5036 - mse: 1870.5038 - mae: 29.6502 - val_loss: 17733.3650 - val_mse: 17733.3633 - val_mae: 35.8110\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 249us/step - loss: 2057.4138 - mse: 2057.4138 - mae: 30.7048 - val_loss: 17834.7997 - val_mse: 17834.8008 - val_mae: 35.9977\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 256us/step - loss: 2070.2519 - mse: 2070.2520 - mae: 31.5938 - val_loss: 17732.0955 - val_mse: 17732.0957 - val_mae: 35.8244\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 1983.0627 - mse: 1983.0626 - mae: 30.7448 - val_loss: 17720.9785 - val_mse: 17720.9785 - val_mae: 35.8291\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 257us/step - loss: 1980.1475 - mse: 1980.1476 - mae: 30.3309 - val_loss: 17795.2797 - val_mse: 17795.2793 - val_mae: 35.9236\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 338us/step - loss: 1951.1490 - mse: 1951.1490 - mae: 29.9952 - val_loss: 17735.5106 - val_mse: 17735.5098 - val_mae: 35.8514\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 398us/step - loss: 2110.7289 - mse: 2110.7290 - mae: 31.1886 - val_loss: 17707.3863 - val_mse: 17707.3867 - val_mae: 35.8584\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 386us/step - loss: 1831.8847 - mse: 1831.8846 - mae: 28.1178 - val_loss: 17614.9680 - val_mse: 17614.9668 - val_mae: 35.9070\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 377us/step - loss: 1969.0346 - mse: 1969.0345 - mae: 31.0860 - val_loss: 17745.9414 - val_mse: 17745.9414 - val_mae: 35.8750\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 1912.0876 - mse: 1912.0876 - mae: 29.0726 - val_loss: 17667.4413 - val_mse: 17667.4395 - val_mae: 35.8857\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 377us/step - loss: 1796.4717 - mse: 1796.4717 - mae: 29.8946 - val_loss: 17499.7923 - val_mse: 17499.7930 - val_mae: 36.1471\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 350us/step - loss: 1962.1018 - mse: 1962.1017 - mae: 30.8699 - val_loss: 17733.8808 - val_mse: 17733.8828 - val_mae: 35.8942\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 281us/step - loss: 1862.8301 - mse: 1862.8298 - mae: 28.9791 - val_loss: 17530.1402 - val_mse: 17530.1406 - val_mae: 36.0978\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 290us/step - loss: 1979.3730 - mse: 1979.3732 - mae: 30.2812 - val_loss: 17855.6470 - val_mse: 17855.6465 - val_mae: 36.1222\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2032.3512 - mse: 2032.3512 - mae: 30.5307 - val_loss: 17798.6881 - val_mse: 17798.6875 - val_mae: 35.9885\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 1979.4596 - mse: 1979.4595 - mae: 30.9142 - val_loss: 17736.4293 - val_mse: 17736.4297 - val_mae: 35.9341\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 276us/step - loss: 1986.0602 - mse: 1986.0603 - mae: 30.2064 - val_loss: 17675.5630 - val_mse: 17675.5645 - val_mae: 35.9494\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4331.3775 - mse: 4331.3774 - mae: 34.5781 - val_loss: 2125.5985 - val_mse: 2125.5984 - val_mae: 31.4470\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4128.0403 - mse: 4128.0405 - mae: 34.9761 - val_loss: 2237.8483 - val_mse: 2237.8479 - val_mae: 31.7523\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4237.1221 - mse: 4237.1221 - mae: 35.1649 - val_loss: 2276.5471 - val_mse: 2276.5471 - val_mae: 31.8609\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4204.8670 - mse: 4204.8672 - mae: 34.8585 - val_loss: 2307.2364 - val_mse: 2307.2366 - val_mae: 31.9636\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4214.9831 - mse: 4214.9829 - mae: 35.6465 - val_loss: 2314.1301 - val_mse: 2314.1299 - val_mae: 31.9872\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4305.3755 - mse: 4305.3750 - mae: 35.3884 - val_loss: 2360.3846 - val_mse: 2360.3848 - val_mae: 32.1423\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4077.7411 - mse: 4077.7415 - mae: 34.2554 - val_loss: 2291.5350 - val_mse: 2291.5347 - val_mae: 31.9121\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4129.4118 - mse: 4129.4116 - mae: 34.3687 - val_loss: 2297.9553 - val_mse: 2297.9551 - val_mae: 31.9347\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 338us/step - loss: 4281.1742 - mse: 4281.1743 - mae: 35.2239 - val_loss: 2394.1290 - val_mse: 2394.1292 - val_mae: 32.2525\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4251.8960 - mse: 4251.8960 - mae: 34.9249 - val_loss: 2291.9866 - val_mse: 2291.9868 - val_mae: 31.9163\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4210.2741 - mse: 4210.2744 - mae: 34.4102 - val_loss: 2325.6999 - val_mse: 2325.7000 - val_mae: 32.0277\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4354.6269 - mse: 4354.6274 - mae: 35.1502 - val_loss: 2307.5577 - val_mse: 2307.5579 - val_mae: 31.9680\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4143.7765 - mse: 4143.7764 - mae: 34.2918 - val_loss: 2349.4724 - val_mse: 2349.4724 - val_mae: 32.1041\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 280us/step - loss: 4349.7202 - mse: 4349.7197 - mae: 34.8279 - val_loss: 2303.1455 - val_mse: 2303.1455 - val_mae: 31.9543\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4107.7758 - mse: 4107.7759 - mae: 34.2247 - val_loss: 2271.7697 - val_mse: 2271.7695 - val_mae: 31.8479\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 3989.7603 - mse: 3989.7610 - mae: 33.1716 - val_loss: 2237.7022 - val_mse: 2237.7019 - val_mae: 31.7396\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4143.3337 - mse: 4143.3340 - mae: 35.0776 - val_loss: 2313.8943 - val_mse: 2313.8943 - val_mae: 31.9922\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4263.5707 - mse: 4263.5708 - mae: 34.8150 - val_loss: 2332.1768 - val_mse: 2332.1765 - val_mae: 32.0542\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4287.2743 - mse: 4287.2744 - mae: 34.3899 - val_loss: 2367.5841 - val_mse: 2367.5840 - val_mae: 32.1709\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 373us/step - loss: 4112.5915 - mse: 4112.5913 - mae: 34.0832 - val_loss: 2331.5201 - val_mse: 2331.5205 - val_mae: 32.0535\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 346us/step - loss: 4293.8658 - mse: 4293.8662 - mae: 34.5355 - val_loss: 2398.6311 - val_mse: 2398.6311 - val_mae: 32.2670\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 4139.5275 - mse: 4139.5273 - mae: 33.5194 - val_loss: 2380.5912 - val_mse: 2380.5913 - val_mae: 32.2127\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 4147.4195 - mse: 4147.4194 - mae: 34.3048 - val_loss: 2302.4926 - val_mse: 2302.4927 - val_mae: 31.9525\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4106.7595 - mse: 4106.7598 - mae: 34.0282 - val_loss: 2338.8535 - val_mse: 2338.8533 - val_mae: 32.0788\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 4103.4769 - mse: 4103.4766 - mae: 33.7931 - val_loss: 2316.4632 - val_mse: 2316.4629 - val_mae: 32.0007\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4023.4100 - mse: 4023.4092 - mae: 33.8313 - val_loss: 2334.6775 - val_mse: 2334.6775 - val_mae: 32.0632\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 280us/step - loss: 4172.4482 - mse: 4172.4482 - mae: 33.7706 - val_loss: 2310.0365 - val_mse: 2310.0366 - val_mae: 31.9756\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 3918.1575 - mse: 3918.1570 - mae: 33.3773 - val_loss: 2298.6847 - val_mse: 2298.6848 - val_mae: 31.9343\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 337us/step - loss: 4324.5650 - mse: 4324.5654 - mae: 34.7281 - val_loss: 2378.3179 - val_mse: 2378.3179 - val_mae: 32.1992\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 338us/step - loss: 4029.3180 - mse: 4029.3176 - mae: 32.6914 - val_loss: 2355.9177 - val_mse: 2355.9177 - val_mae: 32.1272\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4130.6607 - mse: 4130.6606 - mae: 34.2352 - val_loss: 2340.8204 - val_mse: 2340.8203 - val_mae: 32.0784\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4217.4030 - mse: 4217.4033 - mae: 33.8394 - val_loss: 2384.7560 - val_mse: 2384.7559 - val_mae: 32.2164\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4015.4737 - mse: 4015.4741 - mae: 32.8040 - val_loss: 2295.5571 - val_mse: 2295.5569 - val_mae: 31.9194\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4078.9786 - mse: 4078.9783 - mae: 33.9493 - val_loss: 2304.1326 - val_mse: 2304.1326 - val_mae: 31.9488\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4232.4101 - mse: 4232.4102 - mae: 34.1431 - val_loss: 2329.4637 - val_mse: 2329.4641 - val_mae: 32.0366\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 3973.3760 - mse: 3973.3765 - mae: 32.6036 - val_loss: 2308.7846 - val_mse: 2308.7847 - val_mae: 31.9654\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4085.1395 - mse: 4085.1396 - mae: 32.9530 - val_loss: 2269.7789 - val_mse: 2269.7788 - val_mae: 31.8264\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 336us/step - loss: 4246.6717 - mse: 4246.6719 - mae: 34.0156 - val_loss: 2337.5880 - val_mse: 2337.5876 - val_mae: 32.0637\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 357us/step - loss: 4185.3721 - mse: 4185.3726 - mae: 33.1366 - val_loss: 2294.6041 - val_mse: 2294.6042 - val_mae: 31.9138\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 4182.8036 - mse: 4182.8032 - mae: 34.2776 - val_loss: 2360.5619 - val_mse: 2360.5618 - val_mae: 32.1388\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4190.5503 - mse: 4190.5503 - mae: 33.0076 - val_loss: 2308.3588 - val_mse: 2308.3584 - val_mae: 31.9600\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 4178.5166 - mse: 4178.5166 - mae: 33.5337 - val_loss: 2331.7665 - val_mse: 2331.7668 - val_mae: 32.0399\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 3911.8664 - mse: 3911.8665 - mae: 33.9355 - val_loss: 2322.2138 - val_mse: 2322.2139 - val_mae: 32.0068\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 4095.9180 - mse: 4095.9180 - mae: 33.5435 - val_loss: 2376.9415 - val_mse: 2376.9414 - val_mae: 32.1883\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 358us/step - loss: 4123.0963 - mse: 4123.0972 - mae: 33.4389 - val_loss: 2307.3254 - val_mse: 2307.3257 - val_mae: 31.9525\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 4101.3833 - mse: 4101.3833 - mae: 33.9050 - val_loss: 2343.5519 - val_mse: 2343.5518 - val_mae: 32.0767\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 253us/step - loss: 4070.8051 - mse: 4070.8049 - mae: 33.3495 - val_loss: 2344.9319 - val_mse: 2344.9316 - val_mae: 32.0827\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4049.6132 - mse: 4049.6133 - mae: 33.0677 - val_loss: 2330.3801 - val_mse: 2330.3804 - val_mae: 32.0338\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 271us/step - loss: 4032.2355 - mse: 4032.2354 - mae: 33.0679 - val_loss: 2322.3789 - val_mse: 2322.3789 - val_mae: 32.0048\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 4018.3678 - mse: 4018.3674 - mae: 33.1611 - val_loss: 2305.7567 - val_mse: 2305.7568 - val_mae: 31.9446\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 279us/step - loss: 4134.1827 - mse: 4134.1816 - mae: 33.9919 - val_loss: 2393.3037 - val_mse: 2393.3037 - val_mae: 32.2376\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 323us/step - loss: 4025.1077 - mse: 4025.1074 - mae: 32.7535 - val_loss: 2350.2008 - val_mse: 2350.2007 - val_mae: 32.0978\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 297us/step - loss: 4120.2221 - mse: 4120.2217 - mae: 33.7135 - val_loss: 2369.5177 - val_mse: 2369.5178 - val_mae: 32.1606\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 3978.3333 - mse: 3978.3333 - mae: 32.9814 - val_loss: 2308.7552 - val_mse: 2308.7554 - val_mae: 31.9511\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4165.0372 - mse: 4165.0376 - mae: 32.9882 - val_loss: 2385.3083 - val_mse: 2385.3083 - val_mae: 32.2114\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 3994.2624 - mse: 3994.2622 - mae: 32.7686 - val_loss: 2347.1862 - val_mse: 2347.1863 - val_mae: 32.0866\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 349us/step - loss: 4028.9583 - mse: 4028.9590 - mae: 33.5483 - val_loss: 2326.1998 - val_mse: 2326.1997 - val_mae: 32.0126\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4069.0104 - mse: 4069.0100 - mae: 32.7691 - val_loss: 2318.6934 - val_mse: 2318.6936 - val_mae: 31.9841\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 3955.9659 - mse: 3955.9661 - mae: 33.5590 - val_loss: 2344.9563 - val_mse: 2344.9565 - val_mae: 32.0775\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 3839.9088 - mse: 3839.9094 - mae: 33.0693 - val_loss: 2348.3923 - val_mse: 2348.3921 - val_mae: 32.0870\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 359us/step - loss: 4010.9684 - mse: 4010.9685 - mae: 32.8324 - val_loss: 2360.7439 - val_mse: 2360.7437 - val_mae: 32.1281\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 372us/step - loss: 4251.5417 - mse: 4251.5415 - mae: 33.9946 - val_loss: 2395.4212 - val_mse: 2395.4211 - val_mae: 32.2396\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4015.2592 - mse: 4015.2588 - mae: 32.7178 - val_loss: 2374.9837 - val_mse: 2374.9836 - val_mae: 32.1743\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4096.4353 - mse: 4096.4355 - mae: 33.6180 - val_loss: 2380.3141 - val_mse: 2380.3137 - val_mae: 32.1901\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4027.0306 - mse: 4027.0305 - mae: 32.6546 - val_loss: 2360.9993 - val_mse: 2360.9993 - val_mae: 32.1276\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 3991.1277 - mse: 3991.1279 - mae: 33.0839 - val_loss: 2353.1625 - val_mse: 2353.1626 - val_mae: 32.1002\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4025.0285 - mse: 4025.0283 - mae: 32.7911 - val_loss: 2326.2192 - val_mse: 2326.2192 - val_mae: 32.0044\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4044.1384 - mse: 4044.1387 - mae: 32.3716 - val_loss: 2325.5067 - val_mse: 2325.5068 - val_mae: 31.9999\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4203.8745 - mse: 4203.8745 - mae: 34.9688 - val_loss: 2396.1344 - val_mse: 2396.1348 - val_mae: 32.2335\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 3919.9409 - mse: 3919.9414 - mae: 32.8960 - val_loss: 2350.3490 - val_mse: 2350.3491 - val_mae: 32.0821\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 3999.5621 - mse: 3999.5615 - mae: 33.6134 - val_loss: 2361.0181 - val_mse: 2361.0183 - val_mae: 32.1172\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4012.8973 - mse: 4012.8972 - mae: 34.1418 - val_loss: 2341.6810 - val_mse: 2341.6807 - val_mae: 32.0508\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4145.0548 - mse: 4145.0547 - mae: 33.4611 - val_loss: 2326.4483 - val_mse: 2326.4482 - val_mae: 31.9969\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 3834.6079 - mse: 3834.6079 - mae: 31.7737 - val_loss: 2354.6368 - val_mse: 2354.6367 - val_mae: 32.0948\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 240us/step - loss: 3928.0368 - mse: 3928.0369 - mae: 32.4651 - val_loss: 2356.3226 - val_mse: 2356.3225 - val_mae: 32.1000\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 279us/step - loss: 3981.3509 - mse: 3981.3508 - mae: 33.5591 - val_loss: 2359.1021 - val_mse: 2359.1021 - val_mae: 32.1086\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 3873.7010 - mse: 3873.7017 - mae: 32.3740 - val_loss: 2371.7765 - val_mse: 2371.7766 - val_mae: 32.1498\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 3908.5680 - mse: 3908.5681 - mae: 32.2201 - val_loss: 2346.8483 - val_mse: 2346.8481 - val_mae: 32.0649\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 3856.9236 - mse: 3856.9241 - mae: 32.3708 - val_loss: 2334.4110 - val_mse: 2334.4109 - val_mae: 32.0220\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 267us/step - loss: 4102.9828 - mse: 4102.9824 - mae: 33.5387 - val_loss: 2378.9473 - val_mse: 2378.9475 - val_mae: 32.1718\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3412.2378 - mse: 3412.2378 - mae: 32.9041 - val_loss: 1453.5010 - val_mse: 1453.5009 - val_mae: 25.6179\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3442.3529 - mse: 3442.3538 - mae: 32.8234 - val_loss: 1453.7401 - val_mse: 1453.7401 - val_mae: 25.1547\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 261us/step - loss: 3331.0869 - mse: 3331.0867 - mae: 32.5915 - val_loss: 1452.7351 - val_mse: 1452.7351 - val_mae: 25.1639\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3398.6248 - mse: 3398.6250 - mae: 32.7307 - val_loss: 1452.2833 - val_mse: 1452.2832 - val_mae: 25.7886\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3419.8959 - mse: 3419.8970 - mae: 32.5351 - val_loss: 1452.5899 - val_mse: 1452.5897 - val_mae: 25.9733\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3244.4214 - mse: 3244.4224 - mae: 32.0579 - val_loss: 1452.2638 - val_mse: 1452.2637 - val_mae: 26.0238\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3426.3698 - mse: 3426.3689 - mae: 33.0766 - val_loss: 1451.0495 - val_mse: 1451.0494 - val_mae: 25.4158\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 279us/step - loss: 3454.7501 - mse: 3454.7495 - mae: 33.3289 - val_loss: 1453.1645 - val_mse: 1453.1646 - val_mae: 26.0544\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3360.9602 - mse: 3360.9595 - mae: 31.9808 - val_loss: 1451.3925 - val_mse: 1451.3926 - val_mae: 25.4876\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3312.2421 - mse: 3312.2415 - mae: 31.8456 - val_loss: 1451.5456 - val_mse: 1451.5453 - val_mae: 25.3638\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3410.5419 - mse: 3410.5415 - mae: 32.8201 - val_loss: 1451.5184 - val_mse: 1451.5182 - val_mae: 25.2325\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3425.0673 - mse: 3425.0669 - mae: 32.9208 - val_loss: 1451.6421 - val_mse: 1451.6421 - val_mae: 25.5570\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3227.6550 - mse: 3227.6553 - mae: 32.5762 - val_loss: 1452.0221 - val_mse: 1452.0222 - val_mae: 25.4591\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 267us/step - loss: 3293.8699 - mse: 3293.8689 - mae: 31.3693 - val_loss: 1454.6486 - val_mse: 1454.6486 - val_mae: 26.0566\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 351us/step - loss: 3225.5955 - mse: 3225.5957 - mae: 32.6383 - val_loss: 1455.1903 - val_mse: 1455.1902 - val_mae: 26.1322\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3300.0606 - mse: 3300.0605 - mae: 32.6756 - val_loss: 1456.9158 - val_mse: 1456.9158 - val_mae: 26.3108\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 353us/step - loss: 3417.8295 - mse: 3417.8291 - mae: 32.7220 - val_loss: 1453.3371 - val_mse: 1453.3370 - val_mae: 25.4508\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3247.7279 - mse: 3247.7285 - mae: 31.5473 - val_loss: 1453.7704 - val_mse: 1453.7705 - val_mae: 25.8174\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 277us/step - loss: 3326.8072 - mse: 3326.8071 - mae: 32.5234 - val_loss: 1453.9696 - val_mse: 1453.9696 - val_mae: 25.8178\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3429.9349 - mse: 3429.9360 - mae: 32.6536 - val_loss: 1454.0769 - val_mse: 1454.0769 - val_mae: 25.3966\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3384.9696 - mse: 3384.9700 - mae: 32.5271 - val_loss: 1455.0678 - val_mse: 1455.0679 - val_mae: 25.7441\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3369.1904 - mse: 3369.1902 - mae: 33.3124 - val_loss: 1462.2368 - val_mse: 1462.2368 - val_mae: 26.6333\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3298.2832 - mse: 3298.2837 - mae: 32.3434 - val_loss: 1456.3960 - val_mse: 1456.3960 - val_mae: 25.8274\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3368.3926 - mse: 3368.3921 - mae: 32.5586 - val_loss: 1456.0808 - val_mse: 1456.0809 - val_mae: 25.6187\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 318us/step - loss: 3380.0508 - mse: 3380.0513 - mae: 32.5143 - val_loss: 1459.1208 - val_mse: 1459.1206 - val_mae: 26.1468\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3341.6305 - mse: 3341.6304 - mae: 32.0664 - val_loss: 1457.4794 - val_mse: 1457.4795 - val_mae: 25.8599\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3351.5593 - mse: 3351.5586 - mae: 32.2290 - val_loss: 1460.5956 - val_mse: 1460.5956 - val_mae: 26.2385\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3384.0566 - mse: 3384.0569 - mae: 32.0181 - val_loss: 1459.3423 - val_mse: 1459.3423 - val_mae: 26.0138\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3309.1476 - mse: 3309.1475 - mae: 32.5104 - val_loss: 1458.2550 - val_mse: 1458.2550 - val_mae: 25.7249\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 294us/step - loss: 3284.8074 - mse: 3284.8071 - mae: 32.3382 - val_loss: 1464.7397 - val_mse: 1464.7397 - val_mae: 26.5975\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3410.0137 - mse: 3410.0144 - mae: 32.3468 - val_loss: 1459.1462 - val_mse: 1459.1462 - val_mae: 25.8604\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 338us/step - loss: 3342.4031 - mse: 3342.4026 - mae: 32.4596 - val_loss: 1460.2357 - val_mse: 1460.2356 - val_mae: 26.0351\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 331us/step - loss: 3183.7669 - mse: 3183.7678 - mae: 31.1304 - val_loss: 1467.1751 - val_mse: 1467.1749 - val_mae: 26.7465\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 371us/step - loss: 3403.4795 - mse: 3403.4792 - mae: 32.4888 - val_loss: 1461.5037 - val_mse: 1461.5037 - val_mae: 26.1166\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 389us/step - loss: 3387.3403 - mse: 3387.3411 - mae: 31.9626 - val_loss: 1461.3264 - val_mse: 1461.3265 - val_mae: 25.9266\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 367us/step - loss: 3299.8902 - mse: 3299.8906 - mae: 32.2300 - val_loss: 1461.3674 - val_mse: 1461.3673 - val_mae: 25.8877\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 282us/step - loss: 3404.1677 - mse: 3404.1675 - mae: 31.6122 - val_loss: 1463.7875 - val_mse: 1463.7876 - val_mae: 26.2281\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3330.2693 - mse: 3330.2700 - mae: 32.0977 - val_loss: 1465.4232 - val_mse: 1465.4233 - val_mae: 26.3557\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 350us/step - loss: 3244.2594 - mse: 3244.2598 - mae: 31.2407 - val_loss: 1465.4288 - val_mse: 1465.4288 - val_mae: 26.3459\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3387.9284 - mse: 3387.9280 - mae: 32.0681 - val_loss: 1465.6081 - val_mse: 1465.6082 - val_mae: 26.2904\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3234.3558 - mse: 3234.3560 - mae: 32.0458 - val_loss: 1462.8065 - val_mse: 1462.8065 - val_mae: 25.7592\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3257.7498 - mse: 3257.7493 - mae: 31.8693 - val_loss: 1463.7150 - val_mse: 1463.7151 - val_mae: 25.9088\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3386.2999 - mse: 3386.3008 - mae: 32.6007 - val_loss: 1463.4233 - val_mse: 1463.4233 - val_mae: 25.3579\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 340us/step - loss: 3211.7579 - mse: 3211.7576 - mae: 31.2942 - val_loss: 1463.2017 - val_mse: 1463.2017 - val_mae: 25.5830\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 340us/step - loss: 3316.8457 - mse: 3316.8457 - mae: 31.3556 - val_loss: 1465.9273 - val_mse: 1465.9271 - val_mae: 26.2775\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 343us/step - loss: 3313.8009 - mse: 3313.8008 - mae: 32.2515 - val_loss: 1464.5541 - val_mse: 1464.5540 - val_mae: 26.0410\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 232us/step - loss: 3286.4360 - mse: 3286.4360 - mae: 31.9963 - val_loss: 1463.7143 - val_mse: 1463.7144 - val_mae: 25.8667\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 262us/step - loss: 3440.8966 - mse: 3440.8967 - mae: 32.3985 - val_loss: 1463.8044 - val_mse: 1463.8044 - val_mae: 25.3034\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 245us/step - loss: 3364.4898 - mse: 3364.4900 - mae: 32.0979 - val_loss: 1464.5552 - val_mse: 1464.5553 - val_mae: 26.0454\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3240.2987 - mse: 3240.2983 - mae: 31.3485 - val_loss: 1467.5445 - val_mse: 1467.5444 - val_mae: 26.4120\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3325.0592 - mse: 3325.0598 - mae: 32.3636 - val_loss: 1463.6632 - val_mse: 1463.6632 - val_mae: 25.8115\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3270.9271 - mse: 3270.9280 - mae: 31.7064 - val_loss: 1465.7849 - val_mse: 1465.7849 - val_mae: 26.1477\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3288.3194 - mse: 3288.3198 - mae: 31.6645 - val_loss: 1464.7185 - val_mse: 1464.7186 - val_mae: 25.9403\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3177.9402 - mse: 3177.9404 - mae: 31.3621 - val_loss: 1468.9337 - val_mse: 1468.9340 - val_mae: 26.4792\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 272us/step - loss: 3201.8991 - mse: 3201.8984 - mae: 31.6144 - val_loss: 1469.2628 - val_mse: 1469.2627 - val_mae: 26.4925\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3306.3714 - mse: 3306.3716 - mae: 31.4908 - val_loss: 1465.5616 - val_mse: 1465.5616 - val_mae: 25.9133\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 231us/step - loss: 3306.8122 - mse: 3306.8118 - mae: 31.5655 - val_loss: 1466.1971 - val_mse: 1466.1971 - val_mae: 25.9409\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3251.8594 - mse: 3251.8591 - mae: 31.4950 - val_loss: 1466.3040 - val_mse: 1466.3038 - val_mae: 25.9882\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3240.9582 - mse: 3240.9585 - mae: 31.0307 - val_loss: 1468.1663 - val_mse: 1468.1663 - val_mae: 26.2948\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3286.2714 - mse: 3286.2710 - mae: 31.5967 - val_loss: 1465.4429 - val_mse: 1465.4426 - val_mae: 25.7064\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 351us/step - loss: 3297.3048 - mse: 3297.3049 - mae: 31.4450 - val_loss: 1467.0736 - val_mse: 1467.0735 - val_mae: 26.1183\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 362us/step - loss: 3326.8147 - mse: 3326.8154 - mae: 32.1241 - val_loss: 1465.1213 - val_mse: 1465.1213 - val_mae: 25.4295\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 227us/step - loss: 3298.5803 - mse: 3298.5806 - mae: 32.3226 - val_loss: 1467.2641 - val_mse: 1467.2640 - val_mae: 26.1476\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3300.9168 - mse: 3300.9167 - mae: 31.6365 - val_loss: 1466.9330 - val_mse: 1466.9329 - val_mae: 26.0370\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3302.6938 - mse: 3302.6941 - mae: 31.8360 - val_loss: 1467.5513 - val_mse: 1467.5511 - val_mae: 26.1637\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3195.0883 - mse: 3195.0879 - mae: 31.9118 - val_loss: 1466.8534 - val_mse: 1466.8534 - val_mae: 26.0938\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3235.1375 - mse: 3235.1375 - mae: 31.5639 - val_loss: 1465.0512 - val_mse: 1465.0513 - val_mae: 25.7014\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3268.9518 - mse: 3268.9524 - mae: 31.3880 - val_loss: 1467.8748 - val_mse: 1467.8749 - val_mae: 26.2721\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3208.4395 - mse: 3208.4397 - mae: 31.2640 - val_loss: 1465.5721 - val_mse: 1465.5720 - val_mae: 25.8574\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3251.6054 - mse: 3251.6047 - mae: 31.6746 - val_loss: 1468.1851 - val_mse: 1468.1852 - val_mae: 26.2765\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3275.5914 - mse: 3275.5916 - mae: 31.9070 - val_loss: 1466.3662 - val_mse: 1466.3661 - val_mae: 26.0043\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3340.8638 - mse: 3340.8638 - mae: 32.1040 - val_loss: 1466.1200 - val_mse: 1466.1201 - val_mae: 25.9293\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3221.2774 - mse: 3221.2776 - mae: 30.9519 - val_loss: 1467.1606 - val_mse: 1467.1604 - val_mae: 26.1126\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 344us/step - loss: 3267.2981 - mse: 3267.2974 - mae: 31.3518 - val_loss: 1467.5335 - val_mse: 1467.5332 - val_mae: 26.1873\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 273us/step - loss: 3288.5861 - mse: 3288.5867 - mae: 31.7046 - val_loss: 1465.8611 - val_mse: 1465.8611 - val_mae: 25.9236\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3229.3023 - mse: 3229.3022 - mae: 31.3207 - val_loss: 1466.3828 - val_mse: 1466.3827 - val_mae: 25.9729\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3210.2250 - mse: 3210.2251 - mae: 31.6577 - val_loss: 1466.5158 - val_mse: 1466.5157 - val_mae: 25.9865\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3279.0849 - mse: 3279.0850 - mae: 31.3966 - val_loss: 1467.7036 - val_mse: 1467.7035 - val_mae: 26.1572\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3277.0003 - mse: 3277.0007 - mae: 32.1678 - val_loss: 1467.0606 - val_mse: 1467.0605 - val_mae: 26.0731\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3257.0586 - mse: 3257.0581 - mae: 31.4997 - val_loss: 1469.1407 - val_mse: 1469.1405 - val_mae: 26.2935\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2967.4495 - mse: 2967.4500 - mae: 31.6798 - val_loss: 1067.2804 - val_mse: 1067.2804 - val_mae: 24.0025\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 269us/step - loss: 2879.0032 - mse: 2879.0037 - mae: 30.7273 - val_loss: 1068.6434 - val_mse: 1068.6434 - val_mae: 24.9951\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2897.6894 - mse: 2897.6892 - mae: 30.9042 - val_loss: 1066.4792 - val_mse: 1066.4794 - val_mae: 24.6580\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2905.5606 - mse: 2905.5601 - mae: 30.9585 - val_loss: 1065.9286 - val_mse: 1065.9286 - val_mae: 24.0438\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2884.3577 - mse: 2884.3582 - mae: 30.4765 - val_loss: 1065.1122 - val_mse: 1065.1121 - val_mae: 24.5591\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 343us/step - loss: 2964.2990 - mse: 2964.2988 - mae: 30.9179 - val_loss: 1065.0437 - val_mse: 1065.0436 - val_mae: 24.0621\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - ETA: 0s - loss: 2938.2377 - mse: 2938.2378 - mae: 30.83 - 1s 286us/step - loss: 2881.1999 - mse: 2881.2000 - mae: 30.8162 - val_loss: 1065.3327 - val_mse: 1065.3326 - val_mae: 24.7433\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2869.1361 - mse: 2869.1362 - mae: 31.1572 - val_loss: 1066.0048 - val_mse: 1066.0048 - val_mae: 24.9148\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2909.7695 - mse: 2909.7703 - mae: 30.9268 - val_loss: 1065.6839 - val_mse: 1065.6838 - val_mae: 24.8657\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 0s 244us/step - loss: 2993.0275 - mse: 2993.0273 - mae: 31.0959 - val_loss: 1064.0770 - val_mse: 1064.0770 - val_mae: 24.0440\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2829.7434 - mse: 2829.7437 - mae: 30.8599 - val_loss: 1063.9638 - val_mse: 1063.9636 - val_mae: 24.5431\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2874.3415 - mse: 2874.3413 - mae: 30.3352 - val_loss: 1064.2963 - val_mse: 1064.2963 - val_mae: 24.0247\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 252us/step - loss: 2918.6385 - mse: 2918.6375 - mae: 30.7569 - val_loss: 1064.0142 - val_mse: 1064.0142 - val_mae: 24.7220\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2794.4079 - mse: 2794.4080 - mae: 30.1699 - val_loss: 1065.2382 - val_mse: 1065.2379 - val_mae: 24.9900\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2875.1414 - mse: 2875.1406 - mae: 30.9358 - val_loss: 1063.1150 - val_mse: 1063.1150 - val_mae: 24.0961\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2911.7225 - mse: 2911.7227 - mae: 31.3385 - val_loss: 1064.3757 - val_mse: 1064.3756 - val_mae: 23.8874\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2854.2190 - mse: 2854.2190 - mae: 30.6219 - val_loss: 1063.0746 - val_mse: 1063.0747 - val_mae: 24.4748\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2877.0689 - mse: 2877.0686 - mae: 30.7688 - val_loss: 1062.7823 - val_mse: 1062.7825 - val_mae: 24.1976\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2818.0226 - mse: 2818.0225 - mae: 30.6545 - val_loss: 1062.5517 - val_mse: 1062.5518 - val_mae: 24.4826\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2854.6195 - mse: 2854.6199 - mae: 30.0354 - val_loss: 1065.6403 - val_mse: 1065.6404 - val_mae: 25.0703\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2922.1011 - mse: 2922.1006 - mae: 31.2471 - val_loss: 1061.8160 - val_mse: 1061.8160 - val_mae: 24.3657\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2913.0303 - mse: 2913.0298 - mae: 30.7769 - val_loss: 1061.0862 - val_mse: 1061.0863 - val_mae: 24.2397\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2904.4717 - mse: 2904.4719 - mae: 30.6908 - val_loss: 1060.7395 - val_mse: 1060.7395 - val_mae: 24.4441\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2814.8145 - mse: 2814.8137 - mae: 30.4994 - val_loss: 1060.2528 - val_mse: 1060.2529 - val_mae: 24.1373\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 275us/step - loss: 2810.5193 - mse: 2810.5193 - mae: 30.2081 - val_loss: 1059.5994 - val_mse: 1059.5994 - val_mae: 24.5505\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2877.0696 - mse: 2877.0696 - mae: 30.4477 - val_loss: 1059.7627 - val_mse: 1059.7628 - val_mae: 24.5269\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2877.1364 - mse: 2877.1365 - mae: 31.0837 - val_loss: 1060.2208 - val_mse: 1060.2208 - val_mae: 24.7353\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2917.2282 - mse: 2917.2288 - mae: 30.3492 - val_loss: 1059.4984 - val_mse: 1059.4983 - val_mae: 24.6419\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 349us/step - loss: 2833.8072 - mse: 2833.8069 - mae: 30.7529 - val_loss: 1060.8285 - val_mse: 1060.8284 - val_mae: 24.9167\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2827.7463 - mse: 2827.7461 - mae: 30.8622 - val_loss: 1058.3822 - val_mse: 1058.3823 - val_mae: 24.5239\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 343us/step - loss: 2889.5828 - mse: 2889.5825 - mae: 30.6711 - val_loss: 1057.8992 - val_mse: 1057.8992 - val_mae: 24.5193\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 270us/step - loss: 2920.9067 - mse: 2920.9070 - mae: 30.8589 - val_loss: 1059.0780 - val_mse: 1059.0779 - val_mae: 24.7859\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2867.8775 - mse: 2867.8774 - mae: 30.5867 - val_loss: 1059.3292 - val_mse: 1059.3291 - val_mae: 24.8279\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2844.7982 - mse: 2844.7981 - mae: 30.7781 - val_loss: 1059.5161 - val_mse: 1059.5161 - val_mae: 24.8650\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2780.4195 - mse: 2780.4199 - mae: 30.1484 - val_loss: 1058.0620 - val_mse: 1058.0621 - val_mae: 24.6750\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2892.2485 - mse: 2892.2478 - mae: 30.5730 - val_loss: 1060.1413 - val_mse: 1060.1412 - val_mae: 25.0148\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2855.8175 - mse: 2855.8176 - mae: 30.7732 - val_loss: 1056.9240 - val_mse: 1056.9238 - val_mae: 24.5490\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2947.4103 - mse: 2947.4099 - mae: 31.2852 - val_loss: 1056.7785 - val_mse: 1056.7787 - val_mae: 24.0433\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2824.6084 - mse: 2824.6082 - mae: 30.4135 - val_loss: 1057.4516 - val_mse: 1057.4517 - val_mae: 24.8097\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2843.5003 - mse: 2843.5007 - mae: 30.5717 - val_loss: 1056.2736 - val_mse: 1056.2736 - val_mae: 24.6477\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2874.7926 - mse: 2874.7930 - mae: 30.7299 - val_loss: 1055.2504 - val_mse: 1055.2502 - val_mae: 24.5110\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2916.6410 - mse: 2916.6406 - mae: 30.9437 - val_loss: 1055.7557 - val_mse: 1055.7559 - val_mae: 24.7311\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 373us/step - loss: 2911.3778 - mse: 2911.3774 - mae: 30.4804 - val_loss: 1054.2125 - val_mse: 1054.2126 - val_mae: 24.0998\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2750.6036 - mse: 2750.6035 - mae: 30.1763 - val_loss: 1053.6058 - val_mse: 1053.6057 - val_mae: 24.3964\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2726.8391 - mse: 2726.8391 - mae: 29.6197 - val_loss: 1053.9667 - val_mse: 1053.9668 - val_mae: 24.5560\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2724.7083 - mse: 2724.7083 - mae: 29.9289 - val_loss: 1054.7962 - val_mse: 1054.7965 - val_mae: 24.8582\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2820.1111 - mse: 2820.1104 - mae: 30.4353 - val_loss: 1054.3908 - val_mse: 1054.3905 - val_mae: 24.8585\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 268us/step - loss: 2831.4693 - mse: 2831.4695 - mae: 30.6904 - val_loss: 1052.0276 - val_mse: 1052.0276 - val_mae: 24.4152\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2900.1197 - mse: 2900.1191 - mae: 30.7871 - val_loss: 1052.2264 - val_mse: 1052.2266 - val_mae: 24.5787\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2855.0563 - mse: 2855.0559 - mae: 30.7218 - val_loss: 1051.0329 - val_mse: 1051.0328 - val_mae: 24.2398\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2900.8731 - mse: 2900.8735 - mae: 30.6646 - val_loss: 1050.7257 - val_mse: 1050.7256 - val_mae: 24.4097\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2924.3218 - mse: 2924.3210 - mae: 30.7749 - val_loss: 1050.8947 - val_mse: 1050.8947 - val_mae: 24.3600\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2885.0363 - mse: 2885.0354 - mae: 30.3118 - val_loss: 1053.9955 - val_mse: 1053.9955 - val_mae: 24.9651\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2772.2971 - mse: 2772.2971 - mae: 30.3404 - val_loss: 1054.5418 - val_mse: 1054.5419 - val_mae: 25.0558\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 363us/step - loss: 2872.0771 - mse: 2872.0774 - mae: 30.3038 - val_loss: 1050.2435 - val_mse: 1050.2435 - val_mae: 24.1899\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2909.6686 - mse: 2909.6687 - mae: 30.5802 - val_loss: 1049.7613 - val_mse: 1049.7611 - val_mae: 24.3622\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2798.1664 - mse: 2798.1667 - mae: 30.3807 - val_loss: 1050.0735 - val_mse: 1050.0734 - val_mae: 24.5977\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2855.7446 - mse: 2855.7444 - mae: 29.8415 - val_loss: 1049.3819 - val_mse: 1049.3818 - val_mae: 24.5490\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 268us/step - loss: 2855.4719 - mse: 2855.4714 - mae: 30.6091 - val_loss: 1048.4704 - val_mse: 1048.4705 - val_mae: 24.3756\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 341us/step - loss: 2860.0237 - mse: 2860.0242 - mae: 30.1685 - val_loss: 1048.7309 - val_mse: 1048.7311 - val_mae: 24.5467\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2841.6523 - mse: 2841.6526 - mae: 29.9981 - val_loss: 1049.7002 - val_mse: 1049.7001 - val_mae: 24.8501\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 362us/step - loss: 2875.5514 - mse: 2875.5505 - mae: 30.3016 - val_loss: 1047.9155 - val_mse: 1047.9154 - val_mae: 24.6476\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2744.1568 - mse: 2744.1577 - mae: 29.7976 - val_loss: 1046.3277 - val_mse: 1046.3276 - val_mae: 24.5445\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 350us/step - loss: 2800.3093 - mse: 2800.3093 - mae: 30.1184 - val_loss: 1045.2840 - val_mse: 1045.2839 - val_mae: 24.3939\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2836.9837 - mse: 2836.9846 - mae: 30.1409 - val_loss: 1045.5169 - val_mse: 1045.5170 - val_mae: 24.5696\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2890.7435 - mse: 2890.7434 - mae: 31.1101 - val_loss: 1044.5300 - val_mse: 1044.5299 - val_mae: 24.0778\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2840.3474 - mse: 2840.3477 - mae: 30.1082 - val_loss: 1046.7384 - val_mse: 1046.7385 - val_mae: 24.6609\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2829.2877 - mse: 2829.2881 - mae: 30.3298 - val_loss: 1045.6414 - val_mse: 1045.6414 - val_mae: 24.4025\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 330us/step - loss: 2798.6088 - mse: 2798.6086 - mae: 29.8041 - val_loss: 1046.3200 - val_mse: 1046.3201 - val_mae: 24.6286\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2811.7323 - mse: 2811.7327 - mae: 30.2933 - val_loss: 1046.2499 - val_mse: 1046.2500 - val_mae: 24.7025\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2905.6182 - mse: 2905.6184 - mae: 30.6955 - val_loss: 1046.5081 - val_mse: 1046.5082 - val_mae: 24.5695\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 316us/step - loss: 2822.2741 - mse: 2822.2744 - mae: 30.5943 - val_loss: 1046.1641 - val_mse: 1046.1641 - val_mae: 24.7180\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 0s 245us/step - loss: 2843.1527 - mse: 2843.1531 - mae: 30.2209 - val_loss: 1046.1993 - val_mse: 1046.1992 - val_mae: 24.8052\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 0s 230us/step - loss: 2783.5988 - mse: 2783.5981 - mae: 29.6924 - val_loss: 1043.6493 - val_mse: 1043.6493 - val_mae: 24.5835\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2897.0096 - mse: 2897.0100 - mae: 30.3669 - val_loss: 1042.2731 - val_mse: 1042.2731 - val_mae: 24.2747\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2828.3291 - mse: 2828.3296 - mae: 30.3238 - val_loss: 1041.3641 - val_mse: 1041.3641 - val_mae: 24.1491\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2901.3159 - mse: 2901.3157 - mae: 30.4372 - val_loss: 1041.0121 - val_mse: 1041.0121 - val_mae: 24.0849\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2830.8747 - mse: 2830.8750 - mae: 29.8548 - val_loss: 1041.8678 - val_mse: 1041.8678 - val_mae: 24.6288\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2816.7242 - mse: 2816.7241 - mae: 29.9333 - val_loss: 1040.4055 - val_mse: 1040.4054 - val_mae: 24.5009\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2818.8799 - mse: 2818.8806 - mae: 30.0865 - val_loss: 1038.8041 - val_mse: 1038.8042 - val_mae: 24.3996\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2556.0266 - mse: 2556.0271 - mae: 30.0600 - val_loss: 1537.2533 - val_mse: 1537.2533 - val_mae: 27.4588\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2513.1474 - mse: 2513.1475 - mae: 29.7965 - val_loss: 1534.8774 - val_mse: 1534.8772 - val_mae: 27.4278\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2537.6978 - mse: 2537.6975 - mae: 29.3106 - val_loss: 1534.0136 - val_mse: 1534.0138 - val_mae: 27.3811\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2512.4441 - mse: 2512.4436 - mae: 29.8037 - val_loss: 1536.5547 - val_mse: 1536.5544 - val_mae: 27.2888\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2517.1826 - mse: 2517.1826 - mae: 29.5931 - val_loss: 1536.5024 - val_mse: 1536.5024 - val_mae: 27.2367\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 273us/step - loss: 2552.7275 - mse: 2552.7275 - mae: 29.6910 - val_loss: 1527.2450 - val_mse: 1527.2450 - val_mae: 27.4315\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2581.6353 - mse: 2581.6357 - mae: 29.7352 - val_loss: 1514.3517 - val_mse: 1514.3517 - val_mae: 27.7790\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 250us/step - loss: 2521.6248 - mse: 2521.6248 - mae: 29.3764 - val_loss: 1539.1772 - val_mse: 1539.1771 - val_mae: 27.0498\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 270us/step - loss: 2584.6278 - mse: 2584.6284 - mae: 29.6054 - val_loss: 1520.1400 - val_mse: 1520.1400 - val_mae: 27.5099\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2570.1732 - mse: 2570.1733 - mae: 29.9811 - val_loss: 1518.2456 - val_mse: 1518.2457 - val_mae: 27.4957\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2572.0884 - mse: 2572.0876 - mae: 29.7839 - val_loss: 1526.7876 - val_mse: 1526.7876 - val_mae: 27.2070\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2599.9812 - mse: 2599.9810 - mae: 30.0255 - val_loss: 1532.0178 - val_mse: 1532.0177 - val_mae: 27.0537\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2555.4670 - mse: 2555.4666 - mae: 29.3456 - val_loss: 1524.8346 - val_mse: 1524.8347 - val_mae: 27.1724\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2484.4994 - mse: 2484.4990 - mae: 29.2526 - val_loss: 1515.6522 - val_mse: 1515.6523 - val_mae: 27.3463\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2514.8034 - mse: 2514.8037 - mae: 29.1580 - val_loss: 1522.7567 - val_mse: 1522.7568 - val_mae: 27.1021\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2480.1900 - mse: 2480.1899 - mae: 29.1263 - val_loss: 1507.0743 - val_mse: 1507.0741 - val_mae: 27.4379\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2516.9465 - mse: 2516.9473 - mae: 29.3088 - val_loss: 1528.0101 - val_mse: 1528.0101 - val_mae: 26.8854\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2524.9841 - mse: 2524.9839 - mae: 29.6065 - val_loss: 1509.8087 - val_mse: 1509.8087 - val_mae: 27.2857\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2447.9360 - mse: 2447.9351 - mae: 29.0266 - val_loss: 1509.8059 - val_mse: 1509.8060 - val_mae: 27.2710\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2457.2649 - mse: 2457.2649 - mae: 29.1416 - val_loss: 1508.1945 - val_mse: 1508.1945 - val_mae: 27.2787\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2488.0285 - mse: 2488.0286 - mae: 29.0095 - val_loss: 1500.7016 - val_mse: 1500.7019 - val_mae: 27.4309\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 358us/step - loss: 2471.5454 - mse: 2471.5459 - mae: 29.1623 - val_loss: 1506.8897 - val_mse: 1506.8898 - val_mae: 27.1794\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2480.3443 - mse: 2480.3445 - mae: 28.8807 - val_loss: 1500.4776 - val_mse: 1500.4775 - val_mae: 27.2655\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2540.2104 - mse: 2540.2112 - mae: 29.4286 - val_loss: 1514.7087 - val_mse: 1514.7089 - val_mae: 26.8182\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 342us/step - loss: 2557.8452 - mse: 2557.8447 - mae: 29.1138 - val_loss: 1505.7128 - val_mse: 1505.7126 - val_mae: 26.9732\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2531.5483 - mse: 2531.5486 - mae: 29.5803 - val_loss: 1509.1640 - val_mse: 1509.1641 - val_mae: 26.9109\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 332us/step - loss: 2459.1643 - mse: 2459.1646 - mae: 28.9908 - val_loss: 1503.5084 - val_mse: 1503.5084 - val_mae: 27.0357\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2488.6940 - mse: 2488.6934 - mae: 29.2974 - val_loss: 1499.8725 - val_mse: 1499.8726 - val_mae: 27.0064\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2490.7591 - mse: 2490.7595 - mae: 28.9565 - val_loss: 1498.2145 - val_mse: 1498.2146 - val_mae: 26.9867\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 347us/step - loss: 2456.6667 - mse: 2456.6665 - mae: 28.9192 - val_loss: 1497.0430 - val_mse: 1497.0430 - val_mae: 27.0253\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2471.6168 - mse: 2471.6160 - mae: 29.0673 - val_loss: 1496.6509 - val_mse: 1496.6511 - val_mae: 26.9930\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2465.1864 - mse: 2465.1873 - mae: 29.0331 - val_loss: 1491.0553 - val_mse: 1491.0554 - val_mae: 27.0995\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2507.1537 - mse: 2507.1541 - mae: 29.0563 - val_loss: 1493.4434 - val_mse: 1493.4431 - val_mae: 27.0921\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 277us/step - loss: 2544.9728 - mse: 2544.9729 - mae: 29.5232 - val_loss: 1497.2458 - val_mse: 1497.2456 - val_mae: 26.9585\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 261us/step - loss: 2516.9304 - mse: 2516.9314 - mae: 29.1337 - val_loss: 1494.7898 - val_mse: 1494.7897 - val_mae: 26.9845\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2481.5088 - mse: 2481.5093 - mae: 29.1707 - val_loss: 1487.4254 - val_mse: 1487.4253 - val_mae: 27.1307\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2471.3072 - mse: 2471.3076 - mae: 29.1020 - val_loss: 1490.9782 - val_mse: 1490.9783 - val_mae: 26.9807\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2479.7819 - mse: 2479.7822 - mae: 29.3149 - val_loss: 1491.7347 - val_mse: 1491.7347 - val_mae: 26.9497\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2483.9309 - mse: 2483.9304 - mae: 29.2029 - val_loss: 1499.6052 - val_mse: 1499.6053 - val_mae: 26.7276\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2522.5588 - mse: 2522.5586 - mae: 29.1796 - val_loss: 1483.2176 - val_mse: 1483.2177 - val_mae: 27.0982\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2480.3581 - mse: 2480.3584 - mae: 29.1029 - val_loss: 1493.6862 - val_mse: 1493.6864 - val_mae: 26.8047\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2512.3051 - mse: 2512.3052 - mae: 29.2224 - val_loss: 1498.5155 - val_mse: 1498.5155 - val_mae: 26.7254\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2492.8814 - mse: 2492.8813 - mae: 28.8687 - val_loss: 1480.8349 - val_mse: 1480.8348 - val_mae: 27.1686\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2434.5391 - mse: 2434.5398 - mae: 28.8572 - val_loss: 1476.8018 - val_mse: 1476.8016 - val_mae: 27.2704\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2430.0720 - mse: 2430.0718 - mae: 28.9693 - val_loss: 1481.5702 - val_mse: 1481.5701 - val_mae: 27.0423\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2483.5906 - mse: 2483.5906 - mae: 29.1712 - val_loss: 1482.3744 - val_mse: 1482.3745 - val_mae: 27.0344\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2457.9773 - mse: 2457.9771 - mae: 28.9014 - val_loss: 1481.0347 - val_mse: 1481.0348 - val_mae: 27.0519\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2409.0967 - mse: 2409.0964 - mae: 29.0190 - val_loss: 1489.0567 - val_mse: 1489.0565 - val_mae: 26.9366\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2446.8526 - mse: 2446.8525 - mae: 29.0329 - val_loss: 1482.7431 - val_mse: 1482.7430 - val_mae: 27.0309\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2515.1258 - mse: 2515.1260 - mae: 29.0047 - val_loss: 1481.9375 - val_mse: 1481.9375 - val_mae: 27.0395\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2472.8115 - mse: 2472.8105 - mae: 29.1216 - val_loss: 1483.3390 - val_mse: 1483.3391 - val_mae: 26.9628\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2452.4368 - mse: 2452.4363 - mae: 29.4323 - val_loss: 1480.2302 - val_mse: 1480.2303 - val_mae: 26.9885\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 271us/step - loss: 2469.9297 - mse: 2469.9297 - mae: 28.7200 - val_loss: 1489.9285 - val_mse: 1489.9285 - val_mae: 26.7023\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2518.8421 - mse: 2518.8418 - mae: 29.3163 - val_loss: 1480.8250 - val_mse: 1480.8251 - val_mae: 26.9196\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2493.3930 - mse: 2493.3936 - mae: 28.6020 - val_loss: 1476.3686 - val_mse: 1476.3687 - val_mae: 27.0400\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2470.0492 - mse: 2470.0488 - mae: 28.9116 - val_loss: 1487.1150 - val_mse: 1487.1150 - val_mae: 26.7367\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2462.6568 - mse: 2462.6560 - mae: 29.0977 - val_loss: 1476.9198 - val_mse: 1476.9198 - val_mae: 27.0268\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 263us/step - loss: 2397.7553 - mse: 2397.7559 - mae: 28.4021 - val_loss: 1470.7196 - val_mse: 1470.7195 - val_mae: 27.1899\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2484.8207 - mse: 2484.8201 - mae: 29.4284 - val_loss: 1484.8898 - val_mse: 1484.8899 - val_mae: 26.8074\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2459.3499 - mse: 2459.3501 - mae: 28.5878 - val_loss: 1476.2514 - val_mse: 1476.2515 - val_mae: 27.0243\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2388.8969 - mse: 2388.8965 - mae: 28.4802 - val_loss: 1479.7912 - val_mse: 1479.7913 - val_mae: 26.9357\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 351us/step - loss: 2435.5809 - mse: 2435.5808 - mae: 29.2626 - val_loss: 1479.9768 - val_mse: 1479.9768 - val_mae: 26.9239\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 268us/step - loss: 2486.2570 - mse: 2486.2581 - mae: 29.0018 - val_loss: 1470.2374 - val_mse: 1470.2375 - val_mae: 27.1729\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2547.3454 - mse: 2547.3462 - mae: 28.9264 - val_loss: 1481.0635 - val_mse: 1481.0636 - val_mae: 26.9180\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 263us/step - loss: 2495.1957 - mse: 2495.1960 - mae: 28.8859 - val_loss: 1475.2309 - val_mse: 1475.2307 - val_mae: 27.0452\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 340us/step - loss: 2474.0032 - mse: 2474.0034 - mae: 28.9830 - val_loss: 1476.0172 - val_mse: 1476.0172 - val_mae: 27.0697\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2472.8982 - mse: 2472.8977 - mae: 28.9618 - val_loss: 1473.2651 - val_mse: 1473.2653 - val_mae: 27.0654\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2465.3318 - mse: 2465.3318 - mae: 29.2817 - val_loss: 1472.9840 - val_mse: 1472.9840 - val_mae: 27.0528\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2436.9098 - mse: 2436.9109 - mae: 28.9937 - val_loss: 1474.9317 - val_mse: 1474.9315 - val_mae: 27.0302\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2442.5436 - mse: 2442.5439 - mae: 28.7914 - val_loss: 1471.6863 - val_mse: 1471.6863 - val_mae: 27.1285\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2474.4081 - mse: 2474.4077 - mae: 29.2002 - val_loss: 1473.5464 - val_mse: 1473.5465 - val_mae: 27.0536\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2469.3767 - mse: 2469.3762 - mae: 29.0081 - val_loss: 1474.9759 - val_mse: 1474.9760 - val_mae: 27.0419\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2441.0498 - mse: 2441.0488 - mae: 28.6358 - val_loss: 1471.4655 - val_mse: 1471.4656 - val_mae: 27.1393\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2397.9239 - mse: 2397.9236 - mae: 28.4553 - val_loss: 1467.1086 - val_mse: 1467.1086 - val_mae: 27.2978\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2496.8960 - mse: 2496.8965 - mae: 29.1923 - val_loss: 1474.2583 - val_mse: 1474.2582 - val_mae: 27.0560\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2435.8433 - mse: 2435.8433 - mae: 29.2533 - val_loss: 1477.5129 - val_mse: 1477.5132 - val_mae: 26.8528\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2557.9604 - mse: 2557.9600 - mae: 29.1572 - val_loss: 1479.0608 - val_mse: 1479.0608 - val_mae: 26.8675\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2436.8968 - mse: 2436.8977 - mae: 28.7554 - val_loss: 1473.6072 - val_mse: 1473.6072 - val_mae: 26.9321\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2404.2510 - mse: 2404.2502 - mae: 28.4648 - val_loss: 1469.8825 - val_mse: 1469.8823 - val_mae: 27.1741\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2438.0627 - mse: 2438.0627 - mae: 28.7140 - val_loss: 1477.3979 - val_mse: 1477.3977 - val_mae: 26.9273\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2323.3683 - mse: 2323.3689 - mae: 29.1393 - val_loss: 3691.2427 - val_mse: 3691.2429 - val_mae: 24.3260\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2349.7540 - mse: 2349.7544 - mae: 29.2496 - val_loss: 3691.7292 - val_mse: 3691.7295 - val_mae: 24.3759\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2357.6544 - mse: 2357.6550 - mae: 29.4674 - val_loss: 3693.0124 - val_mse: 3693.0125 - val_mae: 24.6058\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2339.1908 - mse: 2339.1912 - mae: 28.9528 - val_loss: 3691.0158 - val_mse: 3691.0154 - val_mae: 24.1967\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2344.2977 - mse: 2344.2976 - mae: 28.8108 - val_loss: 3695.4350 - val_mse: 3695.4348 - val_mae: 24.8930\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2363.0040 - mse: 2363.0039 - mae: 29.4753 - val_loss: 3692.2538 - val_mse: 3692.2539 - val_mae: 24.4360\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 367us/step - loss: 2351.6622 - mse: 2351.6631 - mae: 29.2959 - val_loss: 3691.4509 - val_mse: 3691.4502 - val_mae: 24.4060\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 365us/step - loss: 2287.8608 - mse: 2287.8613 - mae: 28.9897 - val_loss: 3690.4521 - val_mse: 3690.4519 - val_mae: 24.2140\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2372.0028 - mse: 2372.0029 - mae: 29.5798 - val_loss: 3691.7450 - val_mse: 3691.7461 - val_mae: 24.3239\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2351.1771 - mse: 2351.1780 - mae: 29.5888 - val_loss: 3694.4258 - val_mse: 3694.4258 - val_mae: 24.7600\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2303.2371 - mse: 2303.2373 - mae: 28.8881 - val_loss: 3693.5491 - val_mse: 3693.5493 - val_mae: 24.6468\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 339us/step - loss: 2367.2637 - mse: 2367.2639 - mae: 29.2839 - val_loss: 3691.6174 - val_mse: 3691.6167 - val_mae: 24.2510\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2367.9309 - mse: 2367.9316 - mae: 29.3967 - val_loss: 3690.4068 - val_mse: 3690.4067 - val_mae: 24.1204\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2345.9218 - mse: 2345.9221 - mae: 29.2665 - val_loss: 3694.0048 - val_mse: 3694.0044 - val_mae: 24.6503\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2308.2266 - mse: 2308.2266 - mae: 28.9389 - val_loss: 3695.2803 - val_mse: 3695.2800 - val_mae: 24.7861\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2290.9911 - mse: 2290.9912 - mae: 29.0644 - val_loss: 3694.7520 - val_mse: 3694.7520 - val_mae: 24.6346\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2330.0421 - mse: 2330.0422 - mae: 29.1901 - val_loss: 3690.9751 - val_mse: 3690.9756 - val_mae: 23.8385\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2302.3184 - mse: 2302.3181 - mae: 28.9567 - val_loss: 3690.9606 - val_mse: 3690.9604 - val_mae: 23.9370\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 366us/step - loss: 2347.8889 - mse: 2347.8884 - mae: 29.0503 - val_loss: 3693.2005 - val_mse: 3693.1997 - val_mae: 24.4624\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2380.9673 - mse: 2380.9668 - mae: 29.2798 - val_loss: 3694.0439 - val_mse: 3694.0439 - val_mae: 24.6181\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2300.7009 - mse: 2300.7014 - mae: 28.9183 - val_loss: 3694.9809 - val_mse: 3694.9819 - val_mae: 24.6760\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2341.3104 - mse: 2341.3108 - mae: 28.7982 - val_loss: 3692.7930 - val_mse: 3692.7942 - val_mae: 24.4048\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2364.9900 - mse: 2364.9890 - mae: 29.2648 - val_loss: 3692.3271 - val_mse: 3692.3276 - val_mae: 24.3263\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2381.4911 - mse: 2381.4910 - mae: 29.4721 - val_loss: 3691.6100 - val_mse: 3691.6104 - val_mae: 24.1952\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2344.0383 - mse: 2344.0386 - mae: 28.9035 - val_loss: 3694.4934 - val_mse: 3694.4939 - val_mae: 24.7128\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2308.2532 - mse: 2308.2537 - mae: 28.9671 - val_loss: 3692.8967 - val_mse: 3692.8962 - val_mae: 24.4845\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2407.0515 - mse: 2407.0515 - mae: 29.4956 - val_loss: 3692.6862 - val_mse: 3692.6865 - val_mae: 24.4589\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2359.6454 - mse: 2359.6445 - mae: 29.0700 - val_loss: 3693.6345 - val_mse: 3693.6348 - val_mae: 24.5567\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2283.0882 - mse: 2283.0881 - mae: 29.0850 - val_loss: 3694.2917 - val_mse: 3694.2915 - val_mae: 24.5971\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2357.2755 - mse: 2357.2749 - mae: 29.1273 - val_loss: 3692.1652 - val_mse: 3692.1658 - val_mae: 24.3007\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2318.7469 - mse: 2318.7468 - mae: 28.8365 - val_loss: 3692.6032 - val_mse: 3692.6023 - val_mae: 24.3998\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2344.2216 - mse: 2344.2222 - mae: 28.8750 - val_loss: 3694.7148 - val_mse: 3694.7151 - val_mae: 24.7419\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2337.2785 - mse: 2337.2786 - mae: 28.8276 - val_loss: 3691.4318 - val_mse: 3691.4312 - val_mae: 24.2598\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2362.6269 - mse: 2362.6262 - mae: 29.2044 - val_loss: 3691.6465 - val_mse: 3691.6462 - val_mae: 24.2889\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2279.3985 - mse: 2279.3987 - mae: 29.0671 - val_loss: 3693.6610 - val_mse: 3693.6597 - val_mae: 24.7322\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 242us/step - loss: 2302.1351 - mse: 2302.1348 - mae: 28.8918 - val_loss: 3691.7656 - val_mse: 3691.7654 - val_mae: 24.4029\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2323.8224 - mse: 2323.8225 - mae: 28.5904 - val_loss: 3691.0471 - val_mse: 3691.0476 - val_mae: 24.2294\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2347.7161 - mse: 2347.7148 - mae: 29.1117 - val_loss: 3691.4852 - val_mse: 3691.4854 - val_mae: 24.3196\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2280.4945 - mse: 2280.4954 - mae: 28.7546 - val_loss: 3693.1480 - val_mse: 3693.1477 - val_mae: 24.4932\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 330us/step - loss: 2300.9148 - mse: 2300.9146 - mae: 29.1796 - val_loss: 3691.7155 - val_mse: 3691.7148 - val_mae: 24.2498\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2306.3051 - mse: 2306.3049 - mae: 28.9122 - val_loss: 3691.4960 - val_mse: 3691.4954 - val_mae: 24.2316\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2305.5367 - mse: 2305.5371 - mae: 28.7736 - val_loss: 3692.2025 - val_mse: 3692.2029 - val_mae: 24.3513\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2355.9573 - mse: 2355.9568 - mae: 28.6711 - val_loss: 3690.5344 - val_mse: 3690.5344 - val_mae: 24.1085\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2348.0015 - mse: 2348.0015 - mae: 29.1928 - val_loss: 3690.8861 - val_mse: 3690.8872 - val_mae: 23.9663\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2293.0228 - mse: 2293.0227 - mae: 28.4732 - val_loss: 3692.6900 - val_mse: 3692.6902 - val_mae: 24.2750\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2316.6765 - mse: 2316.6763 - mae: 29.0374 - val_loss: 3691.9888 - val_mse: 3691.9890 - val_mae: 24.3358\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2305.7157 - mse: 2305.7156 - mae: 28.9528 - val_loss: 3697.0875 - val_mse: 3697.0876 - val_mae: 25.0679\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2298.0237 - mse: 2298.0239 - mae: 29.0494 - val_loss: 3691.2688 - val_mse: 3691.2688 - val_mae: 24.1875\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2326.3579 - mse: 2326.3574 - mae: 28.6322 - val_loss: 3696.2354 - val_mse: 3696.2356 - val_mae: 24.9712\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2337.2855 - mse: 2337.2852 - mae: 29.1937 - val_loss: 3690.9616 - val_mse: 3690.9622 - val_mae: 24.0113\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2329.0293 - mse: 2329.0291 - mae: 28.9792 - val_loss: 3690.3919 - val_mse: 3690.3923 - val_mae: 24.1573\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2306.0504 - mse: 2306.0508 - mae: 28.9583 - val_loss: 3691.2739 - val_mse: 3691.2744 - val_mae: 24.2383\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 342us/step - loss: 2328.3226 - mse: 2328.3218 - mae: 29.0548 - val_loss: 3693.1304 - val_mse: 3693.1301 - val_mae: 24.6350\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 376us/step - loss: 2280.7420 - mse: 2280.7422 - mae: 28.7604 - val_loss: 3693.7283 - val_mse: 3693.7285 - val_mae: 24.6557\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 343us/step - loss: 2292.9579 - mse: 2292.9583 - mae: 28.7377 - val_loss: 3695.3125 - val_mse: 3695.3130 - val_mae: 24.9161\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 351us/step - loss: 2355.5725 - mse: 2355.5723 - mae: 29.0349 - val_loss: 3690.7278 - val_mse: 3690.7266 - val_mae: 24.3880\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2278.5719 - mse: 2278.5715 - mae: 29.0805 - val_loss: 3691.4903 - val_mse: 3691.4902 - val_mae: 24.4294\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 345us/step - loss: 2292.1052 - mse: 2292.1052 - mae: 28.7316 - val_loss: 3689.8699 - val_mse: 3689.8701 - val_mae: 24.1952\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2307.7411 - mse: 2307.7410 - mae: 28.7836 - val_loss: 3693.5692 - val_mse: 3693.5696 - val_mae: 24.7742\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2298.2451 - mse: 2298.2458 - mae: 28.5253 - val_loss: 3697.1373 - val_mse: 3697.1375 - val_mae: 25.1213\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 357us/step - loss: 2309.8238 - mse: 2309.8242 - mae: 28.6832 - val_loss: 3695.1688 - val_mse: 3695.1689 - val_mae: 24.8083\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2324.5676 - mse: 2324.5684 - mae: 29.0284 - val_loss: 3692.1977 - val_mse: 3692.1982 - val_mae: 24.3020\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2309.1579 - mse: 2309.1572 - mae: 28.8836 - val_loss: 3694.7526 - val_mse: 3694.7520 - val_mae: 24.6741\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2329.7345 - mse: 2329.7349 - mae: 29.1558 - val_loss: 3692.2930 - val_mse: 3692.2925 - val_mae: 24.5149\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2267.5947 - mse: 2267.5952 - mae: 28.4674 - val_loss: 3692.6701 - val_mse: 3692.6704 - val_mae: 24.6455\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2322.7280 - mse: 2322.7290 - mae: 29.1907 - val_loss: 3692.1233 - val_mse: 3692.1235 - val_mae: 24.5564\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2353.8559 - mse: 2353.8557 - mae: 29.1434 - val_loss: 3692.3041 - val_mse: 3692.3044 - val_mae: 24.4982\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2377.2740 - mse: 2377.2747 - mae: 28.9291 - val_loss: 3690.2588 - val_mse: 3690.2590 - val_mae: 23.9691\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2286.1051 - mse: 2286.1055 - mae: 28.9453 - val_loss: 3691.5795 - val_mse: 3691.5796 - val_mae: 24.4217\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2310.4287 - mse: 2310.4282 - mae: 29.0642 - val_loss: 3690.8928 - val_mse: 3690.8928 - val_mae: 24.1366\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2326.4536 - mse: 2326.4539 - mae: 28.7002 - val_loss: 3692.6972 - val_mse: 3692.6987 - val_mae: 24.3835\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2355.8248 - mse: 2355.8242 - mae: 29.3162 - val_loss: 3693.5979 - val_mse: 3693.5981 - val_mae: 24.5563\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2319.0865 - mse: 2319.0864 - mae: 28.9560 - val_loss: 3696.1639 - val_mse: 3696.1636 - val_mae: 24.8633\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2306.7663 - mse: 2306.7649 - mae: 28.7878 - val_loss: 3694.6554 - val_mse: 3694.6553 - val_mae: 24.5471\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 252us/step - loss: 2304.0434 - mse: 2304.0430 - mae: 28.7664 - val_loss: 3695.2833 - val_mse: 3695.2842 - val_mae: 24.6526\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 267us/step - loss: 2247.6270 - mse: 2247.6267 - mae: 28.4154 - val_loss: 3700.9867 - val_mse: 3700.9873 - val_mae: 25.3433\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2289.9378 - mse: 2289.9370 - mae: 28.9431 - val_loss: 3694.3294 - val_mse: 3694.3301 - val_mae: 24.7314\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2312.8772 - mse: 2312.8772 - mae: 28.5626 - val_loss: 3693.5166 - val_mse: 3693.5164 - val_mae: 24.5648\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2249.4815 - mse: 2249.4827 - mae: 28.7327 - val_loss: 3693.7325 - val_mse: 3693.7322 - val_mae: 24.5434\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2304.5695 - mse: 2304.5698 - mae: 28.5841 - val_loss: 3694.0139 - val_mse: 3694.0144 - val_mae: 24.7540\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2686.9902 - mse: 2686.9907 - mae: 28.6524 - val_loss: 2174.4933 - val_mse: 2174.4932 - val_mae: 26.3709\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2720.7854 - mse: 2720.7856 - mae: 28.3608 - val_loss: 2171.6417 - val_mse: 2171.6416 - val_mae: 26.6591\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2644.0692 - mse: 2644.0693 - mae: 28.1802 - val_loss: 2175.6670 - val_mse: 2175.6667 - val_mae: 26.7668\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2720.5476 - mse: 2720.5476 - mae: 28.4716 - val_loss: 2175.6377 - val_mse: 2175.6377 - val_mae: 26.9426\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2705.1040 - mse: 2705.1035 - mae: 28.4586 - val_loss: 2184.5971 - val_mse: 2184.5974 - val_mae: 26.5041\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2611.1261 - mse: 2611.1260 - mae: 27.4612 - val_loss: 2176.4334 - val_mse: 2176.4331 - val_mae: 26.9160\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2683.2914 - mse: 2683.2915 - mae: 28.2970 - val_loss: 2181.8743 - val_mse: 2181.8743 - val_mae: 26.7832\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2638.0303 - mse: 2638.0300 - mae: 27.9298 - val_loss: 2187.9893 - val_mse: 2187.9893 - val_mae: 26.5402\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2688.2825 - mse: 2688.2820 - mae: 28.2103 - val_loss: 2182.3137 - val_mse: 2182.3132 - val_mae: 26.5621\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2735.5248 - mse: 2735.5251 - mae: 28.6051 - val_loss: 2181.4671 - val_mse: 2181.4673 - val_mae: 26.7247\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2690.9534 - mse: 2690.9529 - mae: 28.2223 - val_loss: 2183.1007 - val_mse: 2183.1003 - val_mae: 26.4677\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2679.8281 - mse: 2679.8279 - mae: 27.8769 - val_loss: 2172.8646 - val_mse: 2172.8645 - val_mae: 26.5236\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2705.9912 - mse: 2705.9905 - mae: 28.0814 - val_loss: 2190.1671 - val_mse: 2190.1670 - val_mae: 26.2101\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2682.6200 - mse: 2682.6199 - mae: 27.9763 - val_loss: 2182.3488 - val_mse: 2182.3489 - val_mae: 26.2908\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2666.4011 - mse: 2666.4014 - mae: 28.1705 - val_loss: 2176.9172 - val_mse: 2176.9175 - val_mae: 26.7314\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2672.8435 - mse: 2672.8435 - mae: 28.3290 - val_loss: 2184.3535 - val_mse: 2184.3538 - val_mae: 26.8103\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2665.0398 - mse: 2665.0405 - mae: 28.0819 - val_loss: 2194.1453 - val_mse: 2194.1453 - val_mae: 26.7421\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2680.9800 - mse: 2680.9807 - mae: 28.2659 - val_loss: 2188.3914 - val_mse: 2188.3914 - val_mae: 26.9444\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2650.4742 - mse: 2650.4746 - mae: 28.3328 - val_loss: 2195.1052 - val_mse: 2195.1050 - val_mae: 26.5379\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2711.8204 - mse: 2711.8198 - mae: 28.3683 - val_loss: 2185.4394 - val_mse: 2185.4395 - val_mae: 26.8323\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2678.0509 - mse: 2678.0510 - mae: 28.1965 - val_loss: 2191.6011 - val_mse: 2191.6013 - val_mae: 26.3821\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2676.2050 - mse: 2676.2043 - mae: 27.8009 - val_loss: 2184.7251 - val_mse: 2184.7249 - val_mae: 26.7464\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2678.5105 - mse: 2678.5095 - mae: 28.0467 - val_loss: 2184.9247 - val_mse: 2184.9246 - val_mae: 26.8791\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2707.8642 - mse: 2707.8643 - mae: 28.4246 - val_loss: 2193.8075 - val_mse: 2193.8071 - val_mae: 26.6538\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2682.9803 - mse: 2682.9790 - mae: 28.3014 - val_loss: 2197.6142 - val_mse: 2197.6140 - val_mae: 26.5375\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2693.6397 - mse: 2693.6396 - mae: 28.4160 - val_loss: 2198.8343 - val_mse: 2198.8342 - val_mae: 26.5880\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 329us/step - loss: 2714.6583 - mse: 2714.6580 - mae: 28.1753 - val_loss: 2197.4239 - val_mse: 2197.4238 - val_mae: 27.0358\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2700.0720 - mse: 2700.0718 - mae: 28.1770 - val_loss: 2204.6778 - val_mse: 2204.6780 - val_mae: 26.7459\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2711.0067 - mse: 2711.0071 - mae: 28.2906 - val_loss: 2208.7072 - val_mse: 2208.7070 - val_mae: 26.7171\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2722.1732 - mse: 2722.1738 - mae: 28.4033 - val_loss: 2207.7347 - val_mse: 2207.7341 - val_mae: 26.6197\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2707.5072 - mse: 2707.5076 - mae: 28.4443 - val_loss: 2218.8321 - val_mse: 2218.8320 - val_mae: 26.5503\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2675.9483 - mse: 2675.9480 - mae: 28.2344 - val_loss: 2208.0614 - val_mse: 2208.0618 - val_mae: 26.7234\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2664.3363 - mse: 2664.3362 - mae: 28.2015 - val_loss: 2198.0516 - val_mse: 2198.0515 - val_mae: 26.8821\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2676.9229 - mse: 2676.9236 - mae: 28.2438 - val_loss: 2212.7379 - val_mse: 2212.7380 - val_mae: 26.6529\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 263us/step - loss: 2668.8694 - mse: 2668.8689 - mae: 28.0621 - val_loss: 2210.3352 - val_mse: 2210.3354 - val_mae: 26.6350\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2688.0208 - mse: 2688.0210 - mae: 28.0639 - val_loss: 2205.4837 - val_mse: 2205.4839 - val_mae: 26.9519\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2735.2759 - mse: 2735.2754 - mae: 28.3571 - val_loss: 2215.7259 - val_mse: 2215.7258 - val_mae: 26.3712\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2711.2776 - mse: 2711.2776 - mae: 28.2805 - val_loss: 2198.5133 - val_mse: 2198.5134 - val_mae: 26.8220\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2696.1744 - mse: 2696.1753 - mae: 28.2447 - val_loss: 2206.8243 - val_mse: 2206.8245 - val_mae: 26.5871\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2656.1862 - mse: 2656.1855 - mae: 28.3488 - val_loss: 2203.1133 - val_mse: 2203.1133 - val_mae: 26.7493\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2707.2736 - mse: 2707.2732 - mae: 28.3077 - val_loss: 2213.3151 - val_mse: 2213.3152 - val_mae: 26.6472\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2674.2546 - mse: 2674.2539 - mae: 27.9862 - val_loss: 2202.0033 - val_mse: 2202.0034 - val_mae: 26.9038\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2684.7353 - mse: 2684.7366 - mae: 28.3394 - val_loss: 2206.0046 - val_mse: 2206.0044 - val_mae: 27.0277\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2737.7139 - mse: 2737.7139 - mae: 28.8137 - val_loss: 2222.7352 - val_mse: 2222.7351 - val_mae: 26.5374\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 266us/step - loss: 2659.8590 - mse: 2659.8589 - mae: 27.9768 - val_loss: 2210.4225 - val_mse: 2210.4226 - val_mae: 26.9041\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2720.1689 - mse: 2720.1685 - mae: 28.2568 - val_loss: 2211.9309 - val_mse: 2211.9312 - val_mae: 26.9538\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2671.6847 - mse: 2671.6848 - mae: 27.8835 - val_loss: 2215.8088 - val_mse: 2215.8086 - val_mae: 26.7999\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2675.7018 - mse: 2675.7021 - mae: 28.1434 - val_loss: 2208.3344 - val_mse: 2208.3345 - val_mae: 26.7548\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2678.5824 - mse: 2678.5828 - mae: 27.8993 - val_loss: 2204.5945 - val_mse: 2204.5945 - val_mae: 27.0800\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2653.9295 - mse: 2653.9292 - mae: 28.2646 - val_loss: 2213.5174 - val_mse: 2213.5171 - val_mae: 26.6799\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2693.2421 - mse: 2693.2417 - mae: 28.3443 - val_loss: 2217.3330 - val_mse: 2217.3330 - val_mae: 26.8215\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2643.7469 - mse: 2643.7466 - mae: 27.7356 - val_loss: 2218.3368 - val_mse: 2218.3362 - val_mae: 27.0067\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2687.8463 - mse: 2687.8467 - mae: 28.3877 - val_loss: 2219.4556 - val_mse: 2219.4556 - val_mae: 26.7523\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2628.3845 - mse: 2628.3843 - mae: 27.7857 - val_loss: 2204.7577 - val_mse: 2204.7578 - val_mae: 26.8997\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2683.2370 - mse: 2683.2371 - mae: 27.9825 - val_loss: 2209.3230 - val_mse: 2209.3232 - val_mae: 26.5540\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2681.4144 - mse: 2681.4143 - mae: 28.1163 - val_loss: 2206.3905 - val_mse: 2206.3904 - val_mae: 26.6746\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2642.4304 - mse: 2642.4299 - mae: 27.9790 - val_loss: 2201.7075 - val_mse: 2201.7075 - val_mae: 26.7704\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 341us/step - loss: 2631.7984 - mse: 2631.7981 - mae: 27.6612 - val_loss: 2205.0450 - val_mse: 2205.0449 - val_mae: 26.6766\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2662.7237 - mse: 2662.7239 - mae: 27.8247 - val_loss: 2198.8982 - val_mse: 2198.8987 - val_mae: 26.9458\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2658.6871 - mse: 2658.6863 - mae: 27.9868 - val_loss: 2207.6695 - val_mse: 2207.6697 - val_mae: 26.7745\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2663.7649 - mse: 2663.7644 - mae: 28.2455 - val_loss: 2206.6241 - val_mse: 2206.6243 - val_mae: 26.7195\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 249us/step - loss: 2635.1553 - mse: 2635.1553 - mae: 27.9225 - val_loss: 2203.6787 - val_mse: 2203.6790 - val_mae: 26.9589\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2646.2173 - mse: 2646.2178 - mae: 27.9946 - val_loss: 2197.0156 - val_mse: 2197.0151 - val_mae: 27.0349\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2617.3482 - mse: 2617.3491 - mae: 28.0298 - val_loss: 2202.1883 - val_mse: 2202.1885 - val_mae: 26.5472\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2700.5577 - mse: 2700.5579 - mae: 28.2154 - val_loss: 2199.5335 - val_mse: 2199.5337 - val_mae: 26.7455\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2655.5212 - mse: 2655.5210 - mae: 28.1514 - val_loss: 2199.3716 - val_mse: 2199.3716 - val_mae: 26.6960\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2685.5205 - mse: 2685.5203 - mae: 28.2105 - val_loss: 2211.4359 - val_mse: 2211.4360 - val_mae: 26.4937\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2643.3850 - mse: 2643.3853 - mae: 27.8397 - val_loss: 2214.1891 - val_mse: 2214.1895 - val_mae: 26.5458\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2657.0971 - mse: 2657.0977 - mae: 28.0289 - val_loss: 2198.7467 - val_mse: 2198.7468 - val_mae: 26.8126\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2678.5872 - mse: 2678.5872 - mae: 28.0299 - val_loss: 2200.5640 - val_mse: 2200.5640 - val_mae: 26.6939\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2689.7554 - mse: 2689.7556 - mae: 28.0108 - val_loss: 2208.4023 - val_mse: 2208.4028 - val_mae: 26.8256\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2674.4458 - mse: 2674.4456 - mae: 28.1619 - val_loss: 2211.5360 - val_mse: 2211.5361 - val_mae: 26.6995\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2685.4831 - mse: 2685.4832 - mae: 28.2066 - val_loss: 2213.4518 - val_mse: 2213.4519 - val_mae: 26.7195\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2646.8033 - mse: 2646.8040 - mae: 28.0380 - val_loss: 2214.7810 - val_mse: 2214.7810 - val_mae: 26.9138\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2650.4839 - mse: 2650.4844 - mae: 27.9157 - val_loss: 2212.4367 - val_mse: 2212.4368 - val_mae: 26.7460\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2643.3111 - mse: 2643.3108 - mae: 27.9572 - val_loss: 2211.2606 - val_mse: 2211.2605 - val_mae: 26.9419\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2628.6951 - mse: 2628.6946 - mae: 27.5333 - val_loss: 2196.6712 - val_mse: 2196.6714 - val_mae: 27.2859\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2685.0443 - mse: 2685.0432 - mae: 28.0550 - val_loss: 2192.4194 - val_mse: 2192.4194 - val_mae: 26.8295\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2630.5081 - mse: 2630.5081 - mae: 27.8940 - val_loss: 2189.9673 - val_mse: 2189.9673 - val_mae: 27.0584\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2686.3336 - mse: 2686.3337 - mae: 28.0283 - val_loss: 2188.9314 - val_mse: 2188.9314 - val_mae: 26.8641\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13266.6048 - mse: 13266.6035 - mae: 109.6220 - val_loss: 34411.0440 - val_mse: 34411.0469 - val_mae: 131.9551\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 388us/step - loss: 12864.9377 - mse: 12864.9395 - mae: 107.8080 - val_loss: 33508.6233 - val_mse: 33508.6211 - val_mae: 128.5228\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 11610.2621 - mse: 11610.2607 - mae: 101.8482 - val_loss: 30876.7352 - val_mse: 30876.7344 - val_mae: 117.9514\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 8680.6969 - mse: 8680.6973 - mae: 85.9714 - val_loss: 24855.2929 - val_mse: 24855.2949 - val_mae: 89.2039\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 4153.9722 - mse: 4153.9722 - mae: 51.0136 - val_loss: 17555.4361 - val_mse: 17555.4355 - val_mae: 36.0699\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 256us/step - loss: 2544.7628 - mse: 2544.7625 - mae: 36.7526 - val_loss: 16992.7029 - val_mse: 16992.7012 - val_mae: 36.4059\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 258us/step - loss: 2356.2822 - mse: 2356.2820 - mae: 34.3513 - val_loss: 17190.1780 - val_mse: 17190.1777 - val_mae: 35.8534\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 379us/step - loss: 2320.2105 - mse: 2320.2102 - mae: 34.2517 - val_loss: 17190.9002 - val_mse: 17190.9004 - val_mae: 35.8660\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2440.6853 - mse: 2440.6853 - mae: 35.6645 - val_loss: 17124.7079 - val_mse: 17124.7070 - val_mae: 35.9590\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 342us/step - loss: 2472.1014 - mse: 2472.1016 - mae: 35.8273 - val_loss: 17151.1810 - val_mse: 17151.1816 - val_mae: 35.9294\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 386us/step - loss: 2388.4015 - mse: 2388.4016 - mae: 35.2891 - val_loss: 17170.6501 - val_mse: 17170.6504 - val_mae: 35.9141\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 385us/step - loss: 2601.1310 - mse: 2601.1309 - mae: 36.8181 - val_loss: 17262.0583 - val_mse: 17262.0586 - val_mae: 35.8600\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 342us/step - loss: 2375.5803 - mse: 2375.5803 - mae: 34.5651 - val_loss: 17089.7169 - val_mse: 17089.7188 - val_mae: 36.0901\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 360us/step - loss: 2450.2629 - mse: 2450.2627 - mae: 34.7246 - val_loss: 17176.6941 - val_mse: 17176.6934 - val_mae: 35.9575\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 263us/step - loss: 2279.3975 - mse: 2279.3975 - mae: 32.7519 - val_loss: 17237.5177 - val_mse: 17237.5176 - val_mae: 35.9305\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2310.2955 - mse: 2310.2954 - mae: 34.5462 - val_loss: 17244.6129 - val_mse: 17244.6133 - val_mae: 35.9412\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 385us/step - loss: 2346.7889 - mse: 2346.7888 - mae: 34.2442 - val_loss: 17075.9834 - val_mse: 17075.9844 - val_mae: 36.1863\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 2400.2537 - mse: 2400.2539 - mae: 35.1078 - val_loss: 17213.5022 - val_mse: 17213.5039 - val_mae: 35.9853\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 274us/step - loss: 2385.0657 - mse: 2385.0654 - mae: 34.8553 - val_loss: 17244.6238 - val_mse: 17244.6270 - val_mae: 35.9715\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2191.4612 - mse: 2191.4612 - mae: 32.9051 - val_loss: 17046.2848 - val_mse: 17046.2832 - val_mae: 36.3273\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 387us/step - loss: 2288.2684 - mse: 2288.2686 - mae: 33.8162 - val_loss: 17232.8414 - val_mse: 17232.8398 - val_mae: 35.9871\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 387us/step - loss: 2147.8115 - mse: 2147.8115 - mae: 31.7875 - val_loss: 17017.3836 - val_mse: 17017.3848 - val_mae: 36.4732\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 329us/step - loss: 2383.1761 - mse: 2383.1760 - mae: 33.7235 - val_loss: 17112.9443 - val_mse: 17112.9434 - val_mae: 36.1638\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2128.0781 - mse: 2128.0784 - mae: 32.5484 - val_loss: 17108.3818 - val_mse: 17108.3828 - val_mae: 36.1905\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 283us/step - loss: 2180.5717 - mse: 2180.5718 - mae: 32.6980 - val_loss: 17047.2027 - val_mse: 17047.2012 - val_mae: 36.3797\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 362us/step - loss: 2347.6022 - mse: 2347.6021 - mae: 34.3652 - val_loss: 17081.9489 - val_mse: 17081.9473 - val_mae: 36.2829\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 258us/step - loss: 2203.8898 - mse: 2203.8896 - mae: 32.9614 - val_loss: 17141.6800 - val_mse: 17141.6797 - val_mae: 36.1650\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 255us/step - loss: 2057.6458 - mse: 2057.6460 - mae: 31.6137 - val_loss: 17078.2431 - val_mse: 17078.2422 - val_mae: 36.3182\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 236us/step - loss: 2309.7516 - mse: 2309.7515 - mae: 33.2571 - val_loss: 17251.2496 - val_mse: 17251.2500 - val_mae: 36.0827\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 262us/step - loss: 2253.0226 - mse: 2253.0225 - mae: 34.0966 - val_loss: 17160.7456 - val_mse: 17160.7461 - val_mae: 36.1802\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 355us/step - loss: 2092.4685 - mse: 2092.4685 - mae: 32.9009 - val_loss: 17064.4696 - val_mse: 17064.4727 - val_mae: 36.3999\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 287us/step - loss: 2003.7335 - mse: 2003.7335 - mae: 32.1509 - val_loss: 16996.4025 - val_mse: 16996.4023 - val_mae: 36.6973\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2206.5139 - mse: 2206.5137 - mae: 33.3625 - val_loss: 17129.4010 - val_mse: 17129.4023 - val_mae: 36.2767\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 2059.1895 - mse: 2059.1897 - mae: 32.5192 - val_loss: 17022.9857 - val_mse: 17022.9863 - val_mae: 36.5926\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 406us/step - loss: 2171.3723 - mse: 2171.3726 - mae: 32.5052 - val_loss: 17104.2810 - val_mse: 17104.2812 - val_mae: 36.3571\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 397us/step - loss: 2206.0585 - mse: 2206.0588 - mae: 31.9599 - val_loss: 17060.9358 - val_mse: 17060.9375 - val_mae: 36.4847\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 268us/step - loss: 2147.5508 - mse: 2147.5508 - mae: 32.9783 - val_loss: 17028.3849 - val_mse: 17028.3848 - val_mae: 36.6091\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 1973.7627 - mse: 1973.7626 - mae: 31.6658 - val_loss: 17044.2145 - val_mse: 17044.2168 - val_mae: 36.5597\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 328us/step - loss: 2203.5983 - mse: 2203.5981 - mae: 32.6438 - val_loss: 17153.0565 - val_mse: 17153.0566 - val_mae: 36.3176\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2046.7298 - mse: 2046.7296 - mae: 31.1613 - val_loss: 17051.8941 - val_mse: 17051.8945 - val_mae: 36.5639\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 353us/step - loss: 2051.6083 - mse: 2051.6084 - mae: 32.2470 - val_loss: 17147.5800 - val_mse: 17147.5801 - val_mae: 36.3505\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 413us/step - loss: 2008.0007 - mse: 2008.0007 - mae: 32.0926 - val_loss: 17091.2504 - val_mse: 17091.2500 - val_mae: 36.4866\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 408us/step - loss: 2067.7412 - mse: 2067.7412 - mae: 32.4518 - val_loss: 17062.6085 - val_mse: 17062.6074 - val_mae: 36.5724\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 395us/step - loss: 2046.6226 - mse: 2046.6227 - mae: 31.0059 - val_loss: 17160.1356 - val_mse: 17160.1367 - val_mae: 36.3551\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 337us/step - loss: 2148.1051 - mse: 2148.1055 - mae: 32.3743 - val_loss: 17090.9176 - val_mse: 17090.9180 - val_mae: 36.5178\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 2146.9300 - mse: 2146.9302 - mae: 32.5304 - val_loss: 17080.6491 - val_mse: 17080.6504 - val_mae: 36.5586\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2221.5425 - mse: 2221.5422 - mae: 32.5617 - val_loss: 17002.9640 - val_mse: 17002.9648 - val_mae: 36.8534\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 283us/step - loss: 1969.3172 - mse: 1969.3173 - mae: 31.4183 - val_loss: 17201.8830 - val_mse: 17201.8848 - val_mae: 36.3399\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 406us/step - loss: 2109.3468 - mse: 2109.3469 - mae: 31.9434 - val_loss: 17186.5225 - val_mse: 17186.5215 - val_mae: 36.3755\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 417us/step - loss: 1972.9093 - mse: 1972.9094 - mae: 29.8433 - val_loss: 17054.9659 - val_mse: 17054.9668 - val_mae: 36.6934\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 439us/step - loss: 2009.2007 - mse: 2009.2007 - mae: 31.5913 - val_loss: 17073.3572 - val_mse: 17073.3555 - val_mae: 36.6455\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 1984.7071 - mse: 1984.7072 - mae: 30.8972 - val_loss: 17172.6754 - val_mse: 17172.6758 - val_mae: 36.4335\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2004.1721 - mse: 2004.1722 - mae: 29.6758 - val_loss: 17165.6152 - val_mse: 17165.6133 - val_mae: 36.4565\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2190.6838 - mse: 2190.6838 - mae: 32.2602 - val_loss: 17166.4561 - val_mse: 17166.4551 - val_mae: 36.4671\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 1820.0420 - mse: 1820.0421 - mae: 30.1628 - val_loss: 17020.4025 - val_mse: 17020.4023 - val_mae: 36.8927\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 298us/step - loss: 2077.4795 - mse: 2077.4797 - mae: 31.6139 - val_loss: 17039.5143 - val_mse: 17039.5156 - val_mae: 36.8316\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 1838.6868 - mse: 1838.6866 - mae: 29.3550 - val_loss: 17128.3643 - val_mse: 17128.3652 - val_mae: 36.5759\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2073.1048 - mse: 2073.1047 - mae: 31.2230 - val_loss: 17260.8748 - val_mse: 17260.8730 - val_mae: 36.3788\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 360us/step - loss: 1914.3558 - mse: 1914.3560 - mae: 30.7919 - val_loss: 17045.0639 - val_mse: 17045.0645 - val_mae: 36.8507\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 1998.4524 - mse: 1998.4523 - mae: 30.4511 - val_loss: 17069.9853 - val_mse: 17069.9863 - val_mae: 36.7759\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2006.5495 - mse: 2006.5493 - mae: 30.1050 - val_loss: 17093.4059 - val_mse: 17093.4062 - val_mae: 36.7101\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2163.1561 - mse: 2163.1560 - mae: 31.7881 - val_loss: 17144.0499 - val_mse: 17144.0488 - val_mae: 36.5919\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 1913.3143 - mse: 1913.3142 - mae: 30.3439 - val_loss: 17125.5116 - val_mse: 17125.5117 - val_mae: 36.6422\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 335us/step - loss: 2009.0407 - mse: 2009.0408 - mae: 31.2416 - val_loss: 17155.3448 - val_mse: 17155.3457 - val_mae: 36.5784\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 1939.6751 - mse: 1939.6749 - mae: 30.1149 - val_loss: 17067.2190 - val_mse: 17067.2188 - val_mae: 36.8390\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2111.4958 - mse: 2111.4961 - mae: 29.9761 - val_loss: 17149.6160 - val_mse: 17149.6152 - val_mae: 36.6175\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 341us/step - loss: 1938.5111 - mse: 1938.5110 - mae: 29.2088 - val_loss: 17034.1930 - val_mse: 17034.1934 - val_mae: 36.9837\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 1907.8250 - mse: 1907.8252 - mae: 29.5771 - val_loss: 17025.0978 - val_mse: 17025.0977 - val_mae: 37.0302\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 1801.6361 - mse: 1801.6362 - mae: 29.0228 - val_loss: 17063.2571 - val_mse: 17063.2578 - val_mae: 36.9006\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 247us/step - loss: 1961.7792 - mse: 1961.7793 - mae: 29.6304 - val_loss: 17024.9284 - val_mse: 17024.9277 - val_mae: 37.0545\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 251us/step - loss: 1730.6558 - mse: 1730.6560 - mae: 28.7761 - val_loss: 17088.3797 - val_mse: 17088.3809 - val_mae: 36.8338\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 1790.2326 - mse: 1790.2327 - mae: 28.7898 - val_loss: 17110.2293 - val_mse: 17110.2305 - val_mae: 36.7715\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 1814.9990 - mse: 1814.9990 - mae: 28.3775 - val_loss: 17111.9888 - val_mse: 17111.9902 - val_mae: 36.7757\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 1996.0975 - mse: 1996.0978 - mae: 31.3014 - val_loss: 17178.7585 - val_mse: 17178.7598 - val_mae: 36.6265\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 1987.9137 - mse: 1987.9135 - mae: 29.6296 - val_loss: 17106.8299 - val_mse: 17106.8301 - val_mae: 36.8124\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 1911.4229 - mse: 1911.4230 - mae: 29.8458 - val_loss: 17108.9117 - val_mse: 17108.9121 - val_mae: 36.8167\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 1822.6915 - mse: 1822.6914 - mae: 29.6805 - val_loss: 17090.8190 - val_mse: 17090.8184 - val_mae: 36.8826\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 384us/step - loss: 1907.8065 - mse: 1907.8066 - mae: 29.9705 - val_loss: 17036.4999 - val_mse: 17036.4980 - val_mae: 37.0868\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 355us/step - loss: 1830.7965 - mse: 1830.7964 - mae: 28.9144 - val_loss: 17075.5131 - val_mse: 17075.5117 - val_mae: 36.9521\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 250us/step - loss: 1877.0306 - mse: 1877.0308 - mae: 29.6954 - val_loss: 17071.4625 - val_mse: 17071.4629 - val_mae: 36.9782\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 4206.6108 - mse: 4206.6108 - mae: 33.4521 - val_loss: 2160.0912 - val_mse: 2160.0911 - val_mae: 29.8485\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 4115.0095 - mse: 4115.0093 - mae: 33.8500 - val_loss: 2219.8515 - val_mse: 2219.8518 - val_mae: 29.9903\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4140.0294 - mse: 4140.0298 - mae: 32.7934 - val_loss: 2205.8632 - val_mse: 2205.8630 - val_mae: 29.9338\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4292.4155 - mse: 4292.4155 - mae: 34.2413 - val_loss: 2319.7639 - val_mse: 2319.7637 - val_mae: 30.4108\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4158.3906 - mse: 4158.3901 - mae: 33.6971 - val_loss: 2175.9658 - val_mse: 2175.9656 - val_mae: 29.8665\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4119.1612 - mse: 4119.1611 - mae: 34.2811 - val_loss: 2203.2000 - val_mse: 2203.2000 - val_mae: 29.9190\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 4110.8305 - mse: 4110.8306 - mae: 33.6991 - val_loss: 2256.7060 - val_mse: 2256.7061 - val_mae: 30.1134\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4190.5992 - mse: 4190.5991 - mae: 34.2881 - val_loss: 2198.0124 - val_mse: 2198.0125 - val_mae: 29.9021\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 351us/step - loss: 3995.9016 - mse: 3995.9016 - mae: 35.0131 - val_loss: 2216.7661 - val_mse: 2216.7661 - val_mae: 29.9535\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 278us/step - loss: 4120.6908 - mse: 4120.6904 - mae: 34.1079 - val_loss: 2198.1741 - val_mse: 2198.1746 - val_mae: 29.8955\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 231us/step - loss: 4176.3727 - mse: 4176.3726 - mae: 34.4535 - val_loss: 2284.1671 - val_mse: 2284.1672 - val_mae: 30.2046\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4180.4243 - mse: 4180.4238 - mae: 34.0794 - val_loss: 2252.2944 - val_mse: 2252.2942 - val_mae: 30.0732\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 256us/step - loss: 3970.6211 - mse: 3970.6218 - mae: 33.7294 - val_loss: 2173.8526 - val_mse: 2173.8528 - val_mae: 29.8457\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 258us/step - loss: 3945.6990 - mse: 3945.6997 - mae: 32.9778 - val_loss: 2262.8872 - val_mse: 2262.8872 - val_mae: 30.1023\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4193.1027 - mse: 4193.1030 - mae: 33.6720 - val_loss: 2297.0499 - val_mse: 2297.0498 - val_mae: 30.2445\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 3892.7957 - mse: 3892.7957 - mae: 32.4298 - val_loss: 2200.0046 - val_mse: 2200.0046 - val_mae: 29.8824\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4155.6079 - mse: 4155.6079 - mae: 34.6413 - val_loss: 2226.8087 - val_mse: 2226.8088 - val_mae: 29.9645\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4176.0320 - mse: 4176.0327 - mae: 34.4573 - val_loss: 2296.5441 - val_mse: 2296.5439 - val_mae: 30.2425\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4002.5506 - mse: 4002.5503 - mae: 32.6064 - val_loss: 2215.5962 - val_mse: 2215.5962 - val_mae: 29.9306\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4015.3296 - mse: 4015.3296 - mae: 32.8034 - val_loss: 2262.5553 - val_mse: 2262.5552 - val_mae: 30.0965\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 252us/step - loss: 4060.0292 - mse: 4060.0295 - mae: 32.5885 - val_loss: 2218.9387 - val_mse: 2218.9385 - val_mae: 29.9364\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4056.3813 - mse: 4056.3816 - mae: 33.6037 - val_loss: 2248.9321 - val_mse: 2248.9321 - val_mae: 30.0399\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 248us/step - loss: 3951.2443 - mse: 3951.2446 - mae: 33.1387 - val_loss: 2231.5165 - val_mse: 2231.5166 - val_mae: 29.9735\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 3926.0816 - mse: 3926.0811 - mae: 32.8443 - val_loss: 2248.6746 - val_mse: 2248.6746 - val_mae: 30.0331\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4368.2180 - mse: 4368.2173 - mae: 34.4715 - val_loss: 2279.6945 - val_mse: 2279.6946 - val_mae: 30.1452\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 343us/step - loss: 4099.5545 - mse: 4099.5542 - mae: 33.9325 - val_loss: 2299.0877 - val_mse: 2299.0876 - val_mae: 30.2307\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 340us/step - loss: 3852.0414 - mse: 3852.0413 - mae: 32.2548 - val_loss: 2222.5908 - val_mse: 2222.5908 - val_mae: 29.9439\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 249us/step - loss: 3879.5513 - mse: 3879.5505 - mae: 32.0459 - val_loss: 2241.3994 - val_mse: 2241.3992 - val_mae: 30.0096\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 323us/step - loss: 4077.7710 - mse: 4077.7710 - mae: 32.6093 - val_loss: 2216.4996 - val_mse: 2216.4995 - val_mae: 29.9345\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4103.3241 - mse: 4103.3242 - mae: 33.6120 - val_loss: 2213.9866 - val_mse: 2213.9866 - val_mae: 29.9277\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 258us/step - loss: 3935.2734 - mse: 3935.2737 - mae: 32.3990 - val_loss: 2232.2989 - val_mse: 2232.2991 - val_mae: 29.9799\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4082.0668 - mse: 4082.0671 - mae: 32.9336 - val_loss: 2276.0281 - val_mse: 2276.0281 - val_mae: 30.1311\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 3897.4313 - mse: 3897.4314 - mae: 33.3512 - val_loss: 2261.9848 - val_mse: 2261.9851 - val_mae: 30.0758\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 357us/step - loss: 3758.6353 - mse: 3758.6360 - mae: 31.8310 - val_loss: 2230.3685 - val_mse: 2230.3684 - val_mae: 29.9656\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4001.7101 - mse: 4001.7097 - mae: 33.5369 - val_loss: 2231.6996 - val_mse: 2231.6995 - val_mae: 29.9680\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 277us/step - loss: 3873.9335 - mse: 3873.9336 - mae: 32.0715 - val_loss: 2263.5526 - val_mse: 2263.5525 - val_mae: 30.0700\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 3791.4927 - mse: 3791.4929 - mae: 31.9403 - val_loss: 2264.8223 - val_mse: 2264.8223 - val_mae: 30.0724\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 276us/step - loss: 3850.6001 - mse: 3850.6003 - mae: 32.8501 - val_loss: 2244.4777 - val_mse: 2244.4775 - val_mae: 30.0045\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 3875.2329 - mse: 3875.2334 - mae: 33.0686 - val_loss: 2218.4643 - val_mse: 2218.4644 - val_mae: 29.9341\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 267us/step - loss: 3926.3149 - mse: 3926.3152 - mae: 33.1740 - val_loss: 2197.7391 - val_mse: 2197.7390 - val_mae: 29.8895\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 350us/step - loss: 3702.4000 - mse: 3702.4004 - mae: 32.2725 - val_loss: 2212.0929 - val_mse: 2212.0930 - val_mae: 29.9161\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 3857.2516 - mse: 3857.2522 - mae: 32.9016 - val_loss: 2186.4870 - val_mse: 2186.4871 - val_mae: 29.8791\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 214us/step - loss: 3953.7489 - mse: 3953.7488 - mae: 32.3921 - val_loss: 2262.2330 - val_mse: 2262.2329 - val_mae: 30.0589\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 295us/step - loss: 4031.1146 - mse: 4031.1147 - mae: 33.1995 - val_loss: 2232.5181 - val_mse: 2232.5181 - val_mae: 29.9669\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 3992.5098 - mse: 3992.5095 - mae: 33.4071 - val_loss: 2269.1621 - val_mse: 2269.1621 - val_mae: 30.0807\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 280us/step - loss: 4194.6589 - mse: 4194.6592 - mae: 33.7456 - val_loss: 2288.7870 - val_mse: 2288.7871 - val_mae: 30.1648\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 216us/step - loss: 3957.9050 - mse: 3957.9053 - mae: 32.6756 - val_loss: 2243.7127 - val_mse: 2243.7129 - val_mae: 30.0014\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 234us/step - loss: 3835.9200 - mse: 3835.9197 - mae: 32.2565 - val_loss: 2207.3555 - val_mse: 2207.3555 - val_mae: 29.9143\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 3996.8016 - mse: 3996.8015 - mae: 32.8438 - val_loss: 2204.8077 - val_mse: 2204.8079 - val_mae: 29.9090\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 3885.3697 - mse: 3885.3694 - mae: 32.7269 - val_loss: 2203.0347 - val_mse: 2203.0344 - val_mae: 29.9060\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 3947.7129 - mse: 3947.7122 - mae: 32.3986 - val_loss: 2266.0581 - val_mse: 2266.0583 - val_mae: 30.0653\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 323us/step - loss: 3942.3214 - mse: 3942.3215 - mae: 33.1801 - val_loss: 2247.8922 - val_mse: 2247.8923 - val_mae: 30.0113\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 289us/step - loss: 3772.0235 - mse: 3772.0229 - mae: 32.1448 - val_loss: 2238.3552 - val_mse: 2238.3552 - val_mae: 29.9853\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 3984.0397 - mse: 3984.0396 - mae: 32.4438 - val_loss: 2211.5683 - val_mse: 2211.5684 - val_mae: 29.9235\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 3817.8946 - mse: 3817.8948 - mae: 31.9973 - val_loss: 2249.6364 - val_mse: 2249.6365 - val_mae: 30.0095\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 242us/step - loss: 3813.5998 - mse: 3813.6003 - mae: 33.2964 - val_loss: 2244.3836 - val_mse: 2244.3835 - val_mae: 29.9984\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 3917.3691 - mse: 3917.3691 - mae: 32.4428 - val_loss: 2315.4217 - val_mse: 2315.4216 - val_mae: 30.2688\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 4029.4898 - mse: 4029.4895 - mae: 32.8908 - val_loss: 2252.4812 - val_mse: 2252.4812 - val_mae: 30.0198\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 295us/step - loss: 4007.4363 - mse: 4007.4365 - mae: 33.0539 - val_loss: 2231.2866 - val_mse: 2231.2869 - val_mae: 29.9660\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 3690.6867 - mse: 3690.6868 - mae: 31.5452 - val_loss: 2210.0684 - val_mse: 2210.0684 - val_mae: 29.9222\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 228us/step - loss: 3810.7799 - mse: 3810.7798 - mae: 31.6763 - val_loss: 2238.3641 - val_mse: 2238.3640 - val_mae: 29.9834\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 231us/step - loss: 3909.4501 - mse: 3909.4504 - mae: 32.1249 - val_loss: 2264.9691 - val_mse: 2264.9690 - val_mae: 30.0528\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 229us/step - loss: 3939.4141 - mse: 3939.4143 - mae: 32.3594 - val_loss: 2232.9784 - val_mse: 2232.9785 - val_mae: 29.9757\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 230us/step - loss: 4116.4369 - mse: 4116.4370 - mae: 32.8914 - val_loss: 2327.3827 - val_mse: 2327.3826 - val_mae: 30.3145\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 267us/step - loss: 3799.4482 - mse: 3799.4480 - mae: 32.9360 - val_loss: 2250.3900 - val_mse: 2250.3896 - val_mae: 30.0127\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 302us/step - loss: 4100.9474 - mse: 4100.9478 - mae: 33.1436 - val_loss: 2275.4687 - val_mse: 2275.4685 - val_mae: 30.0897\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 274us/step - loss: 3911.8812 - mse: 3911.8804 - mae: 32.7539 - val_loss: 2278.1896 - val_mse: 2278.1895 - val_mae: 30.1015\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 3808.5564 - mse: 3808.5564 - mae: 31.5133 - val_loss: 2263.3077 - val_mse: 2263.3079 - val_mae: 30.0524\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 3925.9318 - mse: 3925.9321 - mae: 31.7550 - val_loss: 2238.2686 - val_mse: 2238.2686 - val_mae: 29.9961\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 4024.3255 - mse: 4024.3262 - mae: 33.0979 - val_loss: 2279.2703 - val_mse: 2279.2703 - val_mae: 30.1072\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 297us/step - loss: 4000.4099 - mse: 4000.4099 - mae: 33.0244 - val_loss: 2305.4346 - val_mse: 2305.4346 - val_mae: 30.2176\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 3906.8201 - mse: 3906.8201 - mae: 32.5979 - val_loss: 2265.7288 - val_mse: 2265.7290 - val_mae: 30.0623\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 273us/step - loss: 3734.4570 - mse: 3734.4570 - mae: 31.6256 - val_loss: 2211.6945 - val_mse: 2211.6946 - val_mae: 29.9516\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 215us/step - loss: 4088.7489 - mse: 4088.7493 - mae: 33.3547 - val_loss: 2278.8893 - val_mse: 2278.8892 - val_mae: 30.1047\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 4016.2410 - mse: 4016.2415 - mae: 32.6161 - val_loss: 2271.5378 - val_mse: 2271.5378 - val_mae: 30.0834\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 3929.6393 - mse: 3929.6394 - mae: 31.9012 - val_loss: 2232.0820 - val_mse: 2232.0820 - val_mae: 29.9891\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 250us/step - loss: 3975.0478 - mse: 3975.0476 - mae: 32.7303 - val_loss: 2261.7352 - val_mse: 2261.7354 - val_mae: 30.0533\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 285us/step - loss: 3886.4318 - mse: 3886.4326 - mae: 32.0090 - val_loss: 2252.1874 - val_mse: 2252.1875 - val_mae: 30.0351\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 235us/step - loss: 3810.6995 - mse: 3810.7000 - mae: 32.1816 - val_loss: 2211.3825 - val_mse: 2211.3826 - val_mae: 29.9694\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4183.7399 - mse: 4183.7397 - mae: 32.9767 - val_loss: 2278.2455 - val_mse: 2278.2456 - val_mae: 30.1037\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3348.0197 - mse: 3348.0205 - mae: 32.6556 - val_loss: 1518.5796 - val_mse: 1518.5796 - val_mae: 27.2579\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3349.7262 - mse: 3349.7261 - mae: 32.0248 - val_loss: 1524.4910 - val_mse: 1524.4910 - val_mae: 27.7361\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3243.3025 - mse: 3243.3022 - mae: 32.3104 - val_loss: 1506.9781 - val_mse: 1506.9781 - val_mae: 27.2062\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 279us/step - loss: 3402.8350 - mse: 3402.8347 - mae: 32.8006 - val_loss: 1493.7248 - val_mse: 1493.7246 - val_mae: 26.8667\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3377.3231 - mse: 3377.3237 - mae: 32.3647 - val_loss: 1482.3268 - val_mse: 1482.3269 - val_mae: 26.5951\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 269us/step - loss: 3336.0185 - mse: 3336.0188 - mae: 32.2074 - val_loss: 1480.3458 - val_mse: 1480.3457 - val_mae: 26.8347\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3435.8278 - mse: 3435.8274 - mae: 32.2840 - val_loss: 1471.3291 - val_mse: 1471.3292 - val_mae: 26.6384\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3318.0432 - mse: 3318.0432 - mae: 31.8860 - val_loss: 1456.4664 - val_mse: 1456.4664 - val_mae: 25.8009\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3438.5486 - mse: 3438.5491 - mae: 32.4225 - val_loss: 1460.2880 - val_mse: 1460.2881 - val_mae: 26.4666\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3430.4760 - mse: 3430.4773 - mae: 32.1766 - val_loss: 1463.2708 - val_mse: 1463.2709 - val_mae: 26.7877\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3358.0982 - mse: 3358.0979 - mae: 32.2778 - val_loss: 1456.1344 - val_mse: 1456.1344 - val_mae: 26.5407\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 325us/step - loss: 3321.4216 - mse: 3321.4214 - mae: 32.4739 - val_loss: 1446.2745 - val_mse: 1446.2744 - val_mae: 25.9818\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3301.5012 - mse: 3301.5017 - mae: 32.9122 - val_loss: 1444.4198 - val_mse: 1444.4198 - val_mae: 26.0514\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3282.4981 - mse: 3282.4983 - mae: 32.6751 - val_loss: 1447.3987 - val_mse: 1447.3989 - val_mae: 26.4394\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 275us/step - loss: 3334.0228 - mse: 3334.0227 - mae: 31.7008 - val_loss: 1443.5915 - val_mse: 1443.5914 - val_mae: 26.3461\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3294.7433 - mse: 3294.7432 - mae: 31.9203 - val_loss: 1447.6722 - val_mse: 1447.6720 - val_mae: 26.6801\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3383.5218 - mse: 3383.5225 - mae: 32.0288 - val_loss: 1441.4605 - val_mse: 1441.4606 - val_mae: 26.3557\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3334.1474 - mse: 3334.1475 - mae: 31.7936 - val_loss: 1437.7456 - val_mse: 1437.7456 - val_mae: 26.1475\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3260.5942 - mse: 3260.5952 - mae: 31.6861 - val_loss: 1439.7275 - val_mse: 1439.7274 - val_mae: 26.3062\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3297.9869 - mse: 3297.9871 - mae: 32.3029 - val_loss: 1444.7949 - val_mse: 1444.7952 - val_mae: 26.6655\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 214us/step - loss: 3383.1222 - mse: 3383.1221 - mae: 31.8547 - val_loss: 1433.7710 - val_mse: 1433.7710 - val_mae: 25.9220\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 231us/step - loss: 3342.0287 - mse: 3342.0283 - mae: 31.8175 - val_loss: 1435.9158 - val_mse: 1435.9158 - val_mae: 26.2417\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3325.3046 - mse: 3325.3049 - mae: 32.0532 - val_loss: 1434.4259 - val_mse: 1434.4258 - val_mae: 26.1525\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3367.8851 - mse: 3367.8850 - mae: 32.0852 - val_loss: 1436.7625 - val_mse: 1436.7626 - val_mae: 26.3486\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3228.9382 - mse: 3228.9387 - mae: 31.7044 - val_loss: 1442.7457 - val_mse: 1442.7456 - val_mae: 26.7642\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3256.4361 - mse: 3256.4360 - mae: 31.2135 - val_loss: 1440.3591 - val_mse: 1440.3590 - val_mae: 26.6155\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3276.6677 - mse: 3276.6680 - mae: 31.7485 - val_loss: 1441.8249 - val_mse: 1441.8248 - val_mae: 26.6850\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3224.9820 - mse: 3224.9822 - mae: 31.4573 - val_loss: 1435.8732 - val_mse: 1435.8732 - val_mae: 26.3102\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3271.4533 - mse: 3271.4536 - mae: 32.0217 - val_loss: 1449.8632 - val_mse: 1449.8632 - val_mae: 27.1111\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3197.1403 - mse: 3197.1404 - mae: 31.7305 - val_loss: 1439.5537 - val_mse: 1439.5535 - val_mae: 26.5548\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 351us/step - loss: 3327.3323 - mse: 3327.3325 - mae: 32.3550 - val_loss: 1435.3969 - val_mse: 1435.3969 - val_mae: 26.3392\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3327.6564 - mse: 3327.6562 - mae: 32.0345 - val_loss: 1440.0472 - val_mse: 1440.0471 - val_mae: 26.6264\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3480.7694 - mse: 3480.7686 - mae: 32.9639 - val_loss: 1438.6567 - val_mse: 1438.6566 - val_mae: 26.4790\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3369.1462 - mse: 3369.1455 - mae: 32.3293 - val_loss: 1439.0064 - val_mse: 1439.0062 - val_mae: 26.5538\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3195.0708 - mse: 3195.0708 - mae: 32.2508 - val_loss: 1436.4705 - val_mse: 1436.4706 - val_mae: 26.3433\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 256us/step - loss: 3276.7063 - mse: 3276.7063 - mae: 32.2211 - val_loss: 1433.8862 - val_mse: 1433.8862 - val_mae: 26.0713\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3317.6719 - mse: 3317.6726 - mae: 32.0088 - val_loss: 1436.4345 - val_mse: 1436.4347 - val_mae: 26.2703\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3267.8870 - mse: 3267.8862 - mae: 31.4090 - val_loss: 1438.1878 - val_mse: 1438.1879 - val_mae: 26.4643\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3284.6013 - mse: 3284.6013 - mae: 31.7686 - val_loss: 1434.8309 - val_mse: 1434.8309 - val_mae: 26.1934\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3359.5451 - mse: 3359.5452 - mae: 31.4589 - val_loss: 1434.1166 - val_mse: 1434.1165 - val_mae: 26.1066\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3103.8644 - mse: 3103.8650 - mae: 31.3621 - val_loss: 1444.7611 - val_mse: 1444.7610 - val_mae: 26.8664\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3408.1486 - mse: 3408.1482 - mae: 32.1967 - val_loss: 1432.5179 - val_mse: 1432.5178 - val_mae: 25.8815\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3382.9263 - mse: 3382.9265 - mae: 31.8840 - val_loss: 1438.1806 - val_mse: 1438.1805 - val_mae: 26.4435\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3285.0868 - mse: 3285.0862 - mae: 31.6689 - val_loss: 1440.6757 - val_mse: 1440.6758 - val_mae: 26.5928\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3221.9410 - mse: 3221.9419 - mae: 31.4047 - val_loss: 1443.6335 - val_mse: 1443.6335 - val_mae: 26.7779\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3176.8103 - mse: 3176.8098 - mae: 31.3153 - val_loss: 1437.1585 - val_mse: 1437.1584 - val_mae: 26.4167\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3207.1501 - mse: 3207.1499 - mae: 31.2198 - val_loss: 1439.5994 - val_mse: 1439.5995 - val_mae: 26.6168\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3287.8121 - mse: 3287.8118 - mae: 31.6983 - val_loss: 1436.1535 - val_mse: 1436.1536 - val_mae: 26.3613\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3248.7452 - mse: 3248.7461 - mae: 31.2517 - val_loss: 1443.9841 - val_mse: 1443.9841 - val_mae: 26.8894\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 284us/step - loss: 3237.0942 - mse: 3237.0942 - mae: 31.5157 - val_loss: 1430.5945 - val_mse: 1430.5944 - val_mae: 25.8312\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 220us/step - loss: 3203.0460 - mse: 3203.0459 - mae: 31.0010 - val_loss: 1441.2529 - val_mse: 1441.2528 - val_mae: 26.7568\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 234us/step - loss: 3297.7594 - mse: 3297.7588 - mae: 31.7930 - val_loss: 1434.5745 - val_mse: 1434.5743 - val_mae: 26.2439\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3298.6081 - mse: 3298.6079 - mae: 31.5027 - val_loss: 1438.4466 - val_mse: 1438.4467 - val_mae: 26.5704\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 266us/step - loss: 3107.4428 - mse: 3107.4429 - mae: 31.0243 - val_loss: 1444.2130 - val_mse: 1444.2129 - val_mae: 26.9121\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 331us/step - loss: 3162.8836 - mse: 3162.8831 - mae: 30.9989 - val_loss: 1443.0320 - val_mse: 1443.0322 - val_mae: 26.8361\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3215.6278 - mse: 3215.6272 - mae: 31.2232 - val_loss: 1438.7141 - val_mse: 1438.7139 - val_mae: 26.5867\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 263us/step - loss: 3228.0131 - mse: 3228.0127 - mae: 32.1266 - val_loss: 1438.9210 - val_mse: 1438.9211 - val_mae: 26.5625\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3173.4008 - mse: 3173.4011 - mae: 31.2945 - val_loss: 1440.1911 - val_mse: 1440.1912 - val_mae: 26.6348\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 241us/step - loss: 3180.3980 - mse: 3180.3967 - mae: 31.5632 - val_loss: 1442.9073 - val_mse: 1442.9075 - val_mae: 26.8002\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 214us/step - loss: 3187.5054 - mse: 3187.5056 - mae: 31.2017 - val_loss: 1442.6307 - val_mse: 1442.6307 - val_mae: 26.7841\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 224us/step - loss: 3238.7517 - mse: 3238.7507 - mae: 31.1792 - val_loss: 1437.5164 - val_mse: 1437.5164 - val_mae: 26.4435\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 273us/step - loss: 3093.1247 - mse: 3093.1248 - mae: 30.9922 - val_loss: 1440.6205 - val_mse: 1440.6204 - val_mae: 26.6926\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3219.1805 - mse: 3219.1816 - mae: 31.1129 - val_loss: 1446.6398 - val_mse: 1446.6399 - val_mae: 26.9990\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3309.6421 - mse: 3309.6428 - mae: 31.7927 - val_loss: 1454.7400 - val_mse: 1454.7397 - val_mae: 27.3815\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3263.6954 - mse: 3263.6956 - mae: 32.0689 - val_loss: 1439.7561 - val_mse: 1439.7560 - val_mae: 26.5735\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 331us/step - loss: 3206.2773 - mse: 3206.2778 - mae: 30.9151 - val_loss: 1434.9259 - val_mse: 1434.9258 - val_mae: 26.2081\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3180.3515 - mse: 3180.3508 - mae: 31.4200 - val_loss: 1442.5737 - val_mse: 1442.5737 - val_mae: 26.8011\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3201.1175 - mse: 3201.1182 - mae: 31.6604 - val_loss: 1441.2378 - val_mse: 1441.2378 - val_mae: 26.7013\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3160.6637 - mse: 3160.6636 - mae: 31.0075 - val_loss: 1443.5914 - val_mse: 1443.5912 - val_mae: 26.7951\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 331us/step - loss: 3181.2207 - mse: 3181.2205 - mae: 31.8799 - val_loss: 1438.4880 - val_mse: 1438.4879 - val_mae: 26.4673\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 338us/step - loss: 3287.4991 - mse: 3287.4988 - mae: 32.0143 - val_loss: 1441.4417 - val_mse: 1441.4415 - val_mae: 26.6764\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3279.9659 - mse: 3279.9656 - mae: 31.8511 - val_loss: 1436.8301 - val_mse: 1436.8301 - val_mae: 26.3893\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 342us/step - loss: 3202.7348 - mse: 3202.7339 - mae: 30.8771 - val_loss: 1450.0514 - val_mse: 1450.0515 - val_mae: 27.1661\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3238.0971 - mse: 3238.0977 - mae: 31.4296 - val_loss: 1433.8259 - val_mse: 1433.8259 - val_mae: 26.1713\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 232us/step - loss: 3201.8218 - mse: 3201.8210 - mae: 31.6024 - val_loss: 1435.8301 - val_mse: 1435.8302 - val_mae: 26.3790\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3108.6423 - mse: 3108.6428 - mae: 29.7493 - val_loss: 1447.9805 - val_mse: 1447.9806 - val_mae: 27.1324\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3251.4805 - mse: 3251.4810 - mae: 31.2721 - val_loss: 1448.3996 - val_mse: 1448.3994 - val_mae: 27.1436\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 338us/step - loss: 3291.4085 - mse: 3291.4087 - mae: 31.1420 - val_loss: 1441.2062 - val_mse: 1441.2062 - val_mae: 26.7497\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 279us/step - loss: 3275.1312 - mse: 3275.1309 - mae: 31.1575 - val_loss: 1437.9381 - val_mse: 1437.9380 - val_mae: 26.5370\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3236.0605 - mse: 3236.0596 - mae: 31.0036 - val_loss: 1437.1914 - val_mse: 1437.1915 - val_mae: 26.4931\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2966.4960 - mse: 2966.4958 - mae: 31.5771 - val_loss: 1119.4105 - val_mse: 1119.4106 - val_mae: 22.7307\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2958.2766 - mse: 2958.2776 - mae: 31.0300 - val_loss: 1121.1824 - val_mse: 1121.1826 - val_mae: 22.6825\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2857.7340 - mse: 2857.7346 - mae: 30.0087 - val_loss: 1104.9439 - val_mse: 1104.9438 - val_mae: 22.7878\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2888.6907 - mse: 2888.6902 - mae: 30.8597 - val_loss: 1098.2543 - val_mse: 1098.2542 - val_mae: 22.8667\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2930.6381 - mse: 2930.6379 - mae: 30.7493 - val_loss: 1104.6207 - val_mse: 1104.6206 - val_mae: 22.7598\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2941.0032 - mse: 2941.0032 - mae: 30.7004 - val_loss: 1097.5027 - val_mse: 1097.5026 - val_mae: 22.8508\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2871.4549 - mse: 2871.4553 - mae: 30.4915 - val_loss: 1101.4483 - val_mse: 1101.4482 - val_mae: 22.7548\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 270us/step - loss: 2858.5929 - mse: 2858.5930 - mae: 30.4630 - val_loss: 1091.1235 - val_mse: 1091.1235 - val_mae: 22.8902\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 332us/step - loss: 2869.0461 - mse: 2869.0464 - mae: 30.3842 - val_loss: 1089.0010 - val_mse: 1089.0011 - val_mae: 22.8844\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 0s 234us/step - loss: 2803.2719 - mse: 2803.2717 - mae: 30.1508 - val_loss: 1088.4781 - val_mse: 1088.4780 - val_mae: 22.8733\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2829.3833 - mse: 2829.3835 - mae: 30.1906 - val_loss: 1079.0554 - val_mse: 1079.0554 - val_mae: 23.0735\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2893.5993 - mse: 2893.5996 - mae: 30.6042 - val_loss: 1082.4999 - val_mse: 1082.5000 - val_mae: 22.9624\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 258us/step - loss: 2837.9400 - mse: 2837.9399 - mae: 30.8457 - val_loss: 1081.4242 - val_mse: 1081.4241 - val_mae: 22.9687\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 267us/step - loss: 2886.6691 - mse: 2886.6689 - mae: 31.2060 - val_loss: 1092.5529 - val_mse: 1092.5529 - val_mae: 22.7303\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2857.2287 - mse: 2857.2285 - mae: 31.0166 - val_loss: 1081.8633 - val_mse: 1081.8633 - val_mae: 22.9307\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2907.2529 - mse: 2907.2527 - mae: 31.1083 - val_loss: 1094.3434 - val_mse: 1094.3434 - val_mae: 22.6895\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2880.4671 - mse: 2880.4670 - mae: 30.7267 - val_loss: 1092.4364 - val_mse: 1092.4363 - val_mae: 22.7027\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2855.6744 - mse: 2855.6743 - mae: 30.2224 - val_loss: 1084.2236 - val_mse: 1084.2234 - val_mae: 22.8454\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2880.7820 - mse: 2880.7820 - mae: 30.9720 - val_loss: 1084.0136 - val_mse: 1084.0137 - val_mae: 22.8483\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2907.9559 - mse: 2907.9556 - mae: 30.6217 - val_loss: 1081.9986 - val_mse: 1081.9988 - val_mae: 22.8744\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2887.2594 - mse: 2887.2585 - mae: 30.7900 - val_loss: 1084.4936 - val_mse: 1084.4935 - val_mae: 22.8274\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2842.8166 - mse: 2842.8159 - mae: 30.2364 - val_loss: 1070.1921 - val_mse: 1070.1921 - val_mae: 23.2084\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 319us/step - loss: 2878.7312 - mse: 2878.7312 - mae: 30.9161 - val_loss: 1092.4335 - val_mse: 1092.4336 - val_mae: 22.6726\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2774.2028 - mse: 2774.2019 - mae: 29.9207 - val_loss: 1096.3870 - val_mse: 1096.3871 - val_mae: 22.5935\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 338us/step - loss: 2841.8712 - mse: 2841.8713 - mae: 30.2902 - val_loss: 1080.7922 - val_mse: 1080.7924 - val_mae: 22.8529\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 319us/step - loss: 2863.3463 - mse: 2863.3462 - mae: 30.5027 - val_loss: 1077.3753 - val_mse: 1077.3755 - val_mae: 22.9441\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2898.7169 - mse: 2898.7163 - mae: 30.5480 - val_loss: 1082.6539 - val_mse: 1082.6539 - val_mae: 22.8125\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 252us/step - loss: 2838.7774 - mse: 2838.7783 - mae: 30.3155 - val_loss: 1074.2070 - val_mse: 1074.2070 - val_mae: 22.9827\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 0s 230us/step - loss: 2886.8631 - mse: 2886.8630 - mae: 30.9318 - val_loss: 1107.4630 - val_mse: 1107.4630 - val_mae: 22.4805\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 272us/step - loss: 2866.4563 - mse: 2866.4565 - mae: 31.0882 - val_loss: 1081.7911 - val_mse: 1081.7911 - val_mae: 22.8282\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2894.4060 - mse: 2894.4067 - mae: 30.7239 - val_loss: 1080.3176 - val_mse: 1080.3177 - val_mae: 22.8425\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2900.9818 - mse: 2900.9812 - mae: 30.9922 - val_loss: 1088.7606 - val_mse: 1088.7606 - val_mae: 22.6728\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2687.8368 - mse: 2687.8374 - mae: 30.1031 - val_loss: 1071.1201 - val_mse: 1071.1201 - val_mae: 23.0496\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2781.0046 - mse: 2781.0037 - mae: 30.3005 - val_loss: 1075.4285 - val_mse: 1075.4285 - val_mae: 22.9070\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2817.4891 - mse: 2817.4893 - mae: 29.9883 - val_loss: 1064.7194 - val_mse: 1064.7194 - val_mae: 23.2148\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2821.1878 - mse: 2821.1875 - mae: 30.3973 - val_loss: 1071.7546 - val_mse: 1071.7546 - val_mae: 22.9898\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2848.7850 - mse: 2848.7852 - mae: 30.0669 - val_loss: 1088.8765 - val_mse: 1088.8765 - val_mae: 22.6382\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2870.3763 - mse: 2870.3765 - mae: 30.6232 - val_loss: 1074.8292 - val_mse: 1074.8292 - val_mae: 22.8440\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2905.3551 - mse: 2905.3552 - mae: 30.3944 - val_loss: 1076.3186 - val_mse: 1076.3186 - val_mae: 22.8075\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2814.3756 - mse: 2814.3755 - mae: 30.6149 - val_loss: 1082.9477 - val_mse: 1082.9476 - val_mae: 22.6768\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2798.5645 - mse: 2798.5642 - mae: 29.8740 - val_loss: 1071.4001 - val_mse: 1071.4001 - val_mae: 22.9117\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2938.8843 - mse: 2938.8838 - mae: 30.5732 - val_loss: 1084.8680 - val_mse: 1084.8679 - val_mae: 22.6244\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2848.1442 - mse: 2848.1436 - mae: 30.2435 - val_loss: 1084.0117 - val_mse: 1084.0116 - val_mae: 22.6360\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2831.9147 - mse: 2831.9143 - mae: 30.5336 - val_loss: 1078.5327 - val_mse: 1078.5326 - val_mae: 22.7346\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 0s 251us/step - loss: 2777.3954 - mse: 2777.3948 - mae: 30.3969 - val_loss: 1076.4858 - val_mse: 1076.4858 - val_mae: 22.7484\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2803.1749 - mse: 2803.1753 - mae: 30.6845 - val_loss: 1076.0246 - val_mse: 1076.0247 - val_mae: 22.7468\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2841.0449 - mse: 2841.0452 - mae: 30.5969 - val_loss: 1065.0938 - val_mse: 1065.0940 - val_mae: 22.9725\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2766.7160 - mse: 2766.7161 - mae: 30.0608 - val_loss: 1078.4149 - val_mse: 1078.4149 - val_mae: 22.6693\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2829.2369 - mse: 2829.2363 - mae: 30.3345 - val_loss: 1064.9155 - val_mse: 1064.9155 - val_mae: 22.9801\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2851.0189 - mse: 2851.0195 - mae: 30.2106 - val_loss: 1078.0697 - val_mse: 1078.0698 - val_mae: 22.6193\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2899.0740 - mse: 2899.0747 - mae: 30.4011 - val_loss: 1075.0956 - val_mse: 1075.0956 - val_mae: 22.6596\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2920.1039 - mse: 2920.1035 - mae: 30.5662 - val_loss: 1069.7148 - val_mse: 1069.7148 - val_mae: 22.7875\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2809.2778 - mse: 2809.2778 - mae: 29.8823 - val_loss: 1071.0549 - val_mse: 1071.0548 - val_mae: 22.7361\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2820.6304 - mse: 2820.6313 - mae: 30.3922 - val_loss: 1069.3388 - val_mse: 1069.3387 - val_mae: 22.7794\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2860.4414 - mse: 2860.4417 - mae: 30.2962 - val_loss: 1076.7582 - val_mse: 1076.7582 - val_mae: 22.6295\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 0s 245us/step - loss: 2816.0312 - mse: 2816.0300 - mae: 30.2528 - val_loss: 1070.3062 - val_mse: 1070.3062 - val_mae: 22.7685\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2881.7120 - mse: 2881.7119 - mae: 30.4997 - val_loss: 1083.9748 - val_mse: 1083.9750 - val_mae: 22.5275\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2868.4292 - mse: 2868.4294 - mae: 30.6027 - val_loss: 1084.8351 - val_mse: 1084.8352 - val_mae: 22.4864\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 262us/step - loss: 2813.7839 - mse: 2813.7830 - mae: 30.4005 - val_loss: 1065.2920 - val_mse: 1065.2919 - val_mae: 22.8601\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 357us/step - loss: 2779.6258 - mse: 2779.6260 - mae: 29.6213 - val_loss: 1071.8972 - val_mse: 1071.8972 - val_mae: 22.6780\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2812.4223 - mse: 2812.4216 - mae: 30.2148 - val_loss: 1085.3896 - val_mse: 1085.3895 - val_mae: 22.5100\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 277us/step - loss: 2773.8453 - mse: 2773.8455 - mae: 29.3979 - val_loss: 1075.9982 - val_mse: 1075.9983 - val_mae: 22.6758\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 336us/step - loss: 2776.3233 - mse: 2776.3232 - mae: 29.6382 - val_loss: 1075.0859 - val_mse: 1075.0858 - val_mae: 22.6519\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2793.9876 - mse: 2793.9873 - mae: 29.9171 - val_loss: 1075.4893 - val_mse: 1075.4893 - val_mae: 22.6201\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2804.9441 - mse: 2804.9436 - mae: 30.1181 - val_loss: 1071.6324 - val_mse: 1071.6326 - val_mae: 22.6575\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2848.2086 - mse: 2848.2083 - mae: 29.9634 - val_loss: 1073.0314 - val_mse: 1073.0314 - val_mae: 22.6307\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 337us/step - loss: 2794.2913 - mse: 2794.2913 - mae: 30.0624 - val_loss: 1065.6973 - val_mse: 1065.6971 - val_mae: 22.7482\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2761.1191 - mse: 2761.1191 - mae: 29.4873 - val_loss: 1066.7797 - val_mse: 1066.7797 - val_mae: 22.6669\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2783.9059 - mse: 2783.9053 - mae: 29.6201 - val_loss: 1062.2301 - val_mse: 1062.2300 - val_mae: 22.7693\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2764.9045 - mse: 2764.9058 - mae: 29.9965 - val_loss: 1074.3010 - val_mse: 1074.3013 - val_mae: 22.5677\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2761.1636 - mse: 2761.1626 - mae: 29.6958 - val_loss: 1059.0813 - val_mse: 1059.0812 - val_mae: 22.8400\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2742.8151 - mse: 2742.8149 - mae: 29.1332 - val_loss: 1055.1659 - val_mse: 1055.1660 - val_mae: 22.9554\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2824.8387 - mse: 2824.8391 - mae: 30.1141 - val_loss: 1081.7479 - val_mse: 1081.7480 - val_mae: 22.3917\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 0s 246us/step - loss: 2777.9659 - mse: 2777.9661 - mae: 29.7528 - val_loss: 1067.9788 - val_mse: 1067.9789 - val_mae: 22.5570\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2803.7420 - mse: 2803.7424 - mae: 30.2033 - val_loss: 1069.0296 - val_mse: 1069.0297 - val_mae: 22.5207\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2770.8481 - mse: 2770.8472 - mae: 29.6981 - val_loss: 1068.8137 - val_mse: 1068.8136 - val_mae: 22.5521\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2697.7189 - mse: 2697.7197 - mae: 29.7762 - val_loss: 1063.1804 - val_mse: 1063.1804 - val_mae: 22.6862\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2756.2970 - mse: 2756.2971 - mae: 30.0063 - val_loss: 1070.0701 - val_mse: 1070.0702 - val_mae: 22.5185\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2691.4920 - mse: 2691.4919 - mae: 29.8460 - val_loss: 1058.7166 - val_mse: 1058.7167 - val_mae: 22.7348\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2763.1547 - mse: 2763.1548 - mae: 29.5802 - val_loss: 1059.3892 - val_mse: 1059.3892 - val_mae: 22.7231\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2478.4816 - mse: 2478.4814 - mae: 29.1465 - val_loss: 1514.1134 - val_mse: 1514.1134 - val_mae: 26.5502\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2498.6263 - mse: 2498.6262 - mae: 29.2081 - val_loss: 1507.9102 - val_mse: 1507.9102 - val_mae: 26.6321\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 337us/step - loss: 2444.2225 - mse: 2444.2224 - mae: 29.4787 - val_loss: 1498.4341 - val_mse: 1498.4342 - val_mae: 26.7826\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 337us/step - loss: 2420.2316 - mse: 2420.2312 - mae: 29.2063 - val_loss: 1504.3435 - val_mse: 1504.3435 - val_mae: 26.5753\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2437.1002 - mse: 2437.0996 - mae: 29.1434 - val_loss: 1492.7634 - val_mse: 1492.7635 - val_mae: 26.7803\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2522.7817 - mse: 2522.7812 - mae: 30.1730 - val_loss: 1490.9170 - val_mse: 1490.9169 - val_mae: 26.7359\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2494.3912 - mse: 2494.3921 - mae: 29.1787 - val_loss: 1483.8990 - val_mse: 1483.8990 - val_mae: 26.8307\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 251us/step - loss: 2569.2690 - mse: 2569.2690 - mae: 29.8258 - val_loss: 1490.8618 - val_mse: 1490.8617 - val_mae: 26.6181\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2513.5015 - mse: 2513.5012 - mae: 29.4711 - val_loss: 1490.4873 - val_mse: 1490.4873 - val_mae: 26.5500\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2453.7423 - mse: 2453.7422 - mae: 29.2795 - val_loss: 1496.7452 - val_mse: 1496.7454 - val_mae: 26.3644\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2524.3230 - mse: 2524.3230 - mae: 29.5030 - val_loss: 1507.1037 - val_mse: 1507.1038 - val_mae: 26.1357\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 390us/step - loss: 2472.5998 - mse: 2472.6001 - mae: 29.0207 - val_loss: 1470.5478 - val_mse: 1470.5476 - val_mae: 26.8536\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2490.7414 - mse: 2490.7427 - mae: 29.4405 - val_loss: 1478.8684 - val_mse: 1478.8683 - val_mae: 26.5804\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2451.8467 - mse: 2451.8467 - mae: 29.2442 - val_loss: 1494.2089 - val_mse: 1494.2090 - val_mae: 26.2049\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2447.4650 - mse: 2447.4656 - mae: 29.2459 - val_loss: 1494.5378 - val_mse: 1494.5380 - val_mae: 26.1869\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2513.7891 - mse: 2513.7891 - mae: 29.0376 - val_loss: 1475.7860 - val_mse: 1475.7860 - val_mae: 26.4823\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2477.0376 - mse: 2477.0378 - mae: 29.1241 - val_loss: 1484.9955 - val_mse: 1484.9955 - val_mae: 26.2788\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2419.9826 - mse: 2419.9824 - mae: 28.9218 - val_loss: 1486.0802 - val_mse: 1486.0803 - val_mae: 26.1971\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2487.5860 - mse: 2487.5857 - mae: 29.0480 - val_loss: 1470.3694 - val_mse: 1470.3696 - val_mae: 26.4590\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2428.1027 - mse: 2428.1025 - mae: 29.0120 - val_loss: 1467.0091 - val_mse: 1467.0090 - val_mae: 26.4940\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2470.1998 - mse: 2470.1997 - mae: 28.9607 - val_loss: 1467.2966 - val_mse: 1467.2965 - val_mae: 26.4727\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2540.9113 - mse: 2540.9109 - mae: 29.6239 - val_loss: 1461.8141 - val_mse: 1461.8140 - val_mae: 26.5879\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2525.6022 - mse: 2525.6033 - mae: 29.4997 - val_loss: 1484.5538 - val_mse: 1484.5538 - val_mae: 26.1217\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2469.9608 - mse: 2469.9604 - mae: 28.7722 - val_loss: 1476.9446 - val_mse: 1476.9446 - val_mae: 26.2959\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2404.4795 - mse: 2404.4790 - mae: 28.8925 - val_loss: 1475.5467 - val_mse: 1475.5468 - val_mae: 26.2685\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2446.3108 - mse: 2446.3105 - mae: 28.8748 - val_loss: 1462.1866 - val_mse: 1462.1868 - val_mae: 26.4975\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2511.3510 - mse: 2511.3506 - mae: 29.2890 - val_loss: 1464.0544 - val_mse: 1464.0544 - val_mae: 26.3516\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2444.1696 - mse: 2444.1692 - mae: 29.0831 - val_loss: 1461.6824 - val_mse: 1461.6825 - val_mae: 26.3446\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2455.1088 - mse: 2455.1089 - mae: 29.5114 - val_loss: 1460.6376 - val_mse: 1460.6373 - val_mae: 26.3582\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2463.1194 - mse: 2463.1189 - mae: 29.1252 - val_loss: 1456.3843 - val_mse: 1456.3843 - val_mae: 26.4822\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2440.0683 - mse: 2440.0674 - mae: 28.7253 - val_loss: 1459.8872 - val_mse: 1459.8873 - val_mae: 26.3669\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2433.2199 - mse: 2433.2197 - mae: 29.2427 - val_loss: 1457.2893 - val_mse: 1457.2893 - val_mae: 26.3915\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 273us/step - loss: 2425.5643 - mse: 2425.5647 - mae: 28.6091 - val_loss: 1455.6337 - val_mse: 1455.6335 - val_mae: 26.3714\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2482.3188 - mse: 2482.3186 - mae: 29.0255 - val_loss: 1454.9985 - val_mse: 1454.9985 - val_mae: 26.3975\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2538.1484 - mse: 2538.1487 - mae: 29.3400 - val_loss: 1454.2374 - val_mse: 1454.2374 - val_mae: 26.3982\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2458.5343 - mse: 2458.5342 - mae: 29.1967 - val_loss: 1440.7500 - val_mse: 1440.7501 - val_mae: 26.7422\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2533.1000 - mse: 2533.1001 - mae: 29.5862 - val_loss: 1454.7710 - val_mse: 1454.7710 - val_mae: 26.4054\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2452.0720 - mse: 2452.0710 - mae: 28.7049 - val_loss: 1440.4273 - val_mse: 1440.4272 - val_mae: 26.7107\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2434.3679 - mse: 2434.3674 - mae: 28.7225 - val_loss: 1443.5047 - val_mse: 1443.5046 - val_mae: 26.5519\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 278us/step - loss: 2392.7069 - mse: 2392.7061 - mae: 28.7391 - val_loss: 1449.8515 - val_mse: 1449.8516 - val_mae: 26.3658\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2493.6062 - mse: 2493.6057 - mae: 28.9162 - val_loss: 1450.2054 - val_mse: 1450.2053 - val_mae: 26.3305\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 232us/step - loss: 2486.9300 - mse: 2486.9292 - mae: 28.5964 - val_loss: 1453.9547 - val_mse: 1453.9548 - val_mae: 26.3831\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 267us/step - loss: 2476.3273 - mse: 2476.3284 - mae: 28.7504 - val_loss: 1460.1082 - val_mse: 1460.1083 - val_mae: 26.2442\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2443.9139 - mse: 2443.9141 - mae: 29.0800 - val_loss: 1442.6953 - val_mse: 1442.6954 - val_mae: 26.5090\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 272us/step - loss: 2366.0290 - mse: 2366.0288 - mae: 28.4844 - val_loss: 1446.0431 - val_mse: 1446.0430 - val_mae: 26.4656\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2439.2013 - mse: 2439.2019 - mae: 29.1451 - val_loss: 1452.8809 - val_mse: 1452.8809 - val_mae: 26.2900\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2446.7747 - mse: 2446.7747 - mae: 28.7352 - val_loss: 1438.4999 - val_mse: 1438.5000 - val_mae: 26.6544\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2496.1386 - mse: 2496.1396 - mae: 29.4269 - val_loss: 1460.9350 - val_mse: 1460.9351 - val_mae: 26.1621\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 350us/step - loss: 2449.9129 - mse: 2449.9131 - mae: 28.7161 - val_loss: 1441.1514 - val_mse: 1441.1514 - val_mae: 26.5792\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 241us/step - loss: 2484.2089 - mse: 2484.2083 - mae: 29.0854 - val_loss: 1443.2920 - val_mse: 1443.2920 - val_mae: 26.5483\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 335us/step - loss: 2432.6684 - mse: 2432.6685 - mae: 28.3953 - val_loss: 1438.4246 - val_mse: 1438.4247 - val_mae: 26.6184\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 269us/step - loss: 2456.4408 - mse: 2456.4409 - mae: 29.1341 - val_loss: 1456.0534 - val_mse: 1456.0533 - val_mae: 26.1865\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 267us/step - loss: 2397.1375 - mse: 2397.1375 - mae: 28.8177 - val_loss: 1442.4588 - val_mse: 1442.4587 - val_mae: 26.4403\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2385.6669 - mse: 2385.6677 - mae: 28.9447 - val_loss: 1441.2251 - val_mse: 1441.2252 - val_mae: 26.4682\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 257us/step - loss: 2461.9223 - mse: 2461.9224 - mae: 29.4300 - val_loss: 1443.1821 - val_mse: 1443.1821 - val_mae: 26.3548\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2448.4075 - mse: 2448.4077 - mae: 28.7680 - val_loss: 1442.7948 - val_mse: 1442.7948 - val_mae: 26.4405\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 341us/step - loss: 2465.7840 - mse: 2465.7849 - mae: 28.9413 - val_loss: 1438.9848 - val_mse: 1438.9846 - val_mae: 26.5082\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2464.7501 - mse: 2464.7500 - mae: 28.8424 - val_loss: 1441.7879 - val_mse: 1441.7878 - val_mae: 26.4461\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2395.7703 - mse: 2395.7703 - mae: 28.7479 - val_loss: 1440.0282 - val_mse: 1440.0284 - val_mae: 26.4704\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 336us/step - loss: 2497.4557 - mse: 2497.4551 - mae: 29.3769 - val_loss: 1442.6258 - val_mse: 1442.6259 - val_mae: 26.4093\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2385.6303 - mse: 2385.6299 - mae: 28.2008 - val_loss: 1434.3642 - val_mse: 1434.3640 - val_mae: 26.5162\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2400.3444 - mse: 2400.3445 - mae: 28.5940 - val_loss: 1429.9338 - val_mse: 1429.9338 - val_mae: 26.6018\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 264us/step - loss: 2470.4264 - mse: 2470.4265 - mae: 29.2615 - val_loss: 1445.2965 - val_mse: 1445.2966 - val_mae: 26.3357\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 327us/step - loss: 2425.6538 - mse: 2425.6538 - mae: 28.8183 - val_loss: 1445.4729 - val_mse: 1445.4730 - val_mae: 26.3999\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2464.7101 - mse: 2464.7100 - mae: 29.0729 - val_loss: 1450.5546 - val_mse: 1450.5547 - val_mae: 26.2444\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2455.7233 - mse: 2455.7234 - mae: 28.6018 - val_loss: 1435.8369 - val_mse: 1435.8369 - val_mae: 26.4712\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 262us/step - loss: 2423.7782 - mse: 2423.7788 - mae: 28.6492 - val_loss: 1435.0143 - val_mse: 1435.0144 - val_mae: 26.5341\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2466.5344 - mse: 2466.5344 - mae: 28.8959 - val_loss: 1447.0627 - val_mse: 1447.0627 - val_mae: 26.3054\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2431.9572 - mse: 2431.9568 - mae: 28.7396 - val_loss: 1455.3691 - val_mse: 1455.3691 - val_mae: 26.2002\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 256us/step - loss: 2370.3880 - mse: 2370.3887 - mae: 28.0219 - val_loss: 1435.1839 - val_mse: 1435.1841 - val_mae: 26.5339\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2481.9200 - mse: 2481.9187 - mae: 29.0877 - val_loss: 1447.4902 - val_mse: 1447.4904 - val_mae: 26.2740\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2486.3935 - mse: 2486.3943 - mae: 28.6555 - val_loss: 1439.6880 - val_mse: 1439.6880 - val_mae: 26.4642\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2472.0798 - mse: 2472.0796 - mae: 28.7535 - val_loss: 1446.0767 - val_mse: 1446.0769 - val_mae: 26.3073\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2459.5479 - mse: 2459.5486 - mae: 28.7223 - val_loss: 1444.8907 - val_mse: 1444.8906 - val_mae: 26.4006\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2469.2978 - mse: 2469.2979 - mae: 28.6884 - val_loss: 1438.2014 - val_mse: 1438.2012 - val_mae: 26.5295\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2436.9696 - mse: 2436.9705 - mae: 28.9011 - val_loss: 1441.2750 - val_mse: 1441.2750 - val_mae: 26.4127\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 242us/step - loss: 2460.6501 - mse: 2460.6499 - mae: 28.7964 - val_loss: 1445.1701 - val_mse: 1445.1702 - val_mae: 26.3463\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 274us/step - loss: 2484.3200 - mse: 2484.3206 - mae: 28.6606 - val_loss: 1439.6909 - val_mse: 1439.6908 - val_mae: 26.4309\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 261us/step - loss: 2474.7866 - mse: 2474.7864 - mae: 28.7603 - val_loss: 1437.0693 - val_mse: 1437.0692 - val_mae: 26.5282\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2380.3846 - mse: 2380.3848 - mae: 28.2784 - val_loss: 1440.7060 - val_mse: 1440.7059 - val_mae: 26.4165\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 350us/step - loss: 2314.5455 - mse: 2314.5457 - mae: 28.8786 - val_loss: 3668.8227 - val_mse: 3668.8225 - val_mae: 24.1718\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2312.7506 - mse: 2312.7512 - mae: 28.8103 - val_loss: 3669.9216 - val_mse: 3669.9207 - val_mae: 24.5359\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2385.7618 - mse: 2385.7620 - mae: 29.2703 - val_loss: 3669.5161 - val_mse: 3669.5161 - val_mae: 24.4439\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2294.4422 - mse: 2294.4417 - mae: 29.0032 - val_loss: 3667.0957 - val_mse: 3667.0959 - val_mae: 24.1936\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 265us/step - loss: 2321.5924 - mse: 2321.5923 - mae: 29.0769 - val_loss: 3669.1108 - val_mse: 3669.1111 - val_mae: 24.3292\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2272.1960 - mse: 2272.1960 - mae: 28.8872 - val_loss: 3670.0254 - val_mse: 3670.0254 - val_mae: 24.3518\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2279.8899 - mse: 2279.8896 - mae: 28.9246 - val_loss: 3665.8953 - val_mse: 3665.8953 - val_mae: 24.0166\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2385.0916 - mse: 2385.0918 - mae: 29.5374 - val_loss: 3666.4286 - val_mse: 3666.4285 - val_mae: 24.1779\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2322.3150 - mse: 2322.3149 - mae: 29.1234 - val_loss: 3665.2560 - val_mse: 3665.2554 - val_mae: 23.8378\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 325us/step - loss: 2375.8668 - mse: 2375.8662 - mae: 29.5277 - val_loss: 3666.8247 - val_mse: 3666.8242 - val_mae: 24.4451\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2292.9184 - mse: 2292.9180 - mae: 28.6672 - val_loss: 3668.7023 - val_mse: 3668.7024 - val_mae: 24.4607\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 339us/step - loss: 2334.7279 - mse: 2334.7280 - mae: 29.1677 - val_loss: 3666.1964 - val_mse: 3666.1968 - val_mae: 24.0891\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2365.1481 - mse: 2365.1484 - mae: 28.9569 - val_loss: 3666.8090 - val_mse: 3666.8093 - val_mae: 24.0661\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 339us/step - loss: 2315.9416 - mse: 2315.9407 - mae: 28.8511 - val_loss: 3667.9860 - val_mse: 3667.9856 - val_mae: 24.3534\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2313.6371 - mse: 2313.6377 - mae: 29.0171 - val_loss: 3668.0860 - val_mse: 3668.0859 - val_mae: 24.2842\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2365.0367 - mse: 2365.0364 - mae: 29.2983 - val_loss: 3666.5713 - val_mse: 3666.5701 - val_mae: 24.1377\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2327.6702 - mse: 2327.6707 - mae: 28.9941 - val_loss: 3666.6146 - val_mse: 3666.6140 - val_mae: 24.1087\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2308.0273 - mse: 2308.0273 - mae: 29.2863 - val_loss: 3666.4017 - val_mse: 3666.4016 - val_mae: 23.7559\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2286.6547 - mse: 2286.6548 - mae: 28.8672 - val_loss: 3667.8173 - val_mse: 3667.8174 - val_mae: 24.1275\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2366.7234 - mse: 2366.7236 - mae: 29.1090 - val_loss: 3668.3114 - val_mse: 3668.3115 - val_mae: 24.2617\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2350.6841 - mse: 2350.6838 - mae: 29.0571 - val_loss: 3666.9685 - val_mse: 3666.9692 - val_mae: 24.3436\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2333.3733 - mse: 2333.3735 - mae: 28.8623 - val_loss: 3668.4255 - val_mse: 3668.4253 - val_mae: 24.3408\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2290.9769 - mse: 2290.9763 - mae: 28.8292 - val_loss: 3665.9513 - val_mse: 3665.9512 - val_mae: 23.9125\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2366.6007 - mse: 2366.6006 - mae: 29.2556 - val_loss: 3668.0404 - val_mse: 3668.0405 - val_mae: 24.1894\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2276.6894 - mse: 2276.6892 - mae: 28.6624 - val_loss: 3667.4090 - val_mse: 3667.4094 - val_mae: 24.0155\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2328.1058 - mse: 2328.1060 - mae: 28.9649 - val_loss: 3669.2152 - val_mse: 3669.2158 - val_mae: 24.1369\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2308.5936 - mse: 2308.5942 - mae: 29.0706 - val_loss: 3670.9610 - val_mse: 3670.9595 - val_mae: 24.4014\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2321.9709 - mse: 2321.9705 - mae: 29.4964 - val_loss: 3668.8683 - val_mse: 3668.8684 - val_mae: 23.9157\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2271.4507 - mse: 2271.4507 - mae: 28.9836 - val_loss: 3671.0388 - val_mse: 3671.0386 - val_mae: 24.2783\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 245us/step - loss: 2316.1974 - mse: 2316.1975 - mae: 29.1125 - val_loss: 3675.5965 - val_mse: 3675.5959 - val_mae: 24.7455\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2373.5921 - mse: 2373.5918 - mae: 29.4152 - val_loss: 3672.6686 - val_mse: 3672.6689 - val_mae: 24.4516\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 265us/step - loss: 2378.0330 - mse: 2378.0337 - mae: 29.4584 - val_loss: 3669.8637 - val_mse: 3669.8640 - val_mae: 24.0292\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2319.7531 - mse: 2319.7534 - mae: 28.8008 - val_loss: 3667.4774 - val_mse: 3667.4780 - val_mae: 24.0196\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2303.0581 - mse: 2303.0583 - mae: 28.8743 - val_loss: 3669.3059 - val_mse: 3669.3062 - val_mae: 24.2670\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2294.7605 - mse: 2294.7610 - mae: 29.1973 - val_loss: 3667.9062 - val_mse: 3667.9062 - val_mae: 24.1088\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2308.6139 - mse: 2308.6140 - mae: 28.7325 - val_loss: 3670.0255 - val_mse: 3670.0254 - val_mae: 24.3072\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2301.8554 - mse: 2301.8555 - mae: 28.7376 - val_loss: 3673.2073 - val_mse: 3673.2073 - val_mae: 24.6837\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2350.5121 - mse: 2350.5117 - mae: 29.2607 - val_loss: 3669.0799 - val_mse: 3669.0793 - val_mae: 23.8428\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2276.2135 - mse: 2276.2131 - mae: 28.6451 - val_loss: 3674.7584 - val_mse: 3674.7581 - val_mae: 24.8143\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2289.3962 - mse: 2289.3960 - mae: 28.6359 - val_loss: 3674.0026 - val_mse: 3674.0024 - val_mae: 24.9252\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 251us/step - loss: 2265.8578 - mse: 2265.8574 - mae: 28.8524 - val_loss: 3673.2699 - val_mse: 3673.2698 - val_mae: 24.6049\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2259.0086 - mse: 2259.0088 - mae: 28.7665 - val_loss: 3669.0434 - val_mse: 3669.0425 - val_mae: 24.0163\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 269us/step - loss: 2307.6093 - mse: 2307.6096 - mae: 29.2723 - val_loss: 3670.0097 - val_mse: 3670.0093 - val_mae: 24.2712\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2313.6700 - mse: 2313.6699 - mae: 28.8232 - val_loss: 3669.3297 - val_mse: 3669.3303 - val_mae: 24.1931\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2300.8297 - mse: 2300.8301 - mae: 29.0638 - val_loss: 3668.9734 - val_mse: 3668.9739 - val_mae: 23.9680\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2290.7890 - mse: 2290.7886 - mae: 28.5995 - val_loss: 3667.6330 - val_mse: 3667.6333 - val_mae: 24.0561\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 280us/step - loss: 2278.3259 - mse: 2278.3257 - mae: 28.9482 - val_loss: 3669.4203 - val_mse: 3669.4197 - val_mae: 24.3649\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2310.0831 - mse: 2310.0828 - mae: 28.9984 - val_loss: 3671.8550 - val_mse: 3671.8550 - val_mae: 24.5965\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2315.1197 - mse: 2315.1204 - mae: 28.8455 - val_loss: 3667.3611 - val_mse: 3667.3608 - val_mae: 24.2361\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2318.3420 - mse: 2318.3420 - mae: 29.0560 - val_loss: 3666.7840 - val_mse: 3666.7834 - val_mae: 24.0753\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2328.5647 - mse: 2328.5649 - mae: 29.1152 - val_loss: 3666.2067 - val_mse: 3666.2065 - val_mae: 23.8993\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2344.1821 - mse: 2344.1824 - mae: 29.2377 - val_loss: 3665.4772 - val_mse: 3665.4780 - val_mae: 23.8179\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2216.6122 - mse: 2216.6121 - mae: 28.6562 - val_loss: 3669.6211 - val_mse: 3669.6208 - val_mae: 24.3528\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 251us/step - loss: 2317.8205 - mse: 2317.8208 - mae: 28.9785 - val_loss: 3669.7824 - val_mse: 3669.7825 - val_mae: 24.2391\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 259us/step - loss: 2329.0930 - mse: 2329.0933 - mae: 29.2108 - val_loss: 3669.5390 - val_mse: 3669.5381 - val_mae: 24.3053\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 252us/step - loss: 2313.0091 - mse: 2313.0095 - mae: 28.9625 - val_loss: 3670.1688 - val_mse: 3670.1685 - val_mae: 24.2838\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2306.7743 - mse: 2306.7742 - mae: 28.9079 - val_loss: 3674.2802 - val_mse: 3674.2800 - val_mae: 24.8655\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2304.1674 - mse: 2304.1685 - mae: 28.8588 - val_loss: 3668.4521 - val_mse: 3668.4519 - val_mae: 24.3835\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2313.4135 - mse: 2313.4131 - mae: 28.7847 - val_loss: 3668.6714 - val_mse: 3668.6707 - val_mae: 24.4619\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2240.3513 - mse: 2240.3516 - mae: 28.4867 - val_loss: 3668.3171 - val_mse: 3668.3174 - val_mae: 24.4833\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2260.0277 - mse: 2260.0269 - mae: 28.6337 - val_loss: 3665.8315 - val_mse: 3665.8313 - val_mae: 24.2602\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2284.0854 - mse: 2284.0850 - mae: 28.8821 - val_loss: 3664.4349 - val_mse: 3664.4341 - val_mae: 24.0519\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2238.1536 - mse: 2238.1536 - mae: 28.7028 - val_loss: 3666.3169 - val_mse: 3666.3162 - val_mae: 24.3576\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 378us/step - loss: 2315.9790 - mse: 2315.9792 - mae: 28.9014 - val_loss: 3667.0869 - val_mse: 3667.0874 - val_mae: 24.3991\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2289.9582 - mse: 2289.9575 - mae: 28.4687 - val_loss: 3668.6555 - val_mse: 3668.6553 - val_mae: 24.4644\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2274.5558 - mse: 2274.5549 - mae: 29.0408 - val_loss: 3666.0573 - val_mse: 3666.0576 - val_mae: 24.1260\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2299.9162 - mse: 2299.9163 - mae: 28.6402 - val_loss: 3666.3833 - val_mse: 3666.3833 - val_mae: 24.2551\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 279us/step - loss: 2290.0040 - mse: 2290.0044 - mae: 28.7912 - val_loss: 3666.5876 - val_mse: 3666.5876 - val_mae: 24.3085\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2269.1854 - mse: 2269.1855 - mae: 28.9697 - val_loss: 3664.3040 - val_mse: 3664.3030 - val_mae: 24.0105\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2288.0638 - mse: 2288.0635 - mae: 28.8126 - val_loss: 3663.6073 - val_mse: 3663.6067 - val_mae: 23.8241\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2283.9760 - mse: 2283.9768 - mae: 28.7646 - val_loss: 3664.5919 - val_mse: 3664.5918 - val_mae: 24.1670\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2186.0845 - mse: 2186.0852 - mae: 28.3928 - val_loss: 3666.6364 - val_mse: 3666.6370 - val_mae: 24.3089\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2296.4583 - mse: 2296.4573 - mae: 28.7272 - val_loss: 3666.3131 - val_mse: 3666.3125 - val_mae: 24.1241\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 266us/step - loss: 2283.5398 - mse: 2283.5400 - mae: 28.8417 - val_loss: 3667.6013 - val_mse: 3667.6011 - val_mae: 24.3776\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2294.9852 - mse: 2294.9856 - mae: 28.9051 - val_loss: 3665.4425 - val_mse: 3665.4421 - val_mae: 24.0469\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2272.4255 - mse: 2272.4258 - mae: 28.7378 - val_loss: 3665.3524 - val_mse: 3665.3518 - val_mae: 24.3032\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2229.6299 - mse: 2229.6306 - mae: 28.5468 - val_loss: 3669.1468 - val_mse: 3669.1472 - val_mae: 24.6549\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2262.3700 - mse: 2262.3704 - mae: 28.9683 - val_loss: 3666.5136 - val_mse: 3666.5142 - val_mae: 24.2790\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 371us/step - loss: 2255.9883 - mse: 2255.9885 - mae: 28.4686 - val_loss: 3669.0675 - val_mse: 3669.0684 - val_mae: 24.3537\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2280.0914 - mse: 2280.0916 - mae: 28.8007 - val_loss: 3666.6268 - val_mse: 3666.6270 - val_mae: 24.2254\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 271us/step - loss: 2685.4642 - mse: 2685.4644 - mae: 28.1402 - val_loss: 2191.4119 - val_mse: 2191.4121 - val_mae: 25.9818\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2690.0370 - mse: 2690.0366 - mae: 28.3339 - val_loss: 2190.9764 - val_mse: 2190.9766 - val_mae: 26.0204\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 252us/step - loss: 2662.2152 - mse: 2662.2148 - mae: 28.1095 - val_loss: 2195.6463 - val_mse: 2195.6462 - val_mae: 25.8103\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2659.6526 - mse: 2659.6523 - mae: 28.3070 - val_loss: 2191.2751 - val_mse: 2191.2749 - val_mae: 25.7156\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2678.1365 - mse: 2678.1365 - mae: 28.4683 - val_loss: 2198.7667 - val_mse: 2198.7666 - val_mae: 25.6953\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2701.6655 - mse: 2701.6660 - mae: 28.3188 - val_loss: 2187.9195 - val_mse: 2187.9197 - val_mae: 25.8646\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2654.7856 - mse: 2654.7854 - mae: 27.9466 - val_loss: 2195.8950 - val_mse: 2195.8950 - val_mae: 25.7262\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2655.7200 - mse: 2655.7202 - mae: 28.3402 - val_loss: 2200.8226 - val_mse: 2200.8228 - val_mae: 25.6790\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 270us/step - loss: 2648.5619 - mse: 2648.5625 - mae: 27.7553 - val_loss: 2191.8707 - val_mse: 2191.8706 - val_mae: 25.9059\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 261us/step - loss: 2663.3232 - mse: 2663.3237 - mae: 27.7691 - val_loss: 2197.7455 - val_mse: 2197.7456 - val_mae: 25.8822\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 238us/step - loss: 2656.4082 - mse: 2656.4077 - mae: 28.0192 - val_loss: 2178.9822 - val_mse: 2178.9824 - val_mae: 26.1506\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2695.3339 - mse: 2695.3345 - mae: 28.4170 - val_loss: 2198.2652 - val_mse: 2198.2654 - val_mae: 25.8502\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2601.2163 - mse: 2601.2161 - mae: 27.9083 - val_loss: 2188.4605 - val_mse: 2188.4607 - val_mae: 25.9909\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 255us/step - loss: 2720.1883 - mse: 2720.1882 - mae: 28.4770 - val_loss: 2194.7211 - val_mse: 2194.7209 - val_mae: 25.8425\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2649.5531 - mse: 2649.5527 - mae: 27.9216 - val_loss: 2193.2864 - val_mse: 2193.2866 - val_mae: 26.0600\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2720.5294 - mse: 2720.5288 - mae: 28.6668 - val_loss: 2205.1482 - val_mse: 2205.1482 - val_mae: 25.9623\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2762.0721 - mse: 2762.0720 - mae: 28.6691 - val_loss: 2215.0555 - val_mse: 2215.0557 - val_mae: 25.8421\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2713.0929 - mse: 2713.0928 - mae: 28.4819 - val_loss: 2204.4118 - val_mse: 2204.4116 - val_mae: 26.0644\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2732.8773 - mse: 2732.8770 - mae: 28.4480 - val_loss: 2224.1414 - val_mse: 2224.1416 - val_mae: 25.7018\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2618.8824 - mse: 2618.8833 - mae: 27.7454 - val_loss: 2201.7277 - val_mse: 2201.7275 - val_mae: 25.9213\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2679.8655 - mse: 2679.8655 - mae: 28.0973 - val_loss: 2207.6273 - val_mse: 2207.6277 - val_mae: 25.7686\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2636.0520 - mse: 2636.0520 - mae: 27.7421 - val_loss: 2206.4622 - val_mse: 2206.4624 - val_mae: 25.8617\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2672.4379 - mse: 2672.4380 - mae: 28.2241 - val_loss: 2206.6705 - val_mse: 2206.6709 - val_mae: 25.8432\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2666.6024 - mse: 2666.6035 - mae: 28.1409 - val_loss: 2206.8180 - val_mse: 2206.8179 - val_mae: 25.6748\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2695.8546 - mse: 2695.8555 - mae: 28.1920 - val_loss: 2211.4238 - val_mse: 2211.4238 - val_mae: 25.8354\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 261us/step - loss: 2623.6264 - mse: 2623.6257 - mae: 27.9660 - val_loss: 2215.9642 - val_mse: 2215.9639 - val_mae: 25.7824\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2682.8436 - mse: 2682.8433 - mae: 28.2169 - val_loss: 2210.7258 - val_mse: 2210.7263 - val_mae: 25.9133\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 280us/step - loss: 2700.8483 - mse: 2700.8489 - mae: 28.3493 - val_loss: 2208.1620 - val_mse: 2208.1621 - val_mae: 25.9946\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2651.2941 - mse: 2651.2949 - mae: 28.1638 - val_loss: 2202.3094 - val_mse: 2202.3093 - val_mae: 26.1165\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 247us/step - loss: 2655.2081 - mse: 2655.2075 - mae: 28.3139 - val_loss: 2207.5210 - val_mse: 2207.5212 - val_mae: 26.1230\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2653.3318 - mse: 2653.3318 - mae: 28.2356 - val_loss: 2202.9395 - val_mse: 2202.9397 - val_mae: 26.0807\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2659.3265 - mse: 2659.3257 - mae: 28.2660 - val_loss: 2205.5683 - val_mse: 2205.5684 - val_mae: 25.9669\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2637.0371 - mse: 2637.0361 - mae: 28.0348 - val_loss: 2201.0676 - val_mse: 2201.0679 - val_mae: 26.0562\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2628.2297 - mse: 2628.2297 - mae: 28.0437 - val_loss: 2189.4348 - val_mse: 2189.4348 - val_mae: 26.0186\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2631.8629 - mse: 2631.8630 - mae: 28.0211 - val_loss: 2210.6231 - val_mse: 2210.6235 - val_mae: 25.7260\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2649.6393 - mse: 2649.6396 - mae: 28.0492 - val_loss: 2212.2710 - val_mse: 2212.2710 - val_mae: 25.8764\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2662.5890 - mse: 2662.5891 - mae: 28.0984 - val_loss: 2212.7241 - val_mse: 2212.7239 - val_mae: 25.9422\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2599.5981 - mse: 2599.5979 - mae: 27.8532 - val_loss: 2201.5916 - val_mse: 2201.5916 - val_mae: 26.0666\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2698.6455 - mse: 2698.6462 - mae: 28.1953 - val_loss: 2217.2902 - val_mse: 2217.2900 - val_mae: 25.9085\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2702.1504 - mse: 2702.1504 - mae: 28.2386 - val_loss: 2204.8587 - val_mse: 2204.8591 - val_mae: 25.9947\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2663.9347 - mse: 2663.9346 - mae: 28.6717 - val_loss: 2201.4618 - val_mse: 2201.4619 - val_mae: 26.0037\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2663.3172 - mse: 2663.3169 - mae: 28.0461 - val_loss: 2210.5129 - val_mse: 2210.5129 - val_mae: 25.9999\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 340us/step - loss: 2603.5376 - mse: 2603.5371 - mae: 27.8057 - val_loss: 2196.1148 - val_mse: 2196.1147 - val_mae: 26.2139\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2701.2358 - mse: 2701.2358 - mae: 28.4386 - val_loss: 2196.6083 - val_mse: 2196.6082 - val_mae: 26.0920\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2644.6113 - mse: 2644.6133 - mae: 28.0140 - val_loss: 2200.7129 - val_mse: 2200.7129 - val_mae: 26.0091\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2650.3554 - mse: 2650.3555 - mae: 28.1298 - val_loss: 2210.7164 - val_mse: 2210.7163 - val_mae: 25.8214\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2625.2356 - mse: 2625.2351 - mae: 27.7957 - val_loss: 2188.4811 - val_mse: 2188.4812 - val_mae: 26.0553\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2679.4869 - mse: 2679.4875 - mae: 28.2169 - val_loss: 2189.6272 - val_mse: 2189.6270 - val_mae: 25.8527\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2627.6565 - mse: 2627.6565 - mae: 27.6194 - val_loss: 2189.2544 - val_mse: 2189.2546 - val_mae: 25.9991\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2665.8608 - mse: 2665.8613 - mae: 27.9569 - val_loss: 2202.9287 - val_mse: 2202.9285 - val_mae: 25.8253\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2678.2380 - mse: 2678.2375 - mae: 28.1001 - val_loss: 2205.1723 - val_mse: 2205.1721 - val_mae: 25.9468\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 225us/step - loss: 2687.3364 - mse: 2687.3369 - mae: 28.4089 - val_loss: 2213.7960 - val_mse: 2213.7959 - val_mae: 25.8285\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 327us/step - loss: 2623.7540 - mse: 2623.7534 - mae: 27.8917 - val_loss: 2198.7468 - val_mse: 2198.7463 - val_mae: 26.0640\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2665.7083 - mse: 2665.7087 - mae: 28.5434 - val_loss: 2192.3516 - val_mse: 2192.3518 - val_mae: 26.0321\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2661.9203 - mse: 2661.9202 - mae: 28.2131 - val_loss: 2204.5254 - val_mse: 2204.5256 - val_mae: 25.9486\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2595.8614 - mse: 2595.8608 - mae: 28.1220 - val_loss: 2212.5495 - val_mse: 2212.5498 - val_mae: 25.8143\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 374us/step - loss: 2631.5862 - mse: 2631.5854 - mae: 28.1119 - val_loss: 2207.8462 - val_mse: 2207.8462 - val_mae: 25.9721\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2664.5448 - mse: 2664.5452 - mae: 28.3393 - val_loss: 2212.2283 - val_mse: 2212.2285 - val_mae: 25.9521\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2645.2022 - mse: 2645.2019 - mae: 28.0399 - val_loss: 2200.9663 - val_mse: 2200.9666 - val_mae: 26.1453\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2671.3879 - mse: 2671.3879 - mae: 28.2887 - val_loss: 2218.4451 - val_mse: 2218.4453 - val_mae: 25.8900\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2614.7324 - mse: 2614.7329 - mae: 27.9568 - val_loss: 2213.2776 - val_mse: 2213.2776 - val_mae: 25.9284\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2661.7571 - mse: 2661.7578 - mae: 28.1970 - val_loss: 2216.3317 - val_mse: 2216.3315 - val_mae: 25.9052\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2647.3732 - mse: 2647.3733 - mae: 28.0208 - val_loss: 2221.5175 - val_mse: 2221.5178 - val_mae: 25.8032\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2600.4882 - mse: 2600.4883 - mae: 27.6153 - val_loss: 2190.4113 - val_mse: 2190.4114 - val_mae: 26.3073\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2594.8033 - mse: 2594.8032 - mae: 27.9649 - val_loss: 2195.8933 - val_mse: 2195.8933 - val_mae: 26.0481\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2625.0860 - mse: 2625.0857 - mae: 27.7959 - val_loss: 2207.4554 - val_mse: 2207.4553 - val_mae: 25.9081\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 251us/step - loss: 2623.7550 - mse: 2623.7546 - mae: 28.0340 - val_loss: 2201.8212 - val_mse: 2201.8213 - val_mae: 26.0035\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 223us/step - loss: 2657.4777 - mse: 2657.4783 - mae: 27.9564 - val_loss: 2205.0845 - val_mse: 2205.0845 - val_mae: 26.2013\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2616.9078 - mse: 2616.9082 - mae: 27.9696 - val_loss: 2197.9619 - val_mse: 2197.9617 - val_mae: 26.2207\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2663.7030 - mse: 2663.7021 - mae: 27.9110 - val_loss: 2203.3593 - val_mse: 2203.3591 - val_mae: 26.1179\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2615.1066 - mse: 2615.1067 - mae: 27.8901 - val_loss: 2207.6993 - val_mse: 2207.6992 - val_mae: 25.8664\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2670.0138 - mse: 2670.0132 - mae: 28.1273 - val_loss: 2198.3241 - val_mse: 2198.3242 - val_mae: 26.1835\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 276us/step - loss: 2647.7996 - mse: 2647.8008 - mae: 28.0200 - val_loss: 2217.4770 - val_mse: 2217.4768 - val_mae: 25.9112\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2655.7069 - mse: 2655.7075 - mae: 28.1291 - val_loss: 2204.4473 - val_mse: 2204.4470 - val_mae: 26.0104\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2625.3239 - mse: 2625.3245 - mae: 28.1480 - val_loss: 2209.9106 - val_mse: 2209.9106 - val_mae: 25.9586\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2633.8255 - mse: 2633.8267 - mae: 27.8773 - val_loss: 2226.5075 - val_mse: 2226.5076 - val_mae: 25.8228\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2622.2342 - mse: 2622.2341 - mae: 27.9295 - val_loss: 2201.5683 - val_mse: 2201.5679 - val_mae: 25.9997\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 265us/step - loss: 2646.6877 - mse: 2646.6875 - mae: 27.9801 - val_loss: 2199.3252 - val_mse: 2199.3245 - val_mae: 26.1248\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 266us/step - loss: 2603.1243 - mse: 2603.1238 - mae: 27.8760 - val_loss: 2184.4416 - val_mse: 2184.4417 - val_mae: 26.3481\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2640.2190 - mse: 2640.2188 - mae: 28.0645 - val_loss: 2202.7569 - val_mse: 2202.7571 - val_mae: 25.9628\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13325.3603 - mse: 13325.3594 - mae: 109.8880 - val_loss: 34600.0943 - val_mse: 34600.0977 - val_mae: 132.6616\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 389us/step - loss: 13162.1709 - mse: 13162.1709 - mae: 109.1527 - val_loss: 34264.8192 - val_mse: 34264.8203 - val_mae: 131.4011\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 340us/step - loss: 12672.2105 - mse: 12672.2109 - mae: 106.9112 - val_loss: 33292.2834 - val_mse: 33292.2812 - val_mae: 127.6758\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 11330.8766 - mse: 11330.8770 - mae: 100.4680 - val_loss: 30715.7530 - val_mse: 30715.7520 - val_mae: 117.2402\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 371us/step - loss: 8202.0378 - mse: 8202.0381 - mae: 83.1486 - val_loss: 25041.4983 - val_mse: 25041.4980 - val_mae: 90.1173\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 245us/step - loss: 4019.5652 - mse: 4019.5657 - mae: 49.9204 - val_loss: 18422.2995 - val_mse: 18422.2988 - val_mae: 40.5234\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 386us/step - loss: 2512.9493 - mse: 2512.9495 - mae: 36.5630 - val_loss: 17381.9053 - val_mse: 17381.9062 - val_mae: 35.3230\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 261us/step - loss: 2582.7402 - mse: 2582.7402 - mae: 36.4770 - val_loss: 18004.2452 - val_mse: 18004.2441 - val_mae: 37.0784\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 2781.0034 - mse: 2781.0032 - mae: 37.6556 - val_loss: 17736.0980 - val_mse: 17736.0996 - val_mae: 35.7341\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 397us/step - loss: 2684.1541 - mse: 2684.1541 - mae: 36.7369 - val_loss: 17924.6346 - val_mse: 17924.6367 - val_mae: 36.6271\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 264us/step - loss: 2465.4368 - mse: 2465.4368 - mae: 36.2468 - val_loss: 17976.6745 - val_mse: 17976.6758 - val_mae: 36.9125\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 262us/step - loss: 2554.2527 - mse: 2554.2532 - mae: 36.2603 - val_loss: 17626.1277 - val_mse: 17626.1289 - val_mae: 35.4419\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 427us/step - loss: 2753.3037 - mse: 2753.3035 - mae: 37.5329 - val_loss: 17877.7615 - val_mse: 17877.7617 - val_mae: 36.3649\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2349.6078 - mse: 2349.6082 - mae: 34.5266 - val_loss: 17800.4622 - val_mse: 17800.4629 - val_mae: 35.9823\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2368.6250 - mse: 2368.6250 - mae: 33.7487 - val_loss: 17536.6713 - val_mse: 17536.6699 - val_mae: 35.3352\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 2241.5662 - mse: 2241.5659 - mae: 33.2853 - val_loss: 17514.9994 - val_mse: 17515.0000 - val_mae: 35.3427\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 362us/step - loss: 2461.2785 - mse: 2461.2788 - mae: 34.4051 - val_loss: 17674.0568 - val_mse: 17674.0586 - val_mae: 35.5733\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2455.3962 - mse: 2455.3960 - mae: 35.4172 - val_loss: 17728.7253 - val_mse: 17728.7246 - val_mae: 35.7327\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 391us/step - loss: 2481.8373 - mse: 2481.8374 - mae: 34.9309 - val_loss: 17765.5620 - val_mse: 17765.5625 - val_mae: 35.8636\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 264us/step - loss: 2268.2235 - mse: 2268.2234 - mae: 33.8079 - val_loss: 17753.5952 - val_mse: 17753.5957 - val_mae: 35.8243\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2558.2773 - mse: 2558.2771 - mae: 35.1426 - val_loss: 17827.8603 - val_mse: 17827.8613 - val_mae: 36.1128\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2548.9687 - mse: 2548.9688 - mae: 36.7378 - val_loss: 17885.8042 - val_mse: 17885.8047 - val_mae: 36.4073\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 2218.3224 - mse: 2218.3223 - mae: 33.5475 - val_loss: 17766.2031 - val_mse: 17766.2031 - val_mae: 35.8782\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 2360.4509 - mse: 2360.4504 - mae: 34.3942 - val_loss: 17720.5823 - val_mse: 17720.5820 - val_mae: 35.7352\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2285.4052 - mse: 2285.4050 - mae: 34.4632 - val_loss: 17712.0704 - val_mse: 17712.0703 - val_mae: 35.7173\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 2243.5462 - mse: 2243.5459 - mae: 34.0588 - val_loss: 17691.1866 - val_mse: 17691.1875 - val_mae: 35.6642\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 316us/step - loss: 2192.7172 - mse: 2192.7173 - mae: 33.5494 - val_loss: 17693.2959 - val_mse: 17693.2969 - val_mae: 35.6756\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2227.7376 - mse: 2227.7375 - mae: 32.6107 - val_loss: 17493.9731 - val_mse: 17493.9746 - val_mae: 35.4582\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2286.4239 - mse: 2286.4241 - mae: 33.3269 - val_loss: 17770.8434 - val_mse: 17770.8438 - val_mae: 35.9094\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 325us/step - loss: 2114.6923 - mse: 2114.6924 - mae: 33.2767 - val_loss: 17385.7124 - val_mse: 17385.7129 - val_mae: 35.5543\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2239.4543 - mse: 2239.4543 - mae: 33.2535 - val_loss: 17606.1827 - val_mse: 17606.1816 - val_mae: 35.5103\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2462.8502 - mse: 2462.8501 - mae: 35.0568 - val_loss: 17785.7480 - val_mse: 17785.7480 - val_mae: 35.9594\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2036.3680 - mse: 2036.3683 - mae: 32.4597 - val_loss: 17581.8036 - val_mse: 17581.8047 - val_mae: 35.4869\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2149.6482 - mse: 2149.6482 - mae: 31.5731 - val_loss: 17492.7418 - val_mse: 17492.7422 - val_mae: 35.4957\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2238.6650 - mse: 2238.6648 - mae: 34.6662 - val_loss: 17766.4565 - val_mse: 17766.4570 - val_mae: 35.9065\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 402us/step - loss: 2416.8807 - mse: 2416.8806 - mae: 33.7353 - val_loss: 17617.1983 - val_mse: 17617.1973 - val_mae: 35.5521\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 393us/step - loss: 2116.8704 - mse: 2116.8708 - mae: 32.0654 - val_loss: 17866.0718 - val_mse: 17866.0723 - val_mae: 36.2810\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 402us/step - loss: 2306.3009 - mse: 2306.3013 - mae: 34.4960 - val_loss: 17820.8754 - val_mse: 17820.8730 - val_mae: 36.0993\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 350us/step - loss: 2079.8886 - mse: 2079.8887 - mae: 32.2897 - val_loss: 17565.7751 - val_mse: 17565.7734 - val_mae: 35.5301\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 361us/step - loss: 2087.4435 - mse: 2087.4436 - mae: 31.7635 - val_loss: 17612.8108 - val_mse: 17612.8105 - val_mae: 35.5718\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2159.1643 - mse: 2159.1646 - mae: 32.5981 - val_loss: 17808.4477 - val_mse: 17808.4453 - val_mae: 36.0690\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2121.6230 - mse: 2121.6230 - mae: 31.9468 - val_loss: 17527.9148 - val_mse: 17527.9141 - val_mae: 35.5555\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 2041.7864 - mse: 2041.7866 - mae: 32.6165 - val_loss: 17808.9058 - val_mse: 17808.9062 - val_mae: 36.0791\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 1894.9335 - mse: 1894.9335 - mae: 30.5961 - val_loss: 17627.8213 - val_mse: 17627.8242 - val_mae: 35.6193\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2194.5389 - mse: 2194.5391 - mae: 31.9158 - val_loss: 17544.6730 - val_mse: 17544.6738 - val_mae: 35.5743\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 316us/step - loss: 1964.1483 - mse: 1964.1482 - mae: 32.2596 - val_loss: 17623.9300 - val_mse: 17623.9316 - val_mae: 35.6215\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 2149.5573 - mse: 2149.5571 - mae: 31.7346 - val_loss: 17701.2225 - val_mse: 17701.2246 - val_mae: 35.7729\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2224.7742 - mse: 2224.7742 - mae: 32.1971 - val_loss: 17655.9868 - val_mse: 17655.9863 - val_mae: 35.6837\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 1950.3000 - mse: 1950.3000 - mae: 29.9454 - val_loss: 17530.7303 - val_mse: 17530.7305 - val_mae: 35.6081\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 326us/step - loss: 2069.6470 - mse: 2069.6470 - mae: 31.4947 - val_loss: 17728.8622 - val_mse: 17728.8633 - val_mae: 35.8643\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2037.2063 - mse: 2037.2062 - mae: 31.3201 - val_loss: 17569.1698 - val_mse: 17569.1699 - val_mae: 35.6076\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2062.0727 - mse: 2062.0725 - mae: 31.2887 - val_loss: 17700.4363 - val_mse: 17700.4375 - val_mae: 35.7909\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2225.9308 - mse: 2225.9307 - mae: 32.7266 - val_loss: 17659.1142 - val_mse: 17659.1133 - val_mae: 35.7117\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2090.9274 - mse: 2090.9275 - mae: 31.2066 - val_loss: 17660.6234 - val_mse: 17660.6211 - val_mae: 35.7182\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 1995.1457 - mse: 1995.1456 - mae: 30.1230 - val_loss: 17576.7560 - val_mse: 17576.7559 - val_mae: 35.6364\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2214.0011 - mse: 2214.0010 - mae: 31.6583 - val_loss: 17695.4265 - val_mse: 17695.4258 - val_mae: 35.7991\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2024.4248 - mse: 2024.4247 - mae: 30.6860 - val_loss: 17652.9581 - val_mse: 17652.9570 - val_mae: 35.7230\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 240us/step - loss: 2275.7842 - mse: 2275.7839 - mae: 32.9523 - val_loss: 17745.4954 - val_mse: 17745.4941 - val_mae: 35.9397\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 336us/step - loss: 2230.9599 - mse: 2230.9597 - mae: 32.9003 - val_loss: 17671.1843 - val_mse: 17671.1836 - val_mae: 35.7605\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 257us/step - loss: 1885.7842 - mse: 1885.7841 - mae: 30.1609 - val_loss: 17665.8536 - val_mse: 17665.8535 - val_mae: 35.7565\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2118.6516 - mse: 2118.6519 - mae: 31.4729 - val_loss: 17624.6371 - val_mse: 17624.6387 - val_mae: 35.7009\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 243us/step - loss: 2105.5287 - mse: 2105.5288 - mae: 30.6126 - val_loss: 17547.6532 - val_mse: 17547.6543 - val_mae: 35.7074\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 1942.8210 - mse: 1942.8209 - mae: 29.9691 - val_loss: 17607.6684 - val_mse: 17607.6680 - val_mae: 35.6983\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 1976.4935 - mse: 1976.4937 - mae: 30.9322 - val_loss: 17695.6271 - val_mse: 17695.6289 - val_mae: 35.8364\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 1952.1236 - mse: 1952.1237 - mae: 30.8338 - val_loss: 17526.2449 - val_mse: 17526.2461 - val_mae: 35.7659\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 280us/step - loss: 1922.2712 - mse: 1922.2711 - mae: 30.0551 - val_loss: 17650.5578 - val_mse: 17650.5586 - val_mae: 35.7609\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 1938.3507 - mse: 1938.3505 - mae: 30.1896 - val_loss: 17515.0941 - val_mse: 17515.0938 - val_mae: 35.8054\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 1930.2067 - mse: 1930.2068 - mae: 30.8059 - val_loss: 17636.3561 - val_mse: 17636.3555 - val_mae: 35.7535\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 1931.4096 - mse: 1931.4095 - mae: 29.9035 - val_loss: 17532.4556 - val_mse: 17532.4551 - val_mae: 35.8040\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 209us/step - loss: 2092.7226 - mse: 2092.7224 - mae: 31.6017 - val_loss: 17643.1735 - val_mse: 17643.1738 - val_mae: 35.7707\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 219us/step - loss: 1898.6634 - mse: 1898.6633 - mae: 30.2402 - val_loss: 17495.0725 - val_mse: 17495.0723 - val_mae: 35.8768\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 380us/step - loss: 1982.0626 - mse: 1982.0625 - mae: 30.7817 - val_loss: 17777.0380 - val_mse: 17777.0371 - val_mae: 36.0807\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 258us/step - loss: 1983.0736 - mse: 1983.0735 - mae: 31.1197 - val_loss: 17541.8275 - val_mse: 17541.8262 - val_mae: 35.8311\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 243us/step - loss: 2053.5152 - mse: 2053.5154 - mae: 30.7766 - val_loss: 17629.9167 - val_mse: 17629.9160 - val_mae: 35.7723\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 1892.6114 - mse: 1892.6113 - mae: 29.6151 - val_loss: 17557.3712 - val_mse: 17557.3691 - val_mae: 35.8325\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 1893.7012 - mse: 1893.7013 - mae: 29.7903 - val_loss: 17537.3645 - val_mse: 17537.3633 - val_mae: 35.8739\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2002.7180 - mse: 2002.7183 - mae: 30.2983 - val_loss: 17537.5861 - val_mse: 17537.5859 - val_mae: 35.8896\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 412us/step - loss: 1950.3291 - mse: 1950.3289 - mae: 30.5161 - val_loss: 17643.8460 - val_mse: 17643.8457 - val_mae: 35.8185\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 1854.8324 - mse: 1854.8323 - mae: 29.1202 - val_loss: 17554.8386 - val_mse: 17554.8359 - val_mae: 35.8832\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2017.9768 - mse: 2017.9769 - mae: 31.5701 - val_loss: 17730.6813 - val_mse: 17730.6797 - val_mae: 36.0026\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 343us/step - loss: 4316.8400 - mse: 4316.8394 - mae: 34.7702 - val_loss: 2120.4285 - val_mse: 2120.4285 - val_mae: 31.4395\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4207.7962 - mse: 4207.7959 - mae: 35.3337 - val_loss: 2251.0310 - val_mse: 2251.0308 - val_mae: 31.7485\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4207.8951 - mse: 4207.8950 - mae: 34.5797 - val_loss: 2255.8122 - val_mse: 2255.8120 - val_mae: 31.7610\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4256.2577 - mse: 4256.2578 - mae: 35.1698 - val_loss: 2234.4337 - val_mse: 2234.4338 - val_mae: 31.7023\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4392.5609 - mse: 4392.5610 - mae: 36.1495 - val_loss: 2269.4412 - val_mse: 2269.4407 - val_mae: 31.7993\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4269.9824 - mse: 4269.9819 - mae: 34.6908 - val_loss: 2254.5308 - val_mse: 2254.5308 - val_mae: 31.7576\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4226.9380 - mse: 4226.9380 - mae: 35.1260 - val_loss: 2291.3521 - val_mse: 2291.3521 - val_mae: 31.8635\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4334.3249 - mse: 4334.3252 - mae: 34.2896 - val_loss: 2266.1497 - val_mse: 2266.1494 - val_mae: 31.7884\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4142.7632 - mse: 4142.7632 - mae: 34.5414 - val_loss: 2275.6626 - val_mse: 2275.6626 - val_mae: 31.8132\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4252.2225 - mse: 4252.2222 - mae: 35.2914 - val_loss: 2299.2034 - val_mse: 2299.2034 - val_mae: 31.8845\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4305.1728 - mse: 4305.1724 - mae: 34.5619 - val_loss: 2294.0372 - val_mse: 2294.0374 - val_mae: 31.8676\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4357.7302 - mse: 4357.7305 - mae: 35.4232 - val_loss: 2249.2014 - val_mse: 2249.2014 - val_mae: 31.7400\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4184.4546 - mse: 4184.4556 - mae: 34.8327 - val_loss: 2274.5033 - val_mse: 2274.5032 - val_mae: 31.8089\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4192.3486 - mse: 4192.3486 - mae: 35.2057 - val_loss: 2273.6305 - val_mse: 2273.6306 - val_mae: 31.8088\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4296.8306 - mse: 4296.8311 - mae: 34.9774 - val_loss: 2300.8093 - val_mse: 2300.8096 - val_mae: 31.8870\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 254us/step - loss: 4342.7074 - mse: 4342.7085 - mae: 35.5899 - val_loss: 2294.8708 - val_mse: 2294.8711 - val_mae: 31.8708\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 257us/step - loss: 4178.1012 - mse: 4178.1006 - mae: 35.0071 - val_loss: 2267.4460 - val_mse: 2267.4460 - val_mae: 31.7954\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 354us/step - loss: 4197.1031 - mse: 4197.1030 - mae: 35.3816 - val_loss: 2286.9828 - val_mse: 2286.9827 - val_mae: 31.8497\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 355us/step - loss: 4259.8240 - mse: 4259.8252 - mae: 35.5894 - val_loss: 2262.4524 - val_mse: 2262.4521 - val_mae: 31.7829\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4142.3172 - mse: 4142.3174 - mae: 33.6528 - val_loss: 2272.2086 - val_mse: 2272.2087 - val_mae: 31.8092\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 347us/step - loss: 4305.4241 - mse: 4305.4238 - mae: 34.4356 - val_loss: 2264.1904 - val_mse: 2264.1904 - val_mae: 31.7865\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4221.5728 - mse: 4221.5732 - mae: 35.0510 - val_loss: 2341.7887 - val_mse: 2341.7886 - val_mae: 32.0222\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 4414.5231 - mse: 4414.5229 - mae: 35.9006 - val_loss: 2266.1771 - val_mse: 2266.1770 - val_mae: 31.7930\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4175.0344 - mse: 4175.0342 - mae: 34.2317 - val_loss: 2230.3388 - val_mse: 2230.3389 - val_mae: 31.6985\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4017.8103 - mse: 4017.8105 - mae: 34.6422 - val_loss: 2284.3680 - val_mse: 2284.3679 - val_mae: 31.8425\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4324.8779 - mse: 4324.8770 - mae: 35.0088 - val_loss: 2331.4700 - val_mse: 2331.4700 - val_mae: 31.9856\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 285us/step - loss: 4230.1169 - mse: 4230.1167 - mae: 34.1593 - val_loss: 2337.5355 - val_mse: 2337.5352 - val_mae: 32.0051\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4175.0841 - mse: 4175.0854 - mae: 34.1298 - val_loss: 2302.7888 - val_mse: 2302.7891 - val_mae: 31.8952\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 277us/step - loss: 4201.1213 - mse: 4201.1206 - mae: 35.2349 - val_loss: 2340.8446 - val_mse: 2340.8445 - val_mae: 32.0158\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 231us/step - loss: 4040.8940 - mse: 4040.8933 - mae: 33.8766 - val_loss: 2241.6282 - val_mse: 2241.6279 - val_mae: 31.7224\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4306.0657 - mse: 4306.0659 - mae: 35.2587 - val_loss: 2288.5763 - val_mse: 2288.5764 - val_mae: 31.8513\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4131.2244 - mse: 4131.2246 - mae: 35.4799 - val_loss: 2301.1920 - val_mse: 2301.1917 - val_mae: 31.8876\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 332us/step - loss: 4111.1264 - mse: 4111.1265 - mae: 33.7518 - val_loss: 2258.2299 - val_mse: 2258.2300 - val_mae: 31.7661\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4102.0373 - mse: 4102.0366 - mae: 33.0732 - val_loss: 2314.4961 - val_mse: 2314.4963 - val_mae: 31.9297\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 297us/step - loss: 4217.7915 - mse: 4217.7915 - mae: 34.3417 - val_loss: 2311.7116 - val_mse: 2311.7117 - val_mae: 31.9208\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4240.4847 - mse: 4240.4844 - mae: 35.0815 - val_loss: 2306.7634 - val_mse: 2306.7634 - val_mae: 31.9071\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4020.2670 - mse: 4020.2673 - mae: 34.2801 - val_loss: 2314.9859 - val_mse: 2314.9858 - val_mae: 31.9329\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 265us/step - loss: 4198.3362 - mse: 4198.3359 - mae: 34.8056 - val_loss: 2330.2190 - val_mse: 2330.2190 - val_mae: 31.9825\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 234us/step - loss: 4208.4943 - mse: 4208.4946 - mae: 34.2569 - val_loss: 2273.0560 - val_mse: 2273.0557 - val_mae: 31.8082\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4030.2737 - mse: 4030.2737 - mae: 33.1363 - val_loss: 2306.0793 - val_mse: 2306.0793 - val_mae: 31.9053\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4203.6718 - mse: 4203.6719 - mae: 33.8029 - val_loss: 2284.3285 - val_mse: 2284.3284 - val_mae: 31.8424\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4154.5036 - mse: 4154.5034 - mae: 34.1121 - val_loss: 2232.6033 - val_mse: 2232.6033 - val_mae: 31.7054\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4281.8611 - mse: 4281.8599 - mae: 34.7667 - val_loss: 2306.6945 - val_mse: 2306.6948 - val_mae: 31.9061\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 271us/step - loss: 4169.8360 - mse: 4169.8364 - mae: 34.2681 - val_loss: 2341.5264 - val_mse: 2341.5266 - val_mae: 32.0190\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 246us/step - loss: 4146.2899 - mse: 4146.2905 - mae: 33.1552 - val_loss: 2313.9090 - val_mse: 2313.9089 - val_mae: 31.9286\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4139.3354 - mse: 4139.3354 - mae: 35.2768 - val_loss: 2290.6398 - val_mse: 2290.6399 - val_mae: 31.8573\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 265us/step - loss: 4039.5914 - mse: 4039.5911 - mae: 32.7443 - val_loss: 2285.9986 - val_mse: 2285.9988 - val_mae: 31.8433\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 3948.1434 - mse: 3948.1431 - mae: 32.9671 - val_loss: 2314.4271 - val_mse: 2314.4270 - val_mae: 31.9290\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4128.1125 - mse: 4128.1128 - mae: 33.8516 - val_loss: 2344.1515 - val_mse: 2344.1516 - val_mae: 32.0240\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4084.8838 - mse: 4084.8833 - mae: 33.8924 - val_loss: 2250.6500 - val_mse: 2250.6499 - val_mae: 31.7472\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 349us/step - loss: 4147.1033 - mse: 4147.1030 - mae: 34.0140 - val_loss: 2301.8509 - val_mse: 2301.8508 - val_mae: 31.8891\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 339us/step - loss: 4204.5455 - mse: 4204.5459 - mae: 33.6709 - val_loss: 2265.1034 - val_mse: 2265.1033 - val_mae: 31.7802\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4117.0098 - mse: 4117.0103 - mae: 33.4622 - val_loss: 2373.1963 - val_mse: 2373.1963 - val_mae: 32.1140\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4013.7628 - mse: 4013.7625 - mae: 33.0126 - val_loss: 2247.8825 - val_mse: 2247.8823 - val_mae: 31.7396\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4116.0903 - mse: 4116.0903 - mae: 34.1234 - val_loss: 2247.6499 - val_mse: 2247.6499 - val_mae: 31.7380\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4181.0612 - mse: 4181.0610 - mae: 34.4049 - val_loss: 2320.0234 - val_mse: 2320.0234 - val_mae: 31.9440\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4112.5161 - mse: 4112.5166 - mae: 33.9077 - val_loss: 2324.0521 - val_mse: 2324.0520 - val_mae: 31.9587\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 242us/step - loss: 4228.4061 - mse: 4228.4062 - mae: 34.0477 - val_loss: 2327.9454 - val_mse: 2327.9456 - val_mae: 31.9724\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 339us/step - loss: 3953.8151 - mse: 3953.8147 - mae: 32.8036 - val_loss: 2304.3770 - val_mse: 2304.3772 - val_mae: 31.8964\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4138.4063 - mse: 4138.4058 - mae: 33.2696 - val_loss: 2297.7991 - val_mse: 2297.7993 - val_mae: 31.8770\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 359us/step - loss: 4179.2991 - mse: 4179.2993 - mae: 34.4357 - val_loss: 2296.5748 - val_mse: 2296.5750 - val_mae: 31.8724\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 3935.2244 - mse: 3935.2244 - mae: 33.1511 - val_loss: 2285.4850 - val_mse: 2285.4854 - val_mae: 31.8396\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 3887.5231 - mse: 3887.5227 - mae: 32.7909 - val_loss: 2262.9710 - val_mse: 2262.9712 - val_mae: 31.7788\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 350us/step - loss: 4197.2077 - mse: 4197.2075 - mae: 34.0193 - val_loss: 2325.6215 - val_mse: 2325.6218 - val_mae: 31.9630\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4003.5104 - mse: 4003.5107 - mae: 33.2582 - val_loss: 2329.2416 - val_mse: 2329.2417 - val_mae: 31.9748\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 3868.0639 - mse: 3868.0640 - mae: 33.3947 - val_loss: 2268.7836 - val_mse: 2268.7834 - val_mae: 31.7941\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 256us/step - loss: 4122.4225 - mse: 4122.4224 - mae: 32.7408 - val_loss: 2332.4643 - val_mse: 2332.4644 - val_mae: 31.9831\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 347us/step - loss: 4098.2017 - mse: 4098.2012 - mae: 33.8664 - val_loss: 2327.4322 - val_mse: 2327.4319 - val_mae: 31.9670\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4032.7842 - mse: 4032.7842 - mae: 32.7601 - val_loss: 2339.9151 - val_mse: 2339.9153 - val_mae: 32.0068\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4021.0540 - mse: 4021.0544 - mae: 33.4626 - val_loss: 2300.7207 - val_mse: 2300.7207 - val_mae: 31.8841\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 3976.8030 - mse: 3976.8030 - mae: 33.1447 - val_loss: 2284.6458 - val_mse: 2284.6460 - val_mae: 31.8356\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4067.2726 - mse: 4067.2727 - mae: 33.0322 - val_loss: 2310.9575 - val_mse: 2310.9575 - val_mae: 31.9110\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 328us/step - loss: 4091.1675 - mse: 4091.1677 - mae: 33.5552 - val_loss: 2337.9233 - val_mse: 2337.9236 - val_mae: 31.9969\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4053.0107 - mse: 4053.0105 - mae: 33.1886 - val_loss: 2288.7321 - val_mse: 2288.7322 - val_mae: 31.8459\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4133.7105 - mse: 4133.7100 - mae: 32.9745 - val_loss: 2348.5574 - val_mse: 2348.5576 - val_mae: 32.0293\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4068.2444 - mse: 4068.2444 - mae: 32.6786 - val_loss: 2321.9793 - val_mse: 2321.9792 - val_mae: 31.9422\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 323us/step - loss: 4086.1819 - mse: 4086.1816 - mae: 33.6255 - val_loss: 2262.4701 - val_mse: 2262.4702 - val_mae: 31.7742\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4102.8475 - mse: 4102.8472 - mae: 33.2776 - val_loss: 2316.4503 - val_mse: 2316.4502 - val_mae: 31.9242\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4012.5036 - mse: 4012.5034 - mae: 33.2734 - val_loss: 2309.3409 - val_mse: 2309.3408 - val_mae: 31.9028\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 3867.4669 - mse: 3867.4673 - mae: 32.7628 - val_loss: 2285.5926 - val_mse: 2285.5925 - val_mae: 31.8341\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 240us/step - loss: 3330.5644 - mse: 3330.5635 - mae: 33.2977 - val_loss: 1434.2602 - val_mse: 1434.2603 - val_mae: 25.5481\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3354.3304 - mse: 3354.3301 - mae: 33.1319 - val_loss: 1434.0227 - val_mse: 1434.0226 - val_mae: 25.2386\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 340us/step - loss: 3552.9397 - mse: 3552.9402 - mae: 33.2080 - val_loss: 1435.3660 - val_mse: 1435.3660 - val_mae: 24.9442\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3489.2765 - mse: 3489.2764 - mae: 33.2803 - val_loss: 1436.3585 - val_mse: 1436.3584 - val_mae: 25.7405\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 271us/step - loss: 3290.3007 - mse: 3290.3010 - mae: 32.8809 - val_loss: 1435.8175 - val_mse: 1435.8176 - val_mae: 25.5570\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3435.2935 - mse: 3435.2937 - mae: 32.8374 - val_loss: 1445.7502 - val_mse: 1445.7501 - val_mae: 26.7187\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 368us/step - loss: 3543.1196 - mse: 3543.1196 - mae: 33.8510 - val_loss: 1437.4839 - val_mse: 1437.4838 - val_mae: 24.9245\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3443.8263 - mse: 3443.8271 - mae: 32.8674 - val_loss: 1436.8586 - val_mse: 1436.8586 - val_mae: 25.1750\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 276us/step - loss: 3343.7215 - mse: 3343.7214 - mae: 32.8553 - val_loss: 1437.2125 - val_mse: 1437.2124 - val_mae: 25.1262\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3402.0846 - mse: 3402.0833 - mae: 32.7421 - val_loss: 1440.1504 - val_mse: 1440.1504 - val_mae: 25.9725\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 227us/step - loss: 3493.2766 - mse: 3493.2773 - mae: 33.6108 - val_loss: 1438.1874 - val_mse: 1438.1874 - val_mae: 25.1909\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 274us/step - loss: 3304.5426 - mse: 3304.5427 - mae: 31.6746 - val_loss: 1446.4998 - val_mse: 1446.4996 - val_mae: 26.6018\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3438.1191 - mse: 3438.1191 - mae: 33.0936 - val_loss: 1439.0547 - val_mse: 1439.0548 - val_mae: 25.5601\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3351.2529 - mse: 3351.2529 - mae: 32.7699 - val_loss: 1443.3292 - val_mse: 1443.3291 - val_mae: 26.2322\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3397.0258 - mse: 3397.0269 - mae: 32.6253 - val_loss: 1443.0195 - val_mse: 1443.0194 - val_mae: 26.1677\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3354.5355 - mse: 3354.5359 - mae: 32.7554 - val_loss: 1440.2775 - val_mse: 1440.2776 - val_mae: 25.6717\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3472.0105 - mse: 3472.0107 - mae: 33.4279 - val_loss: 1440.3276 - val_mse: 1440.3274 - val_mae: 25.1438\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3388.9815 - mse: 3388.9812 - mae: 32.2785 - val_loss: 1442.1986 - val_mse: 1442.1985 - val_mae: 25.8721\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3357.8474 - mse: 3357.8479 - mae: 32.9251 - val_loss: 1442.3075 - val_mse: 1442.3075 - val_mae: 25.7641\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3364.6255 - mse: 3364.6255 - mae: 32.6725 - val_loss: 1442.7006 - val_mse: 1442.7006 - val_mae: 25.8438\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3349.5767 - mse: 3349.5767 - mae: 32.4863 - val_loss: 1443.4608 - val_mse: 1443.4607 - val_mae: 25.8737\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3336.4374 - mse: 3336.4373 - mae: 32.11 - 0s 275us/step - loss: 3270.2200 - mse: 3270.2200 - mae: 32.0748 - val_loss: 1442.7920 - val_mse: 1442.7921 - val_mae: 25.1630\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3440.4345 - mse: 3440.4351 - mae: 32.4110 - val_loss: 1443.3212 - val_mse: 1443.3212 - val_mae: 25.5799\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 373us/step - loss: 3401.1044 - mse: 3401.1033 - mae: 32.8555 - val_loss: 1446.2571 - val_mse: 1446.2570 - val_mae: 26.1337\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 352us/step - loss: 3340.7412 - mse: 3340.7422 - mae: 32.5114 - val_loss: 1443.5989 - val_mse: 1443.5989 - val_mae: 25.4373\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 342us/step - loss: 3338.9040 - mse: 3338.9036 - mae: 32.4753 - val_loss: 1447.9584 - val_mse: 1447.9584 - val_mae: 26.2733\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3318.2032 - mse: 3318.2026 - mae: 32.2590 - val_loss: 1445.6910 - val_mse: 1445.6910 - val_mae: 25.9742\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3364.0164 - mse: 3364.0171 - mae: 32.2918 - val_loss: 1443.9843 - val_mse: 1443.9843 - val_mae: 25.2055\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 365us/step - loss: 3337.8785 - mse: 3337.8789 - mae: 33.0678 - val_loss: 1444.5942 - val_mse: 1444.5941 - val_mae: 25.5604\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3343.9159 - mse: 3343.9160 - mae: 32.3207 - val_loss: 1445.3964 - val_mse: 1445.3964 - val_mae: 25.7209\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3469.1449 - mse: 3469.1455 - mae: 33.0868 - val_loss: 1446.0150 - val_mse: 1446.0149 - val_mae: 25.1208\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3410.3236 - mse: 3410.3230 - mae: 32.4283 - val_loss: 1446.0139 - val_mse: 1446.0140 - val_mae: 25.2977\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3320.7509 - mse: 3320.7512 - mae: 31.9943 - val_loss: 1450.1787 - val_mse: 1450.1786 - val_mae: 26.2988\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3208.6906 - mse: 3208.6902 - mae: 32.0686 - val_loss: 1448.7972 - val_mse: 1448.7971 - val_mae: 26.1030\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3286.0701 - mse: 3286.0708 - mae: 31.8780 - val_loss: 1447.7384 - val_mse: 1447.7383 - val_mae: 25.8738\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 270us/step - loss: 3287.2148 - mse: 3287.2146 - mae: 32.2380 - val_loss: 1447.6472 - val_mse: 1447.6471 - val_mae: 25.8081\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3420.6682 - mse: 3420.6682 - mae: 33.3197 - val_loss: 1446.3422 - val_mse: 1446.3420 - val_mae: 25.4229\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 352us/step - loss: 3300.8000 - mse: 3300.7996 - mae: 32.2754 - val_loss: 1447.2761 - val_mse: 1447.2762 - val_mae: 25.7563\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3174.2049 - mse: 3174.2053 - mae: 31.2668 - val_loss: 1450.7011 - val_mse: 1450.7010 - val_mae: 26.2265\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3394.1553 - mse: 3394.1548 - mae: 32.6376 - val_loss: 1449.6494 - val_mse: 1449.6495 - val_mae: 25.9838\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3316.2819 - mse: 3316.2822 - mae: 32.2695 - val_loss: 1453.4238 - val_mse: 1453.4238 - val_mae: 26.4182\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3411.9017 - mse: 3411.9023 - mae: 33.0872 - val_loss: 1449.7649 - val_mse: 1449.7650 - val_mae: 25.1278\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3306.1000 - mse: 3306.1003 - mae: 32.0387 - val_loss: 1450.0629 - val_mse: 1450.0629 - val_mae: 25.9622\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 372us/step - loss: 3332.6344 - mse: 3332.6348 - mae: 32.1904 - val_loss: 1452.9738 - val_mse: 1452.9738 - val_mae: 26.4017\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 339us/step - loss: 3321.5754 - mse: 3321.5757 - mae: 32.1266 - val_loss: 1449.4581 - val_mse: 1449.4580 - val_mae: 25.6837\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3399.8533 - mse: 3399.8538 - mae: 32.4531 - val_loss: 1450.1866 - val_mse: 1450.1866 - val_mae: 25.7745\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 342us/step - loss: 3168.9408 - mse: 3168.9414 - mae: 31.5925 - val_loss: 1455.7751 - val_mse: 1455.7750 - val_mae: 26.4105\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 276us/step - loss: 3226.7401 - mse: 3226.7397 - mae: 31.6469 - val_loss: 1456.1113 - val_mse: 1456.1111 - val_mae: 26.4090\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3357.0938 - mse: 3357.0940 - mae: 31.8551 - val_loss: 1452.0363 - val_mse: 1452.0363 - val_mae: 25.7707\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3280.1584 - mse: 3280.1575 - mae: 31.7831 - val_loss: 1453.2335 - val_mse: 1453.2334 - val_mae: 25.9221\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 328us/step - loss: 3284.6529 - mse: 3284.6526 - mae: 32.0417 - val_loss: 1452.8392 - val_mse: 1452.8391 - val_mae: 25.8168\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3350.4302 - mse: 3350.4302 - mae: 31.6590 - val_loss: 1452.4383 - val_mse: 1452.4384 - val_mae: 25.7424\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 272us/step - loss: 3319.9292 - mse: 3319.9297 - mae: 31.8148 - val_loss: 1454.2026 - val_mse: 1454.2028 - val_mae: 26.0722\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3441.8694 - mse: 3441.8691 - mae: 32.43 - 0s 278us/step - loss: 3396.7947 - mse: 3396.7944 - mae: 32.3629 - val_loss: 1455.7027 - val_mse: 1455.7028 - val_mae: 26.3144\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 232us/step - loss: 3351.1783 - mse: 3351.1787 - mae: 32.0468 - val_loss: 1452.4347 - val_mse: 1452.4347 - val_mae: 25.6404\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3264.8738 - mse: 3264.8733 - mae: 31.0956 - val_loss: 1455.2187 - val_mse: 1455.2186 - val_mae: 26.2132\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3370.9206 - mse: 3370.9207 - mae: 32.6707 - val_loss: 1453.4468 - val_mse: 1453.4469 - val_mae: 25.8060\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3364.5030 - mse: 3364.5024 - mae: 32.4949 - val_loss: 1454.0665 - val_mse: 1454.0665 - val_mae: 25.6134\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3234.2817 - mse: 3234.2805 - mae: 31.5195 - val_loss: 1453.9612 - val_mse: 1453.9611 - val_mae: 25.5473\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 320us/step - loss: 3431.2982 - mse: 3431.2981 - mae: 32.4038 - val_loss: 1454.9955 - val_mse: 1454.9955 - val_mae: 25.8386\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3178.8648 - mse: 3178.8645 - mae: 31.6988 - val_loss: 1460.1279 - val_mse: 1460.1278 - val_mae: 26.4814\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3404.1661 - mse: 3404.1670 - mae: 32.9007 - val_loss: 1456.1732 - val_mse: 1456.1735 - val_mae: 25.2466\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3321.9656 - mse: 3321.9658 - mae: 31.5779 - val_loss: 1455.4252 - val_mse: 1455.4253 - val_mae: 25.6629\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3351.1049 - mse: 3351.1057 - mae: 31.9903 - val_loss: 1455.1940 - val_mse: 1455.1940 - val_mae: 25.4704\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3359.9379 - mse: 3359.9370 - mae: 32.4675 - val_loss: 1457.8025 - val_mse: 1457.8025 - val_mae: 26.1258\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3327.6723 - mse: 3327.6721 - mae: 32.5997 - val_loss: 1456.5436 - val_mse: 1456.5435 - val_mae: 25.7690\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3279.2846 - mse: 3279.2847 - mae: 32.0979 - val_loss: 1456.3860 - val_mse: 1456.3860 - val_mae: 25.8635\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3277.2217 - mse: 3277.2214 - mae: 31.7477 - val_loss: 1458.3837 - val_mse: 1458.3837 - val_mae: 26.0906\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 276us/step - loss: 3391.6363 - mse: 3391.6365 - mae: 32.1771 - val_loss: 1458.5920 - val_mse: 1458.5919 - val_mae: 25.2035\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3349.9081 - mse: 3349.9084 - mae: 32.1434 - val_loss: 1458.2424 - val_mse: 1458.2424 - val_mae: 25.3019\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3264.0015 - mse: 3264.0010 - mae: 31.41 - 0s 309us/step - loss: 3244.9555 - mse: 3244.9548 - mae: 31.3281 - val_loss: 1458.1767 - val_mse: 1458.1766 - val_mae: 25.9118\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3250.5128 - mse: 3250.5129 - mae: 31.3019 - val_loss: 1460.5353 - val_mse: 1460.5353 - val_mae: 26.2128\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3199.4521 - mse: 3199.4521 - mae: 31.6144 - val_loss: 1458.3664 - val_mse: 1458.3665 - val_mae: 25.7131\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3273.6387 - mse: 3273.6389 - mae: 31.5838 - val_loss: 1458.8629 - val_mse: 1458.8629 - val_mae: 25.8873\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3270.5004 - mse: 3270.5010 - mae: 31.2999 - val_loss: 1458.9383 - val_mse: 1458.9384 - val_mae: 25.2983\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3182.6930 - mse: 3182.6926 - mae: 31.4452 - val_loss: 1458.7413 - val_mse: 1458.7413 - val_mae: 25.9089\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 236us/step - loss: 3291.8823 - mse: 3291.8831 - mae: 31.6733 - val_loss: 1458.2341 - val_mse: 1458.2341 - val_mae: 25.9936\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 280us/step - loss: 3211.7648 - mse: 3211.7642 - mae: 31.5529 - val_loss: 1458.3653 - val_mse: 1458.3655 - val_mae: 25.9265\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 284us/step - loss: 3344.9860 - mse: 3344.9863 - mae: 31.5733 - val_loss: 1460.4569 - val_mse: 1460.4570 - val_mae: 26.2390\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3326.8303 - mse: 3326.8303 - mae: 32.1329 - val_loss: 1458.6646 - val_mse: 1458.6646 - val_mae: 25.7063\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2966.9104 - mse: 2966.9104 - mae: 31.6599 - val_loss: 1073.9282 - val_mse: 1073.9282 - val_mae: 23.5207\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2979.1034 - mse: 2979.1033 - mae: 31.8121 - val_loss: 1068.9088 - val_mse: 1068.9088 - val_mae: 23.7842\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2948.1159 - mse: 2948.1165 - mae: 31.1081 - val_loss: 1068.2885 - val_mse: 1068.2887 - val_mae: 23.7603\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 264us/step - loss: 2895.3507 - mse: 2895.3503 - mae: 30.7738 - val_loss: 1067.3324 - val_mse: 1067.3325 - val_mae: 23.7495\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2920.5934 - mse: 2920.5933 - mae: 30.7041 - val_loss: 1064.7955 - val_mse: 1064.7954 - val_mae: 23.9992\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2922.9349 - mse: 2922.9360 - mae: 31.4109 - val_loss: 1066.2465 - val_mse: 1066.2466 - val_mae: 23.6881\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2882.1599 - mse: 2882.1597 - mae: 30.6209 - val_loss: 1068.2927 - val_mse: 1068.2928 - val_mae: 23.5455\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2964.3530 - mse: 2964.3535 - mae: 31.3961 - val_loss: 1065.4108 - val_mse: 1065.4108 - val_mae: 23.7220\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2993.8576 - mse: 2993.8577 - mae: 30.7150 - val_loss: 1075.9671 - val_mse: 1075.9670 - val_mae: 23.1930\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2926.0492 - mse: 2926.0483 - mae: 31.2126 - val_loss: 1064.8120 - val_mse: 1064.8121 - val_mae: 23.7428\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2969.4693 - mse: 2969.4695 - mae: 31.3144 - val_loss: 1071.9173 - val_mse: 1071.9172 - val_mae: 23.2794\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 298us/step - loss: 2857.2529 - mse: 2857.2524 - mae: 30.4913 - val_loss: 1063.8858 - val_mse: 1063.8857 - val_mae: 23.7867\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2830.9282 - mse: 2830.9277 - mae: 30.8639 - val_loss: 1063.5825 - val_mse: 1063.5824 - val_mae: 23.7708\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 253us/step - loss: 2944.9023 - mse: 2944.9023 - mae: 31.1349 - val_loss: 1061.8359 - val_mse: 1061.8359 - val_mae: 23.9167\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2888.5750 - mse: 2888.5747 - mae: 30.9606 - val_loss: 1060.7193 - val_mse: 1060.7195 - val_mae: 24.0207\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2960.6537 - mse: 2960.6533 - mae: 31.3284 - val_loss: 1064.8096 - val_mse: 1064.8096 - val_mae: 23.4770\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 256us/step - loss: 2911.8356 - mse: 2911.8369 - mae: 30.9193 - val_loss: 1063.5415 - val_mse: 1063.5415 - val_mae: 23.5461\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2900.4951 - mse: 2900.4946 - mae: 31.4967 - val_loss: 1061.8830 - val_mse: 1061.8831 - val_mae: 23.6219\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2846.3951 - mse: 2846.3953 - mae: 30.1837 - val_loss: 1059.7893 - val_mse: 1059.7893 - val_mae: 23.8106\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2961.4778 - mse: 2961.4788 - mae: 30.9409 - val_loss: 1060.1520 - val_mse: 1060.1520 - val_mae: 23.7054\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2947.6268 - mse: 2947.6260 - mae: 30.8561 - val_loss: 1058.8140 - val_mse: 1058.8140 - val_mae: 23.8258\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2962.4821 - mse: 2962.4819 - mae: 31.3115 - val_loss: 1064.9860 - val_mse: 1064.9861 - val_mae: 23.3038\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2917.7373 - mse: 2917.7368 - mae: 30.8503 - val_loss: 1059.6298 - val_mse: 1059.6296 - val_mae: 23.6637\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2976.4736 - mse: 2976.4739 - mae: 31.3217 - val_loss: 1058.4291 - val_mse: 1058.4290 - val_mae: 23.8283\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2933.7050 - mse: 2933.7046 - mae: 30.9253 - val_loss: 1059.8018 - val_mse: 1059.8019 - val_mae: 23.6059\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2935.8033 - mse: 2935.8037 - mae: 30.8859 - val_loss: 1057.2772 - val_mse: 1057.2772 - val_mae: 23.8797\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2959.9567 - mse: 2959.9568 - mae: 31.2928 - val_loss: 1058.4897 - val_mse: 1058.4896 - val_mae: 23.6600\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2905.8890 - mse: 2905.8892 - mae: 30.8192 - val_loss: 1057.7028 - val_mse: 1057.7026 - val_mae: 23.7620\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 326us/step - loss: 2887.9502 - mse: 2887.9504 - mae: 30.8196 - val_loss: 1059.2532 - val_mse: 1059.2533 - val_mae: 23.5769\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 384us/step - loss: 2794.5329 - mse: 2794.5337 - mae: 30.0250 - val_loss: 1056.0547 - val_mse: 1056.0546 - val_mae: 23.8636\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 326us/step - loss: 2791.2804 - mse: 2791.2800 - mae: 30.5753 - val_loss: 1055.0506 - val_mse: 1055.0504 - val_mae: 24.0633\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2844.2017 - mse: 2844.2017 - mae: 30.5229 - val_loss: 1056.3618 - val_mse: 1056.3618 - val_mae: 23.7240\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2938.6674 - mse: 2938.6677 - mae: 31.4032 - val_loss: 1056.8912 - val_mse: 1056.8911 - val_mae: 23.6791\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2813.1521 - mse: 2813.1519 - mae: 30.1707 - val_loss: 1055.3741 - val_mse: 1055.3740 - val_mae: 23.8726\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2904.0806 - mse: 2904.0801 - mae: 30.8994 - val_loss: 1054.5250 - val_mse: 1054.5250 - val_mae: 24.0179\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2903.3569 - mse: 2903.3572 - mae: 30.5125 - val_loss: 1054.6346 - val_mse: 1054.6346 - val_mae: 23.7358\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2834.8364 - mse: 2834.8364 - mae: 30.5137 - val_loss: 1053.2393 - val_mse: 1053.2393 - val_mae: 23.7944\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2944.7821 - mse: 2944.7825 - mae: 30.8643 - val_loss: 1052.4014 - val_mse: 1052.4012 - val_mae: 23.8945\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 332us/step - loss: 2809.4934 - mse: 2809.4932 - mae: 30.1453 - val_loss: 1051.8400 - val_mse: 1051.8400 - val_mae: 23.8156\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2908.4692 - mse: 2908.4692 - mae: 31.1371 - val_loss: 1051.2363 - val_mse: 1051.2363 - val_mae: 23.8279\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2740.9623 - mse: 2740.9617 - mae: 29.8974 - val_loss: 1050.8752 - val_mse: 1050.8751 - val_mae: 23.8915\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2898.3190 - mse: 2898.3188 - mae: 30.8528 - val_loss: 1050.2237 - val_mse: 1050.2236 - val_mae: 23.8699\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 368us/step - loss: 2855.3973 - mse: 2855.3970 - mae: 30.4255 - val_loss: 1049.8098 - val_mse: 1049.8097 - val_mae: 23.8905\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 272us/step - loss: 2936.0116 - mse: 2936.0120 - mae: 30.8624 - val_loss: 1052.8199 - val_mse: 1052.8201 - val_mae: 23.5094\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 356us/step - loss: 2818.9500 - mse: 2818.9497 - mae: 30.3704 - val_loss: 1049.6027 - val_mse: 1049.6027 - val_mae: 23.9473\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 329us/step - loss: 2875.5924 - mse: 2875.5920 - mae: 30.6160 - val_loss: 1052.0903 - val_mse: 1052.0903 - val_mae: 23.5296\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2907.3013 - mse: 2907.3018 - mae: 30.7416 - val_loss: 1050.7113 - val_mse: 1050.7112 - val_mae: 23.5999\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2983.7301 - mse: 2983.7302 - mae: 31.4100 - val_loss: 1048.7746 - val_mse: 1048.7744 - val_mae: 23.7780\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2883.7660 - mse: 2883.7656 - mae: 30.2091 - val_loss: 1046.4749 - val_mse: 1046.4749 - val_mae: 24.1658\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 266us/step - loss: 2886.2458 - mse: 2886.2466 - mae: 30.8961 - val_loss: 1053.4094 - val_mse: 1053.4094 - val_mae: 23.2778\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2837.1161 - mse: 2837.1162 - mae: 30.2705 - val_loss: 1046.7513 - val_mse: 1046.7512 - val_mae: 23.8878\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2855.3795 - mse: 2855.3799 - mae: 30.8495 - val_loss: 1045.5264 - val_mse: 1045.5265 - val_mae: 23.9513\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2890.5392 - mse: 2890.5386 - mae: 30.5375 - val_loss: 1044.9742 - val_mse: 1044.9740 - val_mae: 23.9753\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 374us/step - loss: 2816.3073 - mse: 2816.3074 - mae: 30.3741 - val_loss: 1045.5361 - val_mse: 1045.5363 - val_mae: 23.7120\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 337us/step - loss: 2892.1196 - mse: 2892.1187 - mae: 30.6530 - val_loss: 1044.0370 - val_mse: 1044.0370 - val_mae: 23.8387\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2830.3155 - mse: 2830.3152 - mae: 30.6716 - val_loss: 1044.7300 - val_mse: 1044.7300 - val_mae: 23.7056\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 327us/step - loss: 2884.2072 - mse: 2884.2065 - mae: 30.6171 - val_loss: 1043.5605 - val_mse: 1043.5604 - val_mae: 24.1458\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 334us/step - loss: 2839.1375 - mse: 2839.1365 - mae: 30.1719 - val_loss: 1045.6224 - val_mse: 1045.6223 - val_mae: 23.5693\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2794.5343 - mse: 2794.5344 - mae: 29.5426 - val_loss: 1042.3674 - val_mse: 1042.3673 - val_mae: 24.0968\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2850.4175 - mse: 2850.4172 - mae: 30.3882 - val_loss: 1043.5732 - val_mse: 1043.5732 - val_mae: 23.6076\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2840.4471 - mse: 2840.4463 - mae: 30.4225 - val_loss: 1041.4568 - val_mse: 1041.4567 - val_mae: 23.8662\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2766.6628 - mse: 2766.6631 - mae: 29.9641 - val_loss: 1040.6466 - val_mse: 1040.6465 - val_mae: 24.1406\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2845.0664 - mse: 2845.0662 - mae: 30.7685 - val_loss: 1039.7036 - val_mse: 1039.7035 - val_mae: 24.0556\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2827.0203 - mse: 2827.0208 - mae: 30.7008 - val_loss: 1040.6176 - val_mse: 1040.6176 - val_mae: 23.7349\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2812.1257 - mse: 2812.1257 - mae: 30.1805 - val_loss: 1041.1213 - val_mse: 1041.1215 - val_mae: 23.5841\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2811.2984 - mse: 2811.2979 - mae: 30.0707 - val_loss: 1039.0283 - val_mse: 1039.0283 - val_mae: 24.0429\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2918.7595 - mse: 2918.7593 - mae: 30.5993 - val_loss: 1041.7651 - val_mse: 1041.7653 - val_mae: 23.5126\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2766.7725 - mse: 2766.7725 - mae: 29.9401 - val_loss: 1038.2590 - val_mse: 1038.2590 - val_mae: 23.9392\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2848.1449 - mse: 2848.1450 - mae: 30.5841 - val_loss: 1038.7777 - val_mse: 1038.7777 - val_mae: 23.7551\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2845.1504 - mse: 2845.1494 - mae: 30.1118 - val_loss: 1037.2590 - val_mse: 1037.2590 - val_mae: 24.1255\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2803.1885 - mse: 2803.1885 - mae: 30.5193 - val_loss: 1036.3272 - val_mse: 1036.3273 - val_mae: 23.8870\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 268us/step - loss: 2857.6624 - mse: 2857.6624 - mae: 30.5463 - val_loss: 1034.5901 - val_mse: 1034.5901 - val_mae: 23.9582\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2833.3089 - mse: 2833.3088 - mae: 30.4475 - val_loss: 1035.3736 - val_mse: 1035.3737 - val_mae: 23.5380\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2818.4329 - mse: 2818.4321 - mae: 30.2194 - val_loss: 1032.2943 - val_mse: 1032.2943 - val_mae: 23.7552\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2905.9499 - mse: 2905.9497 - mae: 30.3464 - val_loss: 1034.8870 - val_mse: 1034.8871 - val_mae: 23.3483\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2829.2337 - mse: 2829.2332 - mae: 30.1152 - val_loss: 1032.1082 - val_mse: 1032.1083 - val_mae: 23.5341\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 327us/step - loss: 2810.7525 - mse: 2810.7527 - mae: 30.3230 - val_loss: 1029.7802 - val_mse: 1029.7802 - val_mae: 23.7308\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2805.6005 - mse: 2805.6003 - mae: 30.3602 - val_loss: 1029.3441 - val_mse: 1029.3441 - val_mae: 23.6797\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 272us/step - loss: 2790.2983 - mse: 2790.2983 - mae: 29.9259 - val_loss: 1032.2616 - val_mse: 1032.2617 - val_mae: 23.2995\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2793.2761 - mse: 2793.2761 - mae: 30.0256 - val_loss: 1028.7439 - val_mse: 1028.7439 - val_mae: 23.6541\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2587.6908 - mse: 2587.6912 - mae: 30.0410 - val_loss: 1518.6413 - val_mse: 1518.6414 - val_mae: 26.9650\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2461.6065 - mse: 2461.6064 - mae: 29.3853 - val_loss: 1510.1378 - val_mse: 1510.1378 - val_mae: 27.0820\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 339us/step - loss: 2506.1460 - mse: 2506.1455 - mae: 29.7373 - val_loss: 1511.3778 - val_mse: 1511.3779 - val_mae: 27.0131\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2516.3259 - mse: 2516.3254 - mae: 29.2012 - val_loss: 1504.0585 - val_mse: 1504.0586 - val_mae: 27.1543\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 334us/step - loss: 2601.7438 - mse: 2601.7446 - mae: 29.9700 - val_loss: 1515.8259 - val_mse: 1515.8257 - val_mae: 26.8259\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2508.8053 - mse: 2508.8052 - mae: 29.4204 - val_loss: 1499.7862 - val_mse: 1499.7863 - val_mae: 27.1967\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2467.2482 - mse: 2467.2483 - mae: 29.5329 - val_loss: 1493.6186 - val_mse: 1493.6187 - val_mae: 27.3697\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2618.6012 - mse: 2618.6016 - mae: 30.0598 - val_loss: 1497.0683 - val_mse: 1497.0684 - val_mae: 27.1882\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2553.4195 - mse: 2553.4194 - mae: 30.0168 - val_loss: 1500.1493 - val_mse: 1500.1493 - val_mae: 27.0615\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2602.7952 - mse: 2602.7949 - mae: 29.9442 - val_loss: 1513.3034 - val_mse: 1513.3033 - val_mae: 26.7317\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 269us/step - loss: 2458.5289 - mse: 2458.5288 - mae: 28.8291 - val_loss: 1494.1746 - val_mse: 1494.1747 - val_mae: 27.1422\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2540.5948 - mse: 2540.5955 - mae: 29.5552 - val_loss: 1494.5944 - val_mse: 1494.5945 - val_mae: 27.0572\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2489.3797 - mse: 2489.3794 - mae: 29.3844 - val_loss: 1491.0484 - val_mse: 1491.0483 - val_mae: 27.0840\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 268us/step - loss: 2569.9902 - mse: 2569.9893 - mae: 29.9412 - val_loss: 1492.8392 - val_mse: 1492.8391 - val_mae: 26.9881\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 285us/step - loss: 2511.9296 - mse: 2511.9304 - mae: 29.1740 - val_loss: 1484.6941 - val_mse: 1484.6940 - val_mae: 27.1704\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 366us/step - loss: 2572.3167 - mse: 2572.3164 - mae: 29.6799 - val_loss: 1487.1347 - val_mse: 1487.1348 - val_mae: 27.0186\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2589.0053 - mse: 2589.0056 - mae: 29.4548 - val_loss: 1481.4908 - val_mse: 1481.4908 - val_mae: 27.1495\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2547.5246 - mse: 2547.5247 - mae: 29.7840 - val_loss: 1496.4679 - val_mse: 1496.4681 - val_mae: 26.7084\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 368us/step - loss: 2573.9667 - mse: 2573.9666 - mae: 29.8821 - val_loss: 1482.5586 - val_mse: 1482.5586 - val_mae: 26.9892\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2524.9961 - mse: 2524.9966 - mae: 29.8001 - val_loss: 1491.3461 - val_mse: 1491.3461 - val_mae: 26.6670\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2488.5851 - mse: 2488.5854 - mae: 29.0417 - val_loss: 1483.3811 - val_mse: 1483.3812 - val_mae: 26.8343\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 324us/step - loss: 2516.2417 - mse: 2516.2422 - mae: 29.5974 - val_loss: 1474.1474 - val_mse: 1474.1478 - val_mae: 27.1094\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2468.7348 - mse: 2468.7349 - mae: 29.3355 - val_loss: 1478.8021 - val_mse: 1478.8020 - val_mae: 26.8989\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2497.9963 - mse: 2497.9966 - mae: 29.7328 - val_loss: 1473.3457 - val_mse: 1473.3457 - val_mae: 27.0515\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2420.2764 - mse: 2420.2754 - mae: 29.1696 - val_loss: 1473.1081 - val_mse: 1473.1083 - val_mae: 27.0359\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2504.5224 - mse: 2504.5220 - mae: 29.4249 - val_loss: 1473.9027 - val_mse: 1473.9027 - val_mae: 26.9900\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 367us/step - loss: 2499.9380 - mse: 2499.9373 - mae: 29.3708 - val_loss: 1476.6995 - val_mse: 1476.6996 - val_mae: 26.8642\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2554.5396 - mse: 2554.5391 - mae: 29.6724 - val_loss: 1471.7349 - val_mse: 1471.7349 - val_mae: 26.9922\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2495.3031 - mse: 2495.3025 - mae: 28.9720 - val_loss: 1470.9045 - val_mse: 1470.9045 - val_mae: 27.0248\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2512.2358 - mse: 2512.2363 - mae: 29.6027 - val_loss: 1487.5565 - val_mse: 1487.5568 - val_mae: 26.6111\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2486.0115 - mse: 2486.0120 - mae: 28.8201 - val_loss: 1475.7138 - val_mse: 1475.7137 - val_mae: 26.8748\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2506.4393 - mse: 2506.4390 - mae: 29.1507 - val_loss: 1462.0633 - val_mse: 1462.0634 - val_mae: 27.3540\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 363us/step - loss: 2587.5392 - mse: 2587.5396 - mae: 30.0172 - val_loss: 1487.2695 - val_mse: 1487.2694 - val_mae: 26.6245\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 348us/step - loss: 2487.5175 - mse: 2487.5173 - mae: 28.7188 - val_loss: 1471.8072 - val_mse: 1471.8074 - val_mae: 27.0624\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2551.2506 - mse: 2551.2512 - mae: 29.3833 - val_loss: 1472.7926 - val_mse: 1472.7927 - val_mae: 27.0153\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2484.4640 - mse: 2484.4646 - mae: 29.1161 - val_loss: 1473.2987 - val_mse: 1473.2986 - val_mae: 27.0215\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2556.8406 - mse: 2556.8413 - mae: 29.4794 - val_loss: 1479.6144 - val_mse: 1479.6145 - val_mae: 26.7870\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 345us/step - loss: 2433.6681 - mse: 2433.6677 - mae: 29.2243 - val_loss: 1471.7257 - val_mse: 1471.7257 - val_mae: 26.9549\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2509.5079 - mse: 2509.5066 - mae: 29.1731 - val_loss: 1471.4488 - val_mse: 1471.4486 - val_mae: 26.9458\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2481.8664 - mse: 2481.8665 - mae: 29.1180 - val_loss: 1473.5893 - val_mse: 1473.5892 - val_mae: 26.8585\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 332us/step - loss: 2509.7359 - mse: 2509.7351 - mae: 28.9006 - val_loss: 1481.5426 - val_mse: 1481.5426 - val_mae: 26.6204\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2529.3456 - mse: 2529.3455 - mae: 29.3443 - val_loss: 1471.8479 - val_mse: 1471.8479 - val_mae: 26.8778\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 352us/step - loss: 2439.6921 - mse: 2439.6921 - mae: 28.9565 - val_loss: 1476.7692 - val_mse: 1476.7693 - val_mae: 26.7234\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2553.4183 - mse: 2553.4185 - mae: 29.3614 - val_loss: 1471.7253 - val_mse: 1471.7252 - val_mae: 26.8893\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2530.0083 - mse: 2530.0085 - mae: 29.4308 - val_loss: 1474.6545 - val_mse: 1474.6545 - val_mae: 26.7864\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2418.0902 - mse: 2418.0898 - mae: 28.6787 - val_loss: 1469.3541 - val_mse: 1469.3540 - val_mae: 26.9253\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 266us/step - loss: 2489.1667 - mse: 2489.1667 - mae: 29.3876 - val_loss: 1462.6772 - val_mse: 1462.6770 - val_mae: 27.1368\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 256us/step - loss: 2482.7371 - mse: 2482.7375 - mae: 29.2193 - val_loss: 1471.5316 - val_mse: 1471.5315 - val_mae: 26.7625\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 258us/step - loss: 2430.1532 - mse: 2430.1533 - mae: 28.7954 - val_loss: 1461.4883 - val_mse: 1461.4884 - val_mae: 27.0978\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 280us/step - loss: 2515.6910 - mse: 2515.6914 - mae: 29.4025 - val_loss: 1475.6650 - val_mse: 1475.6652 - val_mae: 26.6054\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2449.1136 - mse: 2449.1140 - mae: 29.1487 - val_loss: 1463.5511 - val_mse: 1463.5509 - val_mae: 26.9784\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2448.3943 - mse: 2448.3940 - mae: 28.9782 - val_loss: 1463.4111 - val_mse: 1463.4110 - val_mae: 26.9605\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2480.9045 - mse: 2480.9050 - mae: 29.0908 - val_loss: 1466.1959 - val_mse: 1466.1962 - val_mae: 26.8477\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2520.9331 - mse: 2520.9338 - mae: 29.5550 - val_loss: 1468.1601 - val_mse: 1468.1599 - val_mae: 26.7883\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2431.5586 - mse: 2431.5593 - mae: 28.8083 - val_loss: 1470.6474 - val_mse: 1470.6473 - val_mae: 26.6944\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2529.7006 - mse: 2529.7002 - mae: 29.4324 - val_loss: 1467.8570 - val_mse: 1467.8568 - val_mae: 26.7913\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2534.8326 - mse: 2534.8323 - mae: 29.4104 - val_loss: 1462.4918 - val_mse: 1462.4919 - val_mae: 26.9515\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2527.6230 - mse: 2527.6228 - mae: 29.2898 - val_loss: 1465.1764 - val_mse: 1465.1764 - val_mae: 26.8894\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 330us/step - loss: 2473.4601 - mse: 2473.4607 - mae: 28.9434 - val_loss: 1470.0607 - val_mse: 1470.0608 - val_mae: 26.7216\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2447.9858 - mse: 2447.9854 - mae: 28.8111 - val_loss: 1466.0183 - val_mse: 1466.0184 - val_mae: 26.7851\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2505.5117 - mse: 2505.5112 - mae: 29.0963 - val_loss: 1459.1967 - val_mse: 1459.1969 - val_mae: 27.0071\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2467.2748 - mse: 2467.2749 - mae: 29.1496 - val_loss: 1462.8904 - val_mse: 1462.8905 - val_mae: 26.8667\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 277us/step - loss: 2452.6418 - mse: 2452.6416 - mae: 29.2187 - val_loss: 1467.6832 - val_mse: 1467.6835 - val_mae: 26.7555\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 267us/step - loss: 2507.4306 - mse: 2507.4307 - mae: 29.3245 - val_loss: 1474.9373 - val_mse: 1474.9374 - val_mae: 26.5411\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2464.8496 - mse: 2464.8494 - mae: 29.1265 - val_loss: 1469.6516 - val_mse: 1469.6517 - val_mae: 26.6356\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2477.9729 - mse: 2477.9734 - mae: 29.0617 - val_loss: 1457.2555 - val_mse: 1457.2555 - val_mae: 27.0190\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 351us/step - loss: 2478.8942 - mse: 2478.8945 - mae: 29.3237 - val_loss: 1468.1415 - val_mse: 1468.1416 - val_mae: 26.6892\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 336us/step - loss: 2439.7743 - mse: 2439.7737 - mae: 28.8266 - val_loss: 1461.0750 - val_mse: 1461.0750 - val_mae: 26.8276\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2438.0160 - mse: 2438.0159 - mae: 28.5764 - val_loss: 1470.9048 - val_mse: 1470.9047 - val_mae: 26.5501\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2479.5650 - mse: 2479.5647 - mae: 28.8100 - val_loss: 1457.0592 - val_mse: 1457.0590 - val_mae: 26.9451\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2456.3144 - mse: 2456.3140 - mae: 29.0903 - val_loss: 1466.5498 - val_mse: 1466.5498 - val_mae: 26.6203\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2451.5643 - mse: 2451.5642 - mae: 28.5407 - val_loss: 1459.4746 - val_mse: 1459.4746 - val_mae: 26.8447\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2504.8538 - mse: 2504.8540 - mae: 29.0253 - val_loss: 1456.8268 - val_mse: 1456.8268 - val_mae: 26.9434\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 355us/step - loss: 2469.7886 - mse: 2469.7883 - mae: 29.4286 - val_loss: 1465.4816 - val_mse: 1465.4816 - val_mae: 26.6522\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 276us/step - loss: 2517.8059 - mse: 2517.8064 - mae: 29.3981 - val_loss: 1473.4090 - val_mse: 1473.4091 - val_mae: 26.4779\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2515.6107 - mse: 2515.6106 - mae: 29.0163 - val_loss: 1455.4296 - val_mse: 1455.4296 - val_mae: 26.9826\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2480.1642 - mse: 2480.1646 - mae: 28.7458 - val_loss: 1455.4497 - val_mse: 1455.4497 - val_mae: 26.9466\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2491.8621 - mse: 2491.8613 - mae: 29.2350 - val_loss: 1462.5079 - val_mse: 1462.5077 - val_mae: 26.7321\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2451.1986 - mse: 2451.1987 - mae: 29.0866 - val_loss: 1462.3556 - val_mse: 1462.3557 - val_mae: 26.7171\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2424.8566 - mse: 2424.8557 - mae: 28.7776 - val_loss: 1452.9923 - val_mse: 1452.9924 - val_mae: 27.0351\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2317.0572 - mse: 2317.0571 - mae: 29.0718 - val_loss: 3707.0400 - val_mse: 3707.0405 - val_mae: 24.2499\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2343.8232 - mse: 2343.8235 - mae: 29.2876 - val_loss: 3706.7549 - val_mse: 3706.7546 - val_mae: 23.9193\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2409.1139 - mse: 2409.1147 - mae: 29.5301 - val_loss: 3706.0913 - val_mse: 3706.0923 - val_mae: 23.6277\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 262us/step - loss: 2382.0297 - mse: 2382.0295 - mae: 29.6218 - val_loss: 3705.4757 - val_mse: 3705.4758 - val_mae: 23.6594\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2425.1162 - mse: 2425.1160 - mae: 29.9513 - val_loss: 3705.3643 - val_mse: 3705.3638 - val_mae: 23.4789\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 248us/step - loss: 2395.3223 - mse: 2395.3210 - mae: 29.4603 - val_loss: 3705.2923 - val_mse: 3705.2922 - val_mae: 23.5824\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 353us/step - loss: 2342.1380 - mse: 2342.1384 - mae: 29.1293 - val_loss: 3705.3563 - val_mse: 3705.3564 - val_mae: 23.9866\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2385.0612 - mse: 2385.0613 - mae: 29.5451 - val_loss: 3706.4832 - val_mse: 3706.4832 - val_mae: 23.5852\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 280us/step - loss: 2300.3950 - mse: 2300.3938 - mae: 28.9667 - val_loss: 3708.5331 - val_mse: 3708.5327 - val_mae: 24.1667\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2274.3025 - mse: 2274.3018 - mae: 28.6872 - val_loss: 3708.0975 - val_mse: 3708.0984 - val_mae: 23.7179\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2347.0330 - mse: 2347.0332 - mae: 29.2092 - val_loss: 3708.5085 - val_mse: 3708.5081 - val_mae: 23.4109\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2308.4500 - mse: 2308.4497 - mae: 28.8414 - val_loss: 3708.4033 - val_mse: 3708.4026 - val_mae: 23.5747\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2372.5165 - mse: 2372.5168 - mae: 29.5338 - val_loss: 3708.1217 - val_mse: 3708.1221 - val_mae: 23.6953\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 329us/step - loss: 2356.4400 - mse: 2356.4397 - mae: 29.1989 - val_loss: 3706.8169 - val_mse: 3706.8179 - val_mae: 23.6359\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2341.5382 - mse: 2341.5381 - mae: 29.3575 - val_loss: 3706.3468 - val_mse: 3706.3464 - val_mae: 23.7769\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2364.1808 - mse: 2364.1802 - mae: 29.3535 - val_loss: 3707.1703 - val_mse: 3707.1707 - val_mae: 23.8402\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2356.6610 - mse: 2356.6611 - mae: 29.5933 - val_loss: 3707.0948 - val_mse: 3707.0955 - val_mae: 23.9108\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2315.1381 - mse: 2315.1375 - mae: 29.1399 - val_loss: 3707.1748 - val_mse: 3707.1758 - val_mae: 23.8513\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 288us/step - loss: 2351.5377 - mse: 2351.5381 - mae: 29.3316 - val_loss: 3705.8020 - val_mse: 3705.8015 - val_mae: 23.5325\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2361.8425 - mse: 2361.8418 - mae: 29.4342 - val_loss: 3705.5217 - val_mse: 3705.5215 - val_mae: 23.8759\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2349.0048 - mse: 2349.0042 - mae: 29.1555 - val_loss: 3705.7937 - val_mse: 3705.7927 - val_mae: 23.7500\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2342.7449 - mse: 2342.7444 - mae: 29.1491 - val_loss: 3706.3743 - val_mse: 3706.3743 - val_mae: 23.6825\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 331us/step - loss: 2351.7979 - mse: 2351.7979 - mae: 29.0260 - val_loss: 3708.8407 - val_mse: 3708.8406 - val_mae: 24.4078\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2396.2077 - mse: 2396.2078 - mae: 29.6782 - val_loss: 3706.9795 - val_mse: 3706.9795 - val_mae: 23.6593\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2328.4736 - mse: 2328.4729 - mae: 29.1751 - val_loss: 3706.5565 - val_mse: 3706.5562 - val_mae: 23.8290\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 360us/step - loss: 2324.0467 - mse: 2324.0471 - mae: 29.0749 - val_loss: 3706.6515 - val_mse: 3706.6514 - val_mae: 23.8766\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2356.8357 - mse: 2356.8359 - mae: 29.1692 - val_loss: 3707.8554 - val_mse: 3707.8555 - val_mae: 24.0058\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2370.8212 - mse: 2370.8210 - mae: 29.2313 - val_loss: 3709.3544 - val_mse: 3709.3540 - val_mae: 24.3896\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2334.1606 - mse: 2334.1609 - mae: 29.1201 - val_loss: 3708.0149 - val_mse: 3708.0144 - val_mae: 23.0693\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2403.0150 - mse: 2403.0149 - mae: 29.1969 - val_loss: 3706.8517 - val_mse: 3706.8511 - val_mae: 23.4011\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2331.8381 - mse: 2331.8381 - mae: 29.0072 - val_loss: 3708.3760 - val_mse: 3708.3757 - val_mae: 24.1551\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2369.9153 - mse: 2369.9146 - mae: 29.3929 - val_loss: 3706.7040 - val_mse: 3706.7041 - val_mae: 23.4827\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2336.5281 - mse: 2336.5281 - mae: 29.2807 - val_loss: 3707.2780 - val_mse: 3707.2788 - val_mae: 23.8053\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 337us/step - loss: 2351.6289 - mse: 2351.6296 - mae: 29.5224 - val_loss: 3706.9080 - val_mse: 3706.9089 - val_mae: 22.8787\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2373.4412 - mse: 2373.4417 - mae: 29.2674 - val_loss: 3705.2665 - val_mse: 3705.2664 - val_mae: 23.3604\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2307.0411 - mse: 2307.0408 - mae: 28.8566 - val_loss: 3706.0565 - val_mse: 3706.0566 - val_mae: 23.7237\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2396.3964 - mse: 2396.3967 - mae: 29.4652 - val_loss: 3706.1628 - val_mse: 3706.1633 - val_mae: 23.9924\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2399.5721 - mse: 2399.5723 - mae: 29.4619 - val_loss: 3706.1471 - val_mse: 3706.1470 - val_mae: 23.3715\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2314.0041 - mse: 2314.0039 - mae: 29.3306 - val_loss: 3707.0470 - val_mse: 3707.0471 - val_mae: 23.1288\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2302.7469 - mse: 2302.7461 - mae: 28.8147 - val_loss: 3705.2142 - val_mse: 3705.2141 - val_mae: 23.3417\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2306.6760 - mse: 2306.6748 - mae: 28.7952 - val_loss: 3705.7824 - val_mse: 3705.7825 - val_mae: 23.8063\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 258us/step - loss: 2303.4868 - mse: 2303.4868 - mae: 28.8211 - val_loss: 3705.9508 - val_mse: 3705.9512 - val_mae: 23.8901\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 386us/step - loss: 2310.0308 - mse: 2310.0310 - mae: 28.9693 - val_loss: 3704.5752 - val_mse: 3704.5750 - val_mae: 23.3682\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2335.7733 - mse: 2335.7732 - mae: 28.9974 - val_loss: 3704.5297 - val_mse: 3704.5300 - val_mae: 23.6883\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 356us/step - loss: 2317.2769 - mse: 2317.2776 - mae: 29.0645 - val_loss: 3704.5790 - val_mse: 3704.5796 - val_mae: 23.4850\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 368us/step - loss: 2363.7919 - mse: 2363.7922 - mae: 29.2431 - val_loss: 3705.0796 - val_mse: 3705.0801 - val_mae: 23.6970\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2358.1640 - mse: 2358.1641 - mae: 29.4571 - val_loss: 3704.7830 - val_mse: 3704.7827 - val_mae: 23.3876\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2322.9380 - mse: 2322.9380 - mae: 29.2907 - val_loss: 3704.8176 - val_mse: 3704.8169 - val_mae: 23.5329\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2297.2568 - mse: 2297.2566 - mae: 28.9118 - val_loss: 3704.7849 - val_mse: 3704.7842 - val_mae: 23.3448\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2341.7608 - mse: 2341.7615 - mae: 29.0802 - val_loss: 3706.1275 - val_mse: 3706.1265 - val_mae: 23.4916\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 243us/step - loss: 2326.9544 - mse: 2326.9541 - mae: 29.0877 - val_loss: 3705.0558 - val_mse: 3705.0562 - val_mae: 23.5642\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 234us/step - loss: 2308.3685 - mse: 2308.3689 - mae: 28.6224 - val_loss: 3704.5509 - val_mse: 3704.5508 - val_mae: 23.7846\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2285.7466 - mse: 2285.7461 - mae: 28.6885 - val_loss: 3705.0388 - val_mse: 3705.0381 - val_mae: 23.3438\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2417.9504 - mse: 2417.9507 - mae: 29.2171 - val_loss: 3704.3289 - val_mse: 3704.3286 - val_mae: 23.4784\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2350.2892 - mse: 2350.2896 - mae: 29.4253 - val_loss: 3704.5251 - val_mse: 3704.5247 - val_mae: 23.5163\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2382.9665 - mse: 2382.9663 - mae: 29.2355 - val_loss: 3703.8215 - val_mse: 3703.8210 - val_mae: 23.3158\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2393.0552 - mse: 2393.0554 - mae: 29.4214 - val_loss: 3704.7837 - val_mse: 3704.7837 - val_mae: 23.1740\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2382.3989 - mse: 2382.3982 - mae: 29.2295 - val_loss: 3704.0008 - val_mse: 3704.0017 - val_mae: 23.9092\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 387us/step - loss: 2321.5127 - mse: 2321.5132 - mae: 28.8326 - val_loss: 3704.8305 - val_mse: 3704.8313 - val_mae: 23.9913\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2320.7084 - mse: 2320.7087 - mae: 28.8107 - val_loss: 3705.1484 - val_mse: 3705.1482 - val_mae: 23.5350\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2389.8487 - mse: 2389.8484 - mae: 29.4851 - val_loss: 3705.0395 - val_mse: 3705.0386 - val_mae: 23.1014\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2270.3085 - mse: 2270.3081 - mae: 28.5563 - val_loss: 3704.3953 - val_mse: 3704.3962 - val_mae: 23.5844\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2302.0475 - mse: 2302.0471 - mae: 28.5616 - val_loss: 3704.0768 - val_mse: 3704.0759 - val_mae: 23.7983\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2349.1928 - mse: 2349.1929 - mae: 29.1384 - val_loss: 3703.5908 - val_mse: 3703.5920 - val_mae: 23.5135\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2378.7546 - mse: 2378.7549 - mae: 29.0678 - val_loss: 3704.3661 - val_mse: 3704.3672 - val_mae: 23.7701\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2315.0432 - mse: 2315.0430 - mae: 28.7497 - val_loss: 3702.5675 - val_mse: 3702.5674 - val_mae: 23.7501\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2308.9616 - mse: 2308.9624 - mae: 28.8596 - val_loss: 3702.9135 - val_mse: 3702.9141 - val_mae: 23.7022\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2316.0938 - mse: 2316.0933 - mae: 29.0557 - val_loss: 3703.4106 - val_mse: 3703.4104 - val_mae: 23.2949\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2350.1133 - mse: 2350.1135 - mae: 28.9152 - val_loss: 3704.0091 - val_mse: 3704.0100 - val_mae: 23.6127\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2332.3508 - mse: 2332.3506 - mae: 28.8039 - val_loss: 3704.0348 - val_mse: 3704.0349 - val_mae: 23.8477\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2307.7034 - mse: 2307.7034 - mae: 28.8675 - val_loss: 3705.0813 - val_mse: 3705.0815 - val_mae: 22.9028\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2319.4256 - mse: 2319.4258 - mae: 28.8527 - val_loss: 3703.4858 - val_mse: 3703.4866 - val_mae: 23.3157\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2344.7495 - mse: 2344.7500 - mae: 28.9323 - val_loss: 3703.4586 - val_mse: 3703.4585 - val_mae: 23.1480\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 269us/step - loss: 2301.0489 - mse: 2301.0488 - mae: 28.5917 - val_loss: 3703.2671 - val_mse: 3703.2671 - val_mae: 23.4018\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2294.5446 - mse: 2294.5444 - mae: 28.7388 - val_loss: 3702.8680 - val_mse: 3702.8674 - val_mae: 23.6099\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2294.9147 - mse: 2294.9141 - mae: 28.5148 - val_loss: 3702.5536 - val_mse: 3702.5532 - val_mae: 23.2936\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2300.2814 - mse: 2300.2803 - mae: 29.0132 - val_loss: 3703.8945 - val_mse: 3703.8943 - val_mae: 23.2535\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2266.7905 - mse: 2266.7908 - mae: 28.4188 - val_loss: 3703.3642 - val_mse: 3703.3645 - val_mae: 23.6664\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2323.3067 - mse: 2323.3066 - mae: 28.8179 - val_loss: 3701.5064 - val_mse: 3701.5073 - val_mae: 23.2915\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2323.9329 - mse: 2323.9326 - mae: 28.8141 - val_loss: 3703.0513 - val_mse: 3703.0515 - val_mae: 23.7717\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2673.1784 - mse: 2673.1794 - mae: 28.5207 - val_loss: 2063.8126 - val_mse: 2063.8123 - val_mae: 26.7416\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2707.1786 - mse: 2707.1790 - mae: 27.9849 - val_loss: 2079.5658 - val_mse: 2079.5662 - val_mae: 26.5141\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2697.0366 - mse: 2697.0371 - mae: 28.5276 - val_loss: 2080.9929 - val_mse: 2080.9929 - val_mae: 26.6229\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 335us/step - loss: 2651.7667 - mse: 2651.7661 - mae: 28.1601 - val_loss: 2070.1106 - val_mse: 2070.1106 - val_mae: 27.0247\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2674.0542 - mse: 2674.0542 - mae: 28.1981 - val_loss: 2077.2049 - val_mse: 2077.2051 - val_mae: 26.5440\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 333us/step - loss: 2665.6713 - mse: 2665.6726 - mae: 28.2023 - val_loss: 2069.7265 - val_mse: 2069.7268 - val_mae: 27.3236\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2743.6693 - mse: 2743.6692 - mae: 28.1968 - val_loss: 2081.2727 - val_mse: 2081.2727 - val_mae: 26.7894\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2688.7027 - mse: 2688.7021 - mae: 28.2238 - val_loss: 2087.6633 - val_mse: 2087.6633 - val_mae: 26.5667\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 330us/step - loss: 2681.5760 - mse: 2681.5757 - mae: 28.1920 - val_loss: 2079.6525 - val_mse: 2079.6526 - val_mae: 26.8104\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2672.6184 - mse: 2672.6184 - mae: 28.1555 - val_loss: 2081.5012 - val_mse: 2081.5012 - val_mae: 26.3344\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2718.8468 - mse: 2718.8474 - mae: 28.3646 - val_loss: 2093.5725 - val_mse: 2093.5725 - val_mae: 26.3980\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2656.1073 - mse: 2656.1069 - mae: 27.8425 - val_loss: 2093.2158 - val_mse: 2093.2156 - val_mae: 27.1156\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 336us/step - loss: 2675.9469 - mse: 2675.9470 - mae: 28.2583 - val_loss: 2098.1901 - val_mse: 2098.1899 - val_mae: 26.5037\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 348us/step - loss: 2689.1397 - mse: 2689.1389 - mae: 28.2758 - val_loss: 2103.1139 - val_mse: 2103.1140 - val_mae: 26.4583\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2681.2731 - mse: 2681.2734 - mae: 27.9788 - val_loss: 2106.4960 - val_mse: 2106.4961 - val_mae: 26.6622\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 343us/step - loss: 2703.1720 - mse: 2703.1729 - mae: 28.2551 - val_loss: 2099.5689 - val_mse: 2099.5688 - val_mae: 26.5765\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2706.2062 - mse: 2706.2063 - mae: 28.0908 - val_loss: 2104.7543 - val_mse: 2104.7539 - val_mae: 26.5206\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2701.9232 - mse: 2701.9229 - mae: 28.0899 - val_loss: 2103.5602 - val_mse: 2103.5603 - val_mae: 26.3669\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 327us/step - loss: 2637.9776 - mse: 2637.9771 - mae: 27.8361 - val_loss: 2090.1092 - val_mse: 2090.1096 - val_mae: 26.6593\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2654.4858 - mse: 2654.4866 - mae: 27.6967 - val_loss: 2087.2466 - val_mse: 2087.2466 - val_mae: 26.9760\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2686.5329 - mse: 2686.5334 - mae: 28.2901 - val_loss: 2080.2037 - val_mse: 2080.2036 - val_mae: 26.8115\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 351us/step - loss: 2667.1323 - mse: 2667.1316 - mae: 28.0950 - val_loss: 2096.6939 - val_mse: 2096.6941 - val_mae: 26.2180\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 333us/step - loss: 2681.0220 - mse: 2681.0220 - mae: 28.1568 - val_loss: 2082.9125 - val_mse: 2082.9124 - val_mae: 26.6280\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2649.5953 - mse: 2649.5955 - mae: 28.0719 - val_loss: 2077.9151 - val_mse: 2077.9150 - val_mae: 26.9707\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 340us/step - loss: 2695.2576 - mse: 2695.2573 - mae: 27.8200 - val_loss: 2085.4550 - val_mse: 2085.4551 - val_mae: 26.2721\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2640.0470 - mse: 2640.0459 - mae: 27.9574 - val_loss: 2078.7024 - val_mse: 2078.7021 - val_mae: 26.8578\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2665.8716 - mse: 2665.8711 - mae: 27.9750 - val_loss: 2084.2082 - val_mse: 2084.2083 - val_mae: 26.7598\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2687.3657 - mse: 2687.3655 - mae: 28.2219 - val_loss: 2079.5263 - val_mse: 2079.5266 - val_mae: 26.8010\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2693.0247 - mse: 2693.0247 - mae: 28.1526 - val_loss: 2082.5595 - val_mse: 2082.5596 - val_mae: 26.6134\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2664.7369 - mse: 2664.7371 - mae: 28.2016 - val_loss: 2079.1829 - val_mse: 2079.1831 - val_mae: 27.2321\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2647.7546 - mse: 2647.7542 - mae: 28.0093 - val_loss: 2088.1218 - val_mse: 2088.1226 - val_mae: 26.5649\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 354us/step - loss: 2653.9325 - mse: 2653.9324 - mae: 28.1374 - val_loss: 2090.0226 - val_mse: 2090.0225 - val_mae: 26.7003\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2654.4147 - mse: 2654.4153 - mae: 27.8152 - val_loss: 2092.7694 - val_mse: 2092.7693 - val_mae: 26.5761\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2694.4115 - mse: 2694.4121 - mae: 28.0276 - val_loss: 2082.7547 - val_mse: 2082.7546 - val_mae: 26.7840\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2668.8799 - mse: 2668.8804 - mae: 28.0418 - val_loss: 2082.0173 - val_mse: 2082.0171 - val_mae: 26.8639\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2683.5245 - mse: 2683.5249 - mae: 28.4460 - val_loss: 2085.9609 - val_mse: 2085.9609 - val_mae: 26.3943\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2704.7274 - mse: 2704.7271 - mae: 28.2031 - val_loss: 2092.9087 - val_mse: 2092.9084 - val_mae: 26.3090\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 341us/step - loss: 2688.1301 - mse: 2688.1292 - mae: 28.1646 - val_loss: 2102.8698 - val_mse: 2102.8696 - val_mae: 26.5378\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2628.8399 - mse: 2628.8401 - mae: 27.4328 - val_loss: 2094.8244 - val_mse: 2094.8245 - val_mae: 26.9230\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2676.9828 - mse: 2676.9824 - mae: 28.1394 - val_loss: 2098.9449 - val_mse: 2098.9451 - val_mae: 26.5225\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2667.3912 - mse: 2667.3911 - mae: 27.9383 - val_loss: 2107.4145 - val_mse: 2107.4148 - val_mae: 26.5721\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2678.3137 - mse: 2678.3130 - mae: 28.2676 - val_loss: 2105.0371 - val_mse: 2105.0374 - val_mae: 26.5335\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2661.4429 - mse: 2661.4424 - mae: 28.1256 - val_loss: 2096.9586 - val_mse: 2096.9585 - val_mae: 27.0479\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2711.2821 - mse: 2711.2825 - mae: 28.2558 - val_loss: 2107.2741 - val_mse: 2107.2742 - val_mae: 26.4225\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2717.1397 - mse: 2717.1389 - mae: 28.3294 - val_loss: 2107.1228 - val_mse: 2107.1230 - val_mae: 26.4749\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 330us/step - loss: 2671.0345 - mse: 2671.0342 - mae: 28.1129 - val_loss: 2103.1799 - val_mse: 2103.1807 - val_mae: 26.6627\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 248us/step - loss: 2637.9492 - mse: 2637.9482 - mae: 28.1719 - val_loss: 2098.8968 - val_mse: 2098.8970 - val_mae: 26.5403\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 343us/step - loss: 2683.4802 - mse: 2683.4800 - mae: 28.0003 - val_loss: 2096.9358 - val_mse: 2096.9355 - val_mae: 26.5288\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2707.7070 - mse: 2707.7068 - mae: 28.0719 - val_loss: 2091.9628 - val_mse: 2091.9634 - val_mae: 26.7644\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2610.9353 - mse: 2610.9348 - mae: 27.6540 - val_loss: 2085.1236 - val_mse: 2085.1238 - val_mae: 26.8798\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 329us/step - loss: 2638.6418 - mse: 2638.6426 - mae: 27.8612 - val_loss: 2102.2254 - val_mse: 2102.2251 - val_mae: 26.9426\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2659.8461 - mse: 2659.8462 - mae: 28.0637 - val_loss: 2104.0930 - val_mse: 2104.0928 - val_mae: 26.5639\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 276us/step - loss: 2623.9572 - mse: 2623.9590 - mae: 27.7078 - val_loss: 2102.3806 - val_mse: 2102.3804 - val_mae: 26.6171\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2695.8453 - mse: 2695.8457 - mae: 28.3872 - val_loss: 2109.9474 - val_mse: 2109.9470 - val_mae: 26.2573\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2692.4289 - mse: 2692.4294 - mae: 27.7771 - val_loss: 2111.3699 - val_mse: 2111.3696 - val_mae: 26.6522\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2641.1520 - mse: 2641.1519 - mae: 27.9907 - val_loss: 2113.5036 - val_mse: 2113.5037 - val_mae: 26.3398\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2658.9640 - mse: 2658.9636 - mae: 27.8318 - val_loss: 2100.7023 - val_mse: 2100.7021 - val_mae: 26.6912\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2625.1987 - mse: 2625.1985 - mae: 28.0447 - val_loss: 2099.7786 - val_mse: 2099.7786 - val_mae: 26.7106\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 331us/step - loss: 2655.0120 - mse: 2655.0125 - mae: 27.9653 - val_loss: 2101.5133 - val_mse: 2101.5134 - val_mae: 26.5093\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2673.6772 - mse: 2673.6770 - mae: 27.8266 - val_loss: 2112.9486 - val_mse: 2112.9487 - val_mae: 26.9760\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2656.3364 - mse: 2656.3369 - mae: 27.9833 - val_loss: 2109.0238 - val_mse: 2109.0237 - val_mae: 26.6543\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2667.9128 - mse: 2667.9126 - mae: 28.0665 - val_loss: 2110.1075 - val_mse: 2110.1077 - val_mae: 26.8336\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2670.0786 - mse: 2670.0786 - mae: 27.9232 - val_loss: 2109.0057 - val_mse: 2109.0056 - val_mae: 27.0207\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 327us/step - loss: 2655.7891 - mse: 2655.7883 - mae: 27.8844 - val_loss: 2107.6566 - val_mse: 2107.6560 - val_mae: 26.8831\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2682.5495 - mse: 2682.5496 - mae: 28.1444 - val_loss: 2119.7045 - val_mse: 2119.7043 - val_mae: 26.6489\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 357us/step - loss: 2640.0424 - mse: 2640.0420 - mae: 27.8483 - val_loss: 2129.9523 - val_mse: 2129.9524 - val_mae: 26.3676\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2645.8016 - mse: 2645.8018 - mae: 28.2486 - val_loss: 2114.6034 - val_mse: 2114.6033 - val_mae: 26.7197\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2700.9440 - mse: 2700.9443 - mae: 27.9318 - val_loss: 2108.9755 - val_mse: 2108.9753 - val_mae: 27.0242\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2645.7174 - mse: 2645.7175 - mae: 27.7365 - val_loss: 2113.9568 - val_mse: 2113.9565 - val_mae: 26.8599\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 351us/step - loss: 2658.8241 - mse: 2658.8230 - mae: 28.0458 - val_loss: 2121.7296 - val_mse: 2121.7297 - val_mae: 26.6642\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2645.4372 - mse: 2645.4380 - mae: 28.0132 - val_loss: 2114.5327 - val_mse: 2114.5327 - val_mae: 26.7836\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2693.3836 - mse: 2693.3831 - mae: 28.0734 - val_loss: 2110.0599 - val_mse: 2110.0598 - val_mae: 26.9379\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2678.6156 - mse: 2678.6157 - mae: 27.7202 - val_loss: 2113.6254 - val_mse: 2113.6252 - val_mae: 26.8734\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2723.1003 - mse: 2723.0996 - mae: 28.2567 - val_loss: 2112.1380 - val_mse: 2112.1384 - val_mae: 26.7950\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2675.7506 - mse: 2675.7507 - mae: 28.0530 - val_loss: 2111.8378 - val_mse: 2111.8381 - val_mae: 26.9932\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 325us/step - loss: 2613.0450 - mse: 2613.0452 - mae: 27.5060 - val_loss: 2119.6331 - val_mse: 2119.6331 - val_mae: 26.5799\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2673.9168 - mse: 2673.9170 - mae: 27.7874 - val_loss: 2115.6245 - val_mse: 2115.6245 - val_mae: 26.7327\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2614.7080 - mse: 2614.7073 - mae: 27.6888 - val_loss: 2102.0871 - val_mse: 2102.0874 - val_mae: 26.8666\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2612.8401 - mse: 2612.8401 - mae: 27.8003 - val_loss: 2103.9999 - val_mse: 2103.9998 - val_mae: 26.6511\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 330us/step - loss: 2666.0742 - mse: 2666.0740 - mae: 27.7158 - val_loss: 2103.9368 - val_mse: 2103.9365 - val_mae: 26.6499\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13293.0670 - mse: 13293.0674 - mae: 109.7473 - val_loss: 34526.8316 - val_mse: 34526.8281 - val_mae: 132.3885\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 328us/step - loss: 13043.9136 - mse: 13043.9131 - mae: 108.6186 - val_loss: 34010.8044 - val_mse: 34010.8047 - val_mae: 130.4427\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 12308.9796 - mse: 12308.9785 - mae: 105.1451 - val_loss: 32478.4721 - val_mse: 32478.4707 - val_mae: 124.4898\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 10395.8919 - mse: 10395.8926 - mae: 95.4149 - val_loss: 28612.7606 - val_mse: 28612.7617 - val_mae: 108.0268\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 6462.0224 - mse: 6462.0229 - mae: 70.5656 - val_loss: 21395.0266 - val_mse: 21395.0254 - val_mae: 67.2567\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2772.2268 - mse: 2772.2268 - mae: 39.0172 - val_loss: 17262.8546 - val_mse: 17262.8535 - val_mae: 35.1049\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 277us/step - loss: 2961.1169 - mse: 2961.1167 - mae: 39.3688 - val_loss: 17984.2323 - val_mse: 17984.2324 - val_mae: 36.5727\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 226us/step - loss: 2962.6684 - mse: 2962.6682 - mae: 38.9869 - val_loss: 17786.4829 - val_mse: 17786.4824 - val_mae: 35.7390\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 256us/step - loss: 2730.4721 - mse: 2730.4722 - mae: 37.9286 - val_loss: 17842.3564 - val_mse: 17842.3555 - val_mae: 35.9113\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 226us/step - loss: 2742.0033 - mse: 2742.0034 - mae: 37.6977 - val_loss: 17722.1080 - val_mse: 17722.1074 - val_mae: 35.5518\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 2619.9852 - mse: 2619.9851 - mae: 36.5221 - val_loss: 17733.8680 - val_mse: 17733.8691 - val_mae: 35.5833\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2722.7138 - mse: 2722.7141 - mae: 36.3991 - val_loss: 17508.2922 - val_mse: 17508.2930 - val_mae: 35.2885\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2707.2564 - mse: 2707.2563 - mae: 37.0340 - val_loss: 17834.4784 - val_mse: 17834.4785 - val_mae: 35.8735\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2679.5597 - mse: 2679.5596 - mae: 35.5868 - val_loss: 17630.8733 - val_mse: 17630.8730 - val_mae: 35.4977\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 235us/step - loss: 2818.8184 - mse: 2818.8186 - mae: 37.7937 - val_loss: 17890.1537 - val_mse: 17890.1523 - val_mae: 36.1576\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 275us/step - loss: 2570.8163 - mse: 2570.8162 - mae: 37.5262 - val_loss: 17698.9359 - val_mse: 17698.9355 - val_mae: 35.6581\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 235us/step - loss: 2726.8546 - mse: 2726.8542 - mae: 36.9355 - val_loss: 17795.0143 - val_mse: 17795.0137 - val_mae: 35.9002\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 2443.9904 - mse: 2443.9905 - mae: 35.1616 - val_loss: 17645.2655 - val_mse: 17645.2656 - val_mae: 35.6318\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2602.0742 - mse: 2602.0740 - mae: 36.8020 - val_loss: 17792.3593 - val_mse: 17792.3594 - val_mae: 35.9787\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 271us/step - loss: 2280.2872 - mse: 2280.2871 - mae: 34.3493 - val_loss: 17491.8795 - val_mse: 17491.8809 - val_mae: 35.4904\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2393.9978 - mse: 2393.9980 - mae: 34.7638 - val_loss: 17673.9612 - val_mse: 17673.9609 - val_mae: 35.7539\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2310.4963 - mse: 2310.4966 - mae: 35.4740 - val_loss: 17794.6525 - val_mse: 17794.6523 - val_mae: 36.1173\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 332us/step - loss: 2265.3505 - mse: 2265.3503 - mae: 32.7737 - val_loss: 17453.0227 - val_mse: 17453.0234 - val_mae: 35.6201\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2205.6552 - mse: 2205.6553 - mae: 34.2103 - val_loss: 17638.9642 - val_mse: 17638.9648 - val_mae: 35.8055\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2192.7764 - mse: 2192.7759 - mae: 33.5727 - val_loss: 17534.2857 - val_mse: 17534.2852 - val_mae: 35.7662\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 335us/step - loss: 2348.1867 - mse: 2348.1868 - mae: 34.5675 - val_loss: 17673.8250 - val_mse: 17673.8262 - val_mae: 35.9183\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 327us/step - loss: 2500.5230 - mse: 2500.5229 - mae: 35.7096 - val_loss: 17677.7558 - val_mse: 17677.7559 - val_mae: 35.9541\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 2419.1904 - mse: 2419.1904 - mae: 35.3698 - val_loss: 17673.7009 - val_mse: 17673.6992 - val_mae: 35.9916\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 334us/step - loss: 2247.2842 - mse: 2247.2842 - mae: 33.5131 - val_loss: 17491.8993 - val_mse: 17491.9004 - val_mae: 35.8822\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 316us/step - loss: 2333.3000 - mse: 2333.3000 - mae: 33.5570 - val_loss: 17753.4618 - val_mse: 17753.4629 - val_mae: 36.3244\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 2202.4120 - mse: 2202.4121 - mae: 32.5714 - val_loss: 17520.7016 - val_mse: 17520.7012 - val_mae: 35.9809\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 2294.1291 - mse: 2294.1292 - mae: 33.3759 - val_loss: 17634.2537 - val_mse: 17634.2520 - val_mae: 36.1108\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2442.4095 - mse: 2442.4094 - mae: 34.4325 - val_loss: 17617.8398 - val_mse: 17617.8398 - val_mae: 36.1306\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 260us/step - loss: 2205.8604 - mse: 2205.8604 - mae: 33.6465 - val_loss: 17599.6511 - val_mse: 17599.6523 - val_mae: 36.1419\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 268us/step - loss: 2241.2717 - mse: 2241.2720 - mae: 33.2399 - val_loss: 17688.8656 - val_mse: 17688.8633 - val_mae: 36.3855\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 270us/step - loss: 2309.3938 - mse: 2309.3938 - mae: 33.5646 - val_loss: 17818.5234 - val_mse: 17818.5234 - val_mae: 36.8920\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2189.8629 - mse: 2189.8630 - mae: 33.5380 - val_loss: 17801.5294 - val_mse: 17801.5293 - val_mae: 36.8772\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 399us/step - loss: 2176.3715 - mse: 2176.3718 - mae: 32.2683 - val_loss: 17466.3895 - val_mse: 17466.3926 - val_mae: 36.3434\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2259.8937 - mse: 2259.8938 - mae: 33.5090 - val_loss: 17739.7132 - val_mse: 17739.7129 - val_mae: 36.7712\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2258.7545 - mse: 2258.7544 - mae: 32.9194 - val_loss: 17806.0081 - val_mse: 17806.0078 - val_mae: 37.0826\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2389.9074 - mse: 2389.9075 - mae: 34.4357 - val_loss: 17777.9470 - val_mse: 17777.9473 - val_mae: 37.0399\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 287us/step - loss: 2033.6028 - mse: 2033.6028 - mae: 31.9774 - val_loss: 17718.8830 - val_mse: 17718.8867 - val_mae: 36.8572\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 2251.4647 - mse: 2251.4646 - mae: 32.4378 - val_loss: 17791.1285 - val_mse: 17791.1289 - val_mae: 37.1859\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2105.7622 - mse: 2105.7622 - mae: 32.3119 - val_loss: 17541.9606 - val_mse: 17541.9609 - val_mae: 36.6279\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2013.9783 - mse: 2013.9784 - mae: 32.0662 - val_loss: 17628.7239 - val_mse: 17628.7246 - val_mae: 36.7322\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2204.4832 - mse: 2204.4829 - mae: 33.4705 - val_loss: 17866.5262 - val_mse: 17866.5273 - val_mae: 37.7702\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2303.9697 - mse: 2303.9697 - mae: 33.1845 - val_loss: 17654.0101 - val_mse: 17654.0098 - val_mae: 36.8637\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2110.6792 - mse: 2110.6790 - mae: 33.4361 - val_loss: 17825.0312 - val_mse: 17825.0312 - val_mae: 37.5706\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2032.8896 - mse: 2032.8896 - mae: 31.6065 - val_loss: 17586.4860 - val_mse: 17586.4863 - val_mae: 36.8499\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2267.4795 - mse: 2267.4792 - mae: 33.4396 - val_loss: 17854.2213 - val_mse: 17854.2227 - val_mae: 37.8317\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 327us/step - loss: 2158.9810 - mse: 2158.9807 - mae: 33.4904 - val_loss: 17596.7332 - val_mse: 17596.7344 - val_mae: 36.9199\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2061.2133 - mse: 2061.2131 - mae: 31.7708 - val_loss: 17701.5441 - val_mse: 17701.5469 - val_mae: 37.2001\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 289us/step - loss: 2014.6285 - mse: 2014.6284 - mae: 31.1048 - val_loss: 17553.8918 - val_mse: 17553.8906 - val_mae: 36.9736\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2195.8461 - mse: 2195.8464 - mae: 32.9411 - val_loss: 17605.5676 - val_mse: 17605.5703 - val_mae: 37.0660\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2176.5384 - mse: 2176.5388 - mae: 32.3818 - val_loss: 17729.7146 - val_mse: 17729.7148 - val_mae: 37.3963\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 1990.9781 - mse: 1990.9781 - mae: 31.7889 - val_loss: 17662.6062 - val_mse: 17662.6055 - val_mae: 37.2494\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 345us/step - loss: 1949.1443 - mse: 1949.1445 - mae: 31.5261 - val_loss: 17679.8634 - val_mse: 17679.8613 - val_mae: 37.4209\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 252us/step - loss: 2096.3486 - mse: 2096.3486 - mae: 31.5290 - val_loss: 17717.7717 - val_mse: 17717.7695 - val_mae: 37.5879\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 287us/step - loss: 2092.7111 - mse: 2092.7112 - mae: 31.2361 - val_loss: 17668.5569 - val_mse: 17668.5586 - val_mae: 37.4557\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2232.9283 - mse: 2232.9282 - mae: 32.8071 - val_loss: 18056.6678 - val_mse: 18056.6680 - val_mae: 39.4786\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 2064.6119 - mse: 2064.6118 - mae: 30.7436 - val_loss: 17640.6417 - val_mse: 17640.6426 - val_mae: 37.4133\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 389us/step - loss: 2105.6860 - mse: 2105.6858 - mae: 31.8134 - val_loss: 17681.0768 - val_mse: 17681.0762 - val_mae: 37.5359\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 256us/step - loss: 1908.0218 - mse: 1908.0217 - mae: 29.8205 - val_loss: 17460.3375 - val_mse: 17460.3379 - val_mae: 37.3182\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 260us/step - loss: 2065.4238 - mse: 2065.4238 - mae: 31.8611 - val_loss: 17810.4958 - val_mse: 17810.4961 - val_mae: 38.0623\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 262us/step - loss: 2071.1663 - mse: 2071.1660 - mae: 31.8655 - val_loss: 17771.4100 - val_mse: 17771.4082 - val_mae: 37.9286\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 334us/step - loss: 1916.5904 - mse: 1916.5901 - mae: 30.7989 - val_loss: 17650.8786 - val_mse: 17650.8809 - val_mae: 37.5477\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 2076.7074 - mse: 2076.7073 - mae: 31.1319 - val_loss: 17769.1067 - val_mse: 17769.1074 - val_mae: 37.9841\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 1905.9628 - mse: 1905.9628 - mae: 30.1132 - val_loss: 17678.0207 - val_mse: 17678.0195 - val_mae: 37.7119\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 258us/step - loss: 1949.5013 - mse: 1949.5013 - mae: 30.1050 - val_loss: 17698.9223 - val_mse: 17698.9219 - val_mae: 37.7889\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 1998.0875 - mse: 1998.0874 - mae: 30.8485 - val_loss: 17699.7894 - val_mse: 17699.7891 - val_mae: 37.7919\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 277us/step - loss: 1871.6997 - mse: 1871.6996 - mae: 30.2102 - val_loss: 17614.3444 - val_mse: 17614.3438 - val_mae: 37.5773\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 232us/step - loss: 1852.4036 - mse: 1852.4034 - mae: 30.5210 - val_loss: 17597.2827 - val_mse: 17597.2812 - val_mae: 37.5832\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 1856.5334 - mse: 1856.5337 - mae: 29.3123 - val_loss: 17480.7999 - val_mse: 17480.8008 - val_mae: 37.5249\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 1858.2048 - mse: 1858.2048 - mae: 29.6601 - val_loss: 17735.0319 - val_mse: 17735.0332 - val_mae: 37.9689\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 1884.2696 - mse: 1884.2697 - mae: 30.4869 - val_loss: 17732.0776 - val_mse: 17732.0781 - val_mae: 37.9868\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 264us/step - loss: 1919.7745 - mse: 1919.7744 - mae: 31.2112 - val_loss: 17674.6793 - val_mse: 17674.6777 - val_mae: 37.8232\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 1825.9005 - mse: 1825.9005 - mae: 30.0390 - val_loss: 17806.9362 - val_mse: 17806.9355 - val_mae: 38.3806\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 1989.4382 - mse: 1989.4381 - mae: 30.9746 - val_loss: 17815.3494 - val_mse: 17815.3496 - val_mae: 38.3978\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 1821.8449 - mse: 1821.8447 - mae: 29.2294 - val_loss: 17763.4963 - val_mse: 17763.4961 - val_mae: 38.2046\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 325us/step - loss: 2049.3989 - mse: 2049.3989 - mae: 31.1718 - val_loss: 17787.3403 - val_mse: 17787.3418 - val_mae: 38.2295\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4167.0835 - mse: 4167.0840 - mae: 35.7570 - val_loss: 2046.3591 - val_mse: 2046.3591 - val_mae: 30.1432\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4380.9279 - mse: 4380.9277 - mae: 35.1430 - val_loss: 2161.1333 - val_mse: 2161.1333 - val_mae: 30.5470\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4254.5321 - mse: 4254.5322 - mae: 35.5411 - val_loss: 2176.8436 - val_mse: 2176.8435 - val_mae: 30.6207\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4266.7227 - mse: 4266.7222 - mae: 35.1128 - val_loss: 2207.8851 - val_mse: 2207.8853 - val_mae: 30.7349\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 381us/step - loss: 4216.4540 - mse: 4216.4541 - mae: 35.4646 - val_loss: 2189.2347 - val_mse: 2189.2349 - val_mae: 30.6736\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 3982.7950 - mse: 3982.7954 - mae: 34.0245 - val_loss: 2153.8512 - val_mse: 2153.8511 - val_mae: 30.5423\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 4048.0333 - mse: 4048.0334 - mae: 35.2282 - val_loss: 2236.7502 - val_mse: 2236.7500 - val_mae: 30.8584\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4349.8816 - mse: 4349.8823 - mae: 35.6206 - val_loss: 2223.2024 - val_mse: 2223.2024 - val_mae: 30.7932\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4117.2844 - mse: 4117.2842 - mae: 35.2770 - val_loss: 2207.5914 - val_mse: 2207.5916 - val_mae: 30.7283\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4234.5598 - mse: 4234.5591 - mae: 35.0129 - val_loss: 2234.9781 - val_mse: 2234.9783 - val_mae: 30.8388\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4034.7089 - mse: 4034.7092 - mae: 34.8745 - val_loss: 2195.4382 - val_mse: 2195.4382 - val_mae: 30.6660\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4261.2847 - mse: 4261.2847 - mae: 34.8411 - val_loss: 2186.8098 - val_mse: 2186.8101 - val_mae: 30.6322\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4224.3813 - mse: 4224.3813 - mae: 35.3866 - val_loss: 2239.6788 - val_mse: 2239.6787 - val_mae: 30.8526\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 4050.5230 - mse: 4050.5225 - mae: 34.5095 - val_loss: 2171.4961 - val_mse: 2171.4963 - val_mae: 30.5862\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4272.0200 - mse: 4272.0195 - mae: 35.4364 - val_loss: 2301.5856 - val_mse: 2301.5857 - val_mae: 31.1184\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 340us/step - loss: 4097.3334 - mse: 4097.3335 - mae: 34.5787 - val_loss: 2237.4856 - val_mse: 2237.4856 - val_mae: 30.8489\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 274us/step - loss: 4027.8009 - mse: 4027.8013 - mae: 34.1996 - val_loss: 2172.5449 - val_mse: 2172.5449 - val_mae: 30.6059\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 3965.9392 - mse: 3965.9390 - mae: 34.7434 - val_loss: 2197.7965 - val_mse: 2197.7964 - val_mae: 30.7005\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 4187.6876 - mse: 4187.6880 - mae: 34.5658 - val_loss: 2238.9169 - val_mse: 2238.9170 - val_mae: 30.8673\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 392us/step - loss: 4193.2004 - mse: 4193.2007 - mae: 35.2981 - val_loss: 2210.3378 - val_mse: 2210.3379 - val_mae: 30.7623\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4187.9191 - mse: 4187.9194 - mae: 34.3554 - val_loss: 2199.4480 - val_mse: 2199.4478 - val_mae: 30.7273\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 253us/step - loss: 4202.6501 - mse: 4202.6504 - mae: 34.1986 - val_loss: 2169.0822 - val_mse: 2169.0820 - val_mae: 30.6142\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 249us/step - loss: 4232.3404 - mse: 4232.3398 - mae: 34.1333 - val_loss: 2233.0531 - val_mse: 2233.0532 - val_mae: 30.8645\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4335.3006 - mse: 4335.3008 - mae: 34.8845 - val_loss: 2243.6478 - val_mse: 2243.6479 - val_mae: 30.9059\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4143.9624 - mse: 4143.9624 - mae: 34.4698 - val_loss: 2224.2605 - val_mse: 2224.2605 - val_mae: 30.8348\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4230.6675 - mse: 4230.6675 - mae: 34.5993 - val_loss: 2213.4048 - val_mse: 2213.4050 - val_mae: 30.7916\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4088.1551 - mse: 4088.1553 - mae: 34.9229 - val_loss: 2155.1722 - val_mse: 2155.1724 - val_mae: 30.5797\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 4288.4238 - mse: 4288.4238 - mae: 35.1127 - val_loss: 2161.2244 - val_mse: 2161.2249 - val_mae: 30.6093\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4302.1774 - mse: 4302.1777 - mae: 34.5602 - val_loss: 2266.0067 - val_mse: 2266.0071 - val_mae: 30.9932\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 345us/step - loss: 4014.1032 - mse: 4014.1035 - mae: 33.5261 - val_loss: 2194.7415 - val_mse: 2194.7415 - val_mae: 30.7267\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 4182.9917 - mse: 4182.9917 - mae: 35.0069 - val_loss: 2178.7265 - val_mse: 2178.7263 - val_mae: 30.6740\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 4130.6946 - mse: 4130.6953 - mae: 33.8457 - val_loss: 2247.0310 - val_mse: 2247.0310 - val_mae: 30.9140\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 3920.5830 - mse: 3920.5828 - mae: 33.7183 - val_loss: 2235.1638 - val_mse: 2235.1638 - val_mae: 30.8742\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4178.8285 - mse: 4178.8276 - mae: 34.2484 - val_loss: 2161.0269 - val_mse: 2161.0271 - val_mae: 30.6237\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4037.7959 - mse: 4037.7959 - mae: 34.0987 - val_loss: 2122.9669 - val_mse: 2122.9668 - val_mae: 30.4682\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4058.4466 - mse: 4058.4463 - mae: 34.3939 - val_loss: 2246.9022 - val_mse: 2246.9021 - val_mae: 30.9286\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4233.3401 - mse: 4233.3398 - mae: 34.6549 - val_loss: 2253.5556 - val_mse: 2253.5559 - val_mae: 30.9521\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4031.3314 - mse: 4031.3311 - mae: 33.7561 - val_loss: 2234.3489 - val_mse: 2234.3486 - val_mae: 30.8760\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 321us/step - loss: 4248.9413 - mse: 4248.9414 - mae: 34.6209 - val_loss: 2280.3666 - val_mse: 2280.3667 - val_mae: 31.0495\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4122.1944 - mse: 4122.1948 - mae: 33.9171 - val_loss: 2278.0581 - val_mse: 2278.0581 - val_mae: 31.0451\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4154.4774 - mse: 4154.4775 - mae: 33.3106 - val_loss: 2212.1933 - val_mse: 2212.1934 - val_mae: 30.8024\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 3896.0045 - mse: 3896.0044 - mae: 33.5879 - val_loss: 2220.1938 - val_mse: 2220.1938 - val_mae: 30.8338\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4172.9979 - mse: 4172.9980 - mae: 33.8889 - val_loss: 2214.5814 - val_mse: 2214.5815 - val_mae: 30.8233\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 3911.9531 - mse: 3911.9526 - mae: 33.8657 - val_loss: 2192.1823 - val_mse: 2192.1821 - val_mae: 30.7407\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 3894.1018 - mse: 3894.1016 - mae: 33.6022 - val_loss: 2194.9757 - val_mse: 2194.9761 - val_mae: 30.7469\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 242us/step - loss: 4121.4280 - mse: 4121.4277 - mae: 34.5731 - val_loss: 2340.3573 - val_mse: 2340.3572 - val_mae: 31.2884\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 267us/step - loss: 4013.0589 - mse: 4013.0591 - mae: 33.4491 - val_loss: 2221.3055 - val_mse: 2221.3057 - val_mae: 30.8614\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 302us/step - loss: 4188.4675 - mse: 4188.4668 - mae: 34.7522 - val_loss: 2239.1329 - val_mse: 2239.1328 - val_mae: 30.9195\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4107.4584 - mse: 4107.4590 - mae: 33.8111 - val_loss: 2186.5851 - val_mse: 2186.5847 - val_mae: 30.7335\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4133.9094 - mse: 4133.9097 - mae: 34.4660 - val_loss: 2297.7535 - val_mse: 2297.7537 - val_mae: 31.1265\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 329us/step - loss: 4111.2856 - mse: 4111.2856 - mae: 33.0377 - val_loss: 2232.0240 - val_mse: 2232.0239 - val_mae: 30.8960\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 431us/step - loss: 3972.7466 - mse: 3972.7468 - mae: 32.7976 - val_loss: 2172.2480 - val_mse: 2172.2478 - val_mae: 30.6949\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4059.1541 - mse: 4059.1541 - mae: 34.2462 - val_loss: 2296.7520 - val_mse: 2296.7522 - val_mae: 31.1413\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4211.9115 - mse: 4211.9116 - mae: 33.6529 - val_loss: 2259.4092 - val_mse: 2259.4089 - val_mae: 31.0162\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 301us/step - loss: 3954.1891 - mse: 3954.1882 - mae: 32.8264 - val_loss: 2162.4201 - val_mse: 2162.4199 - val_mae: 30.6620\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 4084.8740 - mse: 4084.8745 - mae: 33.9565 - val_loss: 2205.7836 - val_mse: 2205.7834 - val_mae: 30.8322\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 359us/step - loss: 4098.5740 - mse: 4098.5742 - mae: 33.8168 - val_loss: 2267.3757 - val_mse: 2267.3757 - val_mae: 31.0519\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4044.1463 - mse: 4044.1467 - mae: 33.3181 - val_loss: 2123.6415 - val_mse: 2123.6416 - val_mae: 30.5299\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 4211.6493 - mse: 4211.6489 - mae: 34.9914 - val_loss: 2274.0156 - val_mse: 2274.0154 - val_mae: 31.0903\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 3988.9190 - mse: 3988.9185 - mae: 33.3872 - val_loss: 2271.6053 - val_mse: 2271.6052 - val_mae: 31.0788\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 4083.1453 - mse: 4083.1453 - mae: 33.3943 - val_loss: 2222.4949 - val_mse: 2222.4951 - val_mae: 30.9081\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 384us/step - loss: 4056.1314 - mse: 4056.1309 - mae: 34.0595 - val_loss: 2227.9356 - val_mse: 2227.9358 - val_mae: 30.9306\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 267us/step - loss: 4003.5475 - mse: 4003.5476 - mae: 32.6708 - val_loss: 2202.1037 - val_mse: 2202.1038 - val_mae: 30.8372\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 371us/step - loss: 3989.3665 - mse: 3989.3677 - mae: 33.7441 - val_loss: 2223.8252 - val_mse: 2223.8252 - val_mae: 30.9086\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 3911.2326 - mse: 3911.2332 - mae: 32.5119 - val_loss: 2221.4782 - val_mse: 2221.4783 - val_mae: 30.8804\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 3952.2100 - mse: 3952.2092 - mae: 32.7811 - val_loss: 2180.4248 - val_mse: 2180.4248 - val_mae: 30.7385\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 4157.3655 - mse: 4157.3652 - mae: 33.4314 - val_loss: 2235.5268 - val_mse: 2235.5269 - val_mae: 30.9383\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 367us/step - loss: 3984.1398 - mse: 3984.1396 - mae: 34.7881 - val_loss: 2220.9678 - val_mse: 2220.9680 - val_mae: 30.8771\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 3992.3894 - mse: 3992.3899 - mae: 33.6522 - val_loss: 2205.0604 - val_mse: 2205.0605 - val_mae: 30.8059\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 287us/step - loss: 4075.6259 - mse: 4075.6260 - mae: 33.1014 - val_loss: 2225.4173 - val_mse: 2225.4175 - val_mae: 30.8713\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4088.5311 - mse: 4088.5312 - mae: 33.6995 - val_loss: 2216.4364 - val_mse: 2216.4363 - val_mae: 30.8370\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4050.5649 - mse: 4050.5647 - mae: 33.8236 - val_loss: 2253.8483 - val_mse: 2253.8481 - val_mae: 30.9756\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4021.1963 - mse: 4021.1963 - mae: 33.2915 - val_loss: 2285.1029 - val_mse: 2285.1025 - val_mae: 31.0970\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 3941.4486 - mse: 3941.4478 - mae: 32.8491 - val_loss: 2199.1710 - val_mse: 2199.1711 - val_mae: 30.8141\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4034.8309 - mse: 4034.8311 - mae: 33.8636 - val_loss: 2193.9989 - val_mse: 2193.9988 - val_mae: 30.7981\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 272us/step - loss: 4148.4986 - mse: 4148.4980 - mae: 32.9426 - val_loss: 2253.5209 - val_mse: 2253.5210 - val_mae: 31.0074\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 245us/step - loss: 3965.3307 - mse: 3965.3308 - mae: 33.3605 - val_loss: 2257.9903 - val_mse: 2257.9905 - val_mae: 31.0295\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 4125.4028 - mse: 4125.4023 - mae: 33.9640 - val_loss: 2281.4406 - val_mse: 2281.4407 - val_mae: 31.1098\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 297us/step - loss: 4142.1877 - mse: 4142.1870 - mae: 33.5014 - val_loss: 2284.1002 - val_mse: 2284.1001 - val_mae: 31.1303\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 3963.9779 - mse: 3963.9785 - mae: 32.7599 - val_loss: 2196.6835 - val_mse: 2196.6836 - val_mae: 30.8368\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3359.8296 - mse: 3359.8289 - mae: 32.9921 - val_loss: 1463.0189 - val_mse: 1463.0187 - val_mae: 26.0922\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3379.8307 - mse: 3379.8303 - mae: 32.2641 - val_loss: 1461.2370 - val_mse: 1461.2369 - val_mae: 25.6248\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 271us/step - loss: 3249.7464 - mse: 3249.7466 - mae: 32.4129 - val_loss: 1466.0914 - val_mse: 1466.0912 - val_mae: 26.6000\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3437.4995 - mse: 3437.5002 - mae: 33.3983 - val_loss: 1460.6560 - val_mse: 1460.6561 - val_mae: 25.5131\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 317us/step - loss: 3450.2317 - mse: 3450.2314 - mae: 32.4376 - val_loss: 1460.2976 - val_mse: 1460.2975 - val_mae: 25.8461\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3351.2133 - mse: 3351.2141 - mae: 32.7808 - val_loss: 1460.1816 - val_mse: 1460.1816 - val_mae: 25.6762\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3387.7914 - mse: 3387.7920 - mae: 32.9362 - val_loss: 1459.7372 - val_mse: 1459.7373 - val_mae: 25.8090\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 336us/step - loss: 3464.9717 - mse: 3464.9719 - mae: 33.3857 - val_loss: 1457.9869 - val_mse: 1457.9869 - val_mae: 25.7006\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 328us/step - loss: 3373.6384 - mse: 3373.6389 - mae: 32.8329 - val_loss: 1459.7065 - val_mse: 1459.7065 - val_mae: 25.9018\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3436.9237 - mse: 3436.9243 - mae: 33.4603 - val_loss: 1464.1387 - val_mse: 1464.1387 - val_mae: 25.3507\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3454.4332 - mse: 3454.4331 - mae: 33.4686 - val_loss: 1460.1276 - val_mse: 1460.1277 - val_mae: 25.6088\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3311.4698 - mse: 3311.4685 - mae: 32.4717 - val_loss: 1460.9531 - val_mse: 1460.9532 - val_mae: 26.0034\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 275us/step - loss: 3330.7030 - mse: 3330.7024 - mae: 32.0218 - val_loss: 1463.8943 - val_mse: 1463.8942 - val_mae: 26.3185\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3419.3442 - mse: 3419.3442 - mae: 32.5989 - val_loss: 1462.7458 - val_mse: 1462.7458 - val_mae: 25.8208\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3420.6015 - mse: 3420.6013 - mae: 32.7110 - val_loss: 1463.1910 - val_mse: 1463.1912 - val_mae: 25.6935\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 282us/step - loss: 3242.7162 - mse: 3242.7173 - mae: 32.1120 - val_loss: 1463.6574 - val_mse: 1463.6576 - val_mae: 25.8707\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3296.7547 - mse: 3296.7551 - mae: 32.7526 - val_loss: 1464.6782 - val_mse: 1464.6780 - val_mae: 25.4186\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3227.0884 - mse: 3227.0884 - mae: 31.9829 - val_loss: 1461.5688 - val_mse: 1461.5690 - val_mae: 25.9353\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3392.5246 - mse: 3392.5244 - mae: 32.5852 - val_loss: 1462.9948 - val_mse: 1462.9946 - val_mae: 26.0705\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3421.5313 - mse: 3421.5315 - mae: 32.8688 - val_loss: 1461.1723 - val_mse: 1461.1722 - val_mae: 25.8477\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3314.5446 - mse: 3314.5444 - mae: 32.5507 - val_loss: 1461.6245 - val_mse: 1461.6244 - val_mae: 25.8199\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3421.1898 - mse: 3421.1897 - mae: 33.1166 - val_loss: 1463.1529 - val_mse: 1463.1530 - val_mae: 25.6119\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3330.1854 - mse: 3330.1863 - mae: 32.3960 - val_loss: 1463.9457 - val_mse: 1463.9458 - val_mae: 25.6298\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3489.6796 - mse: 3489.6790 - mae: 32.8867 - val_loss: 1463.2038 - val_mse: 1463.2037 - val_mae: 25.8960\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 353us/step - loss: 3389.2578 - mse: 3389.2585 - mae: 32.8276 - val_loss: 1464.9175 - val_mse: 1464.9175 - val_mae: 25.4344\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3375.5990 - mse: 3375.5986 - mae: 32.3478 - val_loss: 1463.7812 - val_mse: 1463.7812 - val_mae: 25.7882\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 327us/step - loss: 3531.1869 - mse: 3531.1873 - mae: 33.6084 - val_loss: 1466.6525 - val_mse: 1466.6525 - val_mae: 25.5689\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 271us/step - loss: 3282.5254 - mse: 3282.5256 - mae: 32.3892 - val_loss: 1467.7839 - val_mse: 1467.7839 - val_mae: 26.1740\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3333.6071 - mse: 3333.6057 - mae: 31.9488 - val_loss: 1470.2376 - val_mse: 1470.2374 - val_mae: 26.2174\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3235.3487 - mse: 3235.3489 - mae: 31.6152 - val_loss: 1470.3751 - val_mse: 1470.3751 - val_mae: 25.8508\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3268.2337 - mse: 3268.2339 - mae: 31.7073 - val_loss: 1469.6462 - val_mse: 1469.6462 - val_mae: 25.8246\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3272.5781 - mse: 3272.5779 - mae: 31.9203 - val_loss: 1471.0548 - val_mse: 1471.0548 - val_mae: 25.6534\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3227.5965 - mse: 3227.5969 - mae: 31.3326 - val_loss: 1471.3092 - val_mse: 1471.3092 - val_mae: 26.0009\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3337.4692 - mse: 3337.4695 - mae: 32.4540 - val_loss: 1472.2759 - val_mse: 1472.2759 - val_mae: 26.1091\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 338us/step - loss: 3235.4956 - mse: 3235.4949 - mae: 31.9184 - val_loss: 1473.7009 - val_mse: 1473.7009 - val_mae: 26.0005\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 359us/step - loss: 3261.4000 - mse: 3261.3999 - mae: 31.7718 - val_loss: 1474.7915 - val_mse: 1474.7917 - val_mae: 25.9684\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3281.8122 - mse: 3281.8127 - mae: 31.9654 - val_loss: 1476.0872 - val_mse: 1476.0870 - val_mae: 25.7120\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3433.7832 - mse: 3433.7830 - mae: 32.8076 - val_loss: 1475.8092 - val_mse: 1475.8092 - val_mae: 25.7467\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3324.6999 - mse: 3324.6997 - mae: 32.2468 - val_loss: 1475.3541 - val_mse: 1475.3541 - val_mae: 25.8143\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 274us/step - loss: 3336.9162 - mse: 3336.9158 - mae: 32.0398 - val_loss: 1474.7346 - val_mse: 1474.7346 - val_mae: 26.2444\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3308.5481 - mse: 3308.5479 - mae: 32.2988 - val_loss: 1471.9708 - val_mse: 1471.9707 - val_mae: 25.8440\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3271.2436 - mse: 3271.2434 - mae: 31.8212 - val_loss: 1473.2668 - val_mse: 1473.2667 - val_mae: 26.0489\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3236.0019 - mse: 3236.0010 - mae: 31.3338 - val_loss: 1472.9679 - val_mse: 1472.9681 - val_mae: 26.1616\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3287.9805 - mse: 3287.9802 - mae: 32.4371 - val_loss: 1472.3985 - val_mse: 1472.3986 - val_mae: 26.0398\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 249us/step - loss: 3223.3297 - mse: 3223.3298 - mae: 32.3361 - val_loss: 1470.7727 - val_mse: 1470.7727 - val_mae: 25.8711\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 224us/step - loss: 3418.3312 - mse: 3418.3315 - mae: 31.9784 - val_loss: 1471.5037 - val_mse: 1471.5037 - val_mae: 25.7234\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 279us/step - loss: 3453.8314 - mse: 3453.8315 - mae: 32.9641 - val_loss: 1470.6682 - val_mse: 1470.6683 - val_mae: 25.8333\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3188.0194 - mse: 3188.0205 - mae: 31.1936 - val_loss: 1474.8225 - val_mse: 1474.8225 - val_mae: 26.2510\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3314.8007 - mse: 3314.8008 - mae: 32.2257 - val_loss: 1479.0376 - val_mse: 1479.0376 - val_mae: 25.5412\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3241.4693 - mse: 3241.4700 - mae: 32.0340 - val_loss: 1477.2534 - val_mse: 1477.2533 - val_mae: 26.1630\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 346us/step - loss: 3260.2926 - mse: 3260.2932 - mae: 32.2713 - val_loss: 1476.5691 - val_mse: 1476.5688 - val_mae: 25.8770\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3354.6252 - mse: 3354.6248 - mae: 32.0631 - val_loss: 1478.0835 - val_mse: 1478.0835 - val_mae: 25.5562\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3228.0091 - mse: 3228.0083 - mae: 31.3105 - val_loss: 1474.8254 - val_mse: 1474.8253 - val_mae: 25.9925\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3209.8393 - mse: 3209.8398 - mae: 31.2251 - val_loss: 1476.5703 - val_mse: 1476.5702 - val_mae: 26.0869\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 375us/step - loss: 3375.5971 - mse: 3375.5964 - mae: 32.6096 - val_loss: 1477.0388 - val_mse: 1477.0388 - val_mae: 25.9882\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3427.4797 - mse: 3427.4790 - mae: 32.5471 - val_loss: 1477.5435 - val_mse: 1477.5436 - val_mae: 25.9819\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3246.5784 - mse: 3246.5784 - mae: 31.8732 - val_loss: 1477.8855 - val_mse: 1477.8855 - val_mae: 26.1520\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3337.1551 - mse: 3337.1558 - mae: 32.0991 - val_loss: 1477.6462 - val_mse: 1477.6462 - val_mae: 26.2403\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3258.1050 - mse: 3258.1047 - mae: 31.9492 - val_loss: 1477.9571 - val_mse: 1477.9573 - val_mae: 25.7129\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3273.6728 - mse: 3273.6726 - mae: 31.9592 - val_loss: 1479.2833 - val_mse: 1479.2834 - val_mae: 26.3436\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3328.7868 - mse: 3328.7869 - mae: 31.8745 - val_loss: 1478.3876 - val_mse: 1478.3877 - val_mae: 25.8732\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3197.2655 - mse: 3197.2659 - mae: 32.3097 - val_loss: 1478.8817 - val_mse: 1478.8817 - val_mae: 26.0279\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3236.4537 - mse: 3236.4531 - mae: 31.5689 - val_loss: 1477.8186 - val_mse: 1477.8187 - val_mae: 25.8849\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3343.2530 - mse: 3343.2522 - mae: 31.8143 - val_loss: 1479.2762 - val_mse: 1479.2762 - val_mae: 25.9200\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3212.1775 - mse: 3212.1765 - mae: 31.6040 - val_loss: 1477.8206 - val_mse: 1477.8207 - val_mae: 26.1557\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3241.3071 - mse: 3241.3074 - mae: 32.1378 - val_loss: 1478.0219 - val_mse: 1478.0217 - val_mae: 26.2630\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3342.0111 - mse: 3342.0110 - mae: 31.6513 - val_loss: 1477.4565 - val_mse: 1477.4565 - val_mae: 26.2665\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3312.5326 - mse: 3312.5327 - mae: 32.0847 - val_loss: 1476.7122 - val_mse: 1476.7123 - val_mae: 26.0220\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 261us/step - loss: 3240.3738 - mse: 3240.3735 - mae: 31.7035 - val_loss: 1474.0133 - val_mse: 1474.0134 - val_mae: 25.9732\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3302.1255 - mse: 3302.1245 - mae: 31.8765 - val_loss: 1475.8747 - val_mse: 1475.8746 - val_mae: 25.9649\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 353us/step - loss: 3314.1967 - mse: 3314.1960 - mae: 31.9113 - val_loss: 1475.0178 - val_mse: 1475.0177 - val_mae: 26.1481\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3149.7591 - mse: 3149.7595 - mae: 32.1561 - val_loss: 1474.5911 - val_mse: 1474.5912 - val_mae: 25.7963\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 262us/step - loss: 3270.4048 - mse: 3270.4048 - mae: 31.8568 - val_loss: 1477.7212 - val_mse: 1477.7214 - val_mae: 26.5518\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3291.8701 - mse: 3291.8706 - mae: 32.1237 - val_loss: 1474.2997 - val_mse: 1474.2997 - val_mae: 26.1236\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3279.4107 - mse: 3279.4104 - mae: 32.1395 - val_loss: 1473.4176 - val_mse: 1473.4176 - val_mae: 25.7953\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3297.5033 - mse: 3297.5034 - mae: 31.9508 - val_loss: 1474.1476 - val_mse: 1474.1476 - val_mae: 25.6630\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 251us/step - loss: 3222.5217 - mse: 3222.5215 - mae: 31.7185 - val_loss: 1473.1708 - val_mse: 1473.1708 - val_mae: 25.8570\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3308.3977 - mse: 3308.3972 - mae: 31.7193 - val_loss: 1473.3282 - val_mse: 1473.3284 - val_mae: 25.7925\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3254.3249 - mse: 3254.3247 - mae: 31.8310 - val_loss: 1474.0669 - val_mse: 1474.0670 - val_mae: 26.0341\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 254us/step - loss: 3170.9034 - mse: 3170.9041 - mae: 30.8954 - val_loss: 1476.2393 - val_mse: 1476.2393 - val_mae: 26.2355\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2898.6054 - mse: 2898.6052 - mae: 31.0979 - val_loss: 1062.1700 - val_mse: 1062.1698 - val_mae: 23.7233\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 326us/step - loss: 2970.7106 - mse: 2970.7104 - mae: 31.5752 - val_loss: 1060.1403 - val_mse: 1060.1401 - val_mae: 23.7222\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2904.8410 - mse: 2904.8408 - mae: 31.0881 - val_loss: 1058.5769 - val_mse: 1058.5769 - val_mae: 23.7278\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2941.4963 - mse: 2941.4961 - mae: 31.2293 - val_loss: 1055.1910 - val_mse: 1055.1909 - val_mae: 23.8481\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 259us/step - loss: 2925.7325 - mse: 2925.7319 - mae: 31.1801 - val_loss: 1058.8859 - val_mse: 1058.8859 - val_mae: 23.6046\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2904.4459 - mse: 2904.4463 - mae: 31.3888 - val_loss: 1052.3265 - val_mse: 1052.3265 - val_mae: 23.9762\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2864.9055 - mse: 2864.9048 - mae: 31.1006 - val_loss: 1055.8148 - val_mse: 1055.8146 - val_mae: 23.6525\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2864.8502 - mse: 2864.8508 - mae: 31.0675 - val_loss: 1057.4163 - val_mse: 1057.4163 - val_mae: 23.5155\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 255us/step - loss: 2974.3000 - mse: 2974.3000 - mae: 31.5318 - val_loss: 1059.2927 - val_mse: 1059.2928 - val_mae: 23.4033\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2890.7082 - mse: 2890.7078 - mae: 30.7956 - val_loss: 1052.8928 - val_mse: 1052.8929 - val_mae: 23.6888\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2875.9302 - mse: 2875.9302 - mae: 30.9428 - val_loss: 1055.3032 - val_mse: 1055.3032 - val_mae: 23.5208\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2943.5601 - mse: 2943.5598 - mae: 31.3964 - val_loss: 1053.1144 - val_mse: 1053.1145 - val_mae: 23.6685\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 271us/step - loss: 2844.2233 - mse: 2844.2239 - mae: 30.7481 - val_loss: 1054.3093 - val_mse: 1054.3092 - val_mae: 23.5501\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2882.9421 - mse: 2882.9429 - mae: 30.9344 - val_loss: 1056.6425 - val_mse: 1056.6426 - val_mae: 23.4486\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2962.3847 - mse: 2962.3848 - mae: 31.3256 - val_loss: 1054.5098 - val_mse: 1054.5098 - val_mae: 23.4995\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2968.6462 - mse: 2968.6467 - mae: 31.1582 - val_loss: 1057.8835 - val_mse: 1057.8835 - val_mae: 23.4028\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2928.6125 - mse: 2928.6113 - mae: 30.9169 - val_loss: 1047.4879 - val_mse: 1047.4879 - val_mae: 24.0715\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 337us/step - loss: 2904.8141 - mse: 2904.8147 - mae: 31.4168 - val_loss: 1048.3019 - val_mse: 1048.3019 - val_mae: 23.8009\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2904.3736 - mse: 2904.3743 - mae: 30.9510 - val_loss: 1052.5994 - val_mse: 1052.5996 - val_mae: 23.5089\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2802.1257 - mse: 2802.1252 - mae: 30.1268 - val_loss: 1050.8993 - val_mse: 1050.8994 - val_mae: 23.5998\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2938.9588 - mse: 2938.9580 - mae: 31.4046 - val_loss: 1051.1426 - val_mse: 1051.1427 - val_mae: 23.5277\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2915.2028 - mse: 2915.2021 - mae: 30.8119 - val_loss: 1044.6660 - val_mse: 1044.6660 - val_mae: 23.9295\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2896.5381 - mse: 2896.5376 - mae: 31.5616 - val_loss: 1048.2083 - val_mse: 1048.2083 - val_mae: 23.5934\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 274us/step - loss: 2971.2083 - mse: 2971.2092 - mae: 31.5457 - val_loss: 1051.2609 - val_mse: 1051.2609 - val_mae: 23.4201\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2849.6313 - mse: 2849.6326 - mae: 30.4087 - val_loss: 1044.3509 - val_mse: 1044.3510 - val_mae: 23.8413\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 253us/step - loss: 2840.3963 - mse: 2840.3958 - mae: 30.8444 - val_loss: 1049.3006 - val_mse: 1049.3004 - val_mae: 23.5090\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2848.4237 - mse: 2848.4233 - mae: 30.6262 - val_loss: 1050.2773 - val_mse: 1050.2773 - val_mae: 23.4062\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2925.4934 - mse: 2925.4937 - mae: 31.1260 - val_loss: 1040.9746 - val_mse: 1040.9747 - val_mae: 23.9963\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2856.2473 - mse: 2856.2471 - mae: 30.9819 - val_loss: 1042.7387 - val_mse: 1042.7388 - val_mae: 23.6908\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 334us/step - loss: 2841.1586 - mse: 2841.1589 - mae: 30.5374 - val_loss: 1044.1044 - val_mse: 1044.1045 - val_mae: 23.5767\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2910.5984 - mse: 2910.5989 - mae: 30.5048 - val_loss: 1040.1767 - val_mse: 1040.1766 - val_mae: 23.7882\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 341us/step - loss: 2899.0277 - mse: 2899.0273 - mae: 30.9224 - val_loss: 1038.6680 - val_mse: 1038.6680 - val_mae: 23.8655\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 277us/step - loss: 2914.2420 - mse: 2914.2422 - mae: 31.1406 - val_loss: 1038.5229 - val_mse: 1038.5229 - val_mae: 23.8739\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2819.8172 - mse: 2819.8176 - mae: 30.5013 - val_loss: 1039.6918 - val_mse: 1039.6918 - val_mae: 23.6534\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2872.5777 - mse: 2872.5774 - mae: 30.5836 - val_loss: 1037.9775 - val_mse: 1037.9775 - val_mae: 23.7431\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 279us/step - loss: 2797.2780 - mse: 2797.2778 - mae: 30.6371 - val_loss: 1039.2582 - val_mse: 1039.2582 - val_mae: 23.5781\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2909.2778 - mse: 2909.2783 - mae: 31.3048 - val_loss: 1038.8378 - val_mse: 1038.8378 - val_mae: 23.5186\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2894.3095 - mse: 2894.3088 - mae: 30.8049 - val_loss: 1036.9421 - val_mse: 1036.9421 - val_mae: 23.5887\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2877.4418 - mse: 2877.4414 - mae: 30.5852 - val_loss: 1033.1848 - val_mse: 1033.1847 - val_mae: 23.9044\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2934.8329 - mse: 2934.8325 - mae: 31.1287 - val_loss: 1041.1607 - val_mse: 1041.1606 - val_mae: 23.2748\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2923.6162 - mse: 2923.6162 - mae: 31.1491 - val_loss: 1038.5378 - val_mse: 1038.5377 - val_mae: 23.3760\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2854.4701 - mse: 2854.4705 - mae: 30.1452 - val_loss: 1033.4985 - val_mse: 1033.4985 - val_mae: 23.7297\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2931.5933 - mse: 2931.5930 - mae: 30.8885 - val_loss: 1037.9213 - val_mse: 1037.9213 - val_mae: 23.3837\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2854.3869 - mse: 2854.3875 - mae: 30.9063 - val_loss: 1037.1605 - val_mse: 1037.1604 - val_mae: 23.4169\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2905.2223 - mse: 2905.2222 - mae: 31.4769 - val_loss: 1042.6793 - val_mse: 1042.6792 - val_mae: 23.1791\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 339us/step - loss: 2830.6796 - mse: 2830.6797 - mae: 30.1730 - val_loss: 1036.1720 - val_mse: 1036.1720 - val_mae: 23.4015\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 343us/step - loss: 2870.5876 - mse: 2870.5876 - mae: 30.9445 - val_loss: 1033.0291 - val_mse: 1033.0291 - val_mae: 23.5798\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2821.9436 - mse: 2821.9426 - mae: 30.8744 - val_loss: 1029.1366 - val_mse: 1029.1366 - val_mae: 23.9789\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2925.9750 - mse: 2925.9749 - mae: 31.1768 - val_loss: 1033.0770 - val_mse: 1033.0771 - val_mae: 23.4552\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2974.9666 - mse: 2974.9670 - mae: 31.1597 - val_loss: 1030.3817 - val_mse: 1030.3817 - val_mae: 23.5693\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 319us/step - loss: 2852.3450 - mse: 2852.3457 - mae: 30.1496 - val_loss: 1026.9793 - val_mse: 1026.9794 - val_mae: 23.8142\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2852.7701 - mse: 2852.7705 - mae: 31.1237 - val_loss: 1030.2421 - val_mse: 1030.2421 - val_mae: 23.4165\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 319us/step - loss: 2890.3903 - mse: 2890.3896 - mae: 30.9635 - val_loss: 1031.2405 - val_mse: 1031.2405 - val_mae: 23.3409\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2818.4952 - mse: 2818.4941 - mae: 30.4171 - val_loss: 1026.0915 - val_mse: 1026.0914 - val_mae: 23.6006\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2826.4195 - mse: 2826.4199 - mae: 30.9416 - val_loss: 1026.0282 - val_mse: 1026.0282 - val_mae: 23.4947\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2823.8576 - mse: 2823.8572 - mae: 30.1792 - val_loss: 1024.0751 - val_mse: 1024.0752 - val_mae: 23.5792\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2791.0712 - mse: 2791.0710 - mae: 30.5490 - val_loss: 1019.4768 - val_mse: 1019.4767 - val_mae: 23.8849\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2882.3147 - mse: 2882.3140 - mae: 30.9117 - val_loss: 1019.6868 - val_mse: 1019.6869 - val_mae: 23.7069\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2862.1323 - mse: 2862.1328 - mae: 30.7119 - val_loss: 1020.5755 - val_mse: 1020.5753 - val_mae: 23.4738\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2883.0078 - mse: 2883.0083 - mae: 30.6279 - val_loss: 1018.4087 - val_mse: 1018.4087 - val_mae: 23.5416\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 326us/step - loss: 2873.7424 - mse: 2873.7422 - mae: 30.3580 - val_loss: 1019.1099 - val_mse: 1019.1100 - val_mae: 23.4278\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2790.6111 - mse: 2790.6106 - mae: 30.3653 - val_loss: 1013.1247 - val_mse: 1013.1246 - val_mae: 23.9207\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2909.6946 - mse: 2909.6948 - mae: 31.1180 - val_loss: 1020.2806 - val_mse: 1020.2806 - val_mae: 23.1733\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2838.9441 - mse: 2838.9436 - mae: 30.0306 - val_loss: 1013.3252 - val_mse: 1013.3253 - val_mae: 23.5323\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 271us/step - loss: 2796.7796 - mse: 2796.7798 - mae: 30.2211 - val_loss: 1021.4594 - val_mse: 1021.4595 - val_mae: 23.1423\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2844.9667 - mse: 2844.9666 - mae: 30.2154 - val_loss: 1020.1610 - val_mse: 1020.1610 - val_mae: 23.2440\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2846.9992 - mse: 2846.9993 - mae: 30.8503 - val_loss: 1017.5371 - val_mse: 1017.5370 - val_mae: 23.3387\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2790.3838 - mse: 2790.3833 - mae: 30.7254 - val_loss: 1015.5571 - val_mse: 1015.5571 - val_mae: 23.3867\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2843.9159 - mse: 2843.9148 - mae: 30.3572 - val_loss: 1013.0075 - val_mse: 1013.0074 - val_mae: 23.4401\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 349us/step - loss: 2815.2740 - mse: 2815.2739 - mae: 29.9429 - val_loss: 1010.5236 - val_mse: 1010.5236 - val_mae: 23.3610\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 343us/step - loss: 2808.7981 - mse: 2808.7979 - mae: 30.2136 - val_loss: 1011.2942 - val_mse: 1011.2943 - val_mae: 23.2390\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2919.2655 - mse: 2919.2651 - mae: 30.9245 - val_loss: 1013.3589 - val_mse: 1013.3588 - val_mae: 23.1801\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2848.7377 - mse: 2848.7383 - mae: 29.8223 - val_loss: 1009.9754 - val_mse: 1009.9754 - val_mae: 23.3394\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2812.0702 - mse: 2812.0703 - mae: 30.6020 - val_loss: 1011.6747 - val_mse: 1011.6747 - val_mae: 23.1889\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2842.6183 - mse: 2842.6184 - mae: 30.0729 - val_loss: 1008.5966 - val_mse: 1008.5965 - val_mae: 23.3648\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2898.7412 - mse: 2898.7407 - mae: 30.5149 - val_loss: 1009.3693 - val_mse: 1009.3694 - val_mae: 23.2276\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2824.8615 - mse: 2824.8613 - mae: 30.7682 - val_loss: 1008.7564 - val_mse: 1008.7565 - val_mae: 23.1819\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2774.6438 - mse: 2774.6431 - mae: 29.8434 - val_loss: 1006.0247 - val_mse: 1006.0247 - val_mae: 23.2813\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 380us/step - loss: 2761.9588 - mse: 2761.9600 - mae: 30.0873 - val_loss: 999.8243 - val_mse: 999.8243 - val_mae: 23.7017\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 0s 240us/step - loss: 2712.3394 - mse: 2712.3389 - mae: 30.3729 - val_loss: 1004.8815 - val_mse: 1004.8815 - val_mae: 23.1694\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2468.3173 - mse: 2468.3174 - mae: 29.9653 - val_loss: 1484.9529 - val_mse: 1484.9530 - val_mae: 26.9147\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2482.4863 - mse: 2482.4866 - mae: 29.5104 - val_loss: 1489.9628 - val_mse: 1489.9629 - val_mae: 26.7719\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2507.7102 - mse: 2507.7107 - mae: 29.8539 - val_loss: 1478.3599 - val_mse: 1478.3600 - val_mae: 26.9943\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2493.4296 - mse: 2493.4292 - mae: 29.4213 - val_loss: 1490.8051 - val_mse: 1490.8051 - val_mae: 26.6559\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 275us/step - loss: 2551.1458 - mse: 2551.1460 - mae: 29.9644 - val_loss: 1491.4163 - val_mse: 1491.4164 - val_mae: 26.5156\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2515.2136 - mse: 2515.2139 - mae: 29.6474 - val_loss: 1489.8844 - val_mse: 1489.8844 - val_mae: 26.4812\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2502.2387 - mse: 2502.2385 - mae: 29.5724 - val_loss: 1481.5420 - val_mse: 1481.5420 - val_mae: 26.6240\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2496.7751 - mse: 2496.7751 - mae: 29.6097 - val_loss: 1496.8376 - val_mse: 1496.8376 - val_mae: 26.2837\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2566.1433 - mse: 2566.1431 - mae: 30.0022 - val_loss: 1483.0173 - val_mse: 1483.0175 - val_mae: 26.4447\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2506.3273 - mse: 2506.3274 - mae: 30.0059 - val_loss: 1480.5693 - val_mse: 1480.5695 - val_mae: 26.4644\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2490.8180 - mse: 2490.8174 - mae: 29.5614 - val_loss: 1479.1931 - val_mse: 1479.1931 - val_mae: 26.4171\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2508.4629 - mse: 2508.4629 - mae: 29.3628 - val_loss: 1487.6691 - val_mse: 1487.6691 - val_mae: 26.2407\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 337us/step - loss: 2539.7805 - mse: 2539.7800 - mae: 29.1952 - val_loss: 1476.7489 - val_mse: 1476.7488 - val_mae: 26.3495\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2478.8331 - mse: 2478.8333 - mae: 29.3517 - val_loss: 1471.6867 - val_mse: 1471.6866 - val_mae: 26.3896\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2520.6349 - mse: 2520.6348 - mae: 29.6581 - val_loss: 1477.5371 - val_mse: 1477.5371 - val_mae: 26.3000\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2579.0336 - mse: 2579.0334 - mae: 29.7622 - val_loss: 1479.9489 - val_mse: 1479.9491 - val_mae: 26.2333\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 336us/step - loss: 2418.6772 - mse: 2418.6772 - mae: 29.2086 - val_loss: 1461.9079 - val_mse: 1461.9078 - val_mae: 26.5365\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 285us/step - loss: 2510.9136 - mse: 2510.9136 - mae: 29.5576 - val_loss: 1465.5518 - val_mse: 1465.5519 - val_mae: 26.4345\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2534.5324 - mse: 2534.5325 - mae: 29.5397 - val_loss: 1469.5957 - val_mse: 1469.5955 - val_mae: 26.2819\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2487.4617 - mse: 2487.4622 - mae: 29.5281 - val_loss: 1471.6585 - val_mse: 1471.6586 - val_mae: 26.2651\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 263us/step - loss: 2433.5737 - mse: 2433.5725 - mae: 29.4950 - val_loss: 1461.3061 - val_mse: 1461.3062 - val_mae: 26.4044\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 332us/step - loss: 2556.6767 - mse: 2556.6763 - mae: 29.8898 - val_loss: 1479.0320 - val_mse: 1479.0322 - val_mae: 26.1071\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2549.3032 - mse: 2549.3037 - mae: 29.7213 - val_loss: 1481.4661 - val_mse: 1481.4661 - val_mae: 26.0657\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 274us/step - loss: 2520.2618 - mse: 2520.2615 - mae: 29.4070 - val_loss: 1457.9008 - val_mse: 1457.9009 - val_mae: 26.3965\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2489.7635 - mse: 2489.7632 - mae: 29.2789 - val_loss: 1478.2477 - val_mse: 1478.2476 - val_mae: 26.0077\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2542.9878 - mse: 2542.9880 - mae: 29.5074 - val_loss: 1464.0520 - val_mse: 1464.0520 - val_mae: 26.2434\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2449.8523 - mse: 2449.8523 - mae: 28.7283 - val_loss: 1478.7593 - val_mse: 1478.7593 - val_mae: 25.9495\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2479.4371 - mse: 2479.4365 - mae: 29.5481 - val_loss: 1457.5790 - val_mse: 1457.5791 - val_mae: 26.2854\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2429.2883 - mse: 2429.2888 - mae: 29.3571 - val_loss: 1451.9347 - val_mse: 1451.9348 - val_mae: 26.4300\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2568.8976 - mse: 2568.8979 - mae: 29.7176 - val_loss: 1464.4509 - val_mse: 1464.4508 - val_mae: 26.1622\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2483.7273 - mse: 2483.7271 - mae: 29.6446 - val_loss: 1452.0528 - val_mse: 1452.0526 - val_mae: 26.3972\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 370us/step - loss: 2510.9941 - mse: 2510.9941 - mae: 29.4442 - val_loss: 1458.1230 - val_mse: 1458.1232 - val_mae: 26.3240\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2518.8411 - mse: 2518.8401 - mae: 29.6689 - val_loss: 1463.7645 - val_mse: 1463.7645 - val_mae: 26.1682\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 270us/step - loss: 2435.4354 - mse: 2435.4351 - mae: 28.6514 - val_loss: 1454.7978 - val_mse: 1454.7976 - val_mae: 26.3250\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 273us/step - loss: 2435.9666 - mse: 2435.9678 - mae: 29.0104 - val_loss: 1451.9187 - val_mse: 1451.9186 - val_mae: 26.3705\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 361us/step - loss: 2486.4515 - mse: 2486.4517 - mae: 29.4339 - val_loss: 1452.7095 - val_mse: 1452.7096 - val_mae: 26.2643\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 330us/step - loss: 2416.2210 - mse: 2416.2202 - mae: 29.2249 - val_loss: 1442.4035 - val_mse: 1442.4037 - val_mae: 26.3940\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2478.8145 - mse: 2478.8149 - mae: 29.0453 - val_loss: 1463.2402 - val_mse: 1463.2401 - val_mae: 25.9479\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2477.5249 - mse: 2477.5251 - mae: 29.2767 - val_loss: 1457.3583 - val_mse: 1457.3584 - val_mae: 26.0298\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2447.0371 - mse: 2447.0374 - mae: 29.1371 - val_loss: 1454.8984 - val_mse: 1454.8983 - val_mae: 26.0714\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2382.4177 - mse: 2382.4177 - mae: 29.0777 - val_loss: 1441.9503 - val_mse: 1441.9502 - val_mae: 26.3050\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2483.4677 - mse: 2483.4680 - mae: 29.3220 - val_loss: 1460.1166 - val_mse: 1460.1166 - val_mae: 25.9330\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2470.0714 - mse: 2470.0710 - mae: 28.9159 - val_loss: 1459.4033 - val_mse: 1459.4034 - val_mae: 25.9368\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2428.2451 - mse: 2428.2456 - mae: 29.0478 - val_loss: 1444.9745 - val_mse: 1444.9746 - val_mae: 26.1915\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2556.5771 - mse: 2556.5762 - mae: 29.4681 - val_loss: 1445.5317 - val_mse: 1445.5316 - val_mae: 26.2344\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2508.1378 - mse: 2508.1375 - mae: 29.1378 - val_loss: 1454.6053 - val_mse: 1454.6053 - val_mae: 26.0315\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 280us/step - loss: 2456.7792 - mse: 2456.7791 - mae: 29.0760 - val_loss: 1446.5118 - val_mse: 1446.5116 - val_mae: 26.1389\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2503.1471 - mse: 2503.1465 - mae: 29.4038 - val_loss: 1449.2968 - val_mse: 1449.2969 - val_mae: 26.0247\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2470.6112 - mse: 2470.6113 - mae: 29.1233 - val_loss: 1445.2740 - val_mse: 1445.2739 - val_mae: 26.0703\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 341us/step - loss: 2437.0701 - mse: 2437.0703 - mae: 28.9842 - val_loss: 1449.2598 - val_mse: 1449.2599 - val_mae: 25.9825\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2479.5661 - mse: 2479.5662 - mae: 29.0681 - val_loss: 1436.8365 - val_mse: 1436.8365 - val_mae: 26.2717\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2440.4236 - mse: 2440.4236 - mae: 28.8379 - val_loss: 1439.7648 - val_mse: 1439.7649 - val_mae: 26.0760\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 261us/step - loss: 2463.4675 - mse: 2463.4673 - mae: 29.3036 - val_loss: 1434.4088 - val_mse: 1434.4088 - val_mae: 26.1505\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2407.9779 - mse: 2407.9778 - mae: 29.2178 - val_loss: 1454.4395 - val_mse: 1454.4395 - val_mae: 25.7760\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 330us/step - loss: 2406.8742 - mse: 2406.8743 - mae: 28.4564 - val_loss: 1430.5794 - val_mse: 1430.5791 - val_mae: 26.2462\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 287us/step - loss: 2468.6618 - mse: 2468.6616 - mae: 29.3286 - val_loss: 1441.3527 - val_mse: 1441.3530 - val_mae: 25.9223\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 324us/step - loss: 2460.6807 - mse: 2460.6804 - mae: 29.0342 - val_loss: 1432.4309 - val_mse: 1432.4310 - val_mae: 26.0913\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2509.2925 - mse: 2509.2925 - mae: 29.6293 - val_loss: 1449.7817 - val_mse: 1449.7819 - val_mae: 25.7549\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2440.5891 - mse: 2440.5891 - mae: 28.7036 - val_loss: 1428.3367 - val_mse: 1428.3365 - val_mae: 26.1603\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2462.5597 - mse: 2462.5593 - mae: 29.1512 - val_loss: 1433.7828 - val_mse: 1433.7830 - val_mae: 26.0030\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2471.0910 - mse: 2471.0901 - mae: 29.0596 - val_loss: 1435.8136 - val_mse: 1435.8135 - val_mae: 25.9895\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2465.6493 - mse: 2465.6479 - mae: 29.1554 - val_loss: 1431.5680 - val_mse: 1431.5679 - val_mae: 26.1007\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2489.1282 - mse: 2489.1282 - mae: 29.2684 - val_loss: 1445.4870 - val_mse: 1445.4869 - val_mae: 25.8567\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2509.4851 - mse: 2509.4849 - mae: 29.7554 - val_loss: 1434.8205 - val_mse: 1434.8203 - val_mae: 26.0675\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2457.5714 - mse: 2457.5725 - mae: 29.0402 - val_loss: 1425.6562 - val_mse: 1425.6562 - val_mae: 26.3277\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2423.2577 - mse: 2423.2573 - mae: 28.8953 - val_loss: 1437.8522 - val_mse: 1437.8522 - val_mae: 26.0149\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2429.8425 - mse: 2429.8428 - mae: 28.7313 - val_loss: 1430.0115 - val_mse: 1430.0116 - val_mae: 26.1206\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 279us/step - loss: 2451.4639 - mse: 2451.4636 - mae: 28.9635 - val_loss: 1428.1101 - val_mse: 1428.1101 - val_mae: 26.1313\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 294us/step - loss: 2413.6610 - mse: 2413.6611 - mae: 29.1001 - val_loss: 1428.8671 - val_mse: 1428.8671 - val_mae: 26.0860\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2514.4754 - mse: 2514.4746 - mae: 29.0001 - val_loss: 1441.6913 - val_mse: 1441.6913 - val_mae: 25.8628\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2482.1227 - mse: 2482.1228 - mae: 29.4785 - val_loss: 1432.8887 - val_mse: 1432.8887 - val_mae: 25.9704\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 279us/step - loss: 2468.4778 - mse: 2468.4780 - mae: 29.1512 - val_loss: 1420.6242 - val_mse: 1420.6241 - val_mae: 26.2332\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2486.6676 - mse: 2486.6685 - mae: 29.0863 - val_loss: 1440.1968 - val_mse: 1440.1964 - val_mae: 25.8224\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2383.4872 - mse: 2383.4875 - mae: 28.7655 - val_loss: 1418.9453 - val_mse: 1418.9453 - val_mae: 26.2287\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2423.1995 - mse: 2423.1990 - mae: 28.9289 - val_loss: 1427.0929 - val_mse: 1427.0928 - val_mae: 25.9884\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2454.0321 - mse: 2454.0315 - mae: 28.9494 - val_loss: 1414.6280 - val_mse: 1414.6281 - val_mae: 26.2381\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 335us/step - loss: 2460.2892 - mse: 2460.2896 - mae: 28.8512 - val_loss: 1418.5793 - val_mse: 1418.5792 - val_mae: 26.0834\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 353us/step - loss: 2418.0980 - mse: 2418.0977 - mae: 28.6360 - val_loss: 1429.0297 - val_mse: 1429.0295 - val_mae: 25.8154\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2478.6887 - mse: 2478.6885 - mae: 28.8680 - val_loss: 1424.0612 - val_mse: 1424.0613 - val_mae: 25.9068\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 327us/step - loss: 2373.2314 - mse: 2373.2312 - mae: 28.4145 - val_loss: 1432.1178 - val_mse: 1432.1179 - val_mae: 25.7007\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2343.7761 - mse: 2343.7751 - mae: 29.0302 - val_loss: 3695.1741 - val_mse: 3695.1736 - val_mae: 23.5929\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2346.1568 - mse: 2346.1570 - mae: 29.4250 - val_loss: 3693.8488 - val_mse: 3693.8491 - val_mae: 22.6442\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 269us/step - loss: 2383.6710 - mse: 2383.6719 - mae: 30.0467 - val_loss: 3694.5744 - val_mse: 3694.5740 - val_mae: 22.6685\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2375.4998 - mse: 2375.5002 - mae: 29.3650 - val_loss: 3695.1367 - val_mse: 3695.1372 - val_mae: 23.0415\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2422.6003 - mse: 2422.6008 - mae: 29.7117 - val_loss: 3695.9346 - val_mse: 3695.9341 - val_mae: 22.9718\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2302.3421 - mse: 2302.3413 - mae: 29.0075 - val_loss: 3698.6827 - val_mse: 3698.6841 - val_mae: 23.2789\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2321.0571 - mse: 2321.0574 - mae: 29.1020 - val_loss: 3702.3296 - val_mse: 3702.3296 - val_mae: 23.5575\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2250.3027 - mse: 2250.3025 - mae: 28.9593 - val_loss: 3706.0840 - val_mse: 3706.0840 - val_mae: 22.6891\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2379.3482 - mse: 2379.3481 - mae: 29.7807 - val_loss: 3704.3409 - val_mse: 3704.3413 - val_mae: 23.1896\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2397.2105 - mse: 2397.2109 - mae: 29.4829 - val_loss: 3704.6579 - val_mse: 3704.6577 - val_mae: 22.8148\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2404.5390 - mse: 2404.5381 - mae: 29.7387 - val_loss: 3706.1336 - val_mse: 3706.1345 - val_mae: 23.1307\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2336.0131 - mse: 2336.0132 - mae: 28.9660 - val_loss: 3705.8958 - val_mse: 3705.8965 - val_mae: 23.3352\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2351.8042 - mse: 2351.8049 - mae: 29.3299 - val_loss: 3706.0570 - val_mse: 3706.0569 - val_mae: 23.1195\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2354.4443 - mse: 2354.4441 - mae: 29.3363 - val_loss: 3705.3439 - val_mse: 3705.3440 - val_mae: 22.6722\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2368.0085 - mse: 2368.0090 - mae: 29.4770 - val_loss: 3706.2644 - val_mse: 3706.2644 - val_mae: 23.5291\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2324.4009 - mse: 2324.4019 - mae: 29.0229 - val_loss: 3706.4438 - val_mse: 3706.4434 - val_mae: 22.8325\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 357us/step - loss: 2317.5805 - mse: 2317.5801 - mae: 29.0420 - val_loss: 3707.9470 - val_mse: 3707.9475 - val_mae: 23.3737\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2333.7075 - mse: 2333.7083 - mae: 29.3784 - val_loss: 3709.1039 - val_mse: 3709.1038 - val_mae: 23.1019\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2327.8075 - mse: 2327.8071 - mae: 28.9847 - val_loss: 3710.2572 - val_mse: 3710.2566 - val_mae: 22.8214\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2343.4734 - mse: 2343.4731 - mae: 28.9665 - val_loss: 3710.3496 - val_mse: 3710.3503 - val_mae: 22.6471\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2367.2049 - mse: 2367.2041 - mae: 29.5927 - val_loss: 3709.8880 - val_mse: 3709.8882 - val_mae: 23.1113\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 345us/step - loss: 2307.4645 - mse: 2307.4646 - mae: 29.1629 - val_loss: 3712.2388 - val_mse: 3712.2393 - val_mae: 23.0700\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 331us/step - loss: 2333.2707 - mse: 2333.2715 - mae: 29.2745 - val_loss: 3711.1639 - val_mse: 3711.1643 - val_mae: 23.0524\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 257us/step - loss: 2375.6056 - mse: 2375.6050 - mae: 29.4525 - val_loss: 3712.4939 - val_mse: 3712.4937 - val_mae: 22.6278\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2300.0272 - mse: 2300.0266 - mae: 28.8933 - val_loss: 3712.7729 - val_mse: 3712.7727 - val_mae: 22.5873\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2379.6321 - mse: 2379.6326 - mae: 29.4158 - val_loss: 3711.6393 - val_mse: 3711.6387 - val_mae: 23.1616\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2378.0598 - mse: 2378.0603 - mae: 29.2812 - val_loss: 3709.5096 - val_mse: 3709.5090 - val_mae: 23.2982\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2393.2041 - mse: 2393.2041 - mae: 29.6528 - val_loss: 3710.3795 - val_mse: 3710.3801 - val_mae: 23.1765\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2362.7958 - mse: 2362.7954 - mae: 29.5965 - val_loss: 3712.1502 - val_mse: 3712.1501 - val_mae: 22.9808\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 328us/step - loss: 2318.2064 - mse: 2318.2061 - mae: 28.9941 - val_loss: 3716.0644 - val_mse: 3716.0647 - val_mae: 23.5067\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2348.0062 - mse: 2348.0063 - mae: 29.0848 - val_loss: 3711.3507 - val_mse: 3711.3503 - val_mae: 23.1238\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2327.8864 - mse: 2327.8862 - mae: 29.4307 - val_loss: 3713.8951 - val_mse: 3713.8953 - val_mae: 22.6263\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 332us/step - loss: 2383.5010 - mse: 2383.5007 - mae: 29.2097 - val_loss: 3713.0942 - val_mse: 3713.0933 - val_mae: 23.2497\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 389us/step - loss: 2348.1806 - mse: 2348.1812 - mae: 29.0710 - val_loss: 3713.8276 - val_mse: 3713.8274 - val_mae: 22.5230\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 294us/step - loss: 2398.3270 - mse: 2398.3269 - mae: 29.6629 - val_loss: 3714.4187 - val_mse: 3714.4180 - val_mae: 22.5550\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2318.2861 - mse: 2318.2866 - mae: 29.2742 - val_loss: 3711.5748 - val_mse: 3711.5750 - val_mae: 23.1281\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 331us/step - loss: 2354.5488 - mse: 2354.5486 - mae: 29.1006 - val_loss: 3712.5717 - val_mse: 3712.5723 - val_mae: 22.8565\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 276us/step - loss: 2341.9953 - mse: 2341.9949 - mae: 29.4265 - val_loss: 3715.9176 - val_mse: 3715.9170 - val_mae: 23.0750\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2267.8642 - mse: 2267.8647 - mae: 28.9909 - val_loss: 3718.3627 - val_mse: 3718.3628 - val_mae: 23.1898\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2312.1005 - mse: 2312.1013 - mae: 28.9540 - val_loss: 3721.7364 - val_mse: 3721.7363 - val_mae: 23.0108\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 348us/step - loss: 2327.1984 - mse: 2327.1987 - mae: 29.2496 - val_loss: 3720.5709 - val_mse: 3720.5703 - val_mae: 23.4997\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 327us/step - loss: 2344.8179 - mse: 2344.8181 - mae: 29.0759 - val_loss: 3721.8139 - val_mse: 3721.8135 - val_mae: 23.3124\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 331us/step - loss: 2275.6553 - mse: 2275.6555 - mae: 29.2080 - val_loss: 3721.3405 - val_mse: 3721.3406 - val_mae: 23.0256\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2295.5140 - mse: 2295.5142 - mae: 29.3259 - val_loss: 3717.8296 - val_mse: 3717.8291 - val_mae: 22.7992\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2269.0055 - mse: 2269.0056 - mae: 28.6917 - val_loss: 3720.5351 - val_mse: 3720.5354 - val_mae: 23.2840\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 328us/step - loss: 2303.7556 - mse: 2303.7554 - mae: 28.8530 - val_loss: 3723.1011 - val_mse: 3723.1018 - val_mae: 23.4123\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2320.8018 - mse: 2320.8027 - mae: 29.0465 - val_loss: 3727.5446 - val_mse: 3727.5442 - val_mae: 23.5145\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2265.6894 - mse: 2265.6897 - mae: 28.5410 - val_loss: 3722.7052 - val_mse: 3722.7061 - val_mae: 22.9088\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2264.8501 - mse: 2264.8499 - mae: 28.5642 - val_loss: 3722.2597 - val_mse: 3722.2598 - val_mae: 23.3839\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2286.9673 - mse: 2286.9678 - mae: 29.0616 - val_loss: 3724.5411 - val_mse: 3724.5413 - val_mae: 23.5965\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 328us/step - loss: 2265.6664 - mse: 2265.6663 - mae: 28.4379 - val_loss: 3725.2217 - val_mse: 3725.2222 - val_mae: 23.1624\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2292.0535 - mse: 2292.0530 - mae: 29.0582 - val_loss: 3720.3146 - val_mse: 3720.3140 - val_mae: 22.9257\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 350us/step - loss: 2334.0318 - mse: 2334.0320 - mae: 29.0908 - val_loss: 3725.4273 - val_mse: 3725.4263 - val_mae: 23.5564\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2350.0206 - mse: 2350.0205 - mae: 29.5358 - val_loss: 3721.9988 - val_mse: 3721.9998 - val_mae: 23.0346\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 358us/step - loss: 2260.2832 - mse: 2260.2844 - mae: 28.4525 - val_loss: 3720.2777 - val_mse: 3720.2781 - val_mae: 23.4072\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2299.7443 - mse: 2299.7449 - mae: 29.2625 - val_loss: 3716.9092 - val_mse: 3716.9089 - val_mae: 22.8008\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2305.6395 - mse: 2305.6392 - mae: 29.0840 - val_loss: 3723.2871 - val_mse: 3723.2861 - val_mae: 23.6434\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2310.2358 - mse: 2310.2354 - mae: 29.0713 - val_loss: 3722.8312 - val_mse: 3722.8320 - val_mae: 23.0966\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2327.9080 - mse: 2327.9075 - mae: 29.2755 - val_loss: 3729.1913 - val_mse: 3729.1917 - val_mae: 23.7595\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 289us/step - loss: 2291.5613 - mse: 2291.5620 - mae: 28.8979 - val_loss: 3726.0594 - val_mse: 3726.0588 - val_mae: 22.8611\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 335us/step - loss: 2293.9551 - mse: 2293.9543 - mae: 28.9162 - val_loss: 3721.1335 - val_mse: 3721.1338 - val_mae: 22.8659\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2330.7726 - mse: 2330.7729 - mae: 28.9221 - val_loss: 3722.5039 - val_mse: 3722.5039 - val_mae: 23.4508\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2305.0909 - mse: 2305.0903 - mae: 28.7164 - val_loss: 3722.7749 - val_mse: 3722.7759 - val_mae: 23.4971\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2282.3286 - mse: 2282.3279 - mae: 28.8860 - val_loss: 3717.8747 - val_mse: 3717.8752 - val_mae: 22.4324\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 263us/step - loss: 2325.2397 - mse: 2325.2388 - mae: 28.6771 - val_loss: 3722.2601 - val_mse: 3722.2590 - val_mae: 23.7336\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2318.4551 - mse: 2318.4553 - mae: 29.2270 - val_loss: 3724.2809 - val_mse: 3724.2800 - val_mae: 23.4997\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 350us/step - loss: 2284.3117 - mse: 2284.3120 - mae: 29.2071 - val_loss: 3722.3259 - val_mse: 3722.3250 - val_mae: 23.2327\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 352us/step - loss: 2272.2310 - mse: 2272.2305 - mae: 28.9831 - val_loss: 3724.8153 - val_mse: 3724.8152 - val_mae: 23.1730\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2280.8837 - mse: 2280.8835 - mae: 28.7182 - val_loss: 3728.8133 - val_mse: 3728.8130 - val_mae: 23.3047\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2228.8492 - mse: 2228.8484 - mae: 28.5066 - val_loss: 3733.2972 - val_mse: 3733.2976 - val_mae: 23.3785\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2271.2293 - mse: 2271.2295 - mae: 29.1051 - val_loss: 3731.2883 - val_mse: 3731.2881 - val_mae: 22.9334\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2292.9632 - mse: 2292.9641 - mae: 28.9337 - val_loss: 3730.6163 - val_mse: 3730.6165 - val_mae: 23.0078\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 325us/step - loss: 2355.8355 - mse: 2355.8354 - mae: 29.0786 - val_loss: 3724.3966 - val_mse: 3724.3965 - val_mae: 23.0754\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2245.8806 - mse: 2245.8809 - mae: 28.4925 - val_loss: 3730.3359 - val_mse: 3730.3359 - val_mae: 23.5683\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2301.2480 - mse: 2301.2480 - mae: 28.6901 - val_loss: 3732.5992 - val_mse: 3732.5991 - val_mae: 23.3559\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 337us/step - loss: 2297.1687 - mse: 2297.1687 - mae: 28.9501 - val_loss: 3727.8857 - val_mse: 3727.8860 - val_mae: 22.7162\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2294.1511 - mse: 2294.1509 - mae: 28.6411 - val_loss: 3724.0180 - val_mse: 3724.0181 - val_mae: 22.9789\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2290.0514 - mse: 2290.0515 - mae: 28.9469 - val_loss: 3724.7462 - val_mse: 3724.7461 - val_mae: 23.2914\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2306.8787 - mse: 2306.8782 - mae: 28.9420 - val_loss: 3723.2056 - val_mse: 3723.2061 - val_mae: 22.9839\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2282.7826 - mse: 2282.7830 - mae: 28.7736 - val_loss: 3723.9504 - val_mse: 3723.9512 - val_mae: 23.1708\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2709.5738 - mse: 2709.5735 - mae: 28.5161 - val_loss: 2042.1157 - val_mse: 2042.1155 - val_mae: 26.4573\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2718.3771 - mse: 2718.3767 - mae: 28.6224 - val_loss: 2045.3109 - val_mse: 2045.3109 - val_mae: 26.4795\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 271us/step - loss: 2634.5558 - mse: 2634.5562 - mae: 28.3888 - val_loss: 2042.3317 - val_mse: 2042.3315 - val_mae: 26.9027\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2654.1602 - mse: 2654.1589 - mae: 28.3082 - val_loss: 2053.3258 - val_mse: 2053.3254 - val_mae: 26.4288\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2710.4333 - mse: 2710.4326 - mae: 28.3348 - val_loss: 2057.1837 - val_mse: 2057.1838 - val_mae: 26.2734\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2672.2946 - mse: 2672.2944 - mae: 28.1197 - val_loss: 2048.9182 - val_mse: 2048.9182 - val_mae: 26.2313\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2697.4930 - mse: 2697.4932 - mae: 28.4889 - val_loss: 2044.1374 - val_mse: 2044.1373 - val_mae: 26.4321\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2718.7868 - mse: 2718.7861 - mae: 29.1006 - val_loss: 2044.0264 - val_mse: 2044.0266 - val_mae: 26.8331\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 354us/step - loss: 2704.1511 - mse: 2704.1519 - mae: 28.2669 - val_loss: 2039.0978 - val_mse: 2039.0980 - val_mae: 26.8917\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2699.9341 - mse: 2699.9346 - mae: 28.5951 - val_loss: 2046.4640 - val_mse: 2046.4639 - val_mae: 26.8728\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2729.0891 - mse: 2729.0889 - mae: 28.4465 - val_loss: 2049.0508 - val_mse: 2049.0505 - val_mae: 26.6967\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2719.0377 - mse: 2719.0383 - mae: 28.2340 - val_loss: 2054.1862 - val_mse: 2054.1863 - val_mae: 26.5358\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2716.7926 - mse: 2716.7927 - mae: 28.3170 - val_loss: 2057.3290 - val_mse: 2057.3289 - val_mae: 26.3773\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2661.9057 - mse: 2661.9055 - mae: 28.1547 - val_loss: 2057.9476 - val_mse: 2057.9478 - val_mae: 26.2314\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2685.2270 - mse: 2685.2263 - mae: 27.9313 - val_loss: 2046.8864 - val_mse: 2046.8865 - val_mae: 26.6632\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2651.3606 - mse: 2651.3611 - mae: 28.4530 - val_loss: 2038.7023 - val_mse: 2038.7024 - val_mae: 26.9009\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2617.9234 - mse: 2617.9231 - mae: 28.0811 - val_loss: 2042.7867 - val_mse: 2042.7867 - val_mae: 27.2553\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2610.6809 - mse: 2610.6809 - mae: 27.8541 - val_loss: 2041.8005 - val_mse: 2041.8007 - val_mae: 26.8601\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2721.2732 - mse: 2721.2729 - mae: 28.4263 - val_loss: 2044.0885 - val_mse: 2044.0886 - val_mae: 26.5483\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2639.8287 - mse: 2639.8298 - mae: 27.9059 - val_loss: 2044.7896 - val_mse: 2044.7893 - val_mae: 26.6804\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2710.5865 - mse: 2710.5859 - mae: 28.6697 - val_loss: 2059.5053 - val_mse: 2059.5056 - val_mae: 26.6204\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2686.2079 - mse: 2686.2090 - mae: 28.1430 - val_loss: 2054.4992 - val_mse: 2054.4990 - val_mae: 27.1509\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2687.0445 - mse: 2687.0442 - mae: 28.1452 - val_loss: 2049.3093 - val_mse: 2049.3091 - val_mae: 26.7126\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2634.1511 - mse: 2634.1506 - mae: 28.2468 - val_loss: 2056.8981 - val_mse: 2056.8984 - val_mae: 26.4224\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2668.7376 - mse: 2668.7371 - mae: 28.3992 - val_loss: 2054.9914 - val_mse: 2054.9915 - val_mae: 26.8617\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2631.9484 - mse: 2631.9482 - mae: 27.7518 - val_loss: 2063.7019 - val_mse: 2063.7021 - val_mae: 26.6844\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 253us/step - loss: 2643.0742 - mse: 2643.0740 - mae: 28.2083 - val_loss: 2054.6486 - val_mse: 2054.6484 - val_mae: 26.9171\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 245us/step - loss: 2589.5817 - mse: 2589.5808 - mae: 28.1951 - val_loss: 2054.0814 - val_mse: 2054.0815 - val_mae: 27.3500\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2674.5175 - mse: 2674.5186 - mae: 28.6744 - val_loss: 2051.9204 - val_mse: 2051.9202 - val_mae: 26.9890\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2668.7353 - mse: 2668.7358 - mae: 28.4634 - val_loss: 2059.4615 - val_mse: 2059.4612 - val_mae: 26.7306\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 276us/step - loss: 2690.0970 - mse: 2690.0977 - mae: 28.3052 - val_loss: 2065.5969 - val_mse: 2065.5972 - val_mae: 26.4892\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2638.3084 - mse: 2638.3081 - mae: 28.3582 - val_loss: 2061.0275 - val_mse: 2061.0273 - val_mae: 26.8476\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2671.3543 - mse: 2671.3545 - mae: 28.3355 - val_loss: 2063.6081 - val_mse: 2063.6082 - val_mae: 26.4378\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2671.7821 - mse: 2671.7822 - mae: 28.1501 - val_loss: 2065.5141 - val_mse: 2065.5142 - val_mae: 26.2919\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 261us/step - loss: 2629.1766 - mse: 2629.1768 - mae: 28.0130 - val_loss: 2062.6051 - val_mse: 2062.6052 - val_mae: 26.0730\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2669.6878 - mse: 2669.6887 - mae: 28.4321 - val_loss: 2065.8234 - val_mse: 2065.8232 - val_mae: 26.8531\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2695.5993 - mse: 2695.5989 - mae: 28.2598 - val_loss: 2074.3837 - val_mse: 2074.3835 - val_mae: 26.5399\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2710.9902 - mse: 2710.9905 - mae: 28.2048 - val_loss: 2076.9169 - val_mse: 2076.9167 - val_mae: 26.7260\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2657.1358 - mse: 2657.1362 - mae: 27.9350 - val_loss: 2067.3614 - val_mse: 2067.3613 - val_mae: 26.6772\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2696.4081 - mse: 2696.4082 - mae: 28.6450 - val_loss: 2063.9244 - val_mse: 2063.9241 - val_mae: 27.0552\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 267us/step - loss: 2658.1621 - mse: 2658.1619 - mae: 27.9343 - val_loss: 2063.6855 - val_mse: 2063.6858 - val_mae: 26.6377\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 263us/step - loss: 2669.6888 - mse: 2669.6885 - mae: 27.9880 - val_loss: 2063.8314 - val_mse: 2063.8315 - val_mae: 26.5539\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2711.8247 - mse: 2711.8245 - mae: 28.7092 - val_loss: 2068.1600 - val_mse: 2068.1599 - val_mae: 26.5538\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2660.0971 - mse: 2660.0964 - mae: 28.2243 - val_loss: 2066.8301 - val_mse: 2066.8303 - val_mae: 26.7110\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2677.3365 - mse: 2677.3367 - mae: 27.9177 - val_loss: 2060.0046 - val_mse: 2060.0049 - val_mae: 27.0537\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2648.2744 - mse: 2648.2754 - mae: 28.2436 - val_loss: 2058.1415 - val_mse: 2058.1418 - val_mae: 27.6502\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2647.2588 - mse: 2647.2588 - mae: 28.2047 - val_loss: 2057.2459 - val_mse: 2057.2458 - val_mae: 26.9058\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2662.1388 - mse: 2662.1387 - mae: 28.2346 - val_loss: 2066.7861 - val_mse: 2066.7859 - val_mae: 26.3653\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2646.2485 - mse: 2646.2485 - mae: 28.0975 - val_loss: 2060.5824 - val_mse: 2060.5823 - val_mae: 26.8275\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2720.0639 - mse: 2720.0640 - mae: 28.3361 - val_loss: 2069.4826 - val_mse: 2069.4827 - val_mae: 26.5728\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2700.3088 - mse: 2700.3091 - mae: 28.0756 - val_loss: 2069.0167 - val_mse: 2069.0168 - val_mae: 26.6931\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 274us/step - loss: 2647.6353 - mse: 2647.6360 - mae: 28.1263 - val_loss: 2070.0756 - val_mse: 2070.0759 - val_mae: 27.2138\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2545.0127 - mse: 2545.0129 - mae: 27.7377 - val_loss: 2062.0373 - val_mse: 2062.0371 - val_mae: 27.2997\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2671.0172 - mse: 2671.0176 - mae: 28.2522 - val_loss: 2060.5783 - val_mse: 2060.5784 - val_mae: 26.7840\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2610.8358 - mse: 2610.8357 - mae: 27.9791 - val_loss: 2067.0760 - val_mse: 2067.0762 - val_mae: 26.1668\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2644.6034 - mse: 2644.6035 - mae: 27.5769 - val_loss: 2052.2116 - val_mse: 2052.2119 - val_mae: 27.2078\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 238us/step - loss: 2694.1031 - mse: 2694.1030 - mae: 28.4230 - val_loss: 2058.7147 - val_mse: 2058.7148 - val_mae: 26.6155\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2656.0217 - mse: 2656.0217 - mae: 28.1824 - val_loss: 2056.8153 - val_mse: 2056.8154 - val_mae: 26.4617\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2631.6873 - mse: 2631.6880 - mae: 28.1660 - val_loss: 2054.6367 - val_mse: 2054.6370 - val_mae: 27.0491\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2654.6373 - mse: 2654.6367 - mae: 28.3459 - val_loss: 2054.5298 - val_mse: 2054.5298 - val_mae: 27.1359\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 252us/step - loss: 2703.5533 - mse: 2703.5535 - mae: 28.3460 - val_loss: 2064.7101 - val_mse: 2064.7100 - val_mae: 26.5254\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2662.3086 - mse: 2662.3083 - mae: 27.9663 - val_loss: 2066.5042 - val_mse: 2066.5044 - val_mae: 26.3257\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 271us/step - loss: 2634.8171 - mse: 2634.8174 - mae: 27.7856 - val_loss: 2058.1132 - val_mse: 2058.1130 - val_mae: 26.5484\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 338us/step - loss: 2688.2722 - mse: 2688.2727 - mae: 28.4027 - val_loss: 2063.1905 - val_mse: 2063.1904 - val_mae: 26.7123\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2663.2597 - mse: 2663.2603 - mae: 28.1770 - val_loss: 2064.3618 - val_mse: 2064.3621 - val_mae: 26.5274\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2650.7324 - mse: 2650.7322 - mae: 27.8896 - val_loss: 2062.7375 - val_mse: 2062.7378 - val_mae: 26.6521\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2633.0294 - mse: 2633.0288 - mae: 28.0640 - val_loss: 2063.4208 - val_mse: 2063.4209 - val_mae: 26.5760\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2657.7641 - mse: 2657.7642 - mae: 27.9835 - val_loss: 2063.1949 - val_mse: 2063.1948 - val_mae: 26.6887\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2652.0413 - mse: 2652.0420 - mae: 28.1755 - val_loss: 2063.0525 - val_mse: 2063.0525 - val_mae: 26.5650\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2637.8022 - mse: 2637.8030 - mae: 28.1517 - val_loss: 2061.0338 - val_mse: 2061.0334 - val_mae: 27.2330\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2622.4423 - mse: 2622.4417 - mae: 28.1531 - val_loss: 2056.8369 - val_mse: 2056.8372 - val_mae: 26.7084\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2607.9435 - mse: 2607.9434 - mae: 28.1957 - val_loss: 2054.0325 - val_mse: 2054.0327 - val_mae: 26.9465\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 262us/step - loss: 2654.5689 - mse: 2654.5691 - mae: 28.3042 - val_loss: 2054.6886 - val_mse: 2054.6890 - val_mae: 26.6863\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2623.3484 - mse: 2623.3481 - mae: 27.8651 - val_loss: 2060.5019 - val_mse: 2060.5020 - val_mae: 26.5159\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2664.0717 - mse: 2664.0723 - mae: 28.0382 - val_loss: 2049.5432 - val_mse: 2049.5432 - val_mae: 26.8149\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2679.9996 - mse: 2679.9998 - mae: 28.2282 - val_loss: 2050.4630 - val_mse: 2050.4631 - val_mae: 27.1140\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2624.0516 - mse: 2624.0510 - mae: 27.7618 - val_loss: 2047.8925 - val_mse: 2047.8923 - val_mae: 27.6343\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 345us/step - loss: 2634.0232 - mse: 2634.0239 - mae: 27.9531 - val_loss: 2056.7052 - val_mse: 2056.7053 - val_mae: 26.6719\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2639.2618 - mse: 2639.2615 - mae: 27.7999 - val_loss: 2050.2305 - val_mse: 2050.2305 - val_mae: 26.7756\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2666.2254 - mse: 2666.2246 - mae: 27.8820 - val_loss: 2051.3435 - val_mse: 2051.3433 - val_mae: 26.7125\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13285.9446 - mse: 13285.9434 - mae: 109.7138 - val_loss: 34507.5342 - val_mse: 34507.5352 - val_mae: 132.3184\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 12987.4396 - mse: 12987.4395 - mae: 108.3421 - val_loss: 33836.0157 - val_mse: 33836.0156 - val_mae: 129.7888\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 343us/step - loss: 12032.6614 - mse: 12032.6611 - mae: 103.7844 - val_loss: 31866.5609 - val_mse: 31866.5625 - val_mae: 122.0719\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 372us/step - loss: 9671.8066 - mse: 9671.8066 - mae: 91.2887 - val_loss: 27121.7791 - val_mse: 27121.7793 - val_mae: 101.0347\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 354us/step - loss: 5364.2902 - mse: 5364.2905 - mae: 61.1441 - val_loss: 19614.8097 - val_mse: 19614.8086 - val_mae: 52.4366\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2837.0824 - mse: 2837.0823 - mae: 38.8369 - val_loss: 17537.6031 - val_mse: 17537.6035 - val_mae: 34.9334\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 336us/step - loss: 2799.7387 - mse: 2799.7393 - mae: 39.1603 - val_loss: 17988.1988 - val_mse: 17988.1973 - val_mae: 37.5653\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 2701.7190 - mse: 2701.7190 - mae: 37.7174 - val_loss: 18037.4827 - val_mse: 18037.4844 - val_mae: 37.7907\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 376us/step - loss: 2725.6116 - mse: 2725.6116 - mae: 36.9138 - val_loss: 17958.2089 - val_mse: 17958.2070 - val_mae: 36.9745\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 262us/step - loss: 2611.8254 - mse: 2611.8252 - mae: 36.1154 - val_loss: 17878.8468 - val_mse: 17878.8477 - val_mae: 36.1994\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 238us/step - loss: 2994.6199 - mse: 2994.6196 - mae: 38.8985 - val_loss: 18146.7350 - val_mse: 18146.7344 - val_mae: 38.1832\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 2791.6634 - mse: 2791.6636 - mae: 37.7742 - val_loss: 18215.7726 - val_mse: 18215.7734 - val_mae: 38.6600\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2775.9504 - mse: 2775.9502 - mae: 38.0709 - val_loss: 17868.1168 - val_mse: 17868.1152 - val_mae: 35.7914\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 283us/step - loss: 2871.7902 - mse: 2871.7903 - mae: 37.8761 - val_loss: 17967.8203 - val_mse: 17967.8203 - val_mae: 36.3942\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 295us/step - loss: 2913.3103 - mse: 2913.3105 - mae: 37.7993 - val_loss: 18133.8446 - val_mse: 18133.8457 - val_mae: 37.6144\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 2539.3407 - mse: 2539.3411 - mae: 36.0914 - val_loss: 17862.2070 - val_mse: 17862.2070 - val_mae: 35.4995\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 2686.4447 - mse: 2686.4451 - mae: 36.5078 - val_loss: 17923.0755 - val_mse: 17923.0742 - val_mae: 35.8604\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 284us/step - loss: 2462.3196 - mse: 2462.3198 - mae: 35.8970 - val_loss: 18045.7102 - val_mse: 18045.7070 - val_mae: 36.7558\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2408.6842 - mse: 2408.6843 - mae: 34.7939 - val_loss: 17714.6652 - val_mse: 17714.6660 - val_mae: 34.8025\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 337us/step - loss: 2540.8362 - mse: 2540.8362 - mae: 35.9549 - val_loss: 17920.7643 - val_mse: 17920.7637 - val_mae: 35.7780\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 360us/step - loss: 2441.7286 - mse: 2441.7290 - mae: 33.9961 - val_loss: 17886.7354 - val_mse: 17886.7363 - val_mae: 35.5817\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2433.3300 - mse: 2433.3301 - mae: 34.9545 - val_loss: 17558.5215 - val_mse: 17558.5195 - val_mae: 34.5359\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2473.3472 - mse: 2473.3474 - mae: 34.2668 - val_loss: 17982.8297 - val_mse: 17982.8301 - val_mae: 36.1334\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 423us/step - loss: 2507.8190 - mse: 2507.8188 - mae: 36.0677 - val_loss: 17784.1143 - val_mse: 17784.1152 - val_mae: 35.0536\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 331us/step - loss: 2484.0073 - mse: 2484.0073 - mae: 35.4499 - val_loss: 17869.2292 - val_mse: 17869.2305 - val_mae: 35.4655\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 2334.1842 - mse: 2334.1843 - mae: 34.0721 - val_loss: 17699.0582 - val_mse: 17699.0586 - val_mae: 34.8367\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 362us/step - loss: 2176.8238 - mse: 2176.8237 - mae: 33.0463 - val_loss: 17688.2884 - val_mse: 17688.2871 - val_mae: 34.8258\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 247us/step - loss: 2629.3901 - mse: 2629.3899 - mae: 36.8756 - val_loss: 18099.2668 - val_mse: 18099.2656 - val_mae: 36.9744\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 345us/step - loss: 2251.4296 - mse: 2251.4294 - mae: 33.7392 - val_loss: 17756.1096 - val_mse: 17756.1074 - val_mae: 35.0742\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 357us/step - loss: 2379.9710 - mse: 2379.9712 - mae: 34.8548 - val_loss: 17836.5843 - val_mse: 17836.5840 - val_mae: 35.4291\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 388us/step - loss: 2504.2469 - mse: 2504.2471 - mae: 35.3517 - val_loss: 17889.5205 - val_mse: 17889.5195 - val_mae: 35.7395\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 2191.8721 - mse: 2191.8723 - mae: 33.0884 - val_loss: 17743.2387 - val_mse: 17743.2383 - val_mae: 35.0755\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2464.3470 - mse: 2464.3472 - mae: 34.8685 - val_loss: 17875.8399 - val_mse: 17875.8379 - val_mae: 35.6648\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 391us/step - loss: 2221.7495 - mse: 2221.7495 - mae: 33.1642 - val_loss: 17520.9631 - val_mse: 17520.9648 - val_mae: 34.8561\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 352us/step - loss: 2279.2180 - mse: 2279.2183 - mae: 33.0521 - val_loss: 17602.3512 - val_mse: 17602.3496 - val_mae: 34.8935\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 343us/step - loss: 2233.7864 - mse: 2233.7866 - mae: 33.2241 - val_loss: 17793.7267 - val_mse: 17793.7285 - val_mae: 35.3231\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2403.8076 - mse: 2403.8079 - mae: 33.9246 - val_loss: 17628.6927 - val_mse: 17628.6934 - val_mae: 34.9744\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 335us/step - loss: 2229.0124 - mse: 2229.0125 - mae: 33.7381 - val_loss: 17773.3042 - val_mse: 17773.3047 - val_mae: 35.2901\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 286us/step - loss: 2382.2620 - mse: 2382.2620 - mae: 34.3376 - val_loss: 17728.3079 - val_mse: 17728.3086 - val_mae: 35.1860\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 275us/step - loss: 2149.8539 - mse: 2149.8540 - mae: 32.7787 - val_loss: 17800.9896 - val_mse: 17800.9902 - val_mae: 35.4669\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 280us/step - loss: 2209.6645 - mse: 2209.6646 - mae: 32.7906 - val_loss: 17728.9023 - val_mse: 17728.9023 - val_mae: 35.3014\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2299.4865 - mse: 2299.4868 - mae: 34.4871 - val_loss: 17865.1075 - val_mse: 17865.1074 - val_mae: 35.8483\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2237.1557 - mse: 2237.1558 - mae: 33.4478 - val_loss: 17788.4626 - val_mse: 17788.4629 - val_mae: 35.5543\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2301.8439 - mse: 2301.8438 - mae: 33.3927 - val_loss: 17682.4086 - val_mse: 17682.4082 - val_mae: 35.3118\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2282.3557 - mse: 2282.3557 - mae: 34.1740 - val_loss: 17868.2667 - val_mse: 17868.2676 - val_mae: 35.9753\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 221us/step - loss: 2189.1414 - mse: 2189.1416 - mae: 32.0356 - val_loss: 17694.4141 - val_mse: 17694.4141 - val_mae: 35.4068\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 1939.2767 - mse: 1939.2767 - mae: 31.6508 - val_loss: 17895.3702 - val_mse: 17895.3691 - val_mae: 36.1962\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2085.0565 - mse: 2085.0562 - mae: 32.8300 - val_loss: 17671.5494 - val_mse: 17671.5508 - val_mae: 35.4424\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2160.4975 - mse: 2160.4976 - mae: 31.3381 - val_loss: 17646.5458 - val_mse: 17646.5449 - val_mae: 35.4405\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2281.5743 - mse: 2281.5742 - mae: 33.4746 - val_loss: 17918.0301 - val_mse: 17918.0312 - val_mae: 36.4293\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2070.0959 - mse: 2070.0959 - mae: 31.1616 - val_loss: 17773.3587 - val_mse: 17773.3574 - val_mae: 35.8347\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 2084.9260 - mse: 2084.9258 - mae: 32.3798 - val_loss: 17707.8793 - val_mse: 17707.8789 - val_mae: 35.6806\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 297us/step - loss: 2181.5126 - mse: 2181.5127 - mae: 33.2995 - val_loss: 17779.4620 - val_mse: 17779.4648 - val_mae: 35.9101\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 398us/step - loss: 2098.6414 - mse: 2098.6414 - mae: 32.1152 - val_loss: 17683.7976 - val_mse: 17683.7988 - val_mae: 35.6743\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 1923.7026 - mse: 1923.7024 - mae: 31.4332 - val_loss: 17731.2176 - val_mse: 17731.2168 - val_mae: 35.8287\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 323us/step - loss: 2088.6150 - mse: 2088.6150 - mae: 31.1930 - val_loss: 17887.1562 - val_mse: 17887.1582 - val_mae: 36.4823\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 322us/step - loss: 1842.1100 - mse: 1842.1099 - mae: 29.3905 - val_loss: 17533.4047 - val_mse: 17533.4062 - val_mae: 35.6779\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 2164.2839 - mse: 2164.2837 - mae: 32.4874 - val_loss: 17884.9126 - val_mse: 17884.9121 - val_mae: 36.5124\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 2044.6047 - mse: 2044.6046 - mae: 31.3420 - val_loss: 17676.2680 - val_mse: 17676.2656 - val_mae: 35.8512\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2073.7946 - mse: 2073.7949 - mae: 30.3470 - val_loss: 17809.2577 - val_mse: 17809.2578 - val_mae: 36.2461\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 243us/step - loss: 2087.9605 - mse: 2087.9604 - mae: 32.6734 - val_loss: 17761.6555 - val_mse: 17761.6562 - val_mae: 36.1347\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2073.6039 - mse: 2073.6040 - mae: 31.4305 - val_loss: 17915.3502 - val_mse: 17915.3516 - val_mae: 36.8312\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 274us/step - loss: 2073.5686 - mse: 2073.5686 - mae: 31.1482 - val_loss: 17500.9858 - val_mse: 17500.9863 - val_mae: 35.9535\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 309us/step - loss: 2123.7986 - mse: 2123.7983 - mae: 32.3439 - val_loss: 17816.2546 - val_mse: 17816.2539 - val_mae: 36.4007\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 331us/step - loss: 2167.4272 - mse: 2167.4270 - mae: 31.9349 - val_loss: 17771.0445 - val_mse: 17771.0449 - val_mae: 36.2975\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2007.1625 - mse: 2007.1624 - mae: 30.4973 - val_loss: 17777.9995 - val_mse: 17778.0000 - val_mae: 36.3658\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 1952.9706 - mse: 1952.9707 - mae: 30.3714 - val_loss: 17794.8365 - val_mse: 17794.8359 - val_mae: 36.4486\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 378us/step - loss: 1903.8719 - mse: 1903.8719 - mae: 30.7477 - val_loss: 17663.0970 - val_mse: 17663.0977 - val_mae: 36.2066\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 2016.9959 - mse: 2016.9957 - mae: 30.4303 - val_loss: 17556.0251 - val_mse: 17556.0234 - val_mae: 36.1296\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 1988.3530 - mse: 1988.3533 - mae: 31.1503 - val_loss: 17940.8616 - val_mse: 17940.8613 - val_mae: 37.1972\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 1917.1956 - mse: 1917.1958 - mae: 30.5045 - val_loss: 17723.2816 - val_mse: 17723.2812 - val_mae: 36.4135\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 1879.5032 - mse: 1879.5033 - mae: 29.6080 - val_loss: 17850.5358 - val_mse: 17850.5332 - val_mae: 36.8183\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2081.0917 - mse: 2081.0918 - mae: 32.0689 - val_loss: 17665.8519 - val_mse: 17665.8516 - val_mae: 36.4018\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 1792.5383 - mse: 1792.5382 - mae: 29.8808 - val_loss: 17649.8657 - val_mse: 17649.8633 - val_mae: 36.4050\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 1893.8680 - mse: 1893.8680 - mae: 30.1938 - val_loss: 17698.5873 - val_mse: 17698.5879 - val_mae: 36.5265\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 1910.0112 - mse: 1910.0111 - mae: 30.2853 - val_loss: 17847.0494 - val_mse: 17847.0508 - val_mae: 36.9713\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 2001.8742 - mse: 2001.8740 - mae: 30.7716 - val_loss: 17745.4941 - val_mse: 17745.4961 - val_mae: 36.6550\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 352us/step - loss: 1828.1050 - mse: 1828.1049 - mae: 29.6694 - val_loss: 17748.4220 - val_mse: 17748.4219 - val_mae: 36.7262\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 1915.4246 - mse: 1915.4246 - mae: 30.1645 - val_loss: 17606.1433 - val_mse: 17606.1445 - val_mae: 36.5609\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 317us/step - loss: 1930.3681 - mse: 1930.3682 - mae: 30.2883 - val_loss: 17773.9010 - val_mse: 17773.9004 - val_mae: 36.8696\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4150.4918 - mse: 4150.4912 - mae: 32.9417 - val_loss: 2027.1614 - val_mse: 2027.1614 - val_mae: 30.3384\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4406.3379 - mse: 4406.3379 - mae: 36.4375 - val_loss: 2160.1268 - val_mse: 2160.1267 - val_mae: 30.7468\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 322us/step - loss: 4206.1983 - mse: 4206.1978 - mae: 35.4871 - val_loss: 2136.9900 - val_mse: 2136.9897 - val_mae: 30.6133\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4218.5816 - mse: 4218.5811 - mae: 34.8812 - val_loss: 2154.8494 - val_mse: 2154.8494 - val_mae: 30.6505\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4487.3105 - mse: 4487.3101 - mae: 36.2786 - val_loss: 2287.3160 - val_mse: 2287.3159 - val_mae: 31.1975\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4190.7289 - mse: 4190.7285 - mae: 34.5312 - val_loss: 2264.1307 - val_mse: 2264.1309 - val_mae: 31.1121\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4152.9870 - mse: 4152.9873 - mae: 33.8392 - val_loss: 2179.8504 - val_mse: 2179.8503 - val_mae: 30.7821\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4294.7517 - mse: 4294.7524 - mae: 35.5394 - val_loss: 2199.8092 - val_mse: 2199.8093 - val_mae: 30.8660\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4231.1891 - mse: 4231.1885 - mae: 34.6972 - val_loss: 2231.2692 - val_mse: 2231.2695 - val_mae: 31.0143\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 377us/step - loss: 4145.6498 - mse: 4145.6499 - mae: 34.5592 - val_loss: 2159.5523 - val_mse: 2159.5522 - val_mae: 30.7108\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 355us/step - loss: 4148.2227 - mse: 4148.2227 - mae: 34.3457 - val_loss: 2265.1077 - val_mse: 2265.1079 - val_mae: 31.1078\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 3984.0460 - mse: 3984.0457 - mae: 34.6842 - val_loss: 2216.7918 - val_mse: 2216.7920 - val_mae: 30.9030\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 4226.1881 - mse: 4226.1880 - mae: 35.0089 - val_loss: 2254.4203 - val_mse: 2254.4202 - val_mae: 31.0324\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 4329.8620 - mse: 4329.8623 - mae: 35.3441 - val_loss: 2298.8037 - val_mse: 2298.8040 - val_mae: 31.2143\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 331us/step - loss: 4040.5121 - mse: 4040.5122 - mae: 33.4760 - val_loss: 2251.5257 - val_mse: 2251.5254 - val_mae: 31.0286\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - ETA: 0s - loss: 4409.2622 - mse: 4409.2627 - mae: 35.50 - 0s 312us/step - loss: 4348.9483 - mse: 4348.9487 - mae: 35.2979 - val_loss: 2304.4806 - val_mse: 2304.4805 - val_mae: 31.2355\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4198.3907 - mse: 4198.3901 - mae: 34.4306 - val_loss: 2243.8768 - val_mse: 2243.8767 - val_mae: 30.9774\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 261us/step - loss: 4252.4419 - mse: 4252.4414 - mae: 34.5429 - val_loss: 2185.5756 - val_mse: 2185.5757 - val_mae: 30.7632\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 381us/step - loss: 4025.4702 - mse: 4025.4705 - mae: 34.0509 - val_loss: 2231.4930 - val_mse: 2231.4929 - val_mae: 30.9300\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 277us/step - loss: 4106.4273 - mse: 4106.4272 - mae: 33.8553 - val_loss: 2184.6653 - val_mse: 2184.6653 - val_mae: 30.7430\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 372us/step - loss: 4198.0460 - mse: 4198.0464 - mae: 34.1822 - val_loss: 2176.6315 - val_mse: 2176.6318 - val_mae: 30.7118\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 361us/step - loss: 4166.2680 - mse: 4166.2686 - mae: 34.3676 - val_loss: 2274.6621 - val_mse: 2274.6621 - val_mae: 31.0758\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4256.8200 - mse: 4256.8198 - mae: 34.3828 - val_loss: 2281.8519 - val_mse: 2281.8521 - val_mae: 31.1079\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 283us/step - loss: 4176.4513 - mse: 4176.4512 - mae: 33.9562 - val_loss: 2257.4754 - val_mse: 2257.4753 - val_mae: 31.0067\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4105.4859 - mse: 4105.4858 - mae: 34.6351 - val_loss: 2122.8717 - val_mse: 2122.8718 - val_mae: 30.4833\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 4197.3275 - mse: 4197.3276 - mae: 34.7768 - val_loss: 2254.1952 - val_mse: 2254.1953 - val_mae: 30.9780\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4075.9073 - mse: 4075.9072 - mae: 33.5793 - val_loss: 2238.5792 - val_mse: 2238.5796 - val_mae: 30.8926\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 3999.8449 - mse: 3999.8450 - mae: 34.5625 - val_loss: 2267.3956 - val_mse: 2267.3958 - val_mae: 31.0332\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 290us/step - loss: 3996.1848 - mse: 3996.1843 - mae: 34.1013 - val_loss: 2209.8087 - val_mse: 2209.8088 - val_mae: 30.8435\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 308us/step - loss: 4195.6041 - mse: 4195.6040 - mae: 35.0432 - val_loss: 2319.0883 - val_mse: 2319.0881 - val_mae: 31.2720\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4105.5875 - mse: 4105.5879 - mae: 33.5293 - val_loss: 2232.8705 - val_mse: 2232.8706 - val_mae: 30.9636\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4222.7748 - mse: 4222.7749 - mae: 33.6228 - val_loss: 2223.7580 - val_mse: 2223.7583 - val_mae: 30.9370\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 3963.4521 - mse: 3963.4526 - mae: 33.5809 - val_loss: 2237.2518 - val_mse: 2237.2520 - val_mae: 30.9897\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 368us/step - loss: 4137.8353 - mse: 4137.8350 - mae: 33.6743 - val_loss: 2302.1875 - val_mse: 2302.1873 - val_mae: 31.2093\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 3966.3889 - mse: 3966.3889 - mae: 32.8344 - val_loss: 2222.9007 - val_mse: 2222.9004 - val_mae: 30.9092\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 3848.0828 - mse: 3848.0833 - mae: 33.1334 - val_loss: 2134.7197 - val_mse: 2134.7200 - val_mae: 30.5352\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4197.3277 - mse: 4197.3281 - mae: 34.0596 - val_loss: 2242.1397 - val_mse: 2242.1394 - val_mae: 30.9297\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4274.2691 - mse: 4274.2695 - mae: 34.3089 - val_loss: 2324.5741 - val_mse: 2324.5740 - val_mae: 31.2719\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 3960.0064 - mse: 3960.0063 - mae: 33.6642 - val_loss: 2253.2076 - val_mse: 2253.2078 - val_mae: 31.0066\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4107.4309 - mse: 4107.4312 - mae: 33.9124 - val_loss: 2277.3175 - val_mse: 2277.3174 - val_mae: 31.0822\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 3990.3109 - mse: 3990.3118 - mae: 33.0202 - val_loss: 2244.5162 - val_mse: 2244.5161 - val_mae: 30.9610\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 268us/step - loss: 4049.6650 - mse: 4049.6648 - mae: 33.9423 - val_loss: 2288.3092 - val_mse: 2288.3091 - val_mae: 31.1145\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4152.0835 - mse: 4152.0840 - mae: 33.2954 - val_loss: 2257.2325 - val_mse: 2257.2322 - val_mae: 30.9963\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4142.9522 - mse: 4142.9526 - mae: 33.6310 - val_loss: 2206.4526 - val_mse: 2206.4526 - val_mae: 30.7954\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4009.7779 - mse: 4009.7783 - mae: 33.2409 - val_loss: 2216.7012 - val_mse: 2216.7012 - val_mae: 30.8412\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4056.0793 - mse: 4056.0793 - mae: 33.3698 - val_loss: 2231.1356 - val_mse: 2231.1355 - val_mae: 30.8952\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4120.5171 - mse: 4120.5171 - mae: 33.9952 - val_loss: 2304.1694 - val_mse: 2304.1694 - val_mae: 31.1880\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 328us/step - loss: 4132.5907 - mse: 4132.5908 - mae: 33.9567 - val_loss: 2243.3557 - val_mse: 2243.3557 - val_mae: 30.9389\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 234us/step - loss: 3959.7587 - mse: 3959.7585 - mae: 33.1118 - val_loss: 2189.4015 - val_mse: 2189.4019 - val_mae: 30.7356\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 3994.8494 - mse: 3994.8494 - mae: 33.3331 - val_loss: 2259.0627 - val_mse: 2259.0627 - val_mae: 31.0123\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 3925.1584 - mse: 3925.1589 - mae: 33.1039 - val_loss: 2270.4120 - val_mse: 2270.4121 - val_mae: 31.0432\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4070.0209 - mse: 4070.0208 - mae: 33.5535 - val_loss: 2184.1502 - val_mse: 2184.1499 - val_mae: 30.7154\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 356us/step - loss: 4075.8179 - mse: 4075.8174 - mae: 33.5352 - val_loss: 2249.7572 - val_mse: 2249.7573 - val_mae: 30.9851\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 273us/step - loss: 4217.7550 - mse: 4217.7554 - mae: 33.8364 - val_loss: 2309.4739 - val_mse: 2309.4736 - val_mae: 31.2051\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 319us/step - loss: 4241.9408 - mse: 4241.9404 - mae: 34.2812 - val_loss: 2226.5645 - val_mse: 2226.5647 - val_mae: 30.9095\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 3852.2641 - mse: 3852.2634 - mae: 32.2619 - val_loss: 2207.7913 - val_mse: 2207.7910 - val_mae: 30.8376\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 262us/step - loss: 3942.9992 - mse: 3942.9990 - mae: 33.1687 - val_loss: 2203.3391 - val_mse: 2203.3394 - val_mae: 30.8191\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 237us/step - loss: 3811.1664 - mse: 3811.1663 - mae: 32.8042 - val_loss: 2210.2711 - val_mse: 2210.2712 - val_mae: 30.8374\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 244us/step - loss: 4058.1827 - mse: 4058.1829 - mae: 33.7964 - val_loss: 2286.4096 - val_mse: 2286.4097 - val_mae: 31.1260\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4096.0037 - mse: 4096.0034 - mae: 33.5921 - val_loss: 2254.0100 - val_mse: 2254.0103 - val_mae: 30.9886\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 377us/step - loss: 4052.4278 - mse: 4052.4275 - mae: 33.2972 - val_loss: 2283.3307 - val_mse: 2283.3306 - val_mae: 31.0976\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 3976.4762 - mse: 3976.4758 - mae: 32.7866 - val_loss: 2227.9794 - val_mse: 2227.9795 - val_mae: 30.8905\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4078.7313 - mse: 4078.7317 - mae: 33.7382 - val_loss: 2259.2089 - val_mse: 2259.2090 - val_mae: 30.9747\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 389us/step - loss: 3839.8103 - mse: 3839.8105 - mae: 32.0488 - val_loss: 2207.8068 - val_mse: 2207.8066 - val_mae: 30.7915\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 257us/step - loss: 4188.4089 - mse: 4188.4082 - mae: 33.9635 - val_loss: 2232.1509 - val_mse: 2232.1509 - val_mae: 30.9082\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4005.0450 - mse: 4005.0449 - mae: 32.6422 - val_loss: 2251.6329 - val_mse: 2251.6331 - val_mae: 30.9894\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 3964.8765 - mse: 3964.8762 - mae: 33.1049 - val_loss: 2199.2837 - val_mse: 2199.2837 - val_mae: 30.7742\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 355us/step - loss: 3972.3533 - mse: 3972.3528 - mae: 32.7466 - val_loss: 2213.5235 - val_mse: 2213.5232 - val_mae: 30.8112\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 378us/step - loss: 3977.7817 - mse: 3977.7820 - mae: 32.1079 - val_loss: 2220.8399 - val_mse: 2220.8401 - val_mae: 30.8269\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 3771.9306 - mse: 3771.9299 - mae: 32.6356 - val_loss: 2202.4655 - val_mse: 2202.4656 - val_mae: 30.7746\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 350us/step - loss: 4072.1663 - mse: 4072.1655 - mae: 33.3478 - val_loss: 2249.0640 - val_mse: 2249.0640 - val_mae: 30.9564\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 4049.8088 - mse: 4049.8088 - mae: 32.9546 - val_loss: 2262.0951 - val_mse: 2262.0950 - val_mae: 31.0204\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 3882.1165 - mse: 3882.1177 - mae: 32.2494 - val_loss: 2238.7892 - val_mse: 2238.7893 - val_mae: 30.9293\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 370us/step - loss: 4093.6569 - mse: 4093.6570 - mae: 33.1051 - val_loss: 2273.4333 - val_mse: 2273.4333 - val_mae: 31.0900\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 247us/step - loss: 3872.6500 - mse: 3872.6499 - mae: 32.8060 - val_loss: 2241.4362 - val_mse: 2241.4360 - val_mae: 30.9748\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 261us/step - loss: 3883.8449 - mse: 3883.8447 - mae: 32.3481 - val_loss: 2218.5679 - val_mse: 2218.5681 - val_mae: 30.8979\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4053.4473 - mse: 4053.4473 - mae: 33.1537 - val_loss: 2273.5152 - val_mse: 2273.5149 - val_mae: 31.1010\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 3939.1054 - mse: 3939.1047 - mae: 31.9940 - val_loss: 2256.3950 - val_mse: 2256.3950 - val_mae: 31.0308\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 3989.1984 - mse: 3989.1982 - mae: 33.2433 - val_loss: 2270.2361 - val_mse: 2270.2361 - val_mae: 31.0955\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 258us/step - loss: 4098.0254 - mse: 4098.0259 - mae: 33.5640 - val_loss: 2229.9569 - val_mse: 2229.9565 - val_mae: 30.9450\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3446.0321 - mse: 3446.0317 - mae: 32.8989 - val_loss: 1461.0370 - val_mse: 1461.0367 - val_mae: 25.7703\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 345us/step - loss: 3418.2007 - mse: 3418.2009 - mae: 32.9793 - val_loss: 1462.1997 - val_mse: 1462.2001 - val_mae: 25.5068\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3328.2693 - mse: 3328.2695 - mae: 32.3698 - val_loss: 1459.4866 - val_mse: 1459.4866 - val_mae: 25.5544\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3400.3277 - mse: 3400.3279 - mae: 32.9238 - val_loss: 1455.9191 - val_mse: 1455.9191 - val_mae: 25.5283\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3420.5799 - mse: 3420.5789 - mae: 32.9575 - val_loss: 1453.9038 - val_mse: 1453.9037 - val_mae: 25.8307\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3310.0146 - mse: 3310.0154 - mae: 32.1869 - val_loss: 1453.9901 - val_mse: 1453.9901 - val_mae: 25.4425\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3322.8182 - mse: 3322.8181 - mae: 32.4687 - val_loss: 1453.5565 - val_mse: 1453.5565 - val_mae: 25.6651\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 282us/step - loss: 3340.1880 - mse: 3340.1882 - mae: 32.6827 - val_loss: 1454.4809 - val_mse: 1454.4810 - val_mae: 25.5750\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3403.2906 - mse: 3403.2900 - mae: 32.8335 - val_loss: 1452.6108 - val_mse: 1452.6108 - val_mae: 25.5207\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 385us/step - loss: 3369.5994 - mse: 3369.5994 - mae: 32.0137 - val_loss: 1454.2218 - val_mse: 1454.2218 - val_mae: 25.2541\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 253us/step - loss: 3400.4843 - mse: 3400.4839 - mae: 32.7250 - val_loss: 1451.4499 - val_mse: 1451.4501 - val_mae: 25.8507\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3385.3213 - mse: 3385.3213 - mae: 32.6824 - val_loss: 1453.5017 - val_mse: 1453.5015 - val_mae: 25.5245\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3425.8461 - mse: 3425.8455 - mae: 32.7826 - val_loss: 1453.9787 - val_mse: 1453.9786 - val_mae: 25.3950\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 328us/step - loss: 3346.0361 - mse: 3346.0359 - mae: 32.1613 - val_loss: 1453.4383 - val_mse: 1453.4384 - val_mae: 26.0147\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3295.7752 - mse: 3295.7749 - mae: 32.4372 - val_loss: 1454.7271 - val_mse: 1454.7271 - val_mae: 25.5157\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3198.6093 - mse: 3198.6091 - mae: 31.7113 - val_loss: 1454.3138 - val_mse: 1454.3138 - val_mae: 25.9224\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 261us/step - loss: 3360.4809 - mse: 3360.4802 - mae: 31.4085 - val_loss: 1454.0178 - val_mse: 1454.0178 - val_mae: 26.0399\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 367us/step - loss: 3351.9128 - mse: 3351.9126 - mae: 32.8573 - val_loss: 1453.0230 - val_mse: 1453.0229 - val_mae: 25.6192\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 277us/step - loss: 3153.6964 - mse: 3153.6963 - mae: 31.7622 - val_loss: 1455.4433 - val_mse: 1455.4434 - val_mae: 26.1840\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3412.0867 - mse: 3412.0867 - mae: 32.9536 - val_loss: 1453.6146 - val_mse: 1453.6145 - val_mae: 25.4492\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 339us/step - loss: 3362.5883 - mse: 3362.5879 - mae: 31.9828 - val_loss: 1452.7185 - val_mse: 1452.7184 - val_mae: 25.6591\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 398us/step - loss: 3387.3029 - mse: 3387.3032 - mae: 33.0847 - val_loss: 1456.2979 - val_mse: 1456.2981 - val_mae: 25.3815\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3347.4391 - mse: 3347.4395 - mae: 32.5424 - val_loss: 1456.2404 - val_mse: 1456.2404 - val_mae: 25.3426\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 279us/step - loss: 3295.0154 - mse: 3295.0161 - mae: 32.0063 - val_loss: 1454.8355 - val_mse: 1454.8356 - val_mae: 25.6804\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3392.0807 - mse: 3392.0798 - mae: 32.9049 - val_loss: 1456.5887 - val_mse: 1456.5887 - val_mae: 25.6503\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3283.8464 - mse: 3283.8469 - mae: 31.8740 - val_loss: 1457.2188 - val_mse: 1457.2189 - val_mae: 25.7698\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 282us/step - loss: 3213.6502 - mse: 3213.6504 - mae: 31.3783 - val_loss: 1458.5866 - val_mse: 1458.5865 - val_mae: 26.1745\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3386.4075 - mse: 3386.4077 - mae: 32.4084 - val_loss: 1456.6409 - val_mse: 1456.6409 - val_mae: 25.7209\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3212.6054 - mse: 3212.6064 - mae: 31.5727 - val_loss: 1458.0781 - val_mse: 1458.0780 - val_mae: 25.7319\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 270us/step - loss: 3206.5797 - mse: 3206.5796 - mae: 32.3866 - val_loss: 1458.5521 - val_mse: 1458.5520 - val_mae: 25.5891\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 265us/step - loss: 3290.6269 - mse: 3290.6265 - mae: 32.2224 - val_loss: 1459.9266 - val_mse: 1459.9266 - val_mae: 25.5311\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 278us/step - loss: 3281.1293 - mse: 3281.1292 - mae: 32.1011 - val_loss: 1458.4773 - val_mse: 1458.4774 - val_mae: 25.7506\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 338us/step - loss: 3283.3016 - mse: 3283.3020 - mae: 31.6928 - val_loss: 1458.6077 - val_mse: 1458.6078 - val_mae: 25.5366\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3239.6416 - mse: 3239.6421 - mae: 31.8937 - val_loss: 1458.8435 - val_mse: 1458.8434 - val_mae: 25.6221\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3299.4421 - mse: 3299.4419 - mae: 32.7767 - val_loss: 1458.3550 - val_mse: 1458.3550 - val_mae: 25.6802\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3347.5136 - mse: 3347.5137 - mae: 32.2004 - val_loss: 1457.9725 - val_mse: 1457.9724 - val_mae: 26.1369\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 268us/step - loss: 3245.9620 - mse: 3245.9636 - mae: 31.5336 - val_loss: 1456.6349 - val_mse: 1456.6349 - val_mae: 25.8609\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 322us/step - loss: 3216.6425 - mse: 3216.6426 - mae: 31.7593 - val_loss: 1458.3238 - val_mse: 1458.3237 - val_mae: 25.4138\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 364us/step - loss: 3264.8553 - mse: 3264.8560 - mae: 31.6380 - val_loss: 1458.4694 - val_mse: 1458.4692 - val_mae: 25.5223\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 252us/step - loss: 3299.0579 - mse: 3299.0569 - mae: 31.7095 - val_loss: 1459.4888 - val_mse: 1459.4888 - val_mae: 25.5352\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 232us/step - loss: 3233.5651 - mse: 3233.5645 - mae: 31.6928 - val_loss: 1459.3936 - val_mse: 1459.3936 - val_mae: 25.7977\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3274.9528 - mse: 3274.9521 - mae: 31.0007 - val_loss: 1461.7400 - val_mse: 1461.7400 - val_mae: 25.7484\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3205.0439 - mse: 3205.0437 - mae: 31.3475 - val_loss: 1461.5978 - val_mse: 1461.5975 - val_mae: 25.7763\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 327us/step - loss: 3168.1837 - mse: 3168.1836 - mae: 30.9191 - val_loss: 1461.0562 - val_mse: 1461.0563 - val_mae: 25.9329\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3184.8160 - mse: 3184.8154 - mae: 31.0087 - val_loss: 1460.1934 - val_mse: 1460.1935 - val_mae: 25.8660\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3058.2793 - mse: 3058.2793 - mae: 30.8277 - val_loss: 1461.8638 - val_mse: 1461.8638 - val_mae: 26.0094\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 352us/step - loss: 3266.2116 - mse: 3266.2109 - mae: 31.5335 - val_loss: 1464.0593 - val_mse: 1464.0593 - val_mae: 25.4976\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3370.3782 - mse: 3370.3782 - mae: 31.6506 - val_loss: 1465.1758 - val_mse: 1465.1760 - val_mae: 26.2171\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3340.1246 - mse: 3340.1250 - mae: 32.1095 - val_loss: 1462.2595 - val_mse: 1462.2595 - val_mae: 25.7306\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3362.2482 - mse: 3362.2485 - mae: 31.7113 - val_loss: 1462.2274 - val_mse: 1462.2274 - val_mae: 25.6716\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 339us/step - loss: 3286.7223 - mse: 3286.7227 - mae: 31.5222 - val_loss: 1462.4670 - val_mse: 1462.4669 - val_mae: 26.0190\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 271us/step - loss: 3238.5438 - mse: 3238.5435 - mae: 31.0151 - val_loss: 1462.2553 - val_mse: 1462.2551 - val_mae: 26.0870\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3269.3319 - mse: 3269.3320 - mae: 31.5556 - val_loss: 1461.9591 - val_mse: 1461.9591 - val_mae: 25.8066\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 0s 245us/step - loss: 3357.5175 - mse: 3357.5173 - mae: 31.7355 - val_loss: 1462.6355 - val_mse: 1462.6355 - val_mae: 25.7270\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 256us/step - loss: 3292.6064 - mse: 3292.6067 - mae: 31.7553 - val_loss: 1463.8743 - val_mse: 1463.8744 - val_mae: 26.1820\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 285us/step - loss: 3182.2957 - mse: 3182.2959 - mae: 31.6506 - val_loss: 1461.9323 - val_mse: 1461.9321 - val_mae: 25.7987\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 362us/step - loss: 3285.5139 - mse: 3285.5134 - mae: 31.4089 - val_loss: 1461.8041 - val_mse: 1461.8038 - val_mae: 25.6525\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3332.1102 - mse: 3332.1101 - mae: 31.9531 - val_loss: 1462.0911 - val_mse: 1462.0911 - val_mae: 26.2384\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3325.2668 - mse: 3325.2661 - mae: 31.3112 - val_loss: 1461.1462 - val_mse: 1461.1460 - val_mae: 25.6509\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3194.4084 - mse: 3194.4084 - mae: 31.1533 - val_loss: 1458.7156 - val_mse: 1458.7156 - val_mae: 25.7999\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 368us/step - loss: 3151.6228 - mse: 3151.6223 - mae: 31.1782 - val_loss: 1457.5089 - val_mse: 1457.5090 - val_mae: 25.7063\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3110.5778 - mse: 3110.5786 - mae: 30.3295 - val_loss: 1457.5885 - val_mse: 1457.5884 - val_mae: 25.7406\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3265.0495 - mse: 3265.0496 - mae: 31.4629 - val_loss: 1457.7965 - val_mse: 1457.7965 - val_mae: 26.0055\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 321us/step - loss: 3184.9238 - mse: 3184.9231 - mae: 31.2772 - val_loss: 1460.1683 - val_mse: 1460.1683 - val_mae: 26.1899\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3273.1140 - mse: 3273.1145 - mae: 31.8448 - val_loss: 1458.5376 - val_mse: 1458.5376 - val_mae: 26.0359\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 384us/step - loss: 3206.1032 - mse: 3206.1038 - mae: 31.2238 - val_loss: 1458.1189 - val_mse: 1458.1188 - val_mae: 25.7246\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 277us/step - loss: 3287.7504 - mse: 3287.7505 - mae: 31.9798 - val_loss: 1458.6552 - val_mse: 1458.6553 - val_mae: 26.1337\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3133.9571 - mse: 3133.9573 - mae: 30.4465 - val_loss: 1459.9150 - val_mse: 1459.9152 - val_mae: 26.1593\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3314.9327 - mse: 3314.9324 - mae: 31.6009 - val_loss: 1460.1812 - val_mse: 1460.1812 - val_mae: 26.0710\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3225.2413 - mse: 3225.2407 - mae: 31.1796 - val_loss: 1459.3513 - val_mse: 1459.3513 - val_mae: 26.0680\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3272.9154 - mse: 3272.9153 - mae: 31.2851 - val_loss: 1459.2044 - val_mse: 1459.2043 - val_mae: 26.0116\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 391us/step - loss: 3242.2313 - mse: 3242.2312 - mae: 31.4807 - val_loss: 1458.9499 - val_mse: 1458.9501 - val_mae: 26.1679\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 349us/step - loss: 3221.4793 - mse: 3221.4785 - mae: 31.0780 - val_loss: 1459.4491 - val_mse: 1459.4490 - val_mae: 25.9769\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3256.4067 - mse: 3256.4062 - mae: 31.3357 - val_loss: 1459.7586 - val_mse: 1459.7585 - val_mae: 25.6282\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3251.2644 - mse: 3251.2651 - mae: 31.1017 - val_loss: 1460.8317 - val_mse: 1460.8317 - val_mae: 26.2545\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 351us/step - loss: 3355.9902 - mse: 3355.9897 - mae: 31.8329 - val_loss: 1462.1691 - val_mse: 1462.1691 - val_mae: 25.3897\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3282.8450 - mse: 3282.8445 - mae: 31.5004 - val_loss: 1461.8410 - val_mse: 1461.8411 - val_mae: 25.6703\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 256us/step - loss: 3302.8294 - mse: 3302.8298 - mae: 31.5683 - val_loss: 1462.8964 - val_mse: 1462.8964 - val_mae: 26.2148\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 336us/step - loss: 3241.6309 - mse: 3241.6313 - mae: 32.1776 - val_loss: 1462.0690 - val_mse: 1462.0690 - val_mae: 26.0845\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 337us/step - loss: 3128.8607 - mse: 3128.8616 - mae: 31.5502 - val_loss: 1462.6400 - val_mse: 1462.6403 - val_mae: 26.2130\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2827.9790 - mse: 2827.9795 - mae: 30.6223 - val_loss: 1053.8906 - val_mse: 1053.8906 - val_mae: 23.9333\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 333us/step - loss: 2836.4495 - mse: 2836.4492 - mae: 30.5853 - val_loss: 1062.8427 - val_mse: 1062.8428 - val_mae: 23.5063\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2860.4138 - mse: 2860.4133 - mae: 30.5061 - val_loss: 1048.7367 - val_mse: 1048.7365 - val_mae: 24.1945\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 316us/step - loss: 2861.1121 - mse: 2861.1121 - mae: 30.7665 - val_loss: 1050.7216 - val_mse: 1050.7214 - val_mae: 23.8417\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2814.0148 - mse: 2814.0149 - mae: 30.0355 - val_loss: 1058.5594 - val_mse: 1058.5592 - val_mae: 23.5040\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2916.9947 - mse: 2916.9954 - mae: 31.1167 - val_loss: 1058.3737 - val_mse: 1058.3737 - val_mae: 23.4837\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 356us/step - loss: 2910.7298 - mse: 2910.7302 - mae: 30.0860 - val_loss: 1052.7817 - val_mse: 1052.7817 - val_mae: 23.6538\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2817.1783 - mse: 2817.1775 - mae: 30.3858 - val_loss: 1053.9718 - val_mse: 1053.9718 - val_mae: 23.5895\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 0s 228us/step - loss: 2930.3566 - mse: 2930.3564 - mae: 30.4700 - val_loss: 1049.1226 - val_mse: 1049.1224 - val_mae: 23.8017\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2849.1811 - mse: 2849.1814 - mae: 30.1233 - val_loss: 1046.8374 - val_mse: 1046.8373 - val_mae: 23.9700\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2831.8222 - mse: 2831.8223 - mae: 30.3392 - val_loss: 1051.1901 - val_mse: 1051.1902 - val_mae: 23.6784\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2777.4613 - mse: 2777.4609 - mae: 30.6779 - val_loss: 1047.4405 - val_mse: 1047.4407 - val_mae: 23.8017\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2897.3657 - mse: 2897.3655 - mae: 30.6341 - val_loss: 1045.7563 - val_mse: 1045.7563 - val_mae: 23.9352\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2814.8588 - mse: 2814.8589 - mae: 29.7263 - val_loss: 1048.6042 - val_mse: 1048.6041 - val_mae: 23.7053\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2812.1822 - mse: 2812.1816 - mae: 30.2462 - val_loss: 1043.9832 - val_mse: 1043.9832 - val_mae: 23.9759\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 0s 239us/step - loss: 2890.4339 - mse: 2890.4338 - mae: 30.4874 - val_loss: 1051.5405 - val_mse: 1051.5405 - val_mae: 23.5648\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2954.9510 - mse: 2954.9512 - mae: 30.9041 - val_loss: 1050.1184 - val_mse: 1050.1184 - val_mae: 23.5884\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2920.6110 - mse: 2920.6111 - mae: 30.6847 - val_loss: 1051.7783 - val_mse: 1051.7783 - val_mae: 23.5333\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2868.2343 - mse: 2868.2341 - mae: 30.7153 - val_loss: 1051.9192 - val_mse: 1051.9192 - val_mae: 23.4869\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2862.8650 - mse: 2862.8660 - mae: 30.6734 - val_loss: 1048.6251 - val_mse: 1048.6249 - val_mae: 23.6063\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2815.4218 - mse: 2815.4219 - mae: 30.1263 - val_loss: 1047.0522 - val_mse: 1047.0522 - val_mae: 23.6516\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2853.3190 - mse: 2853.3186 - mae: 30.3852 - val_loss: 1046.3429 - val_mse: 1046.3429 - val_mae: 23.6369\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2807.4080 - mse: 2807.4080 - mae: 30.1338 - val_loss: 1048.3260 - val_mse: 1048.3259 - val_mae: 23.5035\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 333us/step - loss: 2890.2362 - mse: 2890.2356 - mae: 30.5948 - val_loss: 1045.4912 - val_mse: 1045.4913 - val_mae: 23.5770\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2862.2384 - mse: 2862.2390 - mae: 30.5883 - val_loss: 1046.0855 - val_mse: 1046.0854 - val_mae: 23.5212\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2814.9198 - mse: 2814.9199 - mae: 29.9705 - val_loss: 1040.2856 - val_mse: 1040.2855 - val_mae: 23.9332\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2809.8012 - mse: 2809.8015 - mae: 29.8708 - val_loss: 1040.4941 - val_mse: 1040.4941 - val_mae: 23.8670\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2897.8096 - mse: 2897.8096 - mae: 30.4045 - val_loss: 1041.2103 - val_mse: 1041.2103 - val_mae: 23.7633\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2802.0403 - mse: 2802.0408 - mae: 30.6318 - val_loss: 1044.9978 - val_mse: 1044.9978 - val_mae: 23.4730\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 283us/step - loss: 2911.3163 - mse: 2911.3159 - mae: 30.7558 - val_loss: 1044.1385 - val_mse: 1044.1384 - val_mae: 23.5063\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2883.3130 - mse: 2883.3130 - mae: 30.4896 - val_loss: 1040.0144 - val_mse: 1040.0144 - val_mae: 23.6096\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 338us/step - loss: 2932.4751 - mse: 2932.4753 - mae: 31.0547 - val_loss: 1035.8035 - val_mse: 1035.8035 - val_mae: 23.8544\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 261us/step - loss: 2790.9083 - mse: 2790.9080 - mae: 29.7964 - val_loss: 1036.1288 - val_mse: 1036.1288 - val_mae: 23.7401\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 0s 226us/step - loss: 2775.9828 - mse: 2775.9827 - mae: 29.5861 - val_loss: 1036.2861 - val_mse: 1036.2861 - val_mae: 23.7623\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 284us/step - loss: 2808.8658 - mse: 2808.8657 - mae: 30.7116 - val_loss: 1034.4190 - val_mse: 1034.4189 - val_mae: 23.8813\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2916.4976 - mse: 2916.4973 - mae: 31.2806 - val_loss: 1040.4185 - val_mse: 1040.4186 - val_mae: 23.4294\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2856.3522 - mse: 2856.3511 - mae: 29.8733 - val_loss: 1036.8864 - val_mse: 1036.8864 - val_mae: 23.6243\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2873.0238 - mse: 2873.0234 - mae: 30.1031 - val_loss: 1037.8953 - val_mse: 1037.8953 - val_mae: 23.5586\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2861.0152 - mse: 2861.0149 - mae: 30.6668 - val_loss: 1037.6036 - val_mse: 1037.6038 - val_mae: 23.5498\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2845.4658 - mse: 2845.4661 - mae: 29.8358 - val_loss: 1043.5407 - val_mse: 1043.5408 - val_mae: 23.3384\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2882.3485 - mse: 2882.3479 - mae: 30.2068 - val_loss: 1033.3688 - val_mse: 1033.3689 - val_mae: 23.8917\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2764.1532 - mse: 2764.1531 - mae: 29.7699 - val_loss: 1038.9263 - val_mse: 1038.9265 - val_mae: 23.4798\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2843.1678 - mse: 2843.1672 - mae: 30.2101 - val_loss: 1034.2392 - val_mse: 1034.2391 - val_mae: 23.7864\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2835.6422 - mse: 2835.6421 - mae: 30.6493 - val_loss: 1034.3140 - val_mse: 1034.3140 - val_mae: 23.7277\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2824.2645 - mse: 2824.2649 - mae: 29.9876 - val_loss: 1035.1334 - val_mse: 1035.1334 - val_mae: 23.6377\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2844.9548 - mse: 2844.9548 - mae: 30.3140 - val_loss: 1032.7804 - val_mse: 1032.7804 - val_mae: 23.7596\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 329us/step - loss: 2900.1747 - mse: 2900.1760 - mae: 30.4764 - val_loss: 1034.7338 - val_mse: 1034.7336 - val_mae: 23.6085\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2787.3392 - mse: 2787.3394 - mae: 29.7507 - val_loss: 1031.7775 - val_mse: 1031.7775 - val_mae: 23.7416\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 338us/step - loss: 2795.0124 - mse: 2795.0120 - mae: 30.2500 - val_loss: 1040.1805 - val_mse: 1040.1805 - val_mae: 23.2890\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2840.0856 - mse: 2840.0852 - mae: 30.3206 - val_loss: 1032.3358 - val_mse: 1032.3359 - val_mae: 23.5733\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 333us/step - loss: 2842.2772 - mse: 2842.2786 - mae: 30.1243 - val_loss: 1028.8094 - val_mse: 1028.8093 - val_mae: 23.7535\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 264us/step - loss: 2900.1543 - mse: 2900.1541 - mae: 30.5079 - val_loss: 1029.2032 - val_mse: 1029.2032 - val_mae: 23.7403\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2844.5154 - mse: 2844.5154 - mae: 29.6905 - val_loss: 1028.8042 - val_mse: 1028.8042 - val_mae: 23.7096\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 272us/step - loss: 2786.0829 - mse: 2786.0820 - mae: 29.9086 - val_loss: 1029.1463 - val_mse: 1029.1462 - val_mae: 23.6686\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2846.6189 - mse: 2846.6184 - mae: 29.8437 - val_loss: 1028.4891 - val_mse: 1028.4891 - val_mae: 23.6231\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2861.1969 - mse: 2861.1968 - mae: 29.9637 - val_loss: 1035.5623 - val_mse: 1035.5623 - val_mae: 23.2953\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 303us/step - loss: 2855.2180 - mse: 2855.2173 - mae: 30.3527 - val_loss: 1028.5518 - val_mse: 1028.5520 - val_mae: 23.5231\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2746.8959 - mse: 2746.8960 - mae: 29.8711 - val_loss: 1028.5719 - val_mse: 1028.5719 - val_mae: 23.5426\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2812.5275 - mse: 2812.5276 - mae: 29.7744 - val_loss: 1028.7456 - val_mse: 1028.7456 - val_mae: 23.6226\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2802.2684 - mse: 2802.2673 - mae: 29.8816 - val_loss: 1028.5357 - val_mse: 1028.5358 - val_mae: 23.6088\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2891.2924 - mse: 2891.2930 - mae: 30.1733 - val_loss: 1028.9630 - val_mse: 1028.9629 - val_mae: 23.5255\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 256us/step - loss: 2893.3366 - mse: 2893.3367 - mae: 30.1715 - val_loss: 1029.4954 - val_mse: 1029.4954 - val_mae: 23.5241\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2764.8554 - mse: 2764.8550 - mae: 29.7709 - val_loss: 1026.6125 - val_mse: 1026.6127 - val_mae: 23.5859\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2824.1853 - mse: 2824.1858 - mae: 29.8442 - val_loss: 1027.1991 - val_mse: 1027.1991 - val_mae: 23.5817\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2780.3831 - mse: 2780.3826 - mae: 29.8091 - val_loss: 1030.9214 - val_mse: 1030.9214 - val_mae: 23.4111\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2794.3743 - mse: 2794.3733 - mae: 29.9492 - val_loss: 1023.3936 - val_mse: 1023.3937 - val_mae: 23.8375\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 329us/step - loss: 2793.4697 - mse: 2793.4685 - mae: 29.9336 - val_loss: 1022.5734 - val_mse: 1022.5735 - val_mae: 23.7496\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2876.9923 - mse: 2876.9922 - mae: 30.5528 - val_loss: 1025.5888 - val_mse: 1025.5887 - val_mae: 23.5086\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 347us/step - loss: 2864.0593 - mse: 2864.0596 - mae: 29.9825 - val_loss: 1027.0588 - val_mse: 1027.0588 - val_mae: 23.4190\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2791.9462 - mse: 2791.9460 - mae: 30.1272 - val_loss: 1020.1717 - val_mse: 1020.1718 - val_mae: 23.7231\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 359us/step - loss: 2791.4177 - mse: 2791.4187 - mae: 29.7869 - val_loss: 1018.8453 - val_mse: 1018.8453 - val_mae: 23.6039\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 338us/step - loss: 2773.3962 - mse: 2773.3955 - mae: 29.7644 - val_loss: 1021.2863 - val_mse: 1021.2863 - val_mae: 23.5101\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2788.4080 - mse: 2788.4072 - mae: 29.8627 - val_loss: 1021.3974 - val_mse: 1021.3975 - val_mae: 23.5677\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2756.3787 - mse: 2756.3784 - mae: 29.5701 - val_loss: 1020.4621 - val_mse: 1020.4620 - val_mae: 23.6393\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2809.7278 - mse: 2809.7280 - mae: 30.2185 - val_loss: 1021.2306 - val_mse: 1021.2305 - val_mae: 23.5342\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2855.5867 - mse: 2855.5862 - mae: 30.2018 - val_loss: 1019.6070 - val_mse: 1019.6072 - val_mae: 23.7043\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2864.9762 - mse: 2864.9768 - mae: 30.0557 - val_loss: 1021.2102 - val_mse: 1021.2102 - val_mae: 23.5152\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2819.1365 - mse: 2819.1367 - mae: 29.9790 - val_loss: 1020.1378 - val_mse: 1020.1379 - val_mae: 23.5121\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2736.6816 - mse: 2736.6819 - mae: 29.5701 - val_loss: 1023.0156 - val_mse: 1023.0155 - val_mae: 23.3505\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 272us/step - loss: 2790.7033 - mse: 2790.7026 - mae: 29.8312 - val_loss: 1016.9079 - val_mse: 1016.9080 - val_mae: 23.7223\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2551.6369 - mse: 2551.6370 - mae: 29.8200 - val_loss: 1568.3151 - val_mse: 1568.3151 - val_mae: 26.7273\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2527.6359 - mse: 2527.6353 - mae: 29.3097 - val_loss: 1561.2477 - val_mse: 1561.2478 - val_mae: 26.8260\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 273us/step - loss: 2485.6750 - mse: 2485.6748 - mae: 29.0445 - val_loss: 1571.2913 - val_mse: 1571.2914 - val_mae: 26.6160\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2440.0965 - mse: 2440.0967 - mae: 29.0512 - val_loss: 1560.9716 - val_mse: 1560.9717 - val_mae: 26.7284\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2519.3537 - mse: 2519.3533 - mae: 29.6728 - val_loss: 1559.2124 - val_mse: 1559.2124 - val_mae: 26.7140\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 349us/step - loss: 2527.9285 - mse: 2527.9287 - mae: 28.9888 - val_loss: 1555.0943 - val_mse: 1555.0944 - val_mae: 26.7471\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 341us/step - loss: 2467.1320 - mse: 2467.1328 - mae: 28.8901 - val_loss: 1563.3691 - val_mse: 1563.3691 - val_mae: 26.5415\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 376us/step - loss: 2520.8442 - mse: 2520.8445 - mae: 29.0701 - val_loss: 1550.1356 - val_mse: 1550.1357 - val_mae: 26.7428\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 352us/step - loss: 2426.6570 - mse: 2426.6575 - mae: 29.1184 - val_loss: 1554.3463 - val_mse: 1554.3464 - val_mae: 26.5935\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 344us/step - loss: 2466.3265 - mse: 2466.3262 - mae: 29.5383 - val_loss: 1569.8239 - val_mse: 1569.8237 - val_mae: 26.3246\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2437.5462 - mse: 2437.5469 - mae: 28.6482 - val_loss: 1565.4056 - val_mse: 1565.4056 - val_mae: 26.3500\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2425.7915 - mse: 2425.7920 - mae: 28.9405 - val_loss: 1551.5779 - val_mse: 1551.5780 - val_mae: 26.4945\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2506.0377 - mse: 2506.0386 - mae: 29.5277 - val_loss: 1551.2664 - val_mse: 1551.2662 - val_mae: 26.4755\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2424.5851 - mse: 2424.5852 - mae: 28.9693 - val_loss: 1539.2309 - val_mse: 1539.2310 - val_mae: 26.6202\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2469.9496 - mse: 2469.9485 - mae: 28.7485 - val_loss: 1538.9641 - val_mse: 1538.9642 - val_mae: 26.5934\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2514.2468 - mse: 2514.2473 - mae: 29.1135 - val_loss: 1546.6975 - val_mse: 1546.6976 - val_mae: 26.4097\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2488.0982 - mse: 2488.0986 - mae: 28.8824 - val_loss: 1531.8208 - val_mse: 1531.8207 - val_mae: 26.6609\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2512.8533 - mse: 2512.8525 - mae: 29.1546 - val_loss: 1539.6716 - val_mse: 1539.6715 - val_mae: 26.4834\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2438.9371 - mse: 2438.9370 - mae: 28.8110 - val_loss: 1547.6089 - val_mse: 1547.6088 - val_mae: 26.2833\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2432.9904 - mse: 2432.9902 - mae: 29.1411 - val_loss: 1543.3467 - val_mse: 1543.3467 - val_mae: 26.3287\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 363us/step - loss: 2439.9937 - mse: 2439.9932 - mae: 29.1027 - val_loss: 1543.9972 - val_mse: 1543.9973 - val_mae: 26.2489\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 345us/step - loss: 2437.4572 - mse: 2437.4568 - mae: 28.7954 - val_loss: 1527.3604 - val_mse: 1527.3604 - val_mae: 26.4638\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 293us/step - loss: 2518.9546 - mse: 2518.9551 - mae: 29.0662 - val_loss: 1531.2373 - val_mse: 1531.2373 - val_mae: 26.3822\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2436.0230 - mse: 2436.0225 - mae: 28.5395 - val_loss: 1515.5477 - val_mse: 1515.5477 - val_mae: 26.5639\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2410.4317 - mse: 2410.4321 - mae: 28.7039 - val_loss: 1533.3081 - val_mse: 1533.3081 - val_mae: 26.2301\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2484.4982 - mse: 2484.4978 - mae: 29.3558 - val_loss: 1535.7366 - val_mse: 1535.7365 - val_mae: 26.1684\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2485.6013 - mse: 2485.6018 - mae: 28.5964 - val_loss: 1527.9000 - val_mse: 1527.9000 - val_mae: 26.2136\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 323us/step - loss: 2428.3171 - mse: 2428.3174 - mae: 28.8057 - val_loss: 1525.3734 - val_mse: 1525.3733 - val_mae: 26.2158\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2527.2017 - mse: 2527.2014 - mae: 29.2307 - val_loss: 1526.2903 - val_mse: 1526.2903 - val_mae: 26.1877\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2500.4846 - mse: 2500.4846 - mae: 29.1402 - val_loss: 1525.9848 - val_mse: 1525.9845 - val_mae: 26.1550\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 331us/step - loss: 2491.1858 - mse: 2491.1860 - mae: 29.2222 - val_loss: 1525.8691 - val_mse: 1525.8691 - val_mae: 26.1861\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2466.0858 - mse: 2466.0859 - mae: 28.8917 - val_loss: 1535.7975 - val_mse: 1535.7975 - val_mae: 26.0504\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 334us/step - loss: 2457.1586 - mse: 2457.1577 - mae: 28.8348 - val_loss: 1526.8430 - val_mse: 1526.8431 - val_mae: 26.1067\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2478.8921 - mse: 2478.8926 - mae: 28.9211 - val_loss: 1521.9500 - val_mse: 1521.9501 - val_mae: 26.1377\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2497.1807 - mse: 2497.1802 - mae: 29.1587 - val_loss: 1531.9432 - val_mse: 1531.9429 - val_mae: 25.9817\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2531.1721 - mse: 2531.1726 - mae: 29.6180 - val_loss: 1520.2220 - val_mse: 1520.2222 - val_mae: 26.0857\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2432.2506 - mse: 2432.2498 - mae: 28.6465 - val_loss: 1513.9761 - val_mse: 1513.9761 - val_mae: 26.1786\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2486.2287 - mse: 2486.2288 - mae: 28.9885 - val_loss: 1514.6158 - val_mse: 1514.6157 - val_mae: 26.0991\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 345us/step - loss: 2452.3342 - mse: 2452.3333 - mae: 28.8061 - val_loss: 1511.1145 - val_mse: 1511.1146 - val_mae: 26.0678\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2464.6878 - mse: 2464.6870 - mae: 28.9587 - val_loss: 1516.7619 - val_mse: 1516.7621 - val_mae: 25.9992\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2490.9770 - mse: 2490.9766 - mae: 29.3935 - val_loss: 1528.0063 - val_mse: 1528.0060 - val_mae: 25.8512\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 343us/step - loss: 2506.3379 - mse: 2506.3379 - mae: 29.1684 - val_loss: 1506.6966 - val_mse: 1506.6965 - val_mae: 26.0170\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2483.3977 - mse: 2483.3977 - mae: 29.0690 - val_loss: 1505.7885 - val_mse: 1505.7886 - val_mae: 26.0093\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2483.8886 - mse: 2483.8882 - mae: 29.1040 - val_loss: 1500.9144 - val_mse: 1500.9146 - val_mae: 26.0513\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2436.3309 - mse: 2436.3308 - mae: 28.9732 - val_loss: 1505.7462 - val_mse: 1505.7462 - val_mae: 25.9248\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 285us/step - loss: 2443.3571 - mse: 2443.3584 - mae: 28.3354 - val_loss: 1496.6399 - val_mse: 1496.6399 - val_mae: 25.9735\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2483.9191 - mse: 2483.9192 - mae: 28.9623 - val_loss: 1510.9232 - val_mse: 1510.9231 - val_mae: 25.7922\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 266us/step - loss: 2454.0140 - mse: 2454.0146 - mae: 28.7161 - val_loss: 1503.1695 - val_mse: 1503.1696 - val_mae: 25.8542\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 379us/step - loss: 2433.7546 - mse: 2433.7542 - mae: 28.4964 - val_loss: 1490.6498 - val_mse: 1490.6497 - val_mae: 25.9662\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 335us/step - loss: 2414.5240 - mse: 2414.5251 - mae: 28.4947 - val_loss: 1486.2424 - val_mse: 1486.2422 - val_mae: 25.9810\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2456.9465 - mse: 2456.9465 - mae: 28.7489 - val_loss: 1500.5393 - val_mse: 1500.5393 - val_mae: 25.7845\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 266us/step - loss: 2487.8696 - mse: 2487.8694 - mae: 29.0542 - val_loss: 1489.0642 - val_mse: 1489.0642 - val_mae: 25.9194\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2455.9729 - mse: 2455.9729 - mae: 28.9733 - val_loss: 1496.6644 - val_mse: 1496.6644 - val_mae: 25.7567\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2410.7868 - mse: 2410.7869 - mae: 28.8922 - val_loss: 1494.5246 - val_mse: 1494.5247 - val_mae: 25.7392\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2457.9331 - mse: 2457.9329 - mae: 28.9044 - val_loss: 1490.4440 - val_mse: 1490.4438 - val_mae: 25.7812\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2456.6131 - mse: 2456.6130 - mae: 28.6969 - val_loss: 1484.1937 - val_mse: 1484.1937 - val_mae: 25.8288\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2475.5143 - mse: 2475.5142 - mae: 29.1950 - val_loss: 1477.5787 - val_mse: 1477.5787 - val_mae: 25.8502\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2456.9765 - mse: 2456.9766 - mae: 29.0729 - val_loss: 1494.1358 - val_mse: 1494.1360 - val_mae: 25.6238\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 364us/step - loss: 2507.9029 - mse: 2507.9038 - mae: 29.1191 - val_loss: 1481.9413 - val_mse: 1481.9413 - val_mae: 25.7748\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 268us/step - loss: 2485.2968 - mse: 2485.2971 - mae: 28.9662 - val_loss: 1497.4567 - val_mse: 1497.4565 - val_mae: 25.5633\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 263us/step - loss: 2461.3824 - mse: 2461.3823 - mae: 28.7956 - val_loss: 1478.0993 - val_mse: 1478.0994 - val_mae: 25.8065\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 279us/step - loss: 2409.1230 - mse: 2409.1230 - mae: 28.6678 - val_loss: 1492.8425 - val_mse: 1492.8425 - val_mae: 25.5988\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2441.1288 - mse: 2441.1282 - mae: 28.8611 - val_loss: 1476.1195 - val_mse: 1476.1193 - val_mae: 25.8023\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 277us/step - loss: 2420.7253 - mse: 2420.7258 - mae: 28.7923 - val_loss: 1481.0252 - val_mse: 1481.0251 - val_mae: 25.6490\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 353us/step - loss: 2443.7464 - mse: 2443.7463 - mae: 28.7741 - val_loss: 1475.0510 - val_mse: 1475.0509 - val_mae: 25.7303\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2474.8399 - mse: 2474.8398 - mae: 28.8820 - val_loss: 1477.2662 - val_mse: 1477.2664 - val_mae: 25.7274\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2392.2066 - mse: 2392.2061 - mae: 28.4796 - val_loss: 1479.4518 - val_mse: 1479.4519 - val_mae: 25.6440\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2467.7181 - mse: 2467.7173 - mae: 28.6667 - val_loss: 1460.9956 - val_mse: 1460.9957 - val_mae: 25.8679\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2392.0668 - mse: 2392.0662 - mae: 28.6020 - val_loss: 1459.9754 - val_mse: 1459.9753 - val_mae: 25.7685\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2382.1225 - mse: 2382.1223 - mae: 28.4592 - val_loss: 1460.7539 - val_mse: 1460.7539 - val_mae: 25.7683\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 360us/step - loss: 2382.2515 - mse: 2382.2517 - mae: 27.9768 - val_loss: 1460.6724 - val_mse: 1460.6724 - val_mae: 25.7758\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 386us/step - loss: 2463.1632 - mse: 2463.1631 - mae: 28.5638 - val_loss: 1462.8346 - val_mse: 1462.8347 - val_mae: 25.7355\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2410.8448 - mse: 2410.8445 - mae: 28.6925 - val_loss: 1464.3926 - val_mse: 1464.3925 - val_mae: 25.6722\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2431.9813 - mse: 2431.9802 - mae: 28.8189 - val_loss: 1472.1575 - val_mse: 1472.1575 - val_mae: 25.5119\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2408.3813 - mse: 2408.3813 - mae: 27.8692 - val_loss: 1456.2677 - val_mse: 1456.2677 - val_mae: 25.7246\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2448.2843 - mse: 2448.2844 - mae: 28.7009 - val_loss: 1461.1806 - val_mse: 1461.1805 - val_mae: 25.6373\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2416.0526 - mse: 2416.0525 - mae: 28.4131 - val_loss: 1463.8657 - val_mse: 1463.8657 - val_mae: 25.5616\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2409.4084 - mse: 2409.4077 - mae: 28.4362 - val_loss: 1465.0170 - val_mse: 1465.0171 - val_mae: 25.5158\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2425.3424 - mse: 2425.3425 - mae: 28.7358 - val_loss: 1467.3033 - val_mse: 1467.3033 - val_mae: 25.5422\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2414.0050 - mse: 2414.0049 - mae: 28.6057 - val_loss: 1476.9928 - val_mse: 1476.9929 - val_mae: 25.4303\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2384.0147 - mse: 2384.0151 - mae: 29.0961 - val_loss: 3665.2773 - val_mse: 3665.2761 - val_mae: 22.1450\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2314.2168 - mse: 2314.2173 - mae: 28.9995 - val_loss: 3664.8729 - val_mse: 3664.8721 - val_mae: 22.7438\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2334.4395 - mse: 2334.4397 - mae: 29.1345 - val_loss: 3668.7784 - val_mse: 3668.7778 - val_mae: 22.2398\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 263us/step - loss: 2307.7363 - mse: 2307.7368 - mae: 28.8072 - val_loss: 3665.3185 - val_mse: 3665.3186 - val_mae: 22.7653\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2313.3053 - mse: 2313.3054 - mae: 28.8691 - val_loss: 3668.2103 - val_mse: 3668.2102 - val_mae: 22.2825\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 275us/step - loss: 2338.6130 - mse: 2338.6123 - mae: 29.1987 - val_loss: 3667.5309 - val_mse: 3667.5308 - val_mae: 22.4856\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2314.5704 - mse: 2314.5706 - mae: 28.8463 - val_loss: 3671.8854 - val_mse: 3671.8853 - val_mae: 22.3481\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2348.3077 - mse: 2348.3076 - mae: 28.8156 - val_loss: 3671.3948 - val_mse: 3671.3943 - val_mae: 22.4739\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2351.7621 - mse: 2351.7617 - mae: 28.9441 - val_loss: 3672.7014 - val_mse: 3672.7012 - val_mae: 22.2253\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2321.3070 - mse: 2321.3074 - mae: 29.1855 - val_loss: 3671.1472 - val_mse: 3671.1472 - val_mae: 22.6842\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2265.7094 - mse: 2265.7100 - mae: 28.4484 - val_loss: 3670.9055 - val_mse: 3670.9053 - val_mae: 22.8775\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 255us/step - loss: 2312.9548 - mse: 2312.9546 - mae: 29.2170 - val_loss: 3672.0212 - val_mse: 3672.0210 - val_mae: 22.6701\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2306.0562 - mse: 2306.0549 - mae: 28.8856 - val_loss: 3670.8998 - val_mse: 3670.8999 - val_mae: 22.5217\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2278.3346 - mse: 2278.3337 - mae: 28.2374 - val_loss: 3671.7773 - val_mse: 3671.7781 - val_mae: 22.5936\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2265.2553 - mse: 2265.2554 - mae: 28.6903 - val_loss: 3676.6350 - val_mse: 3676.6348 - val_mae: 22.8505\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2313.5770 - mse: 2313.5769 - mae: 29.0721 - val_loss: 3675.6882 - val_mse: 3675.6885 - val_mae: 22.7197\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2388.7249 - mse: 2388.7256 - mae: 29.0092 - val_loss: 3676.2895 - val_mse: 3676.2896 - val_mae: 22.9368\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2342.7780 - mse: 2342.7781 - mae: 29.2227 - val_loss: 3677.3132 - val_mse: 3677.3135 - val_mae: 22.5065\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2257.6907 - mse: 2257.6902 - mae: 28.6679 - val_loss: 3677.3271 - val_mse: 3677.3276 - val_mae: 22.8849\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 325us/step - loss: 2313.5177 - mse: 2313.5173 - mae: 28.6830 - val_loss: 3676.5909 - val_mse: 3676.5911 - val_mae: 22.9793\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 327us/step - loss: 2287.0831 - mse: 2287.0830 - mae: 28.7926 - val_loss: 3676.5120 - val_mse: 3676.5115 - val_mae: 22.6143\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2301.5298 - mse: 2301.5293 - mae: 28.6435 - val_loss: 3676.3230 - val_mse: 3676.3232 - val_mae: 22.8507\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 282us/step - loss: 2231.3402 - mse: 2231.3406 - mae: 28.6320 - val_loss: 3679.7075 - val_mse: 3679.7068 - val_mae: 22.5699\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2257.2859 - mse: 2257.2859 - mae: 28.3172 - val_loss: 3679.1766 - val_mse: 3679.1763 - val_mae: 22.8649\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2322.1353 - mse: 2322.1350 - mae: 28.9327 - val_loss: 3680.4602 - val_mse: 3680.4592 - val_mae: 23.2001\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2308.0774 - mse: 2308.0781 - mae: 28.7393 - val_loss: 3678.0593 - val_mse: 3678.0593 - val_mae: 22.7796\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2308.1598 - mse: 2308.1594 - mae: 29.0263 - val_loss: 3678.8533 - val_mse: 3678.8545 - val_mae: 22.2687\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2304.1115 - mse: 2304.1113 - mae: 28.6806 - val_loss: 3679.8672 - val_mse: 3679.8679 - val_mae: 22.4680\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 345us/step - loss: 2269.5282 - mse: 2269.5276 - mae: 29.0445 - val_loss: 3680.5751 - val_mse: 3680.5752 - val_mae: 22.6768\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2270.1903 - mse: 2270.1902 - mae: 28.4131 - val_loss: 3680.0062 - val_mse: 3680.0063 - val_mae: 22.8908\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2260.7610 - mse: 2260.7610 - mae: 28.9196 - val_loss: 3680.2840 - val_mse: 3680.2837 - val_mae: 22.5914\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2294.0484 - mse: 2294.0476 - mae: 28.8395 - val_loss: 3681.0421 - val_mse: 3681.0422 - val_mae: 22.6093\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 321us/step - loss: 2286.9332 - mse: 2286.9333 - mae: 28.5687 - val_loss: 3678.7126 - val_mse: 3678.7131 - val_mae: 22.7502\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2256.5781 - mse: 2256.5784 - mae: 28.7775 - val_loss: 3682.0238 - val_mse: 3682.0244 - val_mae: 22.3052\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2272.3004 - mse: 2272.3003 - mae: 28.6833 - val_loss: 3680.5512 - val_mse: 3680.5508 - val_mae: 22.8153\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2307.9644 - mse: 2307.9641 - mae: 28.3829 - val_loss: 3681.5710 - val_mse: 3681.5703 - val_mae: 22.7835\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 316us/step - loss: 2265.9981 - mse: 2265.9980 - mae: 28.5599 - val_loss: 3682.9252 - val_mse: 3682.9250 - val_mae: 22.4868\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2353.9040 - mse: 2353.9033 - mae: 29.0397 - val_loss: 3683.6260 - val_mse: 3683.6262 - val_mae: 22.9511\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2284.9346 - mse: 2284.9348 - mae: 28.5412 - val_loss: 3682.5897 - val_mse: 3682.5901 - val_mae: 22.8094\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 275us/step - loss: 2288.4867 - mse: 2288.4871 - mae: 28.6711 - val_loss: 3682.7896 - val_mse: 3682.7900 - val_mae: 22.5484\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2269.2629 - mse: 2269.2637 - mae: 28.4845 - val_loss: 3682.2399 - val_mse: 3682.2393 - val_mae: 22.5029\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2302.4122 - mse: 2302.4131 - mae: 28.4296 - val_loss: 3683.5662 - val_mse: 3683.5667 - val_mae: 22.8494\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2253.0318 - mse: 2253.0320 - mae: 28.6036 - val_loss: 3683.1054 - val_mse: 3683.1050 - val_mae: 22.5283\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2304.5328 - mse: 2304.5327 - mae: 28.9247 - val_loss: 3680.9495 - val_mse: 3680.9502 - val_mae: 22.2933\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 335us/step - loss: 2366.7800 - mse: 2366.7798 - mae: 29.1264 - val_loss: 3681.8114 - val_mse: 3681.8103 - val_mae: 22.5199\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2295.2547 - mse: 2295.2549 - mae: 28.2872 - val_loss: 3683.7982 - val_mse: 3683.7986 - val_mae: 22.7398\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2285.6157 - mse: 2285.6167 - mae: 28.8035 - val_loss: 3686.0298 - val_mse: 3686.0300 - val_mae: 22.6827\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2263.8597 - mse: 2263.8604 - mae: 28.5715 - val_loss: 3684.7920 - val_mse: 3684.7915 - val_mae: 22.2569\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2272.7144 - mse: 2272.7146 - mae: 28.3154 - val_loss: 3685.0025 - val_mse: 3685.0027 - val_mae: 22.8109\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2319.7852 - mse: 2319.7844 - mae: 28.7413 - val_loss: 3687.2279 - val_mse: 3687.2275 - val_mae: 22.7680\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2213.5865 - mse: 2213.5872 - mae: 28.0998 - val_loss: 3690.9991 - val_mse: 3690.9998 - val_mae: 22.9035\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2283.4125 - mse: 2283.4131 - mae: 28.5049 - val_loss: 3689.5214 - val_mse: 3689.5215 - val_mae: 22.5927\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 334us/step - loss: 2280.7386 - mse: 2280.7385 - mae: 28.4311 - val_loss: 3691.6518 - val_mse: 3691.6523 - val_mae: 22.9339\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2278.5193 - mse: 2278.5193 - mae: 28.8985 - val_loss: 3692.3613 - val_mse: 3692.3608 - val_mae: 22.7439\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2293.2874 - mse: 2293.2874 - mae: 28.6004 - val_loss: 3691.5327 - val_mse: 3691.5332 - val_mae: 22.9596\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2329.9837 - mse: 2329.9839 - mae: 29.1052 - val_loss: 3691.3932 - val_mse: 3691.3938 - val_mae: 22.4561\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2223.5603 - mse: 2223.5601 - mae: 28.4127 - val_loss: 3694.8786 - val_mse: 3694.8784 - val_mae: 22.7160\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 259us/step - loss: 2303.8771 - mse: 2303.8774 - mae: 28.9888 - val_loss: 3698.0671 - val_mse: 3698.0676 - val_mae: 23.1759\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2294.5729 - mse: 2294.5728 - mae: 28.3782 - val_loss: 3694.5741 - val_mse: 3694.5737 - val_mae: 23.0358\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2229.1940 - mse: 2229.1936 - mae: 28.1720 - val_loss: 3694.6172 - val_mse: 3694.6172 - val_mae: 23.0805\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2283.3865 - mse: 2283.3853 - mae: 28.5803 - val_loss: 3693.7384 - val_mse: 3693.7375 - val_mae: 23.0534\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2248.8029 - mse: 2248.8027 - mae: 28.3647 - val_loss: 3695.9868 - val_mse: 3695.9866 - val_mae: 22.3904\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2290.5991 - mse: 2290.5986 - mae: 28.4909 - val_loss: 3690.2956 - val_mse: 3690.2961 - val_mae: 22.5999\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 238us/step - loss: 2272.7242 - mse: 2272.7234 - mae: 28.3840 - val_loss: 3694.1365 - val_mse: 3694.1372 - val_mae: 22.8341\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2242.4702 - mse: 2242.4705 - mae: 28.6177 - val_loss: 3694.3167 - val_mse: 3694.3167 - val_mae: 22.4251\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 267us/step - loss: 2303.2039 - mse: 2303.2043 - mae: 28.3734 - val_loss: 3690.9064 - val_mse: 3690.9070 - val_mae: 22.5482\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 326us/step - loss: 2270.4962 - mse: 2270.4958 - mae: 28.5930 - val_loss: 3694.8431 - val_mse: 3694.8430 - val_mae: 23.2204\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2285.8686 - mse: 2285.8689 - mae: 28.9241 - val_loss: 3697.7781 - val_mse: 3697.7788 - val_mae: 23.0514\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2294.3936 - mse: 2294.3938 - mae: 28.3187 - val_loss: 3697.1522 - val_mse: 3697.1519 - val_mae: 22.9916\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2302.7334 - mse: 2302.7332 - mae: 28.4470 - val_loss: 3693.2041 - val_mse: 3693.2041 - val_mae: 22.4265\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2285.5637 - mse: 2285.5632 - mae: 28.2691 - val_loss: 3694.9676 - val_mse: 3694.9675 - val_mae: 23.0084\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2295.4791 - mse: 2295.4800 - mae: 28.3549 - val_loss: 3694.6110 - val_mse: 3694.6111 - val_mae: 22.9459\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2294.5761 - mse: 2294.5757 - mae: 28.5678 - val_loss: 3694.1288 - val_mse: 3694.1289 - val_mae: 22.9394\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 345us/step - loss: 2316.5059 - mse: 2316.5059 - mae: 29.3081 - val_loss: 3693.1917 - val_mse: 3693.1912 - val_mae: 22.7607\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2290.1785 - mse: 2290.1794 - mae: 28.3871 - val_loss: 3691.8657 - val_mse: 3691.8662 - val_mae: 22.9026\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 304us/step - loss: 2304.9073 - mse: 2304.9067 - mae: 28.9870 - val_loss: 3692.4619 - val_mse: 3692.4614 - val_mae: 22.6987\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2234.1513 - mse: 2234.1509 - mae: 28.0227 - val_loss: 3699.9511 - val_mse: 3699.9507 - val_mae: 23.1022\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2306.6880 - mse: 2306.6882 - mae: 28.6162 - val_loss: 3691.2272 - val_mse: 3691.2266 - val_mae: 22.7835\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2291.5049 - mse: 2291.5054 - mae: 28.3310 - val_loss: 3695.1094 - val_mse: 3695.1101 - val_mae: 23.3950\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2350.8662 - mse: 2350.8662 - mae: 28.8999 - val_loss: 3690.9972 - val_mse: 3690.9976 - val_mae: 22.8460\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2635.7936 - mse: 2635.7937 - mae: 27.9183 - val_loss: 2075.3612 - val_mse: 2075.3613 - val_mae: 26.5179\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2618.8547 - mse: 2618.8557 - mae: 27.7166 - val_loss: 2081.9492 - val_mse: 2081.9492 - val_mae: 26.1309\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2697.6554 - mse: 2697.6562 - mae: 28.5642 - val_loss: 2097.7640 - val_mse: 2097.7642 - val_mae: 25.9020\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2662.2123 - mse: 2662.2129 - mae: 27.8215 - val_loss: 2098.1537 - val_mse: 2098.1538 - val_mae: 26.1633\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2606.5405 - mse: 2606.5413 - mae: 27.5869 - val_loss: 2091.1395 - val_mse: 2091.1396 - val_mae: 26.3435\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 336us/step - loss: 2678.6288 - mse: 2678.6299 - mae: 28.0031 - val_loss: 2094.8927 - val_mse: 2094.8926 - val_mae: 26.0353\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2691.1994 - mse: 2691.1992 - mae: 27.8935 - val_loss: 2091.6301 - val_mse: 2091.6299 - val_mae: 26.1630\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2627.4259 - mse: 2627.4255 - mae: 27.7200 - val_loss: 2100.2262 - val_mse: 2100.2261 - val_mae: 26.1788\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2679.1951 - mse: 2679.1948 - mae: 28.0342 - val_loss: 2098.0610 - val_mse: 2098.0610 - val_mae: 26.4456\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 333us/step - loss: 2642.2800 - mse: 2642.2795 - mae: 27.7043 - val_loss: 2093.9430 - val_mse: 2093.9429 - val_mae: 26.9127\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2672.4212 - mse: 2672.4207 - mae: 28.2161 - val_loss: 2108.3284 - val_mse: 2108.3286 - val_mae: 26.0229\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2669.1672 - mse: 2669.1670 - mae: 27.8662 - val_loss: 2101.2081 - val_mse: 2101.2075 - val_mae: 26.5186\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 255us/step - loss: 2637.7731 - mse: 2637.7739 - mae: 27.8690 - val_loss: 2095.5029 - val_mse: 2095.5032 - val_mae: 26.4860\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2633.7273 - mse: 2633.7271 - mae: 27.7435 - val_loss: 2099.6151 - val_mse: 2099.6152 - val_mae: 26.5026\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2570.6228 - mse: 2570.6238 - mae: 27.7400 - val_loss: 2106.4147 - val_mse: 2106.4148 - val_mae: 26.3599\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2626.3136 - mse: 2626.3137 - mae: 27.7649 - val_loss: 2104.9782 - val_mse: 2104.9783 - val_mae: 26.5173\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2583.6098 - mse: 2583.6094 - mae: 27.5317 - val_loss: 2106.0526 - val_mse: 2106.0522 - val_mae: 26.7151\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2661.4645 - mse: 2661.4651 - mae: 28.2158 - val_loss: 2110.4300 - val_mse: 2110.4299 - val_mae: 26.3860\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2658.2249 - mse: 2658.2256 - mae: 27.9502 - val_loss: 2107.3234 - val_mse: 2107.3235 - val_mae: 26.4356\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2649.1979 - mse: 2649.1987 - mae: 28.0403 - val_loss: 2099.9034 - val_mse: 2099.9036 - val_mae: 26.7321\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 342us/step - loss: 2619.9923 - mse: 2619.9932 - mae: 27.5165 - val_loss: 2100.4339 - val_mse: 2100.4336 - val_mae: 26.4567\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 354us/step - loss: 2671.2967 - mse: 2671.2966 - mae: 28.2354 - val_loss: 2098.4074 - val_mse: 2098.4077 - val_mae: 26.1277\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 256us/step - loss: 2664.1827 - mse: 2664.1821 - mae: 27.7986 - val_loss: 2091.1461 - val_mse: 2091.1460 - val_mae: 26.6334\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 329us/step - loss: 2633.3526 - mse: 2633.3530 - mae: 27.8052 - val_loss: 2105.8201 - val_mse: 2105.8198 - val_mae: 25.9513\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2669.2660 - mse: 2669.2666 - mae: 27.9889 - val_loss: 2115.7523 - val_mse: 2115.7527 - val_mae: 25.9501\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 252us/step - loss: 2673.1175 - mse: 2673.1174 - mae: 27.7823 - val_loss: 2111.2810 - val_mse: 2111.2808 - val_mae: 26.3473\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 325us/step - loss: 2685.1739 - mse: 2685.1741 - mae: 28.1469 - val_loss: 2106.7027 - val_mse: 2106.7026 - val_mae: 26.5599\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2636.1673 - mse: 2636.1667 - mae: 27.6388 - val_loss: 2104.3246 - val_mse: 2104.3245 - val_mae: 26.3393\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2679.1693 - mse: 2679.1699 - mae: 27.9620 - val_loss: 2102.7499 - val_mse: 2102.7498 - val_mae: 26.7224\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 248us/step - loss: 2650.4173 - mse: 2650.4175 - mae: 27.8241 - val_loss: 2111.5493 - val_mse: 2111.5493 - val_mae: 26.4905\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2668.3819 - mse: 2668.3828 - mae: 27.7912 - val_loss: 2111.2742 - val_mse: 2111.2742 - val_mae: 26.4785\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2629.1008 - mse: 2629.1001 - mae: 27.6619 - val_loss: 2113.2807 - val_mse: 2113.2805 - val_mae: 26.4495\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2641.6670 - mse: 2641.6663 - mae: 27.8028 - val_loss: 2114.9667 - val_mse: 2114.9666 - val_mae: 26.4272\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2621.8280 - mse: 2621.8281 - mae: 27.7469 - val_loss: 2113.8446 - val_mse: 2113.8447 - val_mae: 26.7279\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2641.6187 - mse: 2641.6189 - mae: 27.9165 - val_loss: 2134.3649 - val_mse: 2134.3647 - val_mae: 26.1725\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2600.4957 - mse: 2600.4961 - mae: 27.7028 - val_loss: 2125.0442 - val_mse: 2125.0439 - val_mae: 26.3926\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2664.9933 - mse: 2664.9929 - mae: 28.0764 - val_loss: 2123.7393 - val_mse: 2123.7390 - val_mae: 26.5124\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 331us/step - loss: 2704.6032 - mse: 2704.6040 - mae: 28.2975 - val_loss: 2122.3521 - val_mse: 2122.3518 - val_mae: 26.2226\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 293us/step - loss: 2617.9888 - mse: 2617.9888 - mae: 27.4336 - val_loss: 2117.6185 - val_mse: 2117.6182 - val_mae: 26.4274\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 356us/step - loss: 2623.0694 - mse: 2623.0693 - mae: 27.7671 - val_loss: 2103.8518 - val_mse: 2103.8521 - val_mae: 26.6170\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 325us/step - loss: 2654.5279 - mse: 2654.5281 - mae: 27.9910 - val_loss: 2106.4062 - val_mse: 2106.4065 - val_mae: 26.4092\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 335us/step - loss: 2639.2227 - mse: 2639.2224 - mae: 27.7819 - val_loss: 2122.2737 - val_mse: 2122.2737 - val_mae: 26.0025\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2623.6059 - mse: 2623.6052 - mae: 27.8684 - val_loss: 2109.3905 - val_mse: 2109.3904 - val_mae: 26.5771\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2643.4294 - mse: 2643.4282 - mae: 28.0564 - val_loss: 2113.8623 - val_mse: 2113.8623 - val_mae: 26.1966\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 352us/step - loss: 2636.5702 - mse: 2636.5696 - mae: 27.4446 - val_loss: 2102.9192 - val_mse: 2102.9192 - val_mae: 26.8023\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2629.3332 - mse: 2629.3328 - mae: 27.7899 - val_loss: 2103.0733 - val_mse: 2103.0730 - val_mae: 26.5495\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2651.7105 - mse: 2651.7100 - mae: 28.0470 - val_loss: 2112.8540 - val_mse: 2112.8542 - val_mae: 26.4313\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2634.9126 - mse: 2634.9121 - mae: 27.6866 - val_loss: 2112.6395 - val_mse: 2112.6399 - val_mae: 26.5761\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 249us/step - loss: 2630.7399 - mse: 2630.7397 - mae: 27.6262 - val_loss: 2116.9653 - val_mse: 2116.9653 - val_mae: 26.6917\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 340us/step - loss: 2611.8300 - mse: 2611.8303 - mae: 27.7022 - val_loss: 2114.5559 - val_mse: 2114.5562 - val_mae: 26.4856\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2672.0406 - mse: 2672.0403 - mae: 27.8769 - val_loss: 2129.8951 - val_mse: 2129.8950 - val_mae: 25.9822\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 332us/step - loss: 2656.5004 - mse: 2656.5007 - mae: 27.9680 - val_loss: 2117.9289 - val_mse: 2117.9290 - val_mae: 26.5013\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2642.8007 - mse: 2642.8013 - mae: 27.5238 - val_loss: 2104.8203 - val_mse: 2104.8203 - val_mae: 26.7407\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2617.5225 - mse: 2617.5225 - mae: 27.5693 - val_loss: 2096.2533 - val_mse: 2096.2534 - val_mae: 27.0510\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2648.8872 - mse: 2648.8877 - mae: 27.9354 - val_loss: 2105.6694 - val_mse: 2105.6692 - val_mae: 26.4238\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 338us/step - loss: 2642.5795 - mse: 2642.5796 - mae: 28.0551 - val_loss: 2104.9613 - val_mse: 2104.9612 - val_mae: 26.4399\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2687.1670 - mse: 2687.1672 - mae: 27.8661 - val_loss: 2111.9282 - val_mse: 2111.9280 - val_mae: 26.1801\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 307us/step - loss: 2630.5636 - mse: 2630.5642 - mae: 27.9358 - val_loss: 2099.4567 - val_mse: 2099.4563 - val_mae: 26.5006\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2665.7761 - mse: 2665.7769 - mae: 28.0454 - val_loss: 2104.6424 - val_mse: 2104.6421 - val_mae: 26.6658\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2585.4804 - mse: 2585.4807 - mae: 27.4483 - val_loss: 2103.2211 - val_mse: 2103.2214 - val_mae: 26.6495\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2627.0522 - mse: 2627.0527 - mae: 27.5438 - val_loss: 2106.7963 - val_mse: 2106.7966 - val_mae: 26.4584\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 266us/step - loss: 2634.8359 - mse: 2634.8367 - mae: 27.6736 - val_loss: 2112.2736 - val_mse: 2112.2732 - val_mae: 26.3149\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 270us/step - loss: 2636.4357 - mse: 2636.4358 - mae: 27.5628 - val_loss: 2107.9003 - val_mse: 2107.9001 - val_mae: 26.4546\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2602.6760 - mse: 2602.6763 - mae: 27.7899 - val_loss: 2099.9390 - val_mse: 2099.9387 - val_mae: 26.7031\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2575.3594 - mse: 2575.3594 - mae: 27.3880 - val_loss: 2093.8249 - val_mse: 2093.8252 - val_mae: 26.7072\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2622.1846 - mse: 2622.1843 - mae: 27.5812 - val_loss: 2098.9260 - val_mse: 2098.9260 - val_mae: 26.7918\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2552.5425 - mse: 2552.5435 - mae: 27.3498 - val_loss: 2093.0326 - val_mse: 2093.0325 - val_mae: 27.0126\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2613.8000 - mse: 2613.8005 - mae: 27.5894 - val_loss: 2083.3877 - val_mse: 2083.3879 - val_mae: 26.6592\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2657.6275 - mse: 2657.6274 - mae: 28.2755 - val_loss: 2094.3667 - val_mse: 2094.3665 - val_mae: 25.9188\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2657.5665 - mse: 2657.5662 - mae: 27.8508 - val_loss: 2094.2168 - val_mse: 2094.2168 - val_mae: 26.2658\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2638.5599 - mse: 2638.5596 - mae: 27.7599 - val_loss: 2091.8084 - val_mse: 2091.8086 - val_mae: 26.4725\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 264us/step - loss: 2638.7562 - mse: 2638.7563 - mae: 27.5311 - val_loss: 2107.6420 - val_mse: 2107.6421 - val_mae: 25.9769\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2610.0641 - mse: 2610.0640 - mae: 27.5388 - val_loss: 2090.9943 - val_mse: 2090.9939 - val_mae: 26.7712\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 312us/step - loss: 2640.8421 - mse: 2640.8423 - mae: 27.9787 - val_loss: 2100.5094 - val_mse: 2100.5093 - val_mae: 26.2461\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 336us/step - loss: 2607.8461 - mse: 2607.8459 - mae: 27.5375 - val_loss: 2094.2512 - val_mse: 2094.2510 - val_mae: 26.0528\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 263us/step - loss: 2639.4210 - mse: 2639.4204 - mae: 27.4323 - val_loss: 2110.1039 - val_mse: 2110.1035 - val_mae: 26.1932\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 267us/step - loss: 2637.3374 - mse: 2637.3372 - mae: 27.6395 - val_loss: 2102.4580 - val_mse: 2102.4578 - val_mae: 26.3178\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2644.5585 - mse: 2644.5586 - mae: 27.7047 - val_loss: 2116.4398 - val_mse: 2116.4402 - val_mae: 25.9164\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 344us/step - loss: 2575.4791 - mse: 2575.4795 - mae: 27.3316 - val_loss: 2104.8332 - val_mse: 2104.8335 - val_mae: 26.5470\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2603.2119 - mse: 2603.2114 - mae: 27.7291 - val_loss: 2091.5987 - val_mse: 2091.5986 - val_mae: 26.8595\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13310.2959 - mse: 13310.2959 - mae: 109.8256 - val_loss: 34571.4426 - val_mse: 34571.4414 - val_mae: 132.5512\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 13090.0956 - mse: 13090.0947 - mae: 108.8167 - val_loss: 34172.9032 - val_mse: 34172.9023 - val_mae: 131.0319\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 316us/step - loss: 12492.3104 - mse: 12492.3105 - mae: 106.0279 - val_loss: 32997.1689 - val_mse: 32997.1680 - val_mae: 126.4377\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 10964.7503 - mse: 10964.7490 - mae: 98.3366 - val_loss: 29792.3741 - val_mse: 29792.3730 - val_mae: 112.9420\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 7079.9536 - mse: 7079.9541 - mae: 75.0187 - val_loss: 23277.4044 - val_mse: 23277.4043 - val_mae: 78.4979\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 3386.9294 - mse: 3386.9299 - mae: 44.9922 - val_loss: 18135.1008 - val_mse: 18135.0996 - val_mae: 38.2408\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 363us/step - loss: 2656.5066 - mse: 2656.5066 - mae: 37.8291 - val_loss: 18181.7489 - val_mse: 18181.7480 - val_mae: 38.4493\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 276us/step - loss: 2684.4722 - mse: 2684.4722 - mae: 36.6267 - val_loss: 18136.8981 - val_mse: 18136.8984 - val_mae: 38.2343\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2750.2399 - mse: 2750.2402 - mae: 38.6832 - val_loss: 18498.7195 - val_mse: 18498.7207 - val_mae: 40.2052\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 229us/step - loss: 2438.8797 - mse: 2438.8796 - mae: 36.0600 - val_loss: 18112.4621 - val_mse: 18112.4629 - val_mae: 38.1400\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 260us/step - loss: 2849.2111 - mse: 2849.2112 - mae: 39.1052 - val_loss: 18226.1147 - val_mse: 18226.1152 - val_mae: 38.6620\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2665.6003 - mse: 2665.6006 - mae: 36.3680 - val_loss: 18256.3497 - val_mse: 18256.3496 - val_mae: 38.8041\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 327us/step - loss: 2478.4553 - mse: 2478.4553 - mae: 35.9166 - val_loss: 18070.0156 - val_mse: 18070.0156 - val_mae: 37.9396\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2683.4600 - mse: 2683.4600 - mae: 36.8583 - val_loss: 18397.7626 - val_mse: 18397.7617 - val_mae: 39.6063\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2505.3049 - mse: 2505.3047 - mae: 35.1777 - val_loss: 18215.7081 - val_mse: 18215.7070 - val_mae: 38.6644\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 290us/step - loss: 2377.4816 - mse: 2377.4817 - mae: 34.9950 - val_loss: 18166.5600 - val_mse: 18166.5625 - val_mae: 38.4404\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2549.7487 - mse: 2549.7490 - mae: 36.2740 - val_loss: 18201.2930 - val_mse: 18201.2930 - val_mae: 38.6202\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2695.6500 - mse: 2695.6501 - mae: 36.5732 - val_loss: 18266.1515 - val_mse: 18266.1523 - val_mae: 38.9189\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 234us/step - loss: 2488.2617 - mse: 2488.2617 - mae: 34.2456 - val_loss: 18244.2448 - val_mse: 18244.2461 - val_mae: 38.8356\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2359.5749 - mse: 2359.5747 - mae: 35.0227 - val_loss: 18265.3858 - val_mse: 18265.3867 - val_mae: 38.9797\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 345us/step - loss: 2399.5886 - mse: 2399.5889 - mae: 33.3604 - val_loss: 18065.8013 - val_mse: 18065.8008 - val_mae: 38.0278\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2513.2010 - mse: 2513.2009 - mae: 34.7909 - val_loss: 18119.6320 - val_mse: 18119.6328 - val_mae: 38.2611\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2563.2894 - mse: 2563.2893 - mae: 37.7859 - val_loss: 18356.3342 - val_mse: 18356.3340 - val_mae: 39.5115\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2492.3839 - mse: 2492.3840 - mae: 35.8785 - val_loss: 18289.6166 - val_mse: 18289.6152 - val_mae: 39.1148\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 381us/step - loss: 2009.5729 - mse: 2009.5726 - mae: 31.9636 - val_loss: 17879.6102 - val_mse: 17879.6094 - val_mae: 37.4876\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 371us/step - loss: 2071.6123 - mse: 2071.6123 - mae: 33.3418 - val_loss: 18047.1211 - val_mse: 18047.1191 - val_mae: 37.9850\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2271.8173 - mse: 2271.8169 - mae: 33.6729 - val_loss: 18107.0759 - val_mse: 18107.0762 - val_mae: 38.3090\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2353.2844 - mse: 2353.2847 - mae: 34.1808 - val_loss: 18062.3655 - val_mse: 18062.3652 - val_mae: 38.1324\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 281us/step - loss: 2296.9586 - mse: 2296.9585 - mae: 35.2546 - val_loss: 18110.7788 - val_mse: 18110.7793 - val_mae: 38.3224\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 307us/step - loss: 2556.9843 - mse: 2556.9846 - mae: 35.8868 - val_loss: 18149.2694 - val_mse: 18149.2695 - val_mae: 38.5187\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 321us/step - loss: 2690.5885 - mse: 2690.5884 - mae: 36.7393 - val_loss: 18301.0357 - val_mse: 18301.0352 - val_mae: 39.2875\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2688.9947 - mse: 2688.9951 - mae: 37.1813 - val_loss: 18259.3875 - val_mse: 18259.3887 - val_mae: 38.9679\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 2395.4457 - mse: 2395.4458 - mae: 34.4662 - val_loss: 18009.6311 - val_mse: 18009.6328 - val_mae: 37.8081\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2477.1116 - mse: 2477.1116 - mae: 35.0575 - val_loss: 18174.5577 - val_mse: 18174.5586 - val_mae: 38.4941\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 2479.1310 - mse: 2479.1311 - mae: 35.4160 - val_loss: 18164.0063 - val_mse: 18164.0059 - val_mae: 38.4668\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 318us/step - loss: 2497.9091 - mse: 2497.9089 - mae: 34.2907 - val_loss: 18173.8536 - val_mse: 18173.8535 - val_mae: 38.5223\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 235us/step - loss: 2048.3538 - mse: 2048.3538 - mae: 31.9347 - val_loss: 17799.9013 - val_mse: 17799.9004 - val_mae: 37.4959\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2280.6441 - mse: 2280.6438 - mae: 33.7150 - val_loss: 18124.7530 - val_mse: 18124.7539 - val_mae: 38.3012\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 397us/step - loss: 2407.0331 - mse: 2407.0327 - mae: 34.0613 - val_loss: 17985.8042 - val_mse: 17985.8047 - val_mae: 37.7689\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 235us/step - loss: 2155.4663 - mse: 2155.4661 - mae: 32.6592 - val_loss: 18134.9606 - val_mse: 18134.9629 - val_mae: 38.3637\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 239us/step - loss: 2191.9546 - mse: 2191.9546 - mae: 32.5584 - val_loss: 18048.6523 - val_mse: 18048.6523 - val_mae: 38.0122\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 388us/step - loss: 2095.7177 - mse: 2095.7175 - mae: 33.5390 - val_loss: 18192.9957 - val_mse: 18192.9961 - val_mae: 38.6888\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 284us/step - loss: 2160.2623 - mse: 2160.2625 - mae: 32.1320 - val_loss: 18073.4830 - val_mse: 18073.4824 - val_mae: 38.1206\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 329us/step - loss: 2105.1022 - mse: 2105.1025 - mae: 32.6263 - val_loss: 17823.7928 - val_mse: 17823.7930 - val_mae: 37.5468\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2206.2073 - mse: 2206.2073 - mae: 33.3893 - val_loss: 18009.3106 - val_mse: 18009.3105 - val_mae: 37.8966\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 290us/step - loss: 2311.7578 - mse: 2311.7578 - mae: 32.9540 - val_loss: 18054.4140 - val_mse: 18054.4141 - val_mae: 38.0758\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 2256.9344 - mse: 2256.9348 - mae: 32.3872 - val_loss: 17941.8404 - val_mse: 17941.8398 - val_mae: 37.7175\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 230us/step - loss: 2133.9502 - mse: 2133.9502 - mae: 32.4671 - val_loss: 18171.5477 - val_mse: 18171.5508 - val_mae: 38.5889\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 235us/step - loss: 2258.8494 - mse: 2258.8496 - mae: 33.6177 - val_loss: 17869.0578 - val_mse: 17869.0586 - val_mae: 37.4908\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2038.3446 - mse: 2038.3445 - mae: 31.5844 - val_loss: 17928.9657 - val_mse: 17928.9648 - val_mae: 37.6045\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 384us/step - loss: 2158.2404 - mse: 2158.2407 - mae: 32.1294 - val_loss: 17914.3334 - val_mse: 17914.3320 - val_mae: 37.5486\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 387us/step - loss: 2220.6776 - mse: 2220.6777 - mae: 33.4420 - val_loss: 18033.6245 - val_mse: 18033.6230 - val_mae: 37.9181\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 314us/step - loss: 2025.8312 - mse: 2025.8313 - mae: 30.8459 - val_loss: 17773.9112 - val_mse: 17773.9121 - val_mae: 37.3923\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 301us/step - loss: 2182.6305 - mse: 2182.6309 - mae: 31.8340 - val_loss: 17945.3662 - val_mse: 17945.3633 - val_mae: 37.5110\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 364us/step - loss: 2183.6715 - mse: 2183.6709 - mae: 32.1873 - val_loss: 18078.1638 - val_mse: 18078.1641 - val_mae: 38.0613\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2062.6841 - mse: 2062.6841 - mae: 32.2828 - val_loss: 17928.4848 - val_mse: 17928.4863 - val_mae: 37.4872\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 241us/step - loss: 1931.5076 - mse: 1931.5077 - mae: 29.4606 - val_loss: 17898.7041 - val_mse: 17898.7012 - val_mae: 37.4151\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 339us/step - loss: 1962.4603 - mse: 1962.4603 - mae: 30.2716 - val_loss: 17843.5441 - val_mse: 17843.5449 - val_mae: 37.3174\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 1948.9710 - mse: 1948.9711 - mae: 31.0084 - val_loss: 17739.2263 - val_mse: 17739.2246 - val_mae: 37.3061\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 237us/step - loss: 1993.7342 - mse: 1993.7340 - mae: 30.7680 - val_loss: 17881.4122 - val_mse: 17881.4102 - val_mae: 37.4105\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 333us/step - loss: 2014.1705 - mse: 2014.1707 - mae: 31.6514 - val_loss: 17797.5127 - val_mse: 17797.5117 - val_mae: 37.3688\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 302us/step - loss: 2065.2180 - mse: 2065.2180 - mae: 32.0255 - val_loss: 17890.8519 - val_mse: 17890.8516 - val_mae: 37.4726\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 283us/step - loss: 2042.9978 - mse: 2042.9977 - mae: 31.4438 - val_loss: 17865.9345 - val_mse: 17865.9336 - val_mae: 37.4387\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 234us/step - loss: 1945.4052 - mse: 1945.4055 - mae: 29.8565 - val_loss: 17859.9274 - val_mse: 17859.9277 - val_mae: 37.4519\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 312us/step - loss: 2073.6696 - mse: 2073.6694 - mae: 31.9908 - val_loss: 17909.4574 - val_mse: 17909.4551 - val_mae: 37.5310\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 313us/step - loss: 2006.3956 - mse: 2006.3956 - mae: 31.2467 - val_loss: 17769.3432 - val_mse: 17769.3438 - val_mae: 37.3920\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 320us/step - loss: 2128.1802 - mse: 2128.1802 - mae: 31.2535 - val_loss: 18012.2891 - val_mse: 18012.2891 - val_mae: 37.9660\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 347us/step - loss: 1947.7448 - mse: 1947.7448 - mae: 30.8573 - val_loss: 17758.2158 - val_mse: 17758.2188 - val_mae: 37.3076\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 243us/step - loss: 1944.4549 - mse: 1944.4547 - mae: 30.0411 - val_loss: 17733.7754 - val_mse: 17733.7754 - val_mae: 37.2734\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 1928.6454 - mse: 1928.6455 - mae: 29.9270 - val_loss: 17720.8717 - val_mse: 17720.8711 - val_mae: 37.2855\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 2196.4139 - mse: 2196.4141 - mae: 32.1095 - val_loss: 17937.7912 - val_mse: 17937.7930 - val_mae: 37.6255\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 305us/step - loss: 2047.2279 - mse: 2047.2278 - mae: 31.3407 - val_loss: 17706.4334 - val_mse: 17706.4336 - val_mae: 37.2242\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 1946.7226 - mse: 1946.7227 - mae: 31.1002 - val_loss: 17831.0892 - val_mse: 17831.0898 - val_mae: 37.2579\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 1872.6145 - mse: 1872.6146 - mae: 29.1916 - val_loss: 17715.3444 - val_mse: 17715.3457 - val_mae: 37.1117\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 244us/step - loss: 1978.4794 - mse: 1978.4795 - mae: 30.6388 - val_loss: 17661.6790 - val_mse: 17661.6777 - val_mae: 37.0874\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 1923.8436 - mse: 1923.8436 - mae: 29.7745 - val_loss: 17681.7747 - val_mse: 17681.7754 - val_mae: 37.0517\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 1786.2721 - mse: 1786.2721 - mae: 29.1918 - val_loss: 17738.0370 - val_mse: 17738.0352 - val_mae: 37.1074\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 292us/step - loss: 2018.5994 - mse: 2018.5995 - mae: 31.0550 - val_loss: 17860.6979 - val_mse: 17860.6973 - val_mae: 37.2819\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 231us/step - loss: 1979.5194 - mse: 1979.5193 - mae: 31.2696 - val_loss: 17859.7394 - val_mse: 17859.7383 - val_mae: 37.2737\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 221us/step - loss: 1839.1803 - mse: 1839.1803 - mae: 29.2357 - val_loss: 17646.8005 - val_mse: 17646.8027 - val_mae: 37.0691\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4179.5991 - mse: 4179.5991 - mae: 34.2391 - val_loss: 2027.8177 - val_mse: 2027.8179 - val_mae: 30.4143\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 4168.6062 - mse: 4168.6064 - mae: 35.0921 - val_loss: 2163.8127 - val_mse: 2163.8127 - val_mae: 30.8740\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 263us/step - loss: 4273.5004 - mse: 4273.5010 - mae: 35.0577 - val_loss: 2167.9461 - val_mse: 2167.9463 - val_mae: 30.8914\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 235us/step - loss: 4122.8470 - mse: 4122.8467 - mae: 34.1631 - val_loss: 2190.6100 - val_mse: 2190.6101 - val_mae: 30.9622\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 276us/step - loss: 4097.5034 - mse: 4097.5029 - mae: 34.4611 - val_loss: 2261.3338 - val_mse: 2261.3337 - val_mae: 31.2150\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4308.0267 - mse: 4308.0259 - mae: 35.5779 - val_loss: 2259.8354 - val_mse: 2259.8354 - val_mae: 31.1811\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4090.5079 - mse: 4090.5081 - mae: 34.0746 - val_loss: 2278.2127 - val_mse: 2278.2126 - val_mae: 31.2488\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 3947.2393 - mse: 3947.2400 - mae: 33.5730 - val_loss: 2178.1847 - val_mse: 2178.1851 - val_mae: 30.9027\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4152.7455 - mse: 4152.7456 - mae: 35.8664 - val_loss: 2268.7281 - val_mse: 2268.7283 - val_mae: 31.2249\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 234us/step - loss: 3958.2130 - mse: 3958.2131 - mae: 34.1202 - val_loss: 2215.6354 - val_mse: 2215.6355 - val_mae: 31.0500\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 263us/step - loss: 4236.1759 - mse: 4236.1763 - mae: 34.8732 - val_loss: 2243.8673 - val_mse: 2243.8674 - val_mae: 31.1509\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4091.6068 - mse: 4091.6069 - mae: 34.1940 - val_loss: 2203.2903 - val_mse: 2203.2905 - val_mae: 31.0161\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 4203.5423 - mse: 4203.5425 - mae: 34.9495 - val_loss: 2241.7632 - val_mse: 2241.7632 - val_mae: 31.1408\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4131.6898 - mse: 4131.6899 - mae: 34.9735 - val_loss: 2219.0502 - val_mse: 2219.0503 - val_mae: 31.0637\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 249us/step - loss: 4213.5269 - mse: 4213.5264 - mae: 35.0195 - val_loss: 2240.0918 - val_mse: 2240.0920 - val_mae: 31.1313\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 266us/step - loss: 4187.4310 - mse: 4187.4307 - mae: 33.9434 - val_loss: 2273.6697 - val_mse: 2273.6697 - val_mae: 31.2411\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4235.6754 - mse: 4235.6748 - mae: 34.5973 - val_loss: 2265.9245 - val_mse: 2265.9243 - val_mae: 31.2170\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4032.3563 - mse: 4032.3560 - mae: 33.8157 - val_loss: 2234.6964 - val_mse: 2234.6963 - val_mae: 31.1170\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4212.3217 - mse: 4212.3213 - mae: 34.2941 - val_loss: 2272.7472 - val_mse: 2272.7473 - val_mae: 31.2324\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 333us/step - loss: 3961.9809 - mse: 3961.9807 - mae: 33.6463 - val_loss: 2205.9067 - val_mse: 2205.9067 - val_mae: 31.0117\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 4030.9790 - mse: 4030.9795 - mae: 34.8991 - val_loss: 2319.4534 - val_mse: 2319.4536 - val_mae: 31.4066\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4032.2062 - mse: 4032.2065 - mae: 34.0013 - val_loss: 2202.5251 - val_mse: 2202.5251 - val_mae: 30.9902\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 232us/step - loss: 4094.1971 - mse: 4094.1973 - mae: 33.1955 - val_loss: 2248.6569 - val_mse: 2248.6567 - val_mae: 31.1629\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 325us/step - loss: 3973.9874 - mse: 3973.9875 - mae: 32.9724 - val_loss: 2221.0871 - val_mse: 2221.0869 - val_mae: 31.0695\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 363us/step - loss: 4231.5018 - mse: 4231.5020 - mae: 35.5044 - val_loss: 2214.8149 - val_mse: 2214.8149 - val_mae: 31.0510\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 395us/step - loss: 4046.1274 - mse: 4046.1279 - mae: 33.6617 - val_loss: 2217.9869 - val_mse: 2217.9868 - val_mae: 31.0808\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 386us/step - loss: 3973.4698 - mse: 3973.4700 - mae: 34.1998 - val_loss: 2200.2306 - val_mse: 2200.2305 - val_mae: 31.0111\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 4110.1072 - mse: 4110.1074 - mae: 34.1148 - val_loss: 2241.9723 - val_mse: 2241.9722 - val_mae: 31.1515\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4174.1098 - mse: 4174.1104 - mae: 34.6435 - val_loss: 2256.4899 - val_mse: 2256.4900 - val_mae: 31.1972\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 222us/step - loss: 4000.5310 - mse: 4000.5298 - mae: 33.5371 - val_loss: 2191.6269 - val_mse: 2191.6267 - val_mae: 30.9702\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 223us/step - loss: 3977.2594 - mse: 3977.2585 - mae: 33.5504 - val_loss: 2271.0074 - val_mse: 2271.0073 - val_mae: 31.2524\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 226us/step - loss: 4216.0494 - mse: 4216.0498 - mae: 34.1778 - val_loss: 2290.4633 - val_mse: 2290.4629 - val_mae: 31.3145\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 337us/step - loss: 4134.0222 - mse: 4134.0220 - mae: 33.8639 - val_loss: 2291.6847 - val_mse: 2291.6848 - val_mae: 31.3239\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4215.5838 - mse: 4215.5845 - mae: 34.1970 - val_loss: 2311.2133 - val_mse: 2311.2131 - val_mae: 31.3869\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 3971.6374 - mse: 3971.6377 - mae: 33.5766 - val_loss: 2217.7687 - val_mse: 2217.7688 - val_mae: 31.0640\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4211.2376 - mse: 4211.2378 - mae: 34.3549 - val_loss: 2239.6067 - val_mse: 2239.6064 - val_mae: 31.1385\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 277us/step - loss: 4093.2339 - mse: 4093.2339 - mae: 33.2884 - val_loss: 2201.4652 - val_mse: 2201.4651 - val_mae: 30.9869\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4132.1158 - mse: 4132.1157 - mae: 33.9237 - val_loss: 2204.2961 - val_mse: 2204.2959 - val_mae: 31.0187\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 4107.0864 - mse: 4107.0869 - mae: 34.7001 - val_loss: 2256.1755 - val_mse: 2256.1755 - val_mae: 31.1894\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 330us/step - loss: 3875.7919 - mse: 3875.7920 - mae: 32.0234 - val_loss: 2180.5967 - val_mse: 2180.5967 - val_mae: 30.8985\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 374us/step - loss: 4185.2512 - mse: 4185.2510 - mae: 34.2580 - val_loss: 2282.1134 - val_mse: 2282.1133 - val_mae: 31.2634\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 305us/step - loss: 4061.1678 - mse: 4061.1677 - mae: 34.3224 - val_loss: 2253.0804 - val_mse: 2253.0806 - val_mae: 31.1266\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 3817.3654 - mse: 3817.3652 - mae: 32.1639 - val_loss: 2221.2262 - val_mse: 2221.2263 - val_mae: 31.0108\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 284us/step - loss: 3957.3088 - mse: 3957.3083 - mae: 33.4631 - val_loss: 2279.4008 - val_mse: 2279.4006 - val_mae: 31.2010\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4105.5475 - mse: 4105.5474 - mae: 33.6288 - val_loss: 2340.3064 - val_mse: 2340.3062 - val_mae: 31.3929\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4051.8242 - mse: 4051.8240 - mae: 34.3857 - val_loss: 2287.0436 - val_mse: 2287.0435 - val_mae: 31.2036\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 259us/step - loss: 4181.1155 - mse: 4181.1152 - mae: 33.7450 - val_loss: 2339.2362 - val_mse: 2339.2361 - val_mae: 31.3862\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 227us/step - loss: 3879.1409 - mse: 3879.1406 - mae: 32.6672 - val_loss: 2247.7899 - val_mse: 2247.7900 - val_mae: 31.0707\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 261us/step - loss: 4043.3888 - mse: 4043.3882 - mae: 33.2865 - val_loss: 2267.9881 - val_mse: 2267.9883 - val_mae: 31.1322\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 359us/step - loss: 4240.0495 - mse: 4240.0498 - mae: 34.4119 - val_loss: 2369.9663 - val_mse: 2369.9663 - val_mae: 31.5230\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 3989.8942 - mse: 3989.8945 - mae: 33.2950 - val_loss: 2233.5265 - val_mse: 2233.5266 - val_mae: 31.0440\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4162.5997 - mse: 4162.5996 - mae: 34.8947 - val_loss: 2335.5062 - val_mse: 2335.5063 - val_mae: 31.4194\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 4023.7223 - mse: 4023.7227 - mae: 33.4353 - val_loss: 2233.9030 - val_mse: 2233.9031 - val_mae: 31.0598\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 303us/step - loss: 4099.5840 - mse: 4099.5840 - mae: 33.9681 - val_loss: 2327.5059 - val_mse: 2327.5059 - val_mae: 31.3734\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 3978.7098 - mse: 3978.7100 - mae: 33.7480 - val_loss: 2248.5923 - val_mse: 2248.5923 - val_mae: 31.0883\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4130.5797 - mse: 4130.5801 - mae: 32.9250 - val_loss: 2282.2302 - val_mse: 2282.2300 - val_mae: 31.2089\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 329us/step - loss: 4078.6990 - mse: 4078.6995 - mae: 32.7121 - val_loss: 2242.1110 - val_mse: 2242.1108 - val_mae: 31.0572\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 357us/step - loss: 4270.4876 - mse: 4270.4868 - mae: 34.2093 - val_loss: 2356.6145 - val_mse: 2356.6145 - val_mae: 31.4536\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 379us/step - loss: 4010.0129 - mse: 4010.0129 - mae: 32.8830 - val_loss: 2261.0407 - val_mse: 2261.0408 - val_mae: 31.1131\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 3890.9059 - mse: 3890.9058 - mae: 33.3626 - val_loss: 2178.6999 - val_mse: 2178.7000 - val_mae: 30.8236\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 4081.9276 - mse: 4081.9275 - mae: 33.9678 - val_loss: 2294.4584 - val_mse: 2294.4583 - val_mae: 31.2374\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 339us/step - loss: 3972.7540 - mse: 3972.7532 - mae: 33.1255 - val_loss: 2279.2119 - val_mse: 2279.2119 - val_mae: 31.1817\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 335us/step - loss: 4217.0991 - mse: 4217.0991 - mae: 34.1327 - val_loss: 2355.9966 - val_mse: 2355.9968 - val_mae: 31.4418\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4028.9145 - mse: 4028.9146 - mae: 33.1145 - val_loss: 2240.0375 - val_mse: 2240.0376 - val_mae: 31.0105\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 394us/step - loss: 3943.5406 - mse: 3943.5403 - mae: 33.2579 - val_loss: 2290.6891 - val_mse: 2290.6890 - val_mae: 31.2033\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 268us/step - loss: 3939.1161 - mse: 3939.1162 - mae: 32.2307 - val_loss: 2235.3285 - val_mse: 2235.3286 - val_mae: 30.9903\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4135.6545 - mse: 4135.6538 - mae: 32.6763 - val_loss: 2284.3228 - val_mse: 2284.3228 - val_mae: 31.1601\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 337us/step - loss: 4104.3656 - mse: 4104.3657 - mae: 33.7025 - val_loss: 2287.4765 - val_mse: 2287.4768 - val_mae: 31.1398\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 3999.5604 - mse: 3999.5596 - mae: 33.0640 - val_loss: 2332.4482 - val_mse: 2332.4482 - val_mae: 31.2907\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4074.7154 - mse: 4074.7158 - mae: 32.9221 - val_loss: 2240.1270 - val_mse: 2240.1267 - val_mae: 30.9520\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4018.8898 - mse: 4018.8896 - mae: 32.4849 - val_loss: 2234.1330 - val_mse: 2234.1333 - val_mae: 30.9325\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 3915.7563 - mse: 3915.7563 - mae: 32.9553 - val_loss: 2272.3629 - val_mse: 2272.3628 - val_mae: 31.0743\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 249us/step - loss: 4167.9662 - mse: 4167.9668 - mae: 34.1211 - val_loss: 2316.4491 - val_mse: 2316.4490 - val_mae: 31.2229\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 328us/step - loss: 3881.5420 - mse: 3881.5427 - mae: 32.2135 - val_loss: 2242.0754 - val_mse: 2242.0754 - val_mae: 30.9669\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4008.9788 - mse: 4008.9785 - mae: 32.9371 - val_loss: 2241.2529 - val_mse: 2241.2532 - val_mae: 30.9587\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 4062.9149 - mse: 4062.9148 - mae: 32.6134 - val_loss: 2309.6593 - val_mse: 2309.6592 - val_mae: 31.2097\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 323us/step - loss: 4035.0309 - mse: 4035.0305 - mae: 33.3605 - val_loss: 2231.3484 - val_mse: 2231.3484 - val_mae: 30.9414\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 316us/step - loss: 3980.9961 - mse: 3980.9958 - mae: 32.8063 - val_loss: 2276.4353 - val_mse: 2276.4353 - val_mae: 31.0956\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 296us/step - loss: 3866.2176 - mse: 3866.2178 - mae: 31.6946 - val_loss: 2237.4741 - val_mse: 2237.4741 - val_mae: 30.9539\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 274us/step - loss: 3929.0293 - mse: 3929.0291 - mae: 32.4622 - val_loss: 2258.6784 - val_mse: 2258.6785 - val_mae: 31.0478\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 274us/step - loss: 3187.2676 - mse: 3187.2676 - mae: 32.6379 - val_loss: 1456.6741 - val_mse: 1456.6741 - val_mae: 25.6140\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 325us/step - loss: 3356.9939 - mse: 3356.9937 - mae: 32.1451 - val_loss: 1458.1331 - val_mse: 1458.1331 - val_mae: 25.8890\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3471.4040 - mse: 3471.4036 - mae: 33.3657 - val_loss: 1456.9560 - val_mse: 1456.9561 - val_mae: 25.3471\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 346us/step - loss: 3354.6407 - mse: 3354.6411 - mae: 32.1588 - val_loss: 1455.4476 - val_mse: 1455.4475 - val_mae: 25.6192\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 336us/step - loss: 3333.2924 - mse: 3333.2920 - mae: 32.6283 - val_loss: 1450.7709 - val_mse: 1450.7709 - val_mae: 25.4353\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3295.7782 - mse: 3295.7786 - mae: 32.2488 - val_loss: 1455.3276 - val_mse: 1455.3275 - val_mae: 26.2928\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3394.2739 - mse: 3394.2739 - mae: 32.1171 - val_loss: 1449.5893 - val_mse: 1449.5894 - val_mae: 25.7769\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 291us/step - loss: 3393.2645 - mse: 3393.2634 - mae: 33.3544 - val_loss: 1448.9201 - val_mse: 1448.9203 - val_mae: 25.8109\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3359.0710 - mse: 3359.0708 - mae: 32.4443 - val_loss: 1449.8476 - val_mse: 1449.8478 - val_mae: 25.9182\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3319.5980 - mse: 3319.5974 - mae: 31.9587 - val_loss: 1445.8339 - val_mse: 1445.8339 - val_mae: 25.2779\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3392.4278 - mse: 3392.4275 - mae: 32.3785 - val_loss: 1446.6022 - val_mse: 1446.6021 - val_mae: 25.3442\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3346.6716 - mse: 3346.6721 - mae: 31.6102 - val_loss: 1445.4970 - val_mse: 1445.4969 - val_mae: 25.3079\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3313.4678 - mse: 3313.4685 - mae: 32.6660 - val_loss: 1445.9949 - val_mse: 1445.9949 - val_mae: 25.6619\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3284.0756 - mse: 3284.0757 - mae: 32.8486 - val_loss: 1446.4580 - val_mse: 1446.4580 - val_mae: 25.5203\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3445.4159 - mse: 3445.4165 - mae: 33.0281 - val_loss: 1448.8672 - val_mse: 1448.8673 - val_mae: 26.0001\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3361.9189 - mse: 3361.9197 - mae: 33.5833 - val_loss: 1447.1304 - val_mse: 1447.1304 - val_mae: 25.8304\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 332us/step - loss: 3363.0508 - mse: 3363.0508 - mae: 32.4349 - val_loss: 1447.1328 - val_mse: 1447.1327 - val_mae: 25.8243\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 273us/step - loss: 3283.0719 - mse: 3283.0723 - mae: 31.8574 - val_loss: 1449.7505 - val_mse: 1449.7505 - val_mae: 25.9931\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 335us/step - loss: 3328.0475 - mse: 3328.0481 - mae: 32.3125 - val_loss: 1449.1012 - val_mse: 1449.1013 - val_mae: 25.5067\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 281us/step - loss: 3338.6692 - mse: 3338.6692 - mae: 32.4601 - val_loss: 1451.9496 - val_mse: 1451.9497 - val_mae: 26.0896\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3337.6579 - mse: 3337.6575 - mae: 32.8489 - val_loss: 1448.1320 - val_mse: 1448.1322 - val_mae: 25.5382\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3342.1547 - mse: 3342.1553 - mae: 32.2432 - val_loss: 1447.8907 - val_mse: 1447.8907 - val_mae: 25.5964\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3290.5992 - mse: 3290.5984 - mae: 31.2608 - val_loss: 1448.8410 - val_mse: 1448.8409 - val_mae: 25.5186\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3325.0932 - mse: 3325.0930 - mae: 32.1859 - val_loss: 1448.4008 - val_mse: 1448.4011 - val_mae: 25.6561\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3331.2514 - mse: 3331.2522 - mae: 32.6873 - val_loss: 1450.8325 - val_mse: 1450.8325 - val_mae: 25.7504\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 340us/step - loss: 3270.9156 - mse: 3270.9150 - mae: 31.9361 - val_loss: 1452.6052 - val_mse: 1452.6052 - val_mae: 25.9535\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3533.0847 - mse: 3533.0850 - mae: 33.6368 - val_loss: 1451.7760 - val_mse: 1451.7759 - val_mae: 25.5860\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 292us/step - loss: 3325.9707 - mse: 3325.9712 - mae: 32.1225 - val_loss: 1451.1119 - val_mse: 1451.1119 - val_mae: 25.7907\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3232.7821 - mse: 3232.7822 - mae: 32.2341 - val_loss: 1450.9536 - val_mse: 1450.9535 - val_mae: 25.6006\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3341.3164 - mse: 3341.3162 - mae: 32.5816 - val_loss: 1448.9798 - val_mse: 1448.9797 - val_mae: 25.4602\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 294us/step - loss: 3343.1735 - mse: 3343.1736 - mae: 31.7439 - val_loss: 1452.8591 - val_mse: 1452.8590 - val_mae: 26.0914\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3218.5807 - mse: 3218.5806 - mae: 31.7806 - val_loss: 1450.0601 - val_mse: 1450.0602 - val_mae: 25.7788\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3324.7592 - mse: 3324.7595 - mae: 32.4981 - val_loss: 1450.0505 - val_mse: 1450.0504 - val_mae: 25.8906\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3141.6644 - mse: 3141.6646 - mae: 31.1751 - val_loss: 1450.3643 - val_mse: 1450.3643 - val_mae: 25.6768\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 348us/step - loss: 3443.5569 - mse: 3443.5569 - mae: 33.1888 - val_loss: 1454.6290 - val_mse: 1454.6290 - val_mae: 24.8115\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 339us/step - loss: 3224.5796 - mse: 3224.5793 - mae: 31.2265 - val_loss: 1451.3060 - val_mse: 1451.3060 - val_mae: 25.0507\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 371us/step - loss: 3360.1055 - mse: 3360.1055 - mae: 32.3430 - val_loss: 1450.9200 - val_mse: 1450.9199 - val_mae: 25.5503\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 294us/step - loss: 3226.2995 - mse: 3226.2996 - mae: 31.3077 - val_loss: 1450.3219 - val_mse: 1450.3219 - val_mae: 25.9080\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 282us/step - loss: 3218.5456 - mse: 3218.5459 - mae: 31.6116 - val_loss: 1447.6189 - val_mse: 1447.6190 - val_mae: 25.2683\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3238.0969 - mse: 3238.0967 - mae: 31.6779 - val_loss: 1448.6539 - val_mse: 1448.6537 - val_mae: 25.3302\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 270us/step - loss: 3336.5572 - mse: 3336.5569 - mae: 32.1877 - val_loss: 1449.3332 - val_mse: 1449.3330 - val_mae: 25.6122\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 332us/step - loss: 3371.1613 - mse: 3371.1616 - mae: 32.8890 - val_loss: 1449.8428 - val_mse: 1449.8429 - val_mae: 25.3352\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 295us/step - loss: 3379.4817 - mse: 3379.4810 - mae: 31.8389 - val_loss: 1453.0241 - val_mse: 1453.0240 - val_mae: 25.9810\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 288us/step - loss: 3301.5950 - mse: 3301.5947 - mae: 32.0522 - val_loss: 1452.5167 - val_mse: 1452.5167 - val_mae: 25.7896\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 268us/step - loss: 3333.8945 - mse: 3333.8945 - mae: 32.2543 - val_loss: 1452.8680 - val_mse: 1452.8680 - val_mae: 25.9353\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 330us/step - loss: 3307.1978 - mse: 3307.1980 - mae: 31.8088 - val_loss: 1453.6003 - val_mse: 1453.6002 - val_mae: 25.6387\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3358.4750 - mse: 3358.4756 - mae: 32.1815 - val_loss: 1455.7762 - val_mse: 1455.7762 - val_mae: 25.9062\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3269.9927 - mse: 3269.9929 - mae: 32.3277 - val_loss: 1456.2027 - val_mse: 1456.2028 - val_mae: 25.9646\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 389us/step - loss: 3300.6973 - mse: 3300.6973 - mae: 31.5031 - val_loss: 1456.0617 - val_mse: 1456.0616 - val_mae: 25.8274\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 331us/step - loss: 3209.8375 - mse: 3209.8379 - mae: 31.5689 - val_loss: 1458.0943 - val_mse: 1458.0942 - val_mae: 25.8622\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 309us/step - loss: 3130.5624 - mse: 3130.5630 - mae: 31.4082 - val_loss: 1458.4431 - val_mse: 1458.4432 - val_mae: 25.9649\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 293us/step - loss: 3298.2006 - mse: 3298.2000 - mae: 31.8412 - val_loss: 1456.3608 - val_mse: 1456.3608 - val_mae: 25.7464\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 326us/step - loss: 3255.8576 - mse: 3255.8579 - mae: 31.4761 - val_loss: 1457.1636 - val_mse: 1457.1636 - val_mae: 25.8398\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 366us/step - loss: 3332.7592 - mse: 3332.7593 - mae: 31.4329 - val_loss: 1454.3534 - val_mse: 1454.3533 - val_mae: 25.4412\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3353.2147 - mse: 3353.2153 - mae: 31.4536 - val_loss: 1453.2781 - val_mse: 1453.2781 - val_mae: 25.4011\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 274us/step - loss: 3232.0108 - mse: 3232.0103 - mae: 30.8226 - val_loss: 1454.0780 - val_mse: 1454.0780 - val_mae: 25.7979\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3301.9560 - mse: 3301.9563 - mae: 32.1484 - val_loss: 1455.7226 - val_mse: 1455.7225 - val_mae: 26.0494\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 306us/step - loss: 3317.6438 - mse: 3317.6440 - mae: 32.2105 - val_loss: 1454.9278 - val_mse: 1454.9276 - val_mae: 25.9092\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3269.3808 - mse: 3269.3809 - mae: 31.9372 - val_loss: 1455.7487 - val_mse: 1455.7487 - val_mae: 25.9936\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 297us/step - loss: 3419.8570 - mse: 3419.8572 - mae: 32.9647 - val_loss: 1453.3458 - val_mse: 1453.3459 - val_mae: 25.6003\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 273us/step - loss: 3268.7297 - mse: 3268.7297 - mae: 31.5092 - val_loss: 1454.6130 - val_mse: 1454.6129 - val_mae: 25.7891\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3173.5655 - mse: 3173.5659 - mae: 31.4802 - val_loss: 1460.0015 - val_mse: 1460.0017 - val_mae: 26.4588\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3236.8524 - mse: 3236.8523 - mae: 31.5756 - val_loss: 1452.9249 - val_mse: 1452.9249 - val_mae: 25.8571\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 324us/step - loss: 3311.4854 - mse: 3311.4856 - mae: 31.9464 - val_loss: 1451.4270 - val_mse: 1451.4270 - val_mae: 25.6390\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3251.1959 - mse: 3251.1963 - mae: 31.8474 - val_loss: 1453.0016 - val_mse: 1453.0017 - val_mae: 25.6941\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3187.9206 - mse: 3187.9202 - mae: 31.2105 - val_loss: 1456.1712 - val_mse: 1456.1711 - val_mae: 25.8798\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3320.6105 - mse: 3320.6113 - mae: 32.2612 - val_loss: 1456.7231 - val_mse: 1456.7229 - val_mae: 25.8885\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3447.2464 - mse: 3447.2461 - mae: 31.92 - 0s 311us/step - loss: 3193.0771 - mse: 3193.0769 - mae: 31.4115 - val_loss: 1456.1328 - val_mse: 1456.1327 - val_mae: 25.7975\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3193.5659 - mse: 3193.5652 - mae: 30.6639 - val_loss: 1457.6123 - val_mse: 1457.6123 - val_mae: 25.5716\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3264.8834 - mse: 3264.8835 - mae: 31.2533 - val_loss: 1459.1311 - val_mse: 1459.1310 - val_mae: 26.0193\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 318us/step - loss: 3232.5043 - mse: 3232.5039 - mae: 31.3502 - val_loss: 1457.8402 - val_mse: 1457.8401 - val_mae: 25.7073\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3324.1958 - mse: 3324.1960 - mae: 31.7559 - val_loss: 1464.5586 - val_mse: 1464.5587 - val_mae: 26.4431\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3350.7142 - mse: 3350.7141 - mae: 32.1840 - val_loss: 1459.4134 - val_mse: 1459.4135 - val_mae: 25.5718\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3237.4917 - mse: 3237.4912 - mae: 31.4440 - val_loss: 1458.6472 - val_mse: 1458.6472 - val_mae: 25.7851\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3211.9896 - mse: 3211.9897 - mae: 31.5899 - val_loss: 1456.7515 - val_mse: 1456.7515 - val_mae: 25.7135\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 335us/step - loss: 3226.2209 - mse: 3226.2205 - mae: 30.9612 - val_loss: 1458.6654 - val_mse: 1458.6653 - val_mae: 25.9827\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3220.8402 - mse: 3220.8406 - mae: 31.3431 - val_loss: 1459.7286 - val_mse: 1459.7286 - val_mae: 25.9832\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3116.6631 - mse: 3116.6636 - mae: 31.4340 - val_loss: 1459.9108 - val_mse: 1459.9108 - val_mae: 25.9648\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 304us/step - loss: 3248.0735 - mse: 3248.0735 - mae: 31.7293 - val_loss: 1458.9762 - val_mse: 1458.9761 - val_mae: 25.5928\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 335us/step - loss: 3299.5782 - mse: 3299.5789 - mae: 31.9626 - val_loss: 1457.2114 - val_mse: 1457.2114 - val_mae: 25.7099\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 317us/step - loss: 2981.7153 - mse: 2981.7148 - mae: 32.1173 - val_loss: 1048.7903 - val_mse: 1048.7904 - val_mae: 23.9191\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2947.5459 - mse: 2947.5459 - mae: 30.7238 - val_loss: 1047.2049 - val_mse: 1047.2050 - val_mae: 23.9989\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2883.6318 - mse: 2883.6326 - mae: 31.0909 - val_loss: 1048.2297 - val_mse: 1048.2297 - val_mae: 23.6975\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2947.8086 - mse: 2947.8096 - mae: 31.4220 - val_loss: 1050.2534 - val_mse: 1050.2533 - val_mae: 23.4817\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2939.8339 - mse: 2939.8330 - mae: 30.6713 - val_loss: 1044.3844 - val_mse: 1044.3845 - val_mae: 23.8668\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2895.9478 - mse: 2895.9482 - mae: 30.7726 - val_loss: 1044.4853 - val_mse: 1044.4852 - val_mae: 23.7583\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2856.8298 - mse: 2856.8303 - mae: 31.0093 - val_loss: 1043.6743 - val_mse: 1043.6743 - val_mae: 23.7579\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 267us/step - loss: 2902.0127 - mse: 2902.0129 - mae: 31.3637 - val_loss: 1042.6818 - val_mse: 1042.6819 - val_mae: 23.8619\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 320us/step - loss: 2949.8580 - mse: 2949.8574 - mae: 30.8377 - val_loss: 1042.9003 - val_mse: 1042.9003 - val_mae: 23.7750\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2926.1237 - mse: 2926.1235 - mae: 31.1689 - val_loss: 1043.7102 - val_mse: 1043.7102 - val_mae: 23.5405\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2846.6775 - mse: 2846.6770 - mae: 30.3783 - val_loss: 1042.3363 - val_mse: 1042.3363 - val_mae: 23.5895\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2844.3201 - mse: 2844.3210 - mae: 30.7784 - val_loss: 1041.1828 - val_mse: 1041.1827 - val_mae: 24.0427\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 347us/step - loss: 2930.6223 - mse: 2930.6218 - mae: 31.1710 - val_loss: 1041.3275 - val_mse: 1041.3274 - val_mae: 23.9436\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2913.0598 - mse: 2913.0608 - mae: 30.5089 - val_loss: 1042.5213 - val_mse: 1042.5214 - val_mae: 23.6960\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2933.9865 - mse: 2933.9861 - mae: 30.9092 - val_loss: 1042.0716 - val_mse: 1042.0715 - val_mae: 23.6445\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2908.8279 - mse: 2908.8281 - mae: 30.8758 - val_loss: 1040.1274 - val_mse: 1040.1276 - val_mae: 23.7845\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2854.6173 - mse: 2854.6172 - mae: 30.4031 - val_loss: 1039.4202 - val_mse: 1039.4202 - val_mae: 24.1241\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 354us/step - loss: 2802.9236 - mse: 2802.9233 - mae: 30.5405 - val_loss: 1039.4056 - val_mse: 1039.4055 - val_mae: 24.0271\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2820.8866 - mse: 2820.8870 - mae: 30.5785 - val_loss: 1039.6105 - val_mse: 1039.6107 - val_mae: 24.2403\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2793.3239 - mse: 2793.3237 - mae: 31.0806 - val_loss: 1038.5509 - val_mse: 1038.5510 - val_mae: 23.7457\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 339us/step - loss: 2845.4496 - mse: 2845.4497 - mae: 30.7425 - val_loss: 1038.6905 - val_mse: 1038.6906 - val_mae: 23.7825\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 346us/step - loss: 2906.7733 - mse: 2906.7732 - mae: 30.9408 - val_loss: 1039.1388 - val_mse: 1039.1387 - val_mae: 23.6815\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 275us/step - loss: 2931.9991 - mse: 2931.9990 - mae: 30.4762 - val_loss: 1039.3880 - val_mse: 1039.3878 - val_mae: 23.5840\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2888.6783 - mse: 2888.6790 - mae: 30.8832 - val_loss: 1037.5686 - val_mse: 1037.5687 - val_mae: 23.8994\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2885.8353 - mse: 2885.8359 - mae: 31.1227 - val_loss: 1038.4701 - val_mse: 1038.4701 - val_mae: 23.5713\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2845.1554 - mse: 2845.1560 - mae: 30.2467 - val_loss: 1036.5623 - val_mse: 1036.5624 - val_mae: 23.8754\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2845.6027 - mse: 2845.6025 - mae: 30.3400 - val_loss: 1036.2121 - val_mse: 1036.2122 - val_mae: 23.8620\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2863.7007 - mse: 2863.6995 - mae: 31.2755 - val_loss: 1035.7765 - val_mse: 1035.7766 - val_mae: 23.7134\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 353us/step - loss: 2840.8536 - mse: 2840.8525 - mae: 30.9810 - val_loss: 1035.6038 - val_mse: 1035.6038 - val_mae: 23.5557\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2845.8793 - mse: 2845.8787 - mae: 30.6058 - val_loss: 1034.3809 - val_mse: 1034.3809 - val_mae: 23.7238\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2869.5016 - mse: 2869.5022 - mae: 30.8868 - val_loss: 1033.7541 - val_mse: 1033.7540 - val_mae: 23.8626\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2852.0825 - mse: 2852.0823 - mae: 30.6278 - val_loss: 1033.4517 - val_mse: 1033.4519 - val_mae: 23.7338\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 328us/step - loss: 2809.6253 - mse: 2809.6255 - mae: 30.5813 - val_loss: 1033.1555 - val_mse: 1033.1553 - val_mae: 24.0245\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 343us/step - loss: 2847.2374 - mse: 2847.2373 - mae: 31.1120 - val_loss: 1033.4478 - val_mse: 1033.4478 - val_mae: 23.6479\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 264us/step - loss: 2838.2090 - mse: 2838.2087 - mae: 30.7165 - val_loss: 1032.7689 - val_mse: 1032.7689 - val_mae: 23.9928\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 0s 248us/step - loss: 2833.1481 - mse: 2833.1482 - mae: 30.4697 - val_loss: 1032.5529 - val_mse: 1032.5530 - val_mae: 24.0603\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 310us/step - loss: 2859.7436 - mse: 2859.7437 - mae: 30.2678 - val_loss: 1032.5248 - val_mse: 1032.5248 - val_mae: 23.7783\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2811.7149 - mse: 2811.7156 - mae: 30.4061 - val_loss: 1032.8119 - val_mse: 1032.8120 - val_mae: 24.2679\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 282us/step - loss: 2862.5281 - mse: 2862.5281 - mae: 30.7641 - val_loss: 1035.7837 - val_mse: 1035.7836 - val_mae: 23.2194\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 288us/step - loss: 2821.0642 - mse: 2821.0647 - mae: 30.5212 - val_loss: 1030.2917 - val_mse: 1030.2917 - val_mae: 23.8166\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2818.1583 - mse: 2818.1584 - mae: 30.3440 - val_loss: 1030.1729 - val_mse: 1030.1729 - val_mae: 23.7539\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2803.5604 - mse: 2803.5596 - mae: 30.6110 - val_loss: 1030.9016 - val_mse: 1030.9016 - val_mae: 23.5550\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2871.9127 - mse: 2871.9138 - mae: 30.4179 - val_loss: 1028.8643 - val_mse: 1028.8644 - val_mae: 24.0495\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 330us/step - loss: 2798.8365 - mse: 2798.8362 - mae: 30.5079 - val_loss: 1029.0098 - val_mse: 1029.0098 - val_mae: 24.2094\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 325us/step - loss: 2767.3318 - mse: 2767.3320 - mae: 29.7498 - val_loss: 1028.9595 - val_mse: 1028.9595 - val_mae: 24.2888\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2904.6897 - mse: 2904.6904 - mae: 30.6640 - val_loss: 1027.6449 - val_mse: 1027.6449 - val_mae: 23.5654\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 337us/step - loss: 2843.1544 - mse: 2843.1548 - mae: 30.2682 - val_loss: 1026.8275 - val_mse: 1026.8275 - val_mae: 23.6737\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2924.6723 - mse: 2924.6719 - mae: 30.6571 - val_loss: 1026.6235 - val_mse: 1026.6235 - val_mae: 23.7252\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2819.6429 - mse: 2819.6433 - mae: 30.5872 - val_loss: 1026.8905 - val_mse: 1026.8905 - val_mae: 23.8040\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 355us/step - loss: 2822.6456 - mse: 2822.6455 - mae: 30.7515 - val_loss: 1026.8238 - val_mse: 1026.8239 - val_mae: 23.7247\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2911.8200 - mse: 2911.8206 - mae: 30.9668 - val_loss: 1027.0563 - val_mse: 1027.0564 - val_mae: 23.6334\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2818.2202 - mse: 2818.2209 - mae: 30.4495 - val_loss: 1026.1743 - val_mse: 1026.1742 - val_mae: 23.8087\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 278us/step - loss: 2850.1722 - mse: 2850.1721 - mae: 31.0319 - val_loss: 1025.9948 - val_mse: 1025.9949 - val_mae: 23.9442\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 252us/step - loss: 2848.3657 - mse: 2848.3657 - mae: 30.1322 - val_loss: 1025.8431 - val_mse: 1025.8431 - val_mae: 24.0758\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 332us/step - loss: 2867.8002 - mse: 2867.7996 - mae: 30.3063 - val_loss: 1025.3482 - val_mse: 1025.3484 - val_mae: 23.9859\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 0s 248us/step - loss: 2790.4957 - mse: 2790.4958 - mae: 29.8209 - val_loss: 1025.7052 - val_mse: 1025.7051 - val_mae: 24.1876\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 265us/step - loss: 2805.4988 - mse: 2805.4985 - mae: 30.0430 - val_loss: 1025.1605 - val_mse: 1025.1604 - val_mae: 24.0893\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 289us/step - loss: 2916.4545 - mse: 2916.4546 - mae: 30.8811 - val_loss: 1024.9423 - val_mse: 1024.9424 - val_mae: 23.5407\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2857.0896 - mse: 2857.0898 - mae: 30.6614 - val_loss: 1023.8094 - val_mse: 1023.8093 - val_mae: 23.6508\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2728.4396 - mse: 2728.4397 - mae: 29.6960 - val_loss: 1023.3883 - val_mse: 1023.3884 - val_mae: 23.7598\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2800.8367 - mse: 2800.8379 - mae: 29.9980 - val_loss: 1022.8227 - val_mse: 1022.8227 - val_mae: 23.5601\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 277us/step - loss: 2861.6427 - mse: 2861.6423 - mae: 30.5543 - val_loss: 1024.5612 - val_mse: 1024.5614 - val_mae: 23.3510\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 281us/step - loss: 2819.6640 - mse: 2819.6628 - mae: 29.9825 - val_loss: 1022.1237 - val_mse: 1022.1237 - val_mae: 24.1631\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2819.4842 - mse: 2819.4834 - mae: 30.1078 - val_loss: 1021.1947 - val_mse: 1021.1946 - val_mae: 23.5110\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 332us/step - loss: 2843.7661 - mse: 2843.7671 - mae: 30.4648 - val_loss: 1019.8098 - val_mse: 1019.8098 - val_mae: 23.9882\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 0s 242us/step - loss: 2855.3159 - mse: 2855.3159 - mae: 30.4146 - val_loss: 1019.8216 - val_mse: 1019.8215 - val_mae: 23.5644\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2864.0669 - mse: 2864.0674 - mae: 30.2227 - val_loss: 1018.5439 - val_mse: 1018.5438 - val_mae: 23.7446\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2818.9021 - mse: 2818.9023 - mae: 29.8980 - val_loss: 1019.5981 - val_mse: 1019.5981 - val_mae: 24.1162\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2833.0580 - mse: 2833.0571 - mae: 30.4445 - val_loss: 1018.2716 - val_mse: 1018.2716 - val_mae: 23.8683\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2789.2852 - mse: 2789.2856 - mae: 29.8696 - val_loss: 1017.7049 - val_mse: 1017.7049 - val_mae: 23.5903\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2825.4143 - mse: 2825.4143 - mae: 30.0613 - val_loss: 1017.9671 - val_mse: 1017.9670 - val_mae: 23.6988\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2777.8659 - mse: 2777.8660 - mae: 30.0248 - val_loss: 1018.2813 - val_mse: 1018.2813 - val_mae: 23.9378\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 259us/step - loss: 2765.3275 - mse: 2765.3276 - mae: 30.6666 - val_loss: 1017.5284 - val_mse: 1017.5284 - val_mae: 23.5653\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2817.6029 - mse: 2817.6018 - mae: 30.0972 - val_loss: 1016.7731 - val_mse: 1016.7731 - val_mae: 23.5935\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2833.3160 - mse: 2833.3157 - mae: 30.1574 - val_loss: 1015.4211 - val_mse: 1015.4210 - val_mae: 23.8024\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2810.7518 - mse: 2810.7512 - mae: 30.2524 - val_loss: 1015.6209 - val_mse: 1015.6209 - val_mae: 23.4569\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 275us/step - loss: 2746.1568 - mse: 2746.1570 - mae: 30.0403 - val_loss: 1014.8763 - val_mse: 1014.8763 - val_mae: 23.6596\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2879.5984 - mse: 2879.5991 - mae: 30.7724 - val_loss: 1014.5874 - val_mse: 1014.5875 - val_mae: 23.3986\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 277us/step - loss: 2804.8298 - mse: 2804.8306 - mae: 30.1701 - val_loss: 1013.1398 - val_mse: 1013.1398 - val_mae: 23.5543\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 0s 228us/step - loss: 2763.6810 - mse: 2763.6809 - mae: 30.3244 - val_loss: 1012.4749 - val_mse: 1012.4749 - val_mae: 23.5433\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2515.0330 - mse: 2515.0325 - mae: 29.6814 - val_loss: 1495.1924 - val_mse: 1495.1924 - val_mae: 27.1788\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2500.0051 - mse: 2500.0042 - mae: 29.4263 - val_loss: 1484.9157 - val_mse: 1484.9158 - val_mae: 27.4733\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2517.8088 - mse: 2517.8083 - mae: 29.6121 - val_loss: 1491.8920 - val_mse: 1491.8920 - val_mae: 27.1776\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 315us/step - loss: 2473.2727 - mse: 2473.2720 - mae: 29.1266 - val_loss: 1479.8037 - val_mse: 1479.8038 - val_mae: 27.4924\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2517.8703 - mse: 2517.8704 - mae: 29.3286 - val_loss: 1483.3811 - val_mse: 1483.3810 - val_mae: 27.2510\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2528.5158 - mse: 2528.5151 - mae: 29.5467 - val_loss: 1480.7672 - val_mse: 1480.7672 - val_mae: 27.2485\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2503.7370 - mse: 2503.7373 - mae: 29.5801 - val_loss: 1479.7176 - val_mse: 1479.7177 - val_mae: 27.2306\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 255us/step - loss: 2498.6906 - mse: 2498.6907 - mae: 29.5707 - val_loss: 1481.0986 - val_mse: 1481.0986 - val_mae: 27.1583\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2477.8620 - mse: 2477.8618 - mae: 29.1667 - val_loss: 1478.9493 - val_mse: 1478.9493 - val_mae: 27.1894\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2514.7722 - mse: 2514.7722 - mae: 29.5138 - val_loss: 1478.9430 - val_mse: 1478.9431 - val_mae: 27.1409\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2407.0966 - mse: 2407.0969 - mae: 29.1271 - val_loss: 1468.9421 - val_mse: 1468.9421 - val_mae: 27.3831\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 322us/step - loss: 2491.5859 - mse: 2491.5867 - mae: 29.1171 - val_loss: 1468.4480 - val_mse: 1468.4481 - val_mae: 27.3445\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2442.2133 - mse: 2442.2141 - mae: 29.2674 - val_loss: 1478.5454 - val_mse: 1478.5455 - val_mae: 27.0084\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2455.1561 - mse: 2455.1560 - mae: 29.0416 - val_loss: 1470.5685 - val_mse: 1470.5685 - val_mae: 27.1650\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2482.9206 - mse: 2482.9202 - mae: 29.0859 - val_loss: 1465.3271 - val_mse: 1465.3270 - val_mae: 27.2791\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 320us/step - loss: 2479.5750 - mse: 2479.5754 - mae: 29.2397 - val_loss: 1472.1754 - val_mse: 1472.1754 - val_mae: 27.0070\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2522.5765 - mse: 2522.5764 - mae: 29.2140 - val_loss: 1475.8695 - val_mse: 1475.8693 - val_mae: 26.8619\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 309us/step - loss: 2516.6599 - mse: 2516.6599 - mae: 29.3707 - val_loss: 1471.6485 - val_mse: 1471.6484 - val_mae: 26.9034\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 342us/step - loss: 2512.2900 - mse: 2512.2900 - mae: 29.4698 - val_loss: 1463.0170 - val_mse: 1463.0168 - val_mae: 27.2086\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2473.3769 - mse: 2473.3770 - mae: 29.6311 - val_loss: 1467.2611 - val_mse: 1467.2610 - val_mae: 27.0042\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 342us/step - loss: 2485.9650 - mse: 2485.9641 - mae: 29.3349 - val_loss: 1461.2735 - val_mse: 1461.2733 - val_mae: 27.1674\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2510.5959 - mse: 2510.5962 - mae: 29.4158 - val_loss: 1466.9649 - val_mse: 1466.9650 - val_mae: 26.9972\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 313us/step - loss: 2488.4131 - mse: 2488.4119 - mae: 29.3893 - val_loss: 1464.0937 - val_mse: 1464.0936 - val_mae: 27.0391\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 324us/step - loss: 2533.2928 - mse: 2533.2930 - mae: 29.3502 - val_loss: 1475.8163 - val_mse: 1475.8164 - val_mae: 26.6767\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2415.5808 - mse: 2415.5813 - mae: 28.4645 - val_loss: 1453.3995 - val_mse: 1453.3995 - val_mae: 27.1829\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2510.9143 - mse: 2510.9141 - mae: 29.4418 - val_loss: 1464.2270 - val_mse: 1464.2272 - val_mae: 26.7773\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2428.7914 - mse: 2428.7913 - mae: 29.0559 - val_loss: 1453.7179 - val_mse: 1453.7179 - val_mae: 27.0790\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2510.1489 - mse: 2510.1492 - mae: 29.7407 - val_loss: 1464.3646 - val_mse: 1464.3646 - val_mae: 26.6549\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 271us/step - loss: 2502.1339 - mse: 2502.1328 - mae: 29.5057 - val_loss: 1457.0892 - val_mse: 1457.0892 - val_mae: 26.7864\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2467.5478 - mse: 2467.5479 - mae: 28.8883 - val_loss: 1451.9760 - val_mse: 1451.9761 - val_mae: 26.9448\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 285us/step - loss: 2457.0947 - mse: 2457.0945 - mae: 28.9465 - val_loss: 1447.7576 - val_mse: 1447.7576 - val_mae: 27.0384\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2472.6055 - mse: 2472.6057 - mae: 29.1730 - val_loss: 1450.0936 - val_mse: 1450.0938 - val_mae: 26.9549\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 302us/step - loss: 2461.4566 - mse: 2461.4573 - mae: 29.3350 - val_loss: 1451.5289 - val_mse: 1451.5289 - val_mae: 26.8290\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 277us/step - loss: 2467.8036 - mse: 2467.8027 - mae: 29.4713 - val_loss: 1453.0876 - val_mse: 1453.0876 - val_mae: 26.7772\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2453.7831 - mse: 2453.7827 - mae: 28.7835 - val_loss: 1454.3290 - val_mse: 1454.3291 - val_mae: 26.7001\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2481.5344 - mse: 2481.5344 - mae: 29.4961 - val_loss: 1456.4408 - val_mse: 1456.4408 - val_mae: 26.5382\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 335us/step - loss: 2495.6215 - mse: 2495.6221 - mae: 29.4389 - val_loss: 1451.8374 - val_mse: 1451.8375 - val_mae: 26.6283\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2464.3657 - mse: 2464.3657 - mae: 29.2707 - val_loss: 1446.3346 - val_mse: 1446.3345 - val_mae: 26.6821\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2535.2805 - mse: 2535.2812 - mae: 29.7677 - val_loss: 1449.5591 - val_mse: 1449.5590 - val_mae: 26.5964\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 285us/step - loss: 2472.2173 - mse: 2472.2180 - mae: 28.9655 - val_loss: 1450.9653 - val_mse: 1450.9652 - val_mae: 26.5078\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 263us/step - loss: 2390.1445 - mse: 2390.1440 - mae: 28.6916 - val_loss: 1436.2927 - val_mse: 1436.2928 - val_mae: 26.8052\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 285us/step - loss: 2442.7729 - mse: 2442.7732 - mae: 28.8393 - val_loss: 1443.2959 - val_mse: 1443.2957 - val_mae: 26.5774\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 321us/step - loss: 2488.8448 - mse: 2488.8447 - mae: 29.3224 - val_loss: 1445.2366 - val_mse: 1445.2368 - val_mae: 26.5139\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 343us/step - loss: 2451.5237 - mse: 2451.5244 - mae: 29.2804 - val_loss: 1434.0469 - val_mse: 1434.0470 - val_mae: 26.7628\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2471.7599 - mse: 2471.7605 - mae: 28.5251 - val_loss: 1435.8461 - val_mse: 1435.8462 - val_mae: 26.5850\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2506.8037 - mse: 2506.8032 - mae: 29.3650 - val_loss: 1427.9382 - val_mse: 1427.9381 - val_mae: 26.7368\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2430.9118 - mse: 2430.9124 - mae: 28.8524 - val_loss: 1433.5472 - val_mse: 1433.5471 - val_mae: 26.4309\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 286us/step - loss: 2364.9362 - mse: 2364.9363 - mae: 28.8422 - val_loss: 1430.4626 - val_mse: 1430.4625 - val_mae: 26.4340\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 274us/step - loss: 2377.8127 - mse: 2377.8125 - mae: 29.1444 - val_loss: 1420.7565 - val_mse: 1420.7565 - val_mae: 26.6561\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2431.7145 - mse: 2431.7146 - mae: 28.4838 - val_loss: 1427.3242 - val_mse: 1427.3242 - val_mae: 26.4120\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 351us/step - loss: 2452.2786 - mse: 2452.2788 - mae: 28.6849 - val_loss: 1420.3906 - val_mse: 1420.3906 - val_mae: 26.5860\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2469.2703 - mse: 2469.2703 - mae: 29.1494 - val_loss: 1427.5638 - val_mse: 1427.5640 - val_mae: 26.3035\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2514.0318 - mse: 2514.0310 - mae: 29.8288 - val_loss: 1429.5092 - val_mse: 1429.5093 - val_mae: 26.2598\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2436.7275 - mse: 2436.7271 - mae: 28.7145 - val_loss: 1430.9682 - val_mse: 1430.9681 - val_mae: 26.1960\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2402.9328 - mse: 2402.9326 - mae: 28.5985 - val_loss: 1426.5938 - val_mse: 1426.5940 - val_mae: 26.2157\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2388.0879 - mse: 2388.0884 - mae: 28.8727 - val_loss: 1415.9623 - val_mse: 1415.9624 - val_mae: 26.4341\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2530.3200 - mse: 2530.3206 - mae: 29.5566 - val_loss: 1428.2711 - val_mse: 1428.2711 - val_mae: 26.0813\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2460.9712 - mse: 2460.9712 - mae: 29.1929 - val_loss: 1422.9796 - val_mse: 1422.9795 - val_mae: 26.1867\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2486.8417 - mse: 2486.8406 - mae: 29.0386 - val_loss: 1423.1341 - val_mse: 1423.1340 - val_mae: 26.1976\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2415.6903 - mse: 2415.6907 - mae: 28.5846 - val_loss: 1412.9201 - val_mse: 1412.9199 - val_mae: 26.4715\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2453.4546 - mse: 2453.4543 - mae: 28.7509 - val_loss: 1433.3971 - val_mse: 1433.3972 - val_mae: 25.9691\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 312us/step - loss: 2424.4611 - mse: 2424.4609 - mae: 28.9675 - val_loss: 1434.8104 - val_mse: 1434.8103 - val_mae: 25.9032\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 346us/step - loss: 2355.3840 - mse: 2355.3840 - mae: 28.3242 - val_loss: 1415.6648 - val_mse: 1415.6647 - val_mae: 26.2520\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 282us/step - loss: 2445.9355 - mse: 2445.9355 - mae: 29.2239 - val_loss: 1414.3047 - val_mse: 1414.3046 - val_mae: 26.2564\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 266us/step - loss: 2425.1037 - mse: 2425.1033 - mae: 28.9513 - val_loss: 1417.2413 - val_mse: 1417.2412 - val_mae: 26.2111\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2364.8128 - mse: 2364.8130 - mae: 28.3654 - val_loss: 1416.5642 - val_mse: 1416.5642 - val_mae: 26.1764\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 348us/step - loss: 2404.9121 - mse: 2404.9121 - mae: 28.6027 - val_loss: 1414.6617 - val_mse: 1414.6619 - val_mae: 26.1707\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 357us/step - loss: 2439.3735 - mse: 2439.3733 - mae: 29.3249 - val_loss: 1426.1275 - val_mse: 1426.1274 - val_mae: 25.8887\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2458.4553 - mse: 2458.4556 - mae: 28.7407 - val_loss: 1420.1316 - val_mse: 1420.1315 - val_mae: 25.9539\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 349us/step - loss: 2435.8111 - mse: 2435.8108 - mae: 28.7862 - val_loss: 1421.7111 - val_mse: 1421.7113 - val_mae: 25.9607\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2396.5805 - mse: 2396.5806 - mae: 28.1943 - val_loss: 1415.4445 - val_mse: 1415.4443 - val_mae: 26.0569\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2430.8924 - mse: 2430.8921 - mae: 28.8230 - val_loss: 1418.3224 - val_mse: 1418.3224 - val_mae: 25.9468\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2414.8015 - mse: 2414.8003 - mae: 28.9857 - val_loss: 1421.5876 - val_mse: 1421.5876 - val_mae: 25.8132\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2355.8079 - mse: 2355.8081 - mae: 28.5892 - val_loss: 1416.1411 - val_mse: 1416.1411 - val_mae: 25.8844\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 311us/step - loss: 2446.7491 - mse: 2446.7493 - mae: 28.5373 - val_loss: 1416.1329 - val_mse: 1416.1331 - val_mae: 25.8761\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 327us/step - loss: 2393.6141 - mse: 2393.6150 - mae: 28.4387 - val_loss: 1419.4475 - val_mse: 1419.4476 - val_mae: 25.8231\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 274us/step - loss: 2415.2409 - mse: 2415.2410 - mae: 29.0982 - val_loss: 1431.1904 - val_mse: 1431.1904 - val_mae: 25.5809\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2393.4501 - mse: 2393.4502 - mae: 28.6408 - val_loss: 1403.3838 - val_mse: 1403.3838 - val_mae: 26.1200\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 268us/step - loss: 2399.3484 - mse: 2399.3479 - mae: 28.6449 - val_loss: 1415.0679 - val_mse: 1415.0679 - val_mae: 25.8037\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 342us/step - loss: 2384.3996 - mse: 2384.3994 - mae: 28.7158 - val_loss: 1414.9843 - val_mse: 1414.9844 - val_mae: 25.8273\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2288.2874 - mse: 2288.2876 - mae: 29.1047 - val_loss: 3673.1415 - val_mse: 3673.1411 - val_mae: 22.9363\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2332.5901 - mse: 2332.5896 - mae: 28.9054 - val_loss: 3672.8536 - val_mse: 3672.8535 - val_mae: 22.9786\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 352us/step - loss: 2287.3891 - mse: 2287.3889 - mae: 28.7410 - val_loss: 3675.5011 - val_mse: 3675.5010 - val_mae: 22.5631\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2323.8031 - mse: 2323.8030 - mae: 28.7085 - val_loss: 3675.1221 - val_mse: 3675.1221 - val_mae: 22.7834\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2298.9771 - mse: 2298.9771 - mae: 28.8063 - val_loss: 3672.9059 - val_mse: 3672.9053 - val_mae: 22.8290\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 318us/step - loss: 2349.4852 - mse: 2349.4851 - mae: 29.4538 - val_loss: 3673.0008 - val_mse: 3673.0010 - val_mae: 22.7691\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2307.5933 - mse: 2307.5933 - mae: 29.2321 - val_loss: 3676.7720 - val_mse: 3676.7720 - val_mae: 22.6778\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2321.9016 - mse: 2321.9014 - mae: 29.0783 - val_loss: 3675.7559 - val_mse: 3675.7563 - val_mae: 22.4751\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 324us/step - loss: 2358.7988 - mse: 2358.7986 - mae: 28.9754 - val_loss: 3678.6206 - val_mse: 3678.6208 - val_mae: 21.9601\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2320.1387 - mse: 2320.1387 - mae: 28.8634 - val_loss: 3679.5398 - val_mse: 3679.5405 - val_mae: 22.5915\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2270.8404 - mse: 2270.8398 - mae: 28.1814 - val_loss: 3678.0687 - val_mse: 3678.0686 - val_mae: 22.5185\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2259.8347 - mse: 2259.8350 - mae: 28.8257 - val_loss: 3683.7204 - val_mse: 3683.7200 - val_mae: 21.8652\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2322.9720 - mse: 2322.9722 - mae: 29.0788 - val_loss: 3680.7258 - val_mse: 3680.7253 - val_mae: 22.3764\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2297.8349 - mse: 2297.8350 - mae: 28.5519 - val_loss: 3680.7475 - val_mse: 3680.7473 - val_mae: 22.9073\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 275us/step - loss: 2318.8555 - mse: 2318.8547 - mae: 28.6193 - val_loss: 3685.2170 - val_mse: 3685.2173 - val_mae: 23.0984\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2302.3818 - mse: 2302.3813 - mae: 28.9751 - val_loss: 3685.8281 - val_mse: 3685.8284 - val_mae: 22.3822\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2247.3298 - mse: 2247.3296 - mae: 28.6043 - val_loss: 3684.8264 - val_mse: 3684.8267 - val_mae: 22.8752\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2247.8001 - mse: 2247.7993 - mae: 28.4363 - val_loss: 3687.3661 - val_mse: 3687.3655 - val_mae: 22.9932\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2303.1206 - mse: 2303.1201 - mae: 28.8734 - val_loss: 3692.4620 - val_mse: 3692.4622 - val_mae: 22.5377\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 332us/step - loss: 2304.4670 - mse: 2304.4678 - mae: 28.5780 - val_loss: 3689.2961 - val_mse: 3689.2959 - val_mae: 22.7092\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2311.5737 - mse: 2311.5737 - mae: 29.3553 - val_loss: 3690.2935 - val_mse: 3690.2942 - val_mae: 22.3020\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2302.6760 - mse: 2302.6768 - mae: 29.0248 - val_loss: 3688.0513 - val_mse: 3688.0513 - val_mae: 22.5959\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2292.1492 - mse: 2292.1492 - mae: 28.3763 - val_loss: 3689.7679 - val_mse: 3689.7681 - val_mae: 22.8984\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2262.7346 - mse: 2262.7344 - mae: 28.7993 - val_loss: 3692.6342 - val_mse: 3692.6338 - val_mae: 22.5495\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2285.4064 - mse: 2285.4062 - mae: 28.8576 - val_loss: 3691.8751 - val_mse: 3691.8752 - val_mae: 22.6331\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 329us/step - loss: 2273.3620 - mse: 2273.3621 - mae: 28.5800 - val_loss: 3695.1194 - val_mse: 3695.1191 - val_mae: 23.0931\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2259.1988 - mse: 2259.1992 - mae: 28.8106 - val_loss: 3694.5592 - val_mse: 3694.5586 - val_mae: 22.8321\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2274.1442 - mse: 2274.1440 - mae: 28.6252 - val_loss: 3695.0725 - val_mse: 3695.0720 - val_mae: 22.3284\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 340us/step - loss: 2273.7266 - mse: 2273.7266 - mae: 28.5470 - val_loss: 3695.9950 - val_mse: 3695.9944 - val_mae: 22.8789\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2262.6940 - mse: 2262.6929 - mae: 29.0932 - val_loss: 3693.0674 - val_mse: 3693.0669 - val_mae: 22.5859\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2240.0304 - mse: 2240.0308 - mae: 28.5775 - val_loss: 3694.7504 - val_mse: 3694.7502 - val_mae: 23.1784\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 329us/step - loss: 2335.1917 - mse: 2335.1914 - mae: 29.0637 - val_loss: 3692.5611 - val_mse: 3692.5605 - val_mae: 22.3249\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 348us/step - loss: 2301.1151 - mse: 2301.1157 - mae: 28.7190 - val_loss: 3693.6882 - val_mse: 3693.6875 - val_mae: 22.8735\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 312us/step - loss: 2250.5974 - mse: 2250.5964 - mae: 28.3018 - val_loss: 3693.2977 - val_mse: 3693.2971 - val_mae: 22.6294\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2303.9303 - mse: 2303.9299 - mae: 28.9684 - val_loss: 3697.7099 - val_mse: 3697.7095 - val_mae: 23.0080\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 354us/step - loss: 2246.1065 - mse: 2246.1074 - mae: 28.2868 - val_loss: 3701.5435 - val_mse: 3701.5427 - val_mae: 22.7284\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2371.9673 - mse: 2371.9663 - mae: 29.1878 - val_loss: 3695.4341 - val_mse: 3695.4341 - val_mae: 22.3398\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2296.3109 - mse: 2296.3108 - mae: 28.7566 - val_loss: 3700.3200 - val_mse: 3700.3201 - val_mae: 23.2214\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2294.9679 - mse: 2294.9683 - mae: 28.8180 - val_loss: 3694.8882 - val_mse: 3694.8879 - val_mae: 22.5252\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2273.7632 - mse: 2273.7634 - mae: 28.6158 - val_loss: 3695.5461 - val_mse: 3695.5466 - val_mae: 22.7626\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2296.8207 - mse: 2296.8210 - mae: 28.8926 - val_loss: 3695.2419 - val_mse: 3695.2417 - val_mae: 22.8788\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 263us/step - loss: 2308.1114 - mse: 2308.1113 - mae: 28.8338 - val_loss: 3696.3935 - val_mse: 3696.3936 - val_mae: 22.8529\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 275us/step - loss: 2294.9606 - mse: 2294.9604 - mae: 28.6707 - val_loss: 3696.7989 - val_mse: 3696.7988 - val_mae: 22.8945\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2228.3295 - mse: 2228.3296 - mae: 28.3832 - val_loss: 3697.7868 - val_mse: 3697.7871 - val_mae: 23.3287\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 328us/step - loss: 2281.0272 - mse: 2281.0269 - mae: 28.6857 - val_loss: 3692.1253 - val_mse: 3692.1257 - val_mae: 22.3253\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 348us/step - loss: 2337.9021 - mse: 2337.9026 - mae: 29.1420 - val_loss: 3693.2817 - val_mse: 3693.2825 - val_mae: 23.0466\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2249.5109 - mse: 2249.5115 - mae: 28.1910 - val_loss: 3693.7604 - val_mse: 3693.7603 - val_mae: 23.0468\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2254.4629 - mse: 2254.4624 - mae: 28.3511 - val_loss: 3694.9780 - val_mse: 3694.9790 - val_mae: 23.2717\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2304.7130 - mse: 2304.7124 - mae: 28.8953 - val_loss: 3696.4503 - val_mse: 3696.4507 - val_mae: 22.9263\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2238.0185 - mse: 2238.0181 - mae: 28.1835 - val_loss: 3695.7809 - val_mse: 3695.7810 - val_mae: 23.1315\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 295us/step - loss: 2316.9949 - mse: 2316.9941 - mae: 29.0823 - val_loss: 3698.9348 - val_mse: 3698.9343 - val_mae: 23.1837\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 313us/step - loss: 2258.2496 - mse: 2258.2493 - mae: 28.6624 - val_loss: 3695.1495 - val_mse: 3695.1492 - val_mae: 22.9610\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2287.0876 - mse: 2287.0881 - mae: 28.8919 - val_loss: 3692.2408 - val_mse: 3692.2419 - val_mae: 23.0904\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 280us/step - loss: 2306.1711 - mse: 2306.1711 - mae: 29.0320 - val_loss: 3687.9849 - val_mse: 3687.9846 - val_mae: 22.9512\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2254.4324 - mse: 2254.4326 - mae: 28.7051 - val_loss: 3695.0641 - val_mse: 3695.0642 - val_mae: 22.8191\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2244.9842 - mse: 2244.9832 - mae: 28.4134 - val_loss: 3700.0163 - val_mse: 3700.0168 - val_mae: 23.0797\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 293us/step - loss: 2295.0585 - mse: 2295.0588 - mae: 28.6524 - val_loss: 3692.9469 - val_mse: 3692.9468 - val_mae: 22.5051\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2247.8697 - mse: 2247.8699 - mae: 28.4882 - val_loss: 3694.9864 - val_mse: 3694.9856 - val_mae: 23.2031\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2227.6817 - mse: 2227.6819 - mae: 28.2294 - val_loss: 3697.7629 - val_mse: 3697.7620 - val_mae: 23.4995\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2207.3169 - mse: 2207.3164 - mae: 28.5311 - val_loss: 3698.0630 - val_mse: 3698.0635 - val_mae: 23.3216\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2219.5664 - mse: 2219.5662 - mae: 28.4934 - val_loss: 3702.3823 - val_mse: 3702.3821 - val_mae: 22.9763\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2239.9183 - mse: 2239.9185 - mae: 28.6758 - val_loss: 3696.7763 - val_mse: 3696.7759 - val_mae: 22.9777\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 291us/step - loss: 2225.0023 - mse: 2225.0022 - mae: 28.4755 - val_loss: 3704.2858 - val_mse: 3704.2854 - val_mae: 23.4296\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 281us/step - loss: 2268.9710 - mse: 2268.9700 - mae: 28.2689 - val_loss: 3700.3057 - val_mse: 3700.3059 - val_mae: 22.8998\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2286.5855 - mse: 2286.5859 - mae: 28.5679 - val_loss: 3695.5960 - val_mse: 3695.5959 - val_mae: 22.7269\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 290us/step - loss: 2282.7718 - mse: 2282.7715 - mae: 28.6323 - val_loss: 3694.1623 - val_mse: 3694.1624 - val_mae: 22.8192\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 319us/step - loss: 2243.6953 - mse: 2243.6953 - mae: 28.1383 - val_loss: 3697.4239 - val_mse: 3697.4236 - val_mae: 23.1868\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 336us/step - loss: 2219.7750 - mse: 2219.7742 - mae: 28.4389 - val_loss: 3706.7823 - val_mse: 3706.7822 - val_mae: 23.1345\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 357us/step - loss: 2245.7137 - mse: 2245.7134 - mae: 28.7615 - val_loss: 3701.1027 - val_mse: 3701.1028 - val_mae: 22.5811\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 307us/step - loss: 2279.7016 - mse: 2279.7019 - mae: 28.5658 - val_loss: 3699.0224 - val_mse: 3699.0232 - val_mae: 22.6355\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2287.4726 - mse: 2287.4722 - mae: 28.3234 - val_loss: 3700.9742 - val_mse: 3700.9739 - val_mae: 22.7793\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2308.4775 - mse: 2308.4775 - mae: 28.8924 - val_loss: 3705.4308 - val_mse: 3705.4312 - val_mae: 22.4832\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 268us/step - loss: 2189.8128 - mse: 2189.8130 - mae: 27.9304 - val_loss: 3700.8290 - val_mse: 3700.8301 - val_mae: 22.7315\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 322us/step - loss: 2273.5643 - mse: 2273.5647 - mae: 28.5893 - val_loss: 3698.8520 - val_mse: 3698.8511 - val_mae: 22.9796\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 315us/step - loss: 2276.0785 - mse: 2276.0779 - mae: 28.3754 - val_loss: 3705.0033 - val_mse: 3705.0024 - val_mae: 23.4873\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 230us/step - loss: 2266.0954 - mse: 2266.0964 - mae: 28.5258 - val_loss: 3705.5853 - val_mse: 3705.5850 - val_mae: 23.1575\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2257.6525 - mse: 2257.6533 - mae: 28.3337 - val_loss: 3700.8881 - val_mse: 3700.8879 - val_mae: 22.9156\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 303us/step - loss: 2257.0428 - mse: 2257.0430 - mae: 28.0777 - val_loss: 3704.7165 - val_mse: 3704.7163 - val_mae: 23.3777\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2306.1072 - mse: 2306.1077 - mae: 28.4659 - val_loss: 3703.4291 - val_mse: 3703.4287 - val_mae: 23.3288\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 273us/step - loss: 2241.0245 - mse: 2241.0244 - mae: 28.1970 - val_loss: 3704.8163 - val_mse: 3704.8167 - val_mae: 23.3279\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 352us/step - loss: 2625.1015 - mse: 2625.1013 - mae: 28.1653 - val_loss: 2037.9365 - val_mse: 2037.9362 - val_mae: 26.4912\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 326us/step - loss: 2564.9441 - mse: 2564.9446 - mae: 27.4401 - val_loss: 2041.8444 - val_mse: 2041.8445 - val_mae: 26.6678\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 337us/step - loss: 2626.3731 - mse: 2626.3730 - mae: 27.8944 - val_loss: 2055.9247 - val_mse: 2055.9246 - val_mae: 25.9195\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2690.3220 - mse: 2690.3218 - mae: 28.2818 - val_loss: 2055.5420 - val_mse: 2055.5415 - val_mae: 25.8698\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2657.8909 - mse: 2657.8909 - mae: 28.0475 - val_loss: 2039.1372 - val_mse: 2039.1372 - val_mae: 26.5255\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2667.8145 - mse: 2667.8145 - mae: 28.0902 - val_loss: 2040.3208 - val_mse: 2040.3207 - val_mae: 26.4037\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2599.8251 - mse: 2599.8252 - mae: 27.7535 - val_loss: 2041.9016 - val_mse: 2041.9021 - val_mae: 26.6143\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 325us/step - loss: 2624.5000 - mse: 2624.5005 - mae: 28.0768 - val_loss: 2053.2296 - val_mse: 2053.2300 - val_mae: 26.1533\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2612.3807 - mse: 2612.3796 - mae: 27.7368 - val_loss: 2057.8079 - val_mse: 2057.8079 - val_mae: 26.2799\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2664.5811 - mse: 2664.5808 - mae: 27.8597 - val_loss: 2077.6019 - val_mse: 2077.6016 - val_mae: 26.1062\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 314us/step - loss: 2631.8582 - mse: 2631.8589 - mae: 27.8421 - val_loss: 2068.2889 - val_mse: 2068.2891 - val_mae: 26.2890\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2665.2750 - mse: 2665.2751 - mae: 27.9891 - val_loss: 2075.5469 - val_mse: 2075.5464 - val_mae: 26.4897\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2596.7041 - mse: 2596.7041 - mae: 27.7797 - val_loss: 2063.2218 - val_mse: 2063.2224 - val_mae: 27.0062\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2628.3955 - mse: 2628.3955 - mae: 28.0357 - val_loss: 2061.9960 - val_mse: 2061.9956 - val_mae: 26.6199\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2646.0419 - mse: 2646.0425 - mae: 28.0098 - val_loss: 2069.2369 - val_mse: 2069.2368 - val_mae: 26.6845\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2684.7925 - mse: 2684.7922 - mae: 28.1781 - val_loss: 2075.9669 - val_mse: 2075.9670 - val_mae: 26.4691\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2660.0444 - mse: 2660.0452 - mae: 27.9486 - val_loss: 2082.0184 - val_mse: 2082.0183 - val_mae: 26.4211\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2668.3909 - mse: 2668.3909 - mae: 28.1428 - val_loss: 2069.7774 - val_mse: 2069.7776 - val_mae: 26.3401\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2642.1636 - mse: 2642.1633 - mae: 28.0814 - val_loss: 2066.6803 - val_mse: 2066.6799 - val_mae: 26.0367\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2624.5655 - mse: 2624.5652 - mae: 28.1660 - val_loss: 2059.9383 - val_mse: 2059.9385 - val_mae: 26.6820\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 330us/step - loss: 2651.8906 - mse: 2651.8904 - mae: 28.0727 - val_loss: 2063.2545 - val_mse: 2063.2544 - val_mae: 26.5482\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2616.8388 - mse: 2616.8384 - mae: 27.9609 - val_loss: 2058.6817 - val_mse: 2058.6819 - val_mae: 26.6306\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 346us/step - loss: 2659.0181 - mse: 2659.0181 - mae: 28.0341 - val_loss: 2070.6586 - val_mse: 2070.6584 - val_mae: 25.9471\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2640.5841 - mse: 2640.5842 - mae: 28.1269 - val_loss: 2060.6395 - val_mse: 2060.6396 - val_mae: 26.8319\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 271us/step - loss: 2620.1937 - mse: 2620.1931 - mae: 28.0539 - val_loss: 2057.1182 - val_mse: 2057.1182 - val_mae: 26.6058\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2668.5388 - mse: 2668.5391 - mae: 28.0292 - val_loss: 2053.1802 - val_mse: 2053.1802 - val_mae: 26.7506\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2655.1850 - mse: 2655.1855 - mae: 28.2362 - val_loss: 2058.4289 - val_mse: 2058.4287 - val_mae: 26.1761\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2656.2120 - mse: 2656.2117 - mae: 27.6712 - val_loss: 2060.3083 - val_mse: 2060.3083 - val_mae: 26.3599\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2610.7173 - mse: 2610.7173 - mae: 27.8777 - val_loss: 2053.5712 - val_mse: 2053.5710 - val_mae: 26.6809\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2614.8036 - mse: 2614.8040 - mae: 28.0430 - val_loss: 2054.7625 - val_mse: 2054.7625 - val_mae: 27.1888\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2672.7027 - mse: 2672.7031 - mae: 28.1885 - val_loss: 2064.0775 - val_mse: 2064.0776 - val_mae: 26.6946\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2625.8865 - mse: 2625.8875 - mae: 28.0837 - val_loss: 2062.2824 - val_mse: 2062.2822 - val_mae: 27.0062\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2652.7456 - mse: 2652.7461 - mae: 27.9733 - val_loss: 2068.3155 - val_mse: 2068.3154 - val_mae: 26.8178\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 380us/step - loss: 2597.7893 - mse: 2597.7893 - mae: 27.7061 - val_loss: 2061.9830 - val_mse: 2061.9829 - val_mae: 26.8764\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2597.4848 - mse: 2597.4844 - mae: 27.9492 - val_loss: 2063.5103 - val_mse: 2063.5105 - val_mae: 26.6405\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2637.2089 - mse: 2637.2083 - mae: 27.7067 - val_loss: 2059.7947 - val_mse: 2059.7947 - val_mae: 26.6924\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 297us/step - loss: 2590.2109 - mse: 2590.2109 - mae: 27.7977 - val_loss: 2048.1910 - val_mse: 2048.1909 - val_mae: 26.8154\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 273us/step - loss: 2604.9206 - mse: 2604.9204 - mae: 28.0093 - val_loss: 2056.9918 - val_mse: 2056.9917 - val_mae: 26.3989\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 268us/step - loss: 2665.7835 - mse: 2665.7837 - mae: 28.0351 - val_loss: 2060.0472 - val_mse: 2060.0476 - val_mae: 26.4833\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 309us/step - loss: 2596.3209 - mse: 2596.3210 - mae: 27.7545 - val_loss: 2058.3275 - val_mse: 2058.3274 - val_mae: 27.0758\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 299us/step - loss: 2670.7293 - mse: 2670.7297 - mae: 27.7920 - val_loss: 2061.7211 - val_mse: 2061.7209 - val_mae: 26.4082\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2645.0311 - mse: 2645.0315 - mae: 28.0214 - val_loss: 2062.2482 - val_mse: 2062.2480 - val_mae: 26.6300\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2652.4118 - mse: 2652.4124 - mae: 28.1280 - val_loss: 2061.9680 - val_mse: 2061.9683 - val_mae: 26.2841\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 261us/step - loss: 2673.0412 - mse: 2673.0410 - mae: 28.0833 - val_loss: 2060.9808 - val_mse: 2060.9807 - val_mae: 26.7926\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 367us/step - loss: 2665.1325 - mse: 2665.1331 - mae: 28.0702 - val_loss: 2055.3717 - val_mse: 2055.3721 - val_mae: 26.7768\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2625.6976 - mse: 2625.6973 - mae: 27.6967 - val_loss: 2057.4546 - val_mse: 2057.4543 - val_mae: 26.4857\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2665.3159 - mse: 2665.3154 - mae: 28.1469 - val_loss: 2051.6758 - val_mse: 2051.6758 - val_mae: 26.8318\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2656.5320 - mse: 2656.5317 - mae: 27.9511 - val_loss: 2063.5786 - val_mse: 2063.5784 - val_mae: 26.6361\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2603.2513 - mse: 2603.2510 - mae: 27.8525 - val_loss: 2052.8066 - val_mse: 2052.8066 - val_mae: 27.1552\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2618.3449 - mse: 2618.3442 - mae: 27.5159 - val_loss: 2059.4922 - val_mse: 2059.4924 - val_mae: 26.6378\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 236us/step - loss: 2610.5136 - mse: 2610.5134 - mae: 27.9701 - val_loss: 2063.6331 - val_mse: 2063.6328 - val_mae: 26.3004\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2578.8711 - mse: 2578.8706 - mae: 27.8424 - val_loss: 2061.0670 - val_mse: 2061.0669 - val_mae: 27.2159\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2657.8561 - mse: 2657.8555 - mae: 28.0085 - val_loss: 2062.2183 - val_mse: 2062.2183 - val_mae: 26.3525\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 320us/step - loss: 2690.2104 - mse: 2690.2107 - mae: 28.1200 - val_loss: 2061.9333 - val_mse: 2061.9336 - val_mae: 26.6465\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2577.6570 - mse: 2577.6558 - mae: 27.6265 - val_loss: 2052.9979 - val_mse: 2052.9980 - val_mae: 26.9386\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2604.1023 - mse: 2604.1028 - mae: 27.7460 - val_loss: 2057.6880 - val_mse: 2057.6875 - val_mae: 26.9044\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 286us/step - loss: 2611.4638 - mse: 2611.4639 - mae: 27.7374 - val_loss: 2055.2554 - val_mse: 2055.2554 - val_mae: 26.2325\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 311us/step - loss: 2613.5289 - mse: 2613.5293 - mae: 27.8751 - val_loss: 2053.5628 - val_mse: 2053.5630 - val_mae: 26.7144\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 272us/step - loss: 2641.0439 - mse: 2641.0452 - mae: 28.2216 - val_loss: 2065.0090 - val_mse: 2065.0095 - val_mae: 26.1733\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2622.5109 - mse: 2622.5105 - mae: 27.9711 - val_loss: 2060.2255 - val_mse: 2060.2258 - val_mae: 26.6861\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2604.3237 - mse: 2604.3242 - mae: 27.9315 - val_loss: 2057.5261 - val_mse: 2057.5261 - val_mae: 26.6829\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 285us/step - loss: 2658.3835 - mse: 2658.3833 - mae: 27.9251 - val_loss: 2062.1691 - val_mse: 2062.1694 - val_mae: 26.4231\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2558.6250 - mse: 2558.6248 - mae: 27.4531 - val_loss: 2056.7174 - val_mse: 2056.7178 - val_mae: 26.9070\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 333us/step - loss: 2644.4890 - mse: 2644.4897 - mae: 27.8827 - val_loss: 2062.0485 - val_mse: 2062.0483 - val_mae: 26.5554\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2620.1343 - mse: 2620.1345 - mae: 27.9054 - val_loss: 2064.9655 - val_mse: 2064.9653 - val_mae: 26.7459\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2635.9775 - mse: 2635.9773 - mae: 27.9244 - val_loss: 2056.8665 - val_mse: 2056.8665 - val_mae: 26.6975\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2643.2064 - mse: 2643.2061 - mae: 27.9481 - val_loss: 2061.5898 - val_mse: 2061.5903 - val_mae: 26.6309\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 268us/step - loss: 2628.4665 - mse: 2628.4658 - mae: 27.9191 - val_loss: 2058.9140 - val_mse: 2058.9138 - val_mae: 26.6581\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 349us/step - loss: 2608.4176 - mse: 2608.4170 - mae: 27.6863 - val_loss: 2057.7451 - val_mse: 2057.7446 - val_mae: 26.6816\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2645.0604 - mse: 2645.0603 - mae: 27.6647 - val_loss: 2063.2876 - val_mse: 2063.2878 - val_mae: 26.4148\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 288us/step - loss: 2609.9199 - mse: 2609.9202 - mae: 27.5913 - val_loss: 2063.2140 - val_mse: 2063.2141 - val_mae: 26.6830\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2584.6901 - mse: 2584.6904 - mae: 27.3865 - val_loss: 2061.7110 - val_mse: 2061.7112 - val_mae: 26.8122\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2588.2070 - mse: 2588.2073 - mae: 27.7014 - val_loss: 2059.9420 - val_mse: 2059.9421 - val_mae: 26.6810\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2561.2605 - mse: 2561.2605 - mae: 27.4018 - val_loss: 2066.0444 - val_mse: 2066.0447 - val_mae: 26.2036\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2621.0839 - mse: 2621.0833 - mae: 27.8191 - val_loss: 2053.8569 - val_mse: 2053.8572 - val_mae: 27.1434\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2605.6371 - mse: 2605.6370 - mae: 27.5590 - val_loss: 2057.7122 - val_mse: 2057.7122 - val_mae: 26.5530\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2594.1603 - mse: 2594.1609 - mae: 27.4998 - val_loss: 2066.1687 - val_mse: 2066.1692 - val_mae: 26.2206\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 300us/step - loss: 2585.4052 - mse: 2585.4055 - mae: 27.6738 - val_loss: 2059.0822 - val_mse: 2059.0820 - val_mae: 26.8186\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 269us/step - loss: 2605.2810 - mse: 2605.2810 - mae: 27.7612 - val_loss: 2062.4257 - val_mse: 2062.4258 - val_mae: 26.6419\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2591.3905 - mse: 2591.3906 - mae: 27.5647 - val_loss: 2064.2127 - val_mse: 2064.2126 - val_mae: 26.8614\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 13337.8501 - mse: 13337.8496 - mae: 109.9447 - val_loss: 34645.9633 - val_mse: 34645.9648 - val_mae: 132.8309\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 336us/step - loss: 13226.0873 - mse: 13226.0869 - mae: 109.4390 - val_loss: 34435.2685 - val_mse: 34435.2695 - val_mae: 132.0332\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 330us/step - loss: 12894.0665 - mse: 12894.0674 - mae: 107.9016 - val_loss: 33750.8894 - val_mse: 33750.8867 - val_mae: 129.4081\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 369us/step - loss: 11924.1084 - mse: 11924.1094 - mae: 103.4156 - val_loss: 31745.1162 - val_mse: 31745.1152 - val_mae: 121.3820\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 9400.7935 - mse: 9400.7939 - mae: 89.9295 - val_loss: 26639.8705 - val_mse: 26639.8691 - val_mae: 97.9989\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 365us/step - loss: 4749.8400 - mse: 4749.8398 - mae: 56.3199 - val_loss: 19028.8932 - val_mse: 19028.8926 - val_mae: 44.0731\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 2533.8631 - mse: 2533.8633 - mae: 36.7593 - val_loss: 17818.3946 - val_mse: 17818.3945 - val_mae: 36.0548\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 319us/step - loss: 2813.5171 - mse: 2813.5171 - mae: 37.5796 - val_loss: 18112.0900 - val_mse: 18112.0898 - val_mae: 36.5894\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 363us/step - loss: 2661.2545 - mse: 2661.2542 - mae: 36.5356 - val_loss: 18146.5576 - val_mse: 18146.5605 - val_mae: 36.7737\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 358us/step - loss: 2789.4461 - mse: 2789.4460 - mae: 37.5217 - val_loss: 18154.9365 - val_mse: 18154.9355 - val_mae: 36.8640\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 238us/step - loss: 2495.7314 - mse: 2495.7310 - mae: 35.3354 - val_loss: 18028.0255 - val_mse: 18028.0234 - val_mae: 36.4327\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 373us/step - loss: 2632.5511 - mse: 2632.5513 - mae: 37.4353 - val_loss: 17980.7034 - val_mse: 17980.7012 - val_mae: 36.3821\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2764.0979 - mse: 2764.0977 - mae: 37.3099 - val_loss: 18146.0232 - val_mse: 18146.0234 - val_mae: 36.9704\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 353us/step - loss: 2586.6144 - mse: 2586.6143 - mae: 36.0890 - val_loss: 18155.9892 - val_mse: 18155.9883 - val_mae: 37.0749\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 278us/step - loss: 2437.1584 - mse: 2437.1587 - mae: 35.1224 - val_loss: 18009.6462 - val_mse: 18009.6465 - val_mae: 36.5907\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 343us/step - loss: 2518.0195 - mse: 2518.0198 - mae: 36.1864 - val_loss: 18059.5814 - val_mse: 18059.5820 - val_mae: 36.7587\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 364us/step - loss: 2524.2257 - mse: 2524.2258 - mae: 36.6026 - val_loss: 18134.2610 - val_mse: 18134.2617 - val_mae: 37.1348\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 291us/step - loss: 2408.2232 - mse: 2408.2229 - mae: 35.3513 - val_loss: 18146.4211 - val_mse: 18146.4219 - val_mae: 37.2492\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 2341.2867 - mse: 2341.2866 - mae: 33.7875 - val_loss: 18032.7345 - val_mse: 18032.7344 - val_mae: 36.8588\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 2327.8010 - mse: 2327.8010 - mae: 34.5369 - val_loss: 18000.0203 - val_mse: 18000.0195 - val_mae: 36.8564\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 324us/step - loss: 2251.9041 - mse: 2251.9041 - mae: 34.6911 - val_loss: 18181.1215 - val_mse: 18181.1211 - val_mae: 37.6107\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 304us/step - loss: 2286.5025 - mse: 2286.5024 - mae: 33.3923 - val_loss: 17885.8450 - val_mse: 17885.8457 - val_mae: 36.8266\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 328us/step - loss: 2323.0005 - mse: 2323.0005 - mae: 34.8985 - val_loss: 17973.4046 - val_mse: 17973.4043 - val_mae: 36.9601\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 275us/step - loss: 2604.6543 - mse: 2604.6545 - mae: 36.2075 - val_loss: 18112.7828 - val_mse: 18112.7832 - val_mae: 37.4261\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 282us/step - loss: 2171.8063 - mse: 2171.8062 - mae: 34.2847 - val_loss: 18140.7331 - val_mse: 18140.7344 - val_mae: 37.5888\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2344.7902 - mse: 2344.7900 - mae: 35.1917 - val_loss: 17962.7725 - val_mse: 17962.7734 - val_mae: 37.0488\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2065.8831 - mse: 2065.8831 - mae: 33.1956 - val_loss: 17861.7858 - val_mse: 17861.7832 - val_mae: 36.9941\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 2494.6324 - mse: 2494.6328 - mae: 34.5675 - val_loss: 18064.9564 - val_mse: 18064.9570 - val_mae: 37.3710\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 325us/step - loss: 2280.1627 - mse: 2280.1628 - mae: 33.7056 - val_loss: 17784.6453 - val_mse: 17784.6465 - val_mae: 37.0905\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 335us/step - loss: 2307.3044 - mse: 2307.3044 - mae: 33.6354 - val_loss: 18242.0819 - val_mse: 18242.0820 - val_mae: 38.2655\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 275us/step - loss: 2072.3661 - mse: 2072.3662 - mae: 32.7669 - val_loss: 17964.0847 - val_mse: 17964.0859 - val_mae: 37.1849\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 381us/step - loss: 2222.4155 - mse: 2222.4158 - mae: 34.2002 - val_loss: 18098.2116 - val_mse: 18098.2129 - val_mae: 37.6745\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 347us/step - loss: 2219.9160 - mse: 2219.9160 - mae: 33.6628 - val_loss: 17962.8928 - val_mse: 17962.8926 - val_mae: 37.2581\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 279us/step - loss: 2160.3223 - mse: 2160.3223 - mae: 32.3915 - val_loss: 18089.2220 - val_mse: 18089.2227 - val_mae: 37.7377\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 296us/step - loss: 2316.3234 - mse: 2316.3237 - mae: 33.6550 - val_loss: 18266.3766 - val_mse: 18266.3770 - val_mae: 38.6336\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 366us/step - loss: 2101.8566 - mse: 2101.8567 - mae: 32.5839 - val_loss: 17817.8652 - val_mse: 17817.8652 - val_mae: 37.2693\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 365us/step - loss: 2229.9866 - mse: 2229.9868 - mae: 33.1955 - val_loss: 17945.2986 - val_mse: 17945.3008 - val_mae: 37.3943\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 368us/step - loss: 2179.4853 - mse: 2179.4854 - mae: 32.3847 - val_loss: 18129.1153 - val_mse: 18129.1152 - val_mae: 38.0697\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 253us/step - loss: 2150.5039 - mse: 2150.5039 - mae: 32.3599 - val_loss: 18093.7542 - val_mse: 18093.7539 - val_mae: 37.9462\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 260us/step - loss: 2164.1828 - mse: 2164.1831 - mae: 32.1809 - val_loss: 17976.8555 - val_mse: 17976.8535 - val_mae: 37.5691\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 263us/step - loss: 2225.1662 - mse: 2225.1663 - mae: 33.1811 - val_loss: 18208.5545 - val_mse: 18208.5566 - val_mae: 38.6027\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 300us/step - loss: 2155.3214 - mse: 2155.3213 - mae: 33.0778 - val_loss: 17997.0532 - val_mse: 17997.0527 - val_mae: 37.7569\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 2052.7705 - mse: 2052.7705 - mae: 33.17 - 0s 320us/step - loss: 2301.7359 - mse: 2301.7361 - mae: 33.7533 - val_loss: 18131.7876 - val_mse: 18131.7871 - val_mae: 38.2942\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 346us/step - loss: 2107.8509 - mse: 2107.8511 - mae: 32.5762 - val_loss: 17859.9613 - val_mse: 17859.9629 - val_mae: 37.5325\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 255us/step - loss: 2037.9145 - mse: 2037.9146 - mae: 32.2540 - val_loss: 17933.7258 - val_mse: 17933.7266 - val_mae: 37.6674\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 229us/step - loss: 2230.1993 - mse: 2230.1995 - mae: 32.2456 - val_loss: 18120.8128 - val_mse: 18120.8125 - val_mae: 38.3461\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 267us/step - loss: 2167.7099 - mse: 2167.7097 - mae: 33.0879 - val_loss: 17968.1605 - val_mse: 17968.1582 - val_mae: 37.8501\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 332us/step - loss: 2100.0761 - mse: 2100.0762 - mae: 31.1969 - val_loss: 17964.6322 - val_mse: 17964.6328 - val_mae: 37.8939\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 370us/step - loss: 2177.6483 - mse: 2177.6487 - mae: 33.3444 - val_loss: 18067.0309 - val_mse: 18067.0293 - val_mae: 38.2288\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 374us/step - loss: 2098.1618 - mse: 2098.1621 - mae: 31.5150 - val_loss: 17898.4873 - val_mse: 17898.4883 - val_mae: 37.7624\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 353us/step - loss: 2007.4907 - mse: 2007.4910 - mae: 31.2710 - val_loss: 17860.6852 - val_mse: 17860.6855 - val_mae: 37.7313\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 396us/step - loss: 1968.5381 - mse: 1968.5380 - mae: 30.8600 - val_loss: 17928.0386 - val_mse: 17928.0391 - val_mae: 37.9373\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 406us/step - loss: 2096.0484 - mse: 2096.0488 - mae: 32.3707 - val_loss: 18018.8122 - val_mse: 18018.8125 - val_mae: 38.2280\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 356us/step - loss: 2013.4530 - mse: 2013.4531 - mae: 32.1603 - val_loss: 18137.6833 - val_mse: 18137.6836 - val_mae: 38.7643\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 232us/step - loss: 2129.2335 - mse: 2129.2332 - mae: 31.8714 - val_loss: 18129.1934 - val_mse: 18129.1914 - val_mae: 38.7561\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 354us/step - loss: 2011.8922 - mse: 2011.8920 - mae: 31.0383 - val_loss: 18002.6979 - val_mse: 18002.6953 - val_mae: 38.2724\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 256us/step - loss: 2144.5909 - mse: 2144.5911 - mae: 32.1907 - val_loss: 17989.5346 - val_mse: 17989.5332 - val_mae: 38.2675\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 228us/step - loss: 2054.9317 - mse: 2054.9316 - mae: 31.3840 - val_loss: 18078.5536 - val_mse: 18078.5527 - val_mae: 38.6700\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 268us/step - loss: 2009.1754 - mse: 2009.1755 - mae: 32.1867 - val_loss: 18114.9496 - val_mse: 18114.9473 - val_mae: 38.9016\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 354us/step - loss: 2061.5943 - mse: 2061.5942 - mae: 32.6506 - val_loss: 17958.9734 - val_mse: 17958.9746 - val_mae: 38.2716\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 387us/step - loss: 2060.9516 - mse: 2060.9519 - mae: 31.6090 - val_loss: 18008.6500 - val_mse: 18008.6504 - val_mae: 38.4556\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 383us/step - loss: 2203.4612 - mse: 2203.4609 - mae: 31.9168 - val_loss: 17996.6105 - val_mse: 17996.6094 - val_mae: 38.4316\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 271us/step - loss: 2094.8931 - mse: 2094.8931 - mae: 31.9676 - val_loss: 18018.4755 - val_mse: 18018.4766 - val_mae: 38.4882\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 338us/step - loss: 1812.3529 - mse: 1812.3529 - mae: 29.3782 - val_loss: 17831.4367 - val_mse: 17831.4375 - val_mae: 37.9644\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 276us/step - loss: 2095.1944 - mse: 2095.1946 - mae: 32.0349 - val_loss: 18047.0330 - val_mse: 18047.0332 - val_mae: 38.6275\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 245us/step - loss: 2028.3817 - mse: 2028.3818 - mae: 31.4253 - val_loss: 18048.3400 - val_mse: 18048.3398 - val_mae: 38.6748\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 270us/step - loss: 1949.9210 - mse: 1949.9211 - mae: 31.1611 - val_loss: 17960.8171 - val_mse: 17960.8184 - val_mae: 38.3537\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 1904.7688 - mse: 1904.7689 - mae: 30.9339 - val_loss: 18070.9060 - val_mse: 18070.9062 - val_mae: 38.8189\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 315us/step - loss: 1815.2642 - mse: 1815.2643 - mae: 30.2844 - val_loss: 17830.9062 - val_mse: 17830.9043 - val_mae: 38.0619\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 284us/step - loss: 1873.4209 - mse: 1873.4210 - mae: 30.4823 - val_loss: 17892.5524 - val_mse: 17892.5508 - val_mae: 38.2580\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 281us/step - loss: 2067.4358 - mse: 2067.4358 - mae: 31.8086 - val_loss: 17939.1793 - val_mse: 17939.1797 - val_mae: 38.4130\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 316us/step - loss: 1997.5887 - mse: 1997.5886 - mae: 31.2675 - val_loss: 17947.8930 - val_mse: 17947.8926 - val_mae: 38.4237\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 310us/step - loss: 1860.9705 - mse: 1860.9705 - mae: 29.5038 - val_loss: 17888.5305 - val_mse: 17888.5293 - val_mae: 38.2584\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 293us/step - loss: 2019.7725 - mse: 2019.7722 - mae: 31.0321 - val_loss: 17931.4363 - val_mse: 17931.4355 - val_mae: 38.4009\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 299us/step - loss: 1926.1890 - mse: 1926.1892 - mae: 30.3818 - val_loss: 18029.3888 - val_mse: 18029.3887 - val_mae: 38.8229\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 294us/step - loss: 1971.6452 - mse: 1971.6451 - mae: 30.1929 - val_loss: 17996.4398 - val_mse: 17996.4395 - val_mae: 38.6730\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 311us/step - loss: 1994.3329 - mse: 1994.3329 - mae: 31.9789 - val_loss: 18071.2193 - val_mse: 18071.2207 - val_mae: 39.0838\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 303us/step - loss: 2035.4742 - mse: 2035.4744 - mae: 31.7086 - val_loss: 18096.5922 - val_mse: 18096.5918 - val_mae: 39.2352\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 377us/step - loss: 1890.5458 - mse: 1890.5458 - mae: 29.8692 - val_loss: 17849.9900 - val_mse: 17849.9902 - val_mae: 38.2855\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 283us/step - loss: 1885.0474 - mse: 1885.0475 - mae: 29.4096 - val_loss: 17966.1948 - val_mse: 17966.1934 - val_mae: 38.6718\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4293.0332 - mse: 4293.0327 - mae: 35.0697 - val_loss: 2069.9597 - val_mse: 2069.9597 - val_mae: 30.9862\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 299us/step - loss: 4292.0394 - mse: 4292.0386 - mae: 36.8938 - val_loss: 2233.3452 - val_mse: 2233.3452 - val_mae: 31.4143\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4264.4777 - mse: 4264.4785 - mae: 35.2244 - val_loss: 2197.8938 - val_mse: 2197.8940 - val_mae: 31.2905\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 4328.4121 - mse: 4328.4121 - mae: 35.8979 - val_loss: 2251.2323 - val_mse: 2251.2322 - val_mae: 31.4242\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4291.6236 - mse: 4291.6235 - mae: 35.3445 - val_loss: 2282.4238 - val_mse: 2282.4238 - val_mae: 31.5242\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 348us/step - loss: 4368.4404 - mse: 4368.4399 - mae: 35.4903 - val_loss: 2253.2171 - val_mse: 2253.2170 - val_mae: 31.4226\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4251.0496 - mse: 4251.0493 - mae: 35.9284 - val_loss: 2278.0116 - val_mse: 2278.0115 - val_mae: 31.5043\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 268us/step - loss: 4209.6485 - mse: 4209.6484 - mae: 34.7667 - val_loss: 2253.9954 - val_mse: 2253.9956 - val_mae: 31.4199\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 343us/step - loss: 4294.7128 - mse: 4294.7139 - mae: 35.1765 - val_loss: 2222.8989 - val_mse: 2222.8987 - val_mae: 31.3248\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4417.4164 - mse: 4417.4170 - mae: 36.2747 - val_loss: 2249.4244 - val_mse: 2249.4246 - val_mae: 31.3900\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 265us/step - loss: 4233.7241 - mse: 4233.7241 - mae: 34.5910 - val_loss: 2202.6464 - val_mse: 2202.6462 - val_mae: 31.2461\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 4231.4171 - mse: 4231.4175 - mae: 35.5241 - val_loss: 2168.5933 - val_mse: 2168.5933 - val_mae: 31.1376\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 0s 306us/step - loss: 4327.8085 - mse: 4327.8086 - mae: 35.7910 - val_loss: 2201.2563 - val_mse: 2201.2563 - val_mae: 31.2172\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 0s 304us/step - loss: 4344.1004 - mse: 4344.1001 - mae: 35.8876 - val_loss: 2213.5017 - val_mse: 2213.5017 - val_mae: 31.2410\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 0s 264us/step - loss: 4091.9059 - mse: 4091.9055 - mae: 34.9029 - val_loss: 2268.6433 - val_mse: 2268.6433 - val_mae: 31.3890\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 4108.9745 - mse: 4108.9746 - mae: 33.7517 - val_loss: 2158.6413 - val_mse: 2158.6414 - val_mae: 31.0567\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4366.2285 - mse: 4366.2290 - mae: 35.5061 - val_loss: 2271.8402 - val_mse: 2271.8401 - val_mae: 31.3765\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 310us/step - loss: 4253.4774 - mse: 4253.4771 - mae: 35.5566 - val_loss: 2202.9991 - val_mse: 2202.9990 - val_mae: 31.1643\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 4279.2980 - mse: 4279.2974 - mae: 34.8465 - val_loss: 2222.1519 - val_mse: 2222.1521 - val_mae: 31.2107\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 0s 348us/step - loss: 4155.8876 - mse: 4155.8877 - mae: 35.0481 - val_loss: 2188.7871 - val_mse: 2188.7871 - val_mae: 31.1071\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 4136.1334 - mse: 4136.1333 - mae: 34.9880 - val_loss: 2216.3247 - val_mse: 2216.3247 - val_mae: 31.1909\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 0s 357us/step - loss: 4057.8788 - mse: 4057.8789 - mae: 34.8997 - val_loss: 2215.1893 - val_mse: 2215.1892 - val_mae: 31.1822\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4298.7380 - mse: 4298.7383 - mae: 35.4322 - val_loss: 2285.9725 - val_mse: 2285.9724 - val_mae: 31.4067\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 0s 342us/step - loss: 4205.9848 - mse: 4205.9844 - mae: 35.2332 - val_loss: 2252.7965 - val_mse: 2252.7964 - val_mae: 31.2920\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 318us/step - loss: 4360.6617 - mse: 4360.6616 - mae: 35.1991 - val_loss: 2303.6565 - val_mse: 2303.6565 - val_mae: 31.4331\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 337us/step - loss: 4225.2742 - mse: 4225.2744 - mae: 34.6032 - val_loss: 2241.8968 - val_mse: 2241.8967 - val_mae: 31.2197\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 336us/step - loss: 4313.2643 - mse: 4313.2637 - mae: 35.3881 - val_loss: 2386.7227 - val_mse: 2386.7224 - val_mae: 31.6977\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 298us/step - loss: 4085.6646 - mse: 4085.6658 - mae: 33.1892 - val_loss: 2185.9629 - val_mse: 2185.9629 - val_mae: 31.0236\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 238us/step - loss: 4290.9006 - mse: 4290.9009 - mae: 35.2176 - val_loss: 2239.3379 - val_mse: 2239.3381 - val_mae: 31.1872\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 4304.0761 - mse: 4304.0762 - mae: 34.6748 - val_loss: 2287.3228 - val_mse: 2287.3230 - val_mae: 31.3286\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 254us/step - loss: 4416.3023 - mse: 4416.3018 - mae: 36.1232 - val_loss: 2230.0601 - val_mse: 2230.0601 - val_mae: 31.1350\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 0s 314us/step - loss: 4192.3469 - mse: 4192.3472 - mae: 33.8376 - val_loss: 2196.1509 - val_mse: 2196.1506 - val_mae: 31.0234\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4308.2385 - mse: 4308.2388 - mae: 34.8063 - val_loss: 2257.4494 - val_mse: 2257.4497 - val_mae: 31.2290\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 242us/step - loss: 4044.4551 - mse: 4044.4551 - mae: 33.3341 - val_loss: 2174.9228 - val_mse: 2174.9229 - val_mae: 30.9633\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4187.4219 - mse: 4187.4214 - mae: 34.5820 - val_loss: 2230.3225 - val_mse: 2230.3225 - val_mae: 31.1261\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4175.8291 - mse: 4175.8286 - mae: 34.3020 - val_loss: 2227.9802 - val_mse: 2227.9802 - val_mae: 31.1027\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 0s 300us/step - loss: 4071.7645 - mse: 4071.7646 - mae: 33.7815 - val_loss: 2234.2760 - val_mse: 2234.2756 - val_mae: 31.1145\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 0s 309us/step - loss: 4044.9741 - mse: 4044.9744 - mae: 33.3437 - val_loss: 2199.6847 - val_mse: 2199.6846 - val_mae: 30.9946\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 0s 324us/step - loss: 4085.3998 - mse: 4085.4001 - mae: 34.2211 - val_loss: 2235.7711 - val_mse: 2235.7710 - val_mae: 31.0941\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 0s 301us/step - loss: 4054.7367 - mse: 4054.7371 - mae: 33.7081 - val_loss: 2216.9448 - val_mse: 2216.9448 - val_mae: 31.0375\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 0s 385us/step - loss: 4244.9266 - mse: 4244.9268 - mae: 34.6917 - val_loss: 2242.7496 - val_mse: 2242.7495 - val_mae: 31.1339\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4139.2046 - mse: 4139.2046 - mae: 34.2839 - val_loss: 2229.0740 - val_mse: 2229.0740 - val_mae: 31.0688\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 371us/step - loss: 4106.2814 - mse: 4106.2817 - mae: 34.0966 - val_loss: 2264.4024 - val_mse: 2264.4023 - val_mae: 31.1633\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 311us/step - loss: 4262.0581 - mse: 4262.0581 - mae: 34.6379 - val_loss: 2247.3392 - val_mse: 2247.3394 - val_mae: 31.0860\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 341us/step - loss: 4081.1991 - mse: 4081.1995 - mae: 32.7220 - val_loss: 2176.3887 - val_mse: 2176.3884 - val_mae: 30.8558\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 0s 282us/step - loss: 4259.9784 - mse: 4259.9785 - mae: 34.5667 - val_loss: 2214.4178 - val_mse: 2214.4177 - val_mae: 30.9671\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 0s 270us/step - loss: 4193.9427 - mse: 4193.9424 - mae: 34.3103 - val_loss: 2189.0207 - val_mse: 2189.0208 - val_mae: 30.8788\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 0s 307us/step - loss: 4145.5723 - mse: 4145.5723 - mae: 34.2558 - val_loss: 2270.5276 - val_mse: 2270.5276 - val_mae: 31.1437\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 313us/step - loss: 4238.1255 - mse: 4238.1265 - mae: 34.1599 - val_loss: 2340.4891 - val_mse: 2340.4893 - val_mae: 31.3797\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 0s 262us/step - loss: 4238.3811 - mse: 4238.3818 - mae: 33.9194 - val_loss: 2243.8785 - val_mse: 2243.8784 - val_mae: 31.0644\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 0s 315us/step - loss: 4170.1186 - mse: 4170.1187 - mae: 33.8904 - val_loss: 2281.4215 - val_mse: 2281.4216 - val_mae: 31.1687\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 0s 327us/step - loss: 4039.2228 - mse: 4039.2224 - mae: 33.5049 - val_loss: 2216.0409 - val_mse: 2216.0408 - val_mae: 30.9673\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 4138.3002 - mse: 4138.3003 - mae: 33.9724 - val_loss: 2165.9327 - val_mse: 2165.9329 - val_mae: 30.8025\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 286us/step - loss: 4151.5285 - mse: 4151.5288 - mae: 34.2234 - val_loss: 2179.6153 - val_mse: 2179.6152 - val_mae: 30.8285\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 4099.6601 - mse: 4099.6602 - mae: 33.7206 - val_loss: 2241.9908 - val_mse: 2241.9907 - val_mae: 31.0185\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 0s 317us/step - loss: 4206.0578 - mse: 4206.0581 - mae: 34.2323 - val_loss: 2261.5187 - val_mse: 2261.5186 - val_mae: 31.0937\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 320us/step - loss: 4138.9889 - mse: 4138.9888 - mae: 33.7041 - val_loss: 2252.3316 - val_mse: 2252.3318 - val_mae: 31.0571\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 293us/step - loss: 4208.4401 - mse: 4208.4404 - mae: 33.9538 - val_loss: 2273.7142 - val_mse: 2273.7141 - val_mae: 31.1365\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 0s 248us/step - loss: 4194.6005 - mse: 4194.6006 - mae: 33.7099 - val_loss: 2187.7962 - val_mse: 2187.7961 - val_mae: 30.8250\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 0s 244us/step - loss: 4050.3828 - mse: 4050.3826 - mae: 32.7536 - val_loss: 2174.0096 - val_mse: 2174.0095 - val_mae: 30.7655\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 0s 251us/step - loss: 4089.9416 - mse: 4089.9414 - mae: 33.4649 - val_loss: 2166.0191 - val_mse: 2166.0193 - val_mae: 30.7500\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 0s 294us/step - loss: 4209.1890 - mse: 4209.1895 - mae: 34.0036 - val_loss: 2251.3390 - val_mse: 2251.3391 - val_mae: 31.0340\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 0s 276us/step - loss: 4064.2982 - mse: 4064.2981 - mae: 33.0038 - val_loss: 2225.6444 - val_mse: 2225.6443 - val_mae: 30.9758\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4059.4392 - mse: 4059.4392 - mae: 33.1767 - val_loss: 2174.5879 - val_mse: 2174.5876 - val_mae: 30.8226\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 334us/step - loss: 3991.2976 - mse: 3991.2976 - mae: 33.4790 - val_loss: 2213.2345 - val_mse: 2213.2346 - val_mae: 30.9343\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 355us/step - loss: 4054.2788 - mse: 4054.2786 - mae: 33.9384 - val_loss: 2243.3516 - val_mse: 2243.3518 - val_mae: 31.0314\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 362us/step - loss: 4183.6140 - mse: 4183.6143 - mae: 33.8774 - val_loss: 2256.2918 - val_mse: 2256.2917 - val_mae: 31.0935\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 0s 234us/step - loss: 4112.2460 - mse: 4112.2461 - mae: 33.7538 - val_loss: 2234.5885 - val_mse: 2234.5889 - val_mae: 31.0120\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 0s 278us/step - loss: 4108.1689 - mse: 4108.1689 - mae: 33.8215 - val_loss: 2242.7829 - val_mse: 2242.7830 - val_mae: 31.0449\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 0s 292us/step - loss: 4027.8008 - mse: 4027.8003 - mae: 33.2189 - val_loss: 2273.1017 - val_mse: 2273.1016 - val_mae: 31.1388\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 0s 291us/step - loss: 4036.9274 - mse: 4036.9272 - mae: 32.6381 - val_loss: 2241.5405 - val_mse: 2241.5403 - val_mae: 31.0349\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 3973.0932 - mse: 3973.0935 - mae: 32.5516 - val_loss: 2171.4708 - val_mse: 2171.4707 - val_mae: 30.8019\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 0s 312us/step - loss: 3990.8913 - mse: 3990.8914 - mae: 32.5807 - val_loss: 2253.7443 - val_mse: 2253.7444 - val_mae: 31.0609\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 349us/step - loss: 3977.5376 - mse: 3977.5374 - mae: 32.9887 - val_loss: 2169.7802 - val_mse: 2169.7800 - val_mae: 30.7906\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 0s 326us/step - loss: 3930.4241 - mse: 3930.4236 - mae: 33.6504 - val_loss: 2221.0673 - val_mse: 2221.0671 - val_mae: 30.9699\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 269us/step - loss: 4059.8091 - mse: 4059.8091 - mae: 32.6861 - val_loss: 2199.9674 - val_mse: 2199.9673 - val_mae: 30.9218\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 3922.8551 - mse: 3922.8552 - mae: 32.7143 - val_loss: 2198.4846 - val_mse: 2198.4846 - val_mae: 30.9240\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 281us/step - loss: 4130.4916 - mse: 4130.4912 - mae: 33.9675 - val_loss: 2264.7792 - val_mse: 2264.7793 - val_mae: 31.1166\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 288us/step - loss: 4102.1511 - mse: 4102.1509 - mae: 33.0730 - val_loss: 2237.5682 - val_mse: 2237.5684 - val_mae: 31.0314\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 0s 275us/step - loss: 3918.2918 - mse: 3918.2915 - mae: 31.7346 - val_loss: 2216.6294 - val_mse: 2216.6292 - val_mae: 30.9462\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 0s 311us/step - loss: 3287.2354 - mse: 3287.2358 - mae: 31.9781 - val_loss: 1465.8143 - val_mse: 1465.8143 - val_mae: 26.0740\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3427.3962 - mse: 3427.3962 - mae: 34.1156 - val_loss: 1466.7587 - val_mse: 1466.7587 - val_mae: 25.6164\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3368.8061 - mse: 3368.8049 - mae: 32.3689 - val_loss: 1466.2111 - val_mse: 1466.2111 - val_mae: 25.7838\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 0s 334us/step - loss: 3311.8203 - mse: 3311.8208 - mae: 33.0129 - val_loss: 1466.3846 - val_mse: 1466.3845 - val_mae: 25.9685\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 0s 267us/step - loss: 3338.6634 - mse: 3338.6633 - mae: 33.2420 - val_loss: 1467.9544 - val_mse: 1467.9543 - val_mae: 25.4899\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 0s 313us/step - loss: 3404.2688 - mse: 3404.2688 - mae: 33.1030 - val_loss: 1465.8328 - val_mse: 1465.8328 - val_mae: 25.9661\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 0s 314us/step - loss: 3404.5865 - mse: 3404.5869 - mae: 33.2706 - val_loss: 1464.9759 - val_mse: 1464.9761 - val_mae: 25.6775\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3372.2369 - mse: 3372.2366 - mae: 32.8440 - val_loss: 1464.9490 - val_mse: 1464.9489 - val_mae: 25.8670\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3463.2886 - mse: 3463.2886 - mae: 33.3443 - val_loss: 1468.3098 - val_mse: 1468.3097 - val_mae: 25.3546\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3356.1275 - mse: 3356.1267 - mae: 33.2707 - val_loss: 1466.2573 - val_mse: 1466.2572 - val_mae: 25.8055\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3315.7681 - mse: 3315.7678 - mae: 32.7159 - val_loss: 1466.7638 - val_mse: 1466.7640 - val_mae: 25.4679\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 0s 315us/step - loss: 3361.0997 - mse: 3361.0989 - mae: 33.1336 - val_loss: 1464.7095 - val_mse: 1464.7096 - val_mae: 25.5763\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 0s 248us/step - loss: 3370.6925 - mse: 3370.6929 - mae: 33.1668 - val_loss: 1464.7727 - val_mse: 1464.7727 - val_mae: 25.6912\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3289.6253 - mse: 3289.6248 - mae: 31.7820 - val_loss: 1468.4386 - val_mse: 1468.4387 - val_mae: 25.5867\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 0s 302us/step - loss: 3239.2300 - mse: 3239.2307 - mae: 31.8070 - val_loss: 1467.7537 - val_mse: 1467.7538 - val_mae: 26.1234\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 0s 216us/step - loss: 3340.4897 - mse: 3340.4902 - mae: 32.7603 - val_loss: 1466.8223 - val_mse: 1466.8224 - val_mae: 25.8896\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 0s 253us/step - loss: 3402.2132 - mse: 3402.2141 - mae: 33.0430 - val_loss: 1466.6997 - val_mse: 1466.6997 - val_mae: 25.8591\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 0s 214us/step - loss: 3271.2217 - mse: 3271.2217 - mae: 32.5582 - val_loss: 1465.4492 - val_mse: 1465.4492 - val_mae: 25.6606\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 0s 279us/step - loss: 3350.9366 - mse: 3350.9375 - mae: 32.8170 - val_loss: 1466.5397 - val_mse: 1466.5398 - val_mae: 25.6911\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 0s 266us/step - loss: 3209.4156 - mse: 3209.4153 - mae: 31.7751 - val_loss: 1470.3299 - val_mse: 1470.3298 - val_mae: 26.3126\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 0s 245us/step - loss: 3343.8846 - mse: 3343.8853 - mae: 32.1911 - val_loss: 1470.6462 - val_mse: 1470.6460 - val_mae: 26.1751\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 0s 316us/step - loss: 3511.1553 - mse: 3511.1565 - mae: 33.0995 - val_loss: 1469.9364 - val_mse: 1469.9365 - val_mae: 25.8608\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3372.3705 - mse: 3372.3708 - mae: 32.1763 - val_loss: 1470.1572 - val_mse: 1470.1571 - val_mae: 25.8347\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3343.6137 - mse: 3343.6130 - mae: 32.7368 - val_loss: 1472.6105 - val_mse: 1472.6106 - val_mae: 25.9041\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 0s 275us/step - loss: 3394.4368 - mse: 3394.4365 - mae: 32.6092 - val_loss: 1474.4046 - val_mse: 1474.4048 - val_mae: 25.4538\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 0s 310us/step - loss: 3326.2718 - mse: 3326.2720 - mae: 31.6675 - val_loss: 1475.1108 - val_mse: 1475.1106 - val_mae: 26.3781\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 0s 289us/step - loss: 3337.7850 - mse: 3337.7847 - mae: 32.5858 - val_loss: 1474.4474 - val_mse: 1474.4475 - val_mae: 25.9823\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3328.3564 - mse: 3328.3562 - mae: 32.2259 - val_loss: 1476.1810 - val_mse: 1476.1810 - val_mae: 25.8591\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3350.5383 - mse: 3350.5381 - mae: 33.1324 - val_loss: 1479.0928 - val_mse: 1479.0928 - val_mae: 25.8275\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 0s 290us/step - loss: 3283.3245 - mse: 3283.3245 - mae: 31.6535 - val_loss: 1480.2271 - val_mse: 1480.2273 - val_mae: 26.3745\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 0s 245us/step - loss: 3277.8360 - mse: 3277.8357 - mae: 32.2830 - val_loss: 1477.2277 - val_mse: 1477.2278 - val_mae: 26.1589\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 0s 301us/step - loss: 3379.9041 - mse: 3379.9033 - mae: 32.3566 - val_loss: 1474.3741 - val_mse: 1474.3740 - val_mae: 26.0567\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 0s 299us/step - loss: 3467.0568 - mse: 3467.0566 - mae: 32.8575 - val_loss: 1479.7086 - val_mse: 1479.7085 - val_mae: 25.3676\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 0s 327us/step - loss: 3343.2955 - mse: 3343.2959 - mae: 32.3684 - val_loss: 1476.9669 - val_mse: 1476.9668 - val_mae: 25.5203\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 0s 249us/step - loss: 3346.6985 - mse: 3346.6982 - mae: 32.3947 - val_loss: 1474.9650 - val_mse: 1474.9651 - val_mae: 25.6904\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3336.4670 - mse: 3336.4656 - mae: 31.6499 - val_loss: 1476.0856 - val_mse: 1476.0854 - val_mae: 25.9064\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 0s 276us/step - loss: 3231.4003 - mse: 3231.4004 - mae: 31.8325 - val_loss: 1477.4196 - val_mse: 1477.4197 - val_mae: 25.7832\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 0s 312us/step - loss: 3330.3077 - mse: 3330.3083 - mae: 32.6840 - val_loss: 1477.1825 - val_mse: 1477.1825 - val_mae: 26.1917\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3284.3588 - mse: 3284.3586 - mae: 31.8934 - val_loss: 1478.7240 - val_mse: 1478.7239 - val_mae: 26.1318\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 0s 319us/step - loss: 3371.4527 - mse: 3371.4529 - mae: 31.3371 - val_loss: 1482.9254 - val_mse: 1482.9254 - val_mae: 26.5818\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 0s 329us/step - loss: 3246.3789 - mse: 3246.3789 - mae: 32.5049 - val_loss: 1481.4443 - val_mse: 1481.4443 - val_mae: 25.7365\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 0s 271us/step - loss: 3272.2203 - mse: 3272.2207 - mae: 31.7068 - val_loss: 1482.2241 - val_mse: 1482.2241 - val_mae: 26.3429\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 0s 272us/step - loss: 3289.1524 - mse: 3289.1526 - mae: 32.5680 - val_loss: 1480.0111 - val_mse: 1480.0110 - val_mae: 26.1318\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 0s 296us/step - loss: 3304.3738 - mse: 3304.3733 - mae: 32.4011 - val_loss: 1477.2292 - val_mse: 1477.2292 - val_mae: 26.1476\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 0s 287us/step - loss: 3287.8267 - mse: 3287.8271 - mae: 31.9319 - val_loss: 1476.1757 - val_mse: 1476.1757 - val_mae: 26.2016\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3228.0148 - mse: 3228.0142 - mae: 31.8407 - val_loss: 1477.2932 - val_mse: 1477.2933 - val_mae: 26.4649\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3373.7758 - mse: 3373.7751 - mae: 32.6552 - val_loss: 1476.5343 - val_mse: 1476.5343 - val_mae: 26.2948\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 0s 269us/step - loss: 3236.9320 - mse: 3236.9321 - mae: 32.4218 - val_loss: 1476.2714 - val_mse: 1476.2714 - val_mae: 26.0362\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3222.6794 - mse: 3222.6797 - mae: 31.4839 - val_loss: 1478.3128 - val_mse: 1478.3129 - val_mae: 25.9804\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 0s 332us/step - loss: 3299.3894 - mse: 3299.3896 - mae: 32.2429 - val_loss: 1481.0347 - val_mse: 1481.0345 - val_mae: 25.8443\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3399.5729 - mse: 3399.5735 - mae: 32.5343 - val_loss: 1481.3141 - val_mse: 1481.3143 - val_mae: 25.8564\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 0s 274us/step - loss: 3192.3084 - mse: 3192.3083 - mae: 30.9819 - val_loss: 1481.1533 - val_mse: 1481.1532 - val_mae: 26.3281\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 0s 305us/step - loss: 3363.5401 - mse: 3363.5405 - mae: 32.2674 - val_loss: 1481.4660 - val_mse: 1481.4661 - val_mae: 25.5814\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 345us/step - loss: 3264.0529 - mse: 3264.0522 - mae: 31.9041 - val_loss: 1480.0774 - val_mse: 1480.0774 - val_mae: 26.1149\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 0s 308us/step - loss: 3374.1804 - mse: 3374.1807 - mae: 32.9056 - val_loss: 1481.3788 - val_mse: 1481.3788 - val_mae: 25.6036\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 0s 307us/step - loss: 3233.9333 - mse: 3233.9331 - mae: 31.4168 - val_loss: 1477.8909 - val_mse: 1477.8909 - val_mae: 25.9338\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 0s 298us/step - loss: 3275.8157 - mse: 3275.8162 - mae: 31.7793 - val_loss: 1480.2697 - val_mse: 1480.2698 - val_mae: 26.3699\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 0s 216us/step - loss: 3292.7371 - mse: 3292.7361 - mae: 31.8810 - val_loss: 1479.7367 - val_mse: 1479.7366 - val_mae: 25.7482\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 0s 266us/step - loss: 3247.3337 - mse: 3247.3335 - mae: 31.9202 - val_loss: 1480.3801 - val_mse: 1480.3799 - val_mae: 26.2121\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3192.9263 - mse: 3192.9258 - mae: 31.5940 - val_loss: 1478.7848 - val_mse: 1478.7848 - val_mae: 26.0531\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 0s 283us/step - loss: 3310.6283 - mse: 3310.6282 - mae: 32.1372 - val_loss: 1479.5010 - val_mse: 1479.5010 - val_mae: 25.7484\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 0s 278us/step - loss: 3269.0822 - mse: 3269.0823 - mae: 31.7639 - val_loss: 1477.7057 - val_mse: 1477.7056 - val_mae: 25.9792\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 0s 332us/step - loss: 3360.0417 - mse: 3360.0417 - mae: 31.6428 - val_loss: 1477.8627 - val_mse: 1477.8627 - val_mae: 26.3234\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 0s 333us/step - loss: 3329.1255 - mse: 3329.1257 - mae: 32.5557 - val_loss: 1477.1450 - val_mse: 1477.1450 - val_mae: 25.6649\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 0s 247us/step - loss: 3274.7637 - mse: 3274.7634 - mae: 31.8856 - val_loss: 1476.9859 - val_mse: 1476.9858 - val_mae: 26.3984\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 0s 253us/step - loss: 3322.0950 - mse: 3322.0950 - mae: 32.2090 - val_loss: 1479.1072 - val_mse: 1479.1072 - val_mae: 26.5508\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 0s 219us/step - loss: 3122.5093 - mse: 3122.5093 - mae: 30.9569 - val_loss: 1478.2340 - val_mse: 1478.2340 - val_mae: 26.2181\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 0s 252us/step - loss: 3284.7587 - mse: 3284.7593 - mae: 31.9134 - val_loss: 1476.7166 - val_mse: 1476.7168 - val_mae: 25.7977\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 0s 286us/step - loss: 3282.1743 - mse: 3282.1743 - mae: 31.5043 - val_loss: 1475.9093 - val_mse: 1475.9093 - val_mae: 26.0490\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 0s 323us/step - loss: 3223.2759 - mse: 3223.2756 - mae: 32.5693 - val_loss: 1475.3245 - val_mse: 1475.3246 - val_mae: 26.0324\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 0s 251us/step - loss: 3248.5152 - mse: 3248.5151 - mae: 32.2632 - val_loss: 1475.0386 - val_mse: 1475.0386 - val_mae: 25.7635\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 0s 303us/step - loss: 3232.2631 - mse: 3232.2625 - mae: 31.6061 - val_loss: 1475.0528 - val_mse: 1475.0529 - val_mae: 25.7571\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 0s 256us/step - loss: 3299.8399 - mse: 3299.8401 - mae: 31.7713 - val_loss: 1474.1112 - val_mse: 1474.1111 - val_mae: 25.9690\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 0s 278us/step - loss: 3280.5591 - mse: 3280.5598 - mae: 31.9934 - val_loss: 1478.4839 - val_mse: 1478.4840 - val_mae: 26.6211\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 360us/step - loss: 3360.5986 - mse: 3360.5994 - mae: 32.6004 - val_loss: 1475.6464 - val_mse: 1475.6466 - val_mae: 25.6994\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3307.1044 - mse: 3307.1040 - mae: 31.7531 - val_loss: 1476.9996 - val_mse: 1476.9995 - val_mae: 26.4257\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 0s 260us/step - loss: 3112.8045 - mse: 3112.8040 - mae: 30.9804 - val_loss: 1479.9411 - val_mse: 1479.9410 - val_mae: 26.4060\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 0s 254us/step - loss: 3204.7488 - mse: 3204.7485 - mae: 31.1797 - val_loss: 1479.3157 - val_mse: 1479.3158 - val_mae: 26.3130\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 0s 300us/step - loss: 3163.1622 - mse: 3163.1619 - mae: 31.1655 - val_loss: 1478.4441 - val_mse: 1478.4442 - val_mae: 25.9055\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 0s 215us/step - loss: 3187.6639 - mse: 3187.6636 - mae: 30.9393 - val_loss: 1479.1378 - val_mse: 1479.1377 - val_mae: 25.9677\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 286us/step - loss: 2979.6355 - mse: 2979.6348 - mae: 31.2955 - val_loss: 1060.7120 - val_mse: 1060.7120 - val_mae: 23.6538\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 324us/step - loss: 2956.6222 - mse: 2956.6223 - mae: 30.7621 - val_loss: 1062.7082 - val_mse: 1062.7083 - val_mae: 23.4941\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2952.1557 - mse: 2952.1548 - mae: 31.1224 - val_loss: 1058.0860 - val_mse: 1058.0861 - val_mae: 23.5693\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 295us/step - loss: 2986.4913 - mse: 2986.4919 - mae: 31.6360 - val_loss: 1063.5921 - val_mse: 1063.5922 - val_mae: 23.3284\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 276us/step - loss: 2928.2859 - mse: 2928.2852 - mae: 31.1730 - val_loss: 1062.3581 - val_mse: 1062.3579 - val_mae: 23.2906\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2924.2391 - mse: 2924.2395 - mae: 31.0547 - val_loss: 1060.2712 - val_mse: 1060.2712 - val_mae: 23.3364\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 261us/step - loss: 2895.4490 - mse: 2895.4478 - mae: 31.2809 - val_loss: 1054.1455 - val_mse: 1054.1455 - val_mae: 23.4177\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 291us/step - loss: 2875.9608 - mse: 2875.9609 - mae: 31.0493 - val_loss: 1050.8184 - val_mse: 1050.8184 - val_mae: 23.4271\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 292us/step - loss: 2930.0984 - mse: 2930.0989 - mae: 30.9496 - val_loss: 1049.4385 - val_mse: 1049.4385 - val_mae: 23.5222\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2965.1344 - mse: 2965.1340 - mae: 31.5942 - val_loss: 1049.5344 - val_mse: 1049.5342 - val_mae: 23.4866\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2901.8832 - mse: 2901.8828 - mae: 30.9564 - val_loss: 1050.2668 - val_mse: 1050.2667 - val_mae: 23.4270\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 287us/step - loss: 2923.9398 - mse: 2923.9404 - mae: 31.4551 - val_loss: 1057.1604 - val_mse: 1057.1604 - val_mae: 23.1484\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2805.8092 - mse: 2805.8091 - mae: 30.1511 - val_loss: 1045.0465 - val_mse: 1045.0465 - val_mae: 23.7544\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 335us/step - loss: 2875.5166 - mse: 2875.5166 - mae: 30.9798 - val_loss: 1047.5732 - val_mse: 1047.5732 - val_mae: 23.4958\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2933.0671 - mse: 2933.0671 - mae: 31.0200 - val_loss: 1050.6913 - val_mse: 1050.6914 - val_mae: 23.3043\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2852.3135 - mse: 2852.3135 - mae: 30.3817 - val_loss: 1045.8049 - val_mse: 1045.8048 - val_mae: 23.4909\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 296us/step - loss: 2957.0744 - mse: 2957.0747 - mae: 31.7009 - val_loss: 1046.5721 - val_mse: 1046.5723 - val_mae: 23.5054\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 285us/step - loss: 2777.6557 - mse: 2777.6567 - mae: 30.5929 - val_loss: 1043.8071 - val_mse: 1043.8071 - val_mae: 23.7303\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2900.5195 - mse: 2900.5193 - mae: 30.8275 - val_loss: 1043.8950 - val_mse: 1043.8950 - val_mae: 23.6170\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 299us/step - loss: 2844.6269 - mse: 2844.6272 - mae: 31.1788 - val_loss: 1044.2085 - val_mse: 1044.2084 - val_mae: 23.4920\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 318us/step - loss: 2853.4152 - mse: 2853.4146 - mae: 31.0575 - val_loss: 1040.4375 - val_mse: 1040.4376 - val_mae: 23.8528\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 398us/step - loss: 2852.4149 - mse: 2852.4143 - mae: 31.0730 - val_loss: 1042.5175 - val_mse: 1042.5176 - val_mae: 23.6293\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2833.0324 - mse: 2833.0320 - mae: 30.4226 - val_loss: 1048.7873 - val_mse: 1048.7874 - val_mae: 23.2717\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 308us/step - loss: 2874.0033 - mse: 2874.0032 - mae: 30.8882 - val_loss: 1044.3191 - val_mse: 1044.3190 - val_mae: 23.4601\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 290us/step - loss: 2948.1472 - mse: 2948.1475 - mae: 31.2486 - val_loss: 1040.2884 - val_mse: 1040.2885 - val_mae: 23.7973\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2941.7897 - mse: 2941.7900 - mae: 31.2097 - val_loss: 1040.6356 - val_mse: 1040.6355 - val_mae: 23.7194\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 297us/step - loss: 2849.3394 - mse: 2849.3396 - mae: 31.1283 - val_loss: 1042.2111 - val_mse: 1042.2112 - val_mae: 23.5242\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 334us/step - loss: 2856.1303 - mse: 2856.1309 - mae: 30.5398 - val_loss: 1043.2540 - val_mse: 1043.2539 - val_mae: 23.3682\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 358us/step - loss: 2878.9504 - mse: 2878.9507 - mae: 30.7938 - val_loss: 1040.0645 - val_mse: 1040.0645 - val_mae: 23.6363\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2922.1355 - mse: 2922.1357 - mae: 30.8377 - val_loss: 1040.2757 - val_mse: 1040.2756 - val_mae: 23.5774\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2971.6289 - mse: 2971.6287 - mae: 31.4037 - val_loss: 1049.2136 - val_mse: 1049.2137 - val_mae: 23.1408\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 265us/step - loss: 2918.5495 - mse: 2918.5496 - mae: 30.9530 - val_loss: 1041.5843 - val_mse: 1041.5842 - val_mae: 23.5318\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2892.7458 - mse: 2892.7463 - mae: 30.7716 - val_loss: 1045.2331 - val_mse: 1045.2330 - val_mae: 23.2378\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 273us/step - loss: 2867.4398 - mse: 2867.4399 - mae: 30.4260 - val_loss: 1040.0575 - val_mse: 1040.0575 - val_mae: 23.5248\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 294us/step - loss: 2797.9258 - mse: 2797.9253 - mae: 30.6024 - val_loss: 1041.3180 - val_mse: 1041.3182 - val_mae: 23.3693\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2821.7911 - mse: 2821.7915 - mae: 30.3786 - val_loss: 1036.7951 - val_mse: 1036.7950 - val_mae: 23.5804\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 339us/step - loss: 2849.7735 - mse: 2849.7742 - mae: 30.3217 - val_loss: 1038.2904 - val_mse: 1038.2903 - val_mae: 23.4490\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 0s 222us/step - loss: 2891.7058 - mse: 2891.7048 - mae: 30.4894 - val_loss: 1036.5874 - val_mse: 1036.5874 - val_mae: 23.5731\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 0s 244us/step - loss: 2894.5953 - mse: 2894.5955 - mae: 30.7037 - val_loss: 1035.4864 - val_mse: 1035.4862 - val_mae: 23.7808\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 266us/step - loss: 2874.6605 - mse: 2874.6599 - mae: 30.6303 - val_loss: 1036.4957 - val_mse: 1036.4958 - val_mae: 23.6124\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 253us/step - loss: 2841.2845 - mse: 2841.2834 - mae: 30.6741 - val_loss: 1037.5749 - val_mse: 1037.5748 - val_mae: 23.4412\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 304us/step - loss: 2848.7325 - mse: 2848.7322 - mae: 30.6955 - val_loss: 1037.2341 - val_mse: 1037.2341 - val_mae: 23.4887\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 301us/step - loss: 2887.7872 - mse: 2887.7871 - mae: 30.0475 - val_loss: 1035.4236 - val_mse: 1035.4236 - val_mae: 23.6100\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 312us/step - loss: 2799.5870 - mse: 2799.5874 - mae: 30.5685 - val_loss: 1035.6324 - val_mse: 1035.6324 - val_mae: 23.5048\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 319us/step - loss: 2905.4722 - mse: 2905.4724 - mae: 31.2374 - val_loss: 1034.3091 - val_mse: 1034.3091 - val_mae: 23.5660\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 260us/step - loss: 2817.4861 - mse: 2817.4863 - mae: 30.5040 - val_loss: 1037.4919 - val_mse: 1037.4919 - val_mae: 23.3599\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2926.3177 - mse: 2926.3176 - mae: 30.6741 - val_loss: 1039.5984 - val_mse: 1039.5984 - val_mae: 23.2085\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2846.2229 - mse: 2846.2241 - mae: 30.1565 - val_loss: 1035.3882 - val_mse: 1035.3883 - val_mae: 23.3735\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 327us/step - loss: 2790.8790 - mse: 2790.8799 - mae: 30.6220 - val_loss: 1032.4594 - val_mse: 1032.4595 - val_mae: 23.5854\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 0s 250us/step - loss: 2791.8783 - mse: 2791.8784 - mae: 30.2038 - val_loss: 1030.8742 - val_mse: 1030.8741 - val_mae: 23.8537\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 275us/step - loss: 2847.8701 - mse: 2847.8696 - mae: 30.7276 - val_loss: 1034.2659 - val_mse: 1034.2659 - val_mae: 23.4370\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 0s 223us/step - loss: 2905.4946 - mse: 2905.4934 - mae: 30.7151 - val_loss: 1036.0215 - val_mse: 1036.0216 - val_mae: 23.2939\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 274us/step - loss: 2873.4451 - mse: 2873.4448 - mae: 30.4491 - val_loss: 1033.9306 - val_mse: 1033.9307 - val_mae: 23.4515\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 322us/step - loss: 2884.9126 - mse: 2884.9116 - mae: 30.9180 - val_loss: 1035.7825 - val_mse: 1035.7825 - val_mae: 23.3011\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 315us/step - loss: 2817.4578 - mse: 2817.4578 - mae: 30.4476 - val_loss: 1033.5393 - val_mse: 1033.5393 - val_mae: 23.4508\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 314us/step - loss: 2834.8870 - mse: 2834.8875 - mae: 30.7057 - val_loss: 1033.5805 - val_mse: 1033.5804 - val_mae: 23.4169\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 300us/step - loss: 2776.1168 - mse: 2776.1165 - mae: 30.4239 - val_loss: 1032.9931 - val_mse: 1032.9932 - val_mae: 23.4796\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 305us/step - loss: 2898.2722 - mse: 2898.2712 - mae: 30.6079 - val_loss: 1033.3078 - val_mse: 1033.3077 - val_mae: 23.5464\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2767.2227 - mse: 2767.2227 - mae: 30.1629 - val_loss: 1030.3973 - val_mse: 1030.3973 - val_mae: 23.7679\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 302us/step - loss: 2825.2142 - mse: 2825.2146 - mae: 30.7377 - val_loss: 1032.2037 - val_mse: 1032.2037 - val_mae: 23.5782\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 326us/step - loss: 2697.7387 - mse: 2697.7393 - mae: 30.5379 - val_loss: 1030.9053 - val_mse: 1030.9053 - val_mae: 23.6697\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2851.5183 - mse: 2851.5178 - mae: 30.5844 - val_loss: 1030.1606 - val_mse: 1030.1605 - val_mae: 23.7273\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 311us/step - loss: 2791.1336 - mse: 2791.1326 - mae: 30.2449 - val_loss: 1030.7756 - val_mse: 1030.7756 - val_mae: 23.5736\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 309us/step - loss: 2745.6943 - mse: 2745.6941 - mae: 30.4537 - val_loss: 1028.7625 - val_mse: 1028.7627 - val_mae: 23.6859\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 366us/step - loss: 2797.0582 - mse: 2797.0591 - mae: 30.0499 - val_loss: 1029.2345 - val_mse: 1029.2346 - val_mae: 23.6341\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 265us/step - loss: 2806.9822 - mse: 2806.9822 - mae: 29.9344 - val_loss: 1027.7203 - val_mse: 1027.7202 - val_mae: 23.7719\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 275us/step - loss: 2862.6127 - mse: 2862.6128 - mae: 30.8420 - val_loss: 1028.9469 - val_mse: 1028.9469 - val_mae: 23.5162\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 313us/step - loss: 2873.6260 - mse: 2873.6255 - mae: 30.4272 - val_loss: 1030.4397 - val_mse: 1030.4397 - val_mae: 23.3360\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 323us/step - loss: 2855.6853 - mse: 2855.6853 - mae: 30.6199 - val_loss: 1026.1593 - val_mse: 1026.1593 - val_mae: 23.5399\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 321us/step - loss: 2871.1760 - mse: 2871.1760 - mae: 30.9339 - val_loss: 1027.1193 - val_mse: 1027.1191 - val_mae: 23.3849\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 329us/step - loss: 2837.1857 - mse: 2837.1853 - mae: 29.9195 - val_loss: 1025.8327 - val_mse: 1025.8328 - val_mae: 23.4590\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 306us/step - loss: 2808.0585 - mse: 2808.0593 - mae: 30.1525 - val_loss: 1026.6522 - val_mse: 1026.6522 - val_mae: 23.3546\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 307us/step - loss: 2816.2256 - mse: 2816.2253 - mae: 30.4557 - val_loss: 1024.7690 - val_mse: 1024.7690 - val_mae: 23.5114\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 280us/step - loss: 2826.5889 - mse: 2826.5889 - mae: 30.2888 - val_loss: 1023.5368 - val_mse: 1023.5369 - val_mae: 23.6415\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 293us/step - loss: 2819.8434 - mse: 2819.8425 - mae: 30.3345 - val_loss: 1025.4343 - val_mse: 1025.4343 - val_mae: 23.4551\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 349us/step - loss: 2909.9054 - mse: 2909.9055 - mae: 31.0482 - val_loss: 1030.5325 - val_mse: 1030.5325 - val_mae: 23.0695\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 252us/step - loss: 2732.9585 - mse: 2732.9583 - mae: 29.4774 - val_loss: 1022.3107 - val_mse: 1022.3107 - val_mae: 23.6930\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 331us/step - loss: 2840.1417 - mse: 2840.1411 - mae: 30.5165 - val_loss: 1028.3295 - val_mse: 1028.3296 - val_mae: 23.1458\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 316us/step - loss: 2739.9039 - mse: 2739.9036 - mae: 29.7337 - val_loss: 1022.5567 - val_mse: 1022.5566 - val_mae: 23.7051\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 363us/step - loss: 2852.7886 - mse: 2852.7881 - mae: 30.2184 - val_loss: 1020.9573 - val_mse: 1020.9573 - val_mae: 23.7794\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2532.8721 - mse: 2532.8721 - mae: 29.6393 - val_loss: 1496.3837 - val_mse: 1496.3838 - val_mae: 27.3546\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 290us/step - loss: 2510.5202 - mse: 2510.5200 - mae: 29.8045 - val_loss: 1499.0438 - val_mse: 1499.0438 - val_mae: 27.1950\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 298us/step - loss: 2597.4751 - mse: 2597.4746 - mae: 30.0624 - val_loss: 1501.9800 - val_mse: 1501.9799 - val_mae: 27.0746\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 316us/step - loss: 2515.5199 - mse: 2515.5208 - mae: 29.8695 - val_loss: 1501.3764 - val_mse: 1501.3765 - val_mae: 27.0148\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 338us/step - loss: 2539.2501 - mse: 2539.2495 - mae: 29.6953 - val_loss: 1506.5697 - val_mse: 1506.5696 - val_mae: 26.8857\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2537.3946 - mse: 2537.3940 - mae: 29.6966 - val_loss: 1513.3259 - val_mse: 1513.3259 - val_mae: 26.7208\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2512.5513 - mse: 2512.5515 - mae: 29.2870 - val_loss: 1504.2500 - val_mse: 1504.2498 - val_mae: 26.8792\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2566.0458 - mse: 2566.0454 - mae: 29.9451 - val_loss: 1504.2999 - val_mse: 1504.2999 - val_mae: 26.8548\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2585.1527 - mse: 2585.1521 - mae: 29.7834 - val_loss: 1491.0726 - val_mse: 1491.0728 - val_mae: 27.1099\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2479.2574 - mse: 2479.2573 - mae: 29.3741 - val_loss: 1484.0541 - val_mse: 1484.0542 - val_mae: 27.2165\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2546.8534 - mse: 2546.8533 - mae: 29.7925 - val_loss: 1492.6208 - val_mse: 1492.6207 - val_mae: 26.9931\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 317us/step - loss: 2532.2852 - mse: 2532.2856 - mae: 29.6183 - val_loss: 1476.8567 - val_mse: 1476.8567 - val_mae: 27.3203\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2545.3957 - mse: 2545.3958 - mae: 29.5658 - val_loss: 1507.0290 - val_mse: 1507.0291 - val_mae: 26.4870\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 297us/step - loss: 2553.3056 - mse: 2553.3059 - mae: 29.5415 - val_loss: 1479.3602 - val_mse: 1479.3602 - val_mae: 27.0541\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2538.7903 - mse: 2538.7913 - mae: 29.7466 - val_loss: 1478.7150 - val_mse: 1478.7150 - val_mae: 27.0385\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2486.3838 - mse: 2486.3838 - mae: 29.1205 - val_loss: 1479.7150 - val_mse: 1479.7150 - val_mae: 26.9473\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2520.5969 - mse: 2520.5977 - mae: 29.8174 - val_loss: 1475.8497 - val_mse: 1475.8494 - val_mae: 27.0357\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2524.6244 - mse: 2524.6238 - mae: 29.4334 - val_loss: 1477.1775 - val_mse: 1477.1776 - val_mae: 26.9453\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 271us/step - loss: 2564.0477 - mse: 2564.0474 - mae: 29.7153 - val_loss: 1473.9519 - val_mse: 1473.9519 - val_mae: 26.9871\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 283us/step - loss: 2508.0500 - mse: 2508.0503 - mae: 29.2142 - val_loss: 1487.2834 - val_mse: 1487.2833 - val_mae: 26.6965\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 308us/step - loss: 2546.6152 - mse: 2546.6152 - mae: 29.5098 - val_loss: 1472.3061 - val_mse: 1472.3060 - val_mae: 26.9995\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 288us/step - loss: 2497.8578 - mse: 2497.8574 - mae: 29.5468 - val_loss: 1478.1150 - val_mse: 1478.1151 - val_mae: 26.8218\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 299us/step - loss: 2526.3992 - mse: 2526.3987 - mae: 29.7890 - val_loss: 1480.3770 - val_mse: 1480.3770 - val_mae: 26.6422\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 295us/step - loss: 2566.4924 - mse: 2566.4924 - mae: 29.8408 - val_loss: 1486.5326 - val_mse: 1486.5326 - val_mae: 26.4631\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2476.6102 - mse: 2476.6104 - mae: 29.4579 - val_loss: 1472.1917 - val_mse: 1472.1915 - val_mae: 26.8000\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2538.3677 - mse: 2538.3677 - mae: 29.4445 - val_loss: 1476.1378 - val_mse: 1476.1377 - val_mae: 26.6908\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2509.1890 - mse: 2509.1892 - mae: 29.4244 - val_loss: 1478.4826 - val_mse: 1478.4828 - val_mae: 26.5876\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 333us/step - loss: 2508.6881 - mse: 2508.6877 - mae: 29.6939 - val_loss: 1476.6720 - val_mse: 1476.6720 - val_mae: 26.6106\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2530.8939 - mse: 2530.8943 - mae: 29.2367 - val_loss: 1471.0330 - val_mse: 1471.0330 - val_mae: 26.7405\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2493.1825 - mse: 2493.1826 - mae: 29.6404 - val_loss: 1468.8885 - val_mse: 1468.8885 - val_mae: 26.7066\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 328us/step - loss: 2531.4719 - mse: 2531.4722 - mae: 29.4440 - val_loss: 1475.1426 - val_mse: 1475.1425 - val_mae: 26.5421\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 345us/step - loss: 2553.7507 - mse: 2553.7502 - mae: 29.5634 - val_loss: 1461.5800 - val_mse: 1461.5801 - val_mae: 26.8828\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 301us/step - loss: 2472.0658 - mse: 2472.0657 - mae: 29.3366 - val_loss: 1469.0036 - val_mse: 1469.0037 - val_mae: 26.6239\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 325us/step - loss: 2506.8888 - mse: 2506.8884 - mae: 29.2257 - val_loss: 1466.8818 - val_mse: 1466.8817 - val_mae: 26.6718\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2551.6772 - mse: 2551.6770 - mae: 29.5910 - val_loss: 1465.9061 - val_mse: 1465.9059 - val_mae: 26.6858\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 303us/step - loss: 2517.2683 - mse: 2517.2681 - mae: 29.6455 - val_loss: 1472.6476 - val_mse: 1472.6478 - val_mae: 26.5551\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 340us/step - loss: 2503.6911 - mse: 2503.6921 - mae: 29.5158 - val_loss: 1459.0241 - val_mse: 1459.0240 - val_mae: 26.8603\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 348us/step - loss: 2482.2065 - mse: 2482.2065 - mae: 29.5785 - val_loss: 1451.8253 - val_mse: 1451.8253 - val_mae: 26.9948\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2486.0356 - mse: 2486.0356 - mae: 29.2308 - val_loss: 1457.1385 - val_mse: 1457.1385 - val_mae: 26.8192\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 329us/step - loss: 2540.3350 - mse: 2540.3340 - mae: 29.8954 - val_loss: 1456.2130 - val_mse: 1456.2130 - val_mae: 26.8493\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 269us/step - loss: 2509.8620 - mse: 2509.8618 - mae: 29.5815 - val_loss: 1458.3048 - val_mse: 1458.3048 - val_mae: 26.7449\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 334us/step - loss: 2418.1751 - mse: 2418.1746 - mae: 29.1330 - val_loss: 1459.7174 - val_mse: 1459.7173 - val_mae: 26.6965\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2468.4824 - mse: 2468.4822 - mae: 29.2889 - val_loss: 1459.1760 - val_mse: 1459.1758 - val_mae: 26.6402\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 314us/step - loss: 2502.7456 - mse: 2502.7456 - mae: 29.2681 - val_loss: 1456.7867 - val_mse: 1456.7869 - val_mae: 26.6376\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 304us/step - loss: 2465.0034 - mse: 2465.0039 - mae: 28.9590 - val_loss: 1457.9867 - val_mse: 1457.9868 - val_mae: 26.5789\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 296us/step - loss: 2519.9347 - mse: 2519.9348 - mae: 29.6890 - val_loss: 1478.9203 - val_mse: 1478.9203 - val_mae: 26.1229\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 307us/step - loss: 2515.5641 - mse: 2515.5649 - mae: 29.4641 - val_loss: 1458.0510 - val_mse: 1458.0511 - val_mae: 26.5363\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2552.6706 - mse: 2552.6704 - mae: 29.8251 - val_loss: 1449.2336 - val_mse: 1449.2335 - val_mae: 26.7736\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2467.0156 - mse: 2467.0164 - mae: 29.0981 - val_loss: 1451.4751 - val_mse: 1451.4750 - val_mae: 26.6503\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2557.9232 - mse: 2557.9246 - mae: 29.8366 - val_loss: 1460.9374 - val_mse: 1460.9374 - val_mae: 26.4330\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 231us/step - loss: 2504.9914 - mse: 2504.9915 - mae: 29.3596 - val_loss: 1453.1155 - val_mse: 1453.1154 - val_mae: 26.5791\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 361us/step - loss: 2499.7612 - mse: 2499.7617 - mae: 29.3350 - val_loss: 1452.2297 - val_mse: 1452.2296 - val_mae: 26.5888\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 357us/step - loss: 2456.6591 - mse: 2456.6592 - mae: 29.2416 - val_loss: 1452.9571 - val_mse: 1452.9569 - val_mae: 26.5199\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2478.5579 - mse: 2478.5571 - mae: 29.0818 - val_loss: 1459.2089 - val_mse: 1459.2090 - val_mae: 26.2910\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 257us/step - loss: 2490.2562 - mse: 2490.2563 - mae: 29.3221 - val_loss: 1454.8107 - val_mse: 1454.8108 - val_mae: 26.3160\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2497.3565 - mse: 2497.3569 - mae: 29.3746 - val_loss: 1445.6012 - val_mse: 1445.6012 - val_mae: 26.5528\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2462.4649 - mse: 2462.4653 - mae: 29.0259 - val_loss: 1439.7893 - val_mse: 1439.7892 - val_mae: 26.6440\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 270us/step - loss: 2535.6751 - mse: 2535.6760 - mae: 29.2667 - val_loss: 1443.0655 - val_mse: 1443.0656 - val_mae: 26.6745\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 343us/step - loss: 2453.2931 - mse: 2453.2930 - mae: 29.2101 - val_loss: 1443.0422 - val_mse: 1443.0424 - val_mae: 26.6904\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 326us/step - loss: 2422.0158 - mse: 2422.0159 - mae: 28.8964 - val_loss: 1450.5676 - val_mse: 1450.5676 - val_mae: 26.4530\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 319us/step - loss: 2466.2921 - mse: 2466.2915 - mae: 29.2747 - val_loss: 1449.4203 - val_mse: 1449.4203 - val_mae: 26.4077\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 275us/step - loss: 2450.1131 - mse: 2450.1125 - mae: 29.3806 - val_loss: 1446.8883 - val_mse: 1446.8883 - val_mae: 26.3429\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 260us/step - loss: 2515.7706 - mse: 2515.7705 - mae: 29.9864 - val_loss: 1457.8790 - val_mse: 1457.8790 - val_mae: 26.1000\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 341us/step - loss: 2520.2635 - mse: 2520.2637 - mae: 29.1406 - val_loss: 1444.3292 - val_mse: 1444.3290 - val_mae: 26.3980\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2474.8003 - mse: 2474.8000 - mae: 29.0115 - val_loss: 1444.5635 - val_mse: 1444.5634 - val_mae: 26.3531\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2499.5322 - mse: 2499.5322 - mae: 28.7905 - val_loss: 1449.6190 - val_mse: 1449.6191 - val_mae: 26.1614\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 292us/step - loss: 2459.2074 - mse: 2459.2068 - mae: 29.0243 - val_loss: 1461.5031 - val_mse: 1461.5031 - val_mae: 25.8831\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 272us/step - loss: 2444.6675 - mse: 2444.6680 - mae: 29.3386 - val_loss: 1454.7181 - val_mse: 1454.7181 - val_mae: 25.9181\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 310us/step - loss: 2428.3892 - mse: 2428.3887 - mae: 29.3241 - val_loss: 1443.3135 - val_mse: 1443.3135 - val_mae: 26.0587\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 328us/step - loss: 2528.8339 - mse: 2528.8333 - mae: 29.0837 - val_loss: 1438.0578 - val_mse: 1438.0579 - val_mae: 26.1343\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 300us/step - loss: 2493.7239 - mse: 2493.7246 - mae: 29.3583 - val_loss: 1429.7358 - val_mse: 1429.7357 - val_mae: 26.3717\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 306us/step - loss: 2476.8597 - mse: 2476.8599 - mae: 29.1813 - val_loss: 1446.5416 - val_mse: 1446.5416 - val_mae: 26.0162\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 291us/step - loss: 2455.7873 - mse: 2455.7878 - mae: 29.1175 - val_loss: 1423.9696 - val_mse: 1423.9696 - val_mae: 26.5297\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 318us/step - loss: 2459.7266 - mse: 2459.7271 - mae: 29.0809 - val_loss: 1435.2152 - val_mse: 1435.2153 - val_mae: 26.1670\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 279us/step - loss: 2476.0760 - mse: 2476.0754 - mae: 28.7660 - val_loss: 1435.3417 - val_mse: 1435.3417 - val_mae: 26.1994\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 281us/step - loss: 2441.9134 - mse: 2441.9131 - mae: 28.7423 - val_loss: 1427.2041 - val_mse: 1427.2042 - val_mae: 26.3980\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 289us/step - loss: 2470.5239 - mse: 2470.5244 - mae: 28.8887 - val_loss: 1436.6904 - val_mse: 1436.6904 - val_mae: 26.1053\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 262us/step - loss: 2520.7434 - mse: 2520.7437 - mae: 29.3173 - val_loss: 1428.7919 - val_mse: 1428.7919 - val_mae: 26.3011\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 284us/step - loss: 2443.2976 - mse: 2443.2981 - mae: 29.3548 - val_loss: 1441.8362 - val_mse: 1441.8361 - val_mae: 25.9832\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 305us/step - loss: 2456.9268 - mse: 2456.9270 - mae: 29.0020 - val_loss: 1431.1952 - val_mse: 1431.1951 - val_mae: 26.2062\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2284.7761 - mse: 2284.7751 - mae: 29.4360 - val_loss: 3709.0386 - val_mse: 3709.0381 - val_mae: 23.2053\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2325.4042 - mse: 2325.4038 - mae: 29.2015 - val_loss: 3708.5951 - val_mse: 3708.5947 - val_mae: 23.8091\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2353.7165 - mse: 2353.7158 - mae: 29.3395 - val_loss: 3711.3922 - val_mse: 3711.3926 - val_mae: 24.0027\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 1s 272us/step - loss: 2354.7329 - mse: 2354.7332 - mae: 29.4114 - val_loss: 3712.2248 - val_mse: 3712.2246 - val_mae: 23.6918\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2319.3979 - mse: 2319.3972 - mae: 29.4471 - val_loss: 3711.7645 - val_mse: 3711.7637 - val_mae: 24.0106\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2410.8694 - mse: 2410.8694 - mae: 29.8272 - val_loss: 3711.8312 - val_mse: 3711.8311 - val_mae: 23.8933\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 1s 283us/step - loss: 2371.9192 - mse: 2371.9192 - mae: 29.4209 - val_loss: 3712.6366 - val_mse: 3712.6365 - val_mae: 23.5370\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2325.7582 - mse: 2325.7578 - mae: 29.3526 - val_loss: 3710.4276 - val_mse: 3710.4268 - val_mae: 23.4158\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 1s 350us/step - loss: 2387.3557 - mse: 2387.3562 - mae: 29.6632 - val_loss: 3709.4631 - val_mse: 3709.4629 - val_mae: 22.9185\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2317.9585 - mse: 2317.9587 - mae: 29.2478 - val_loss: 3708.9916 - val_mse: 3708.9917 - val_mae: 23.5289\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2322.5611 - mse: 2322.5610 - mae: 29.1983 - val_loss: 3711.9840 - val_mse: 3711.9836 - val_mae: 23.9143\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2338.6587 - mse: 2338.6597 - mae: 29.3142 - val_loss: 3712.9113 - val_mse: 3712.9119 - val_mae: 23.0036\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 1s 287us/step - loss: 2364.5494 - mse: 2364.5488 - mae: 29.2722 - val_loss: 3714.1333 - val_mse: 3714.1338 - val_mae: 23.6723\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 1s 308us/step - loss: 2288.2889 - mse: 2288.2891 - mae: 28.8226 - val_loss: 3714.4982 - val_mse: 3714.4980 - val_mae: 23.6967\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2401.7739 - mse: 2401.7739 - mae: 29.6130 - val_loss: 3715.2127 - val_mse: 3715.2124 - val_mae: 23.7253\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 1s 243us/step - loss: 2350.7313 - mse: 2350.7317 - mae: 29.4584 - val_loss: 3716.0393 - val_mse: 3716.0398 - val_mae: 23.0449\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2327.9633 - mse: 2327.9631 - mae: 29.2066 - val_loss: 3716.2788 - val_mse: 3716.2781 - val_mae: 23.3903\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2366.0094 - mse: 2366.0093 - mae: 29.3160 - val_loss: 3718.0663 - val_mse: 3718.0669 - val_mae: 23.7309\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2411.8090 - mse: 2411.8101 - mae: 29.8970 - val_loss: 3716.8889 - val_mse: 3716.8899 - val_mae: 23.5908\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2372.8764 - mse: 2372.8767 - mae: 29.4693 - val_loss: 3716.1557 - val_mse: 3716.1560 - val_mae: 22.9300\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 1s 363us/step - loss: 2369.1484 - mse: 2369.1479 - mae: 29.0017 - val_loss: 3718.0897 - val_mse: 3718.0901 - val_mae: 23.1813\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 1s 310us/step - loss: 2270.0193 - mse: 2270.0188 - mae: 28.8900 - val_loss: 3720.2467 - val_mse: 3720.2471 - val_mae: 23.5934\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2302.7190 - mse: 2302.7188 - mae: 29.4042 - val_loss: 3720.2060 - val_mse: 3720.2058 - val_mae: 23.5570\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 1s 286us/step - loss: 2313.4367 - mse: 2313.4365 - mae: 29.3948 - val_loss: 3719.9933 - val_mse: 3719.9929 - val_mae: 23.0027\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 1s 274us/step - loss: 2315.3101 - mse: 2315.3105 - mae: 29.1310 - val_loss: 3720.8069 - val_mse: 3720.8079 - val_mae: 23.7562\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2343.4682 - mse: 2343.4692 - mae: 29.2431 - val_loss: 3722.5771 - val_mse: 3722.5769 - val_mae: 22.9999\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 317us/step - loss: 2328.1491 - mse: 2328.1492 - mae: 29.4108 - val_loss: 3725.7535 - val_mse: 3725.7537 - val_mae: 23.9813\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 1s 255us/step - loss: 2314.6808 - mse: 2314.6814 - mae: 29.0982 - val_loss: 3726.3925 - val_mse: 3726.3928 - val_mae: 23.8305\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 323us/step - loss: 2293.1175 - mse: 2293.1179 - mae: 29.3337 - val_loss: 3725.3793 - val_mse: 3725.3784 - val_mae: 22.9670\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 271us/step - loss: 2362.4839 - mse: 2362.4849 - mae: 29.4806 - val_loss: 3724.5115 - val_mse: 3724.5115 - val_mae: 23.6137\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2325.7829 - mse: 2325.7830 - mae: 29.1395 - val_loss: 3722.1605 - val_mse: 3722.1606 - val_mae: 23.3596\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 1s 298us/step - loss: 2347.7463 - mse: 2347.7461 - mae: 29.0931 - val_loss: 3722.4962 - val_mse: 3722.4973 - val_mae: 23.4719\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2401.3781 - mse: 2401.3784 - mae: 29.5585 - val_loss: 3723.2974 - val_mse: 3723.2971 - val_mae: 23.4307\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 1s 314us/step - loss: 2306.1777 - mse: 2306.1775 - mae: 28.7096 - val_loss: 3721.7237 - val_mse: 3721.7236 - val_mae: 23.0718\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2333.7783 - mse: 2333.7778 - mae: 28.8399 - val_loss: 3722.7762 - val_mse: 3722.7764 - val_mae: 24.1883\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2323.1590 - mse: 2323.1592 - mae: 29.0954 - val_loss: 3720.4323 - val_mse: 3720.4324 - val_mae: 23.2306\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 1s 256us/step - loss: 2328.0359 - mse: 2328.0359 - mae: 29.2653 - val_loss: 3720.6113 - val_mse: 3720.6113 - val_mae: 23.3886\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 297us/step - loss: 2436.6927 - mse: 2436.6931 - mae: 30.0057 - val_loss: 3720.2363 - val_mse: 3720.2366 - val_mae: 23.0419\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2275.8805 - mse: 2275.8806 - mae: 29.0144 - val_loss: 3720.1109 - val_mse: 3720.1101 - val_mae: 24.1143\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2310.5417 - mse: 2310.5415 - mae: 29.3827 - val_loss: 3720.7938 - val_mse: 3720.7942 - val_mae: 23.2897\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2376.5082 - mse: 2376.5078 - mae: 29.2303 - val_loss: 3719.0147 - val_mse: 3719.0154 - val_mae: 23.1860\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 267us/step - loss: 2285.8599 - mse: 2285.8589 - mae: 28.7846 - val_loss: 3720.8525 - val_mse: 3720.8518 - val_mae: 23.8390\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2326.0826 - mse: 2326.0830 - mae: 29.2510 - val_loss: 3726.9634 - val_mse: 3726.9636 - val_mae: 23.9178\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 278us/step - loss: 2297.9287 - mse: 2297.9290 - mae: 29.2214 - val_loss: 3727.8375 - val_mse: 3727.8376 - val_mae: 23.7429\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2344.8494 - mse: 2344.8496 - mae: 29.4420 - val_loss: 3730.7552 - val_mse: 3730.7551 - val_mae: 23.1467\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2281.4051 - mse: 2281.4050 - mae: 28.9788 - val_loss: 3729.9135 - val_mse: 3729.9136 - val_mae: 23.8004\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2294.0765 - mse: 2294.0762 - mae: 29.1841 - val_loss: 3730.9970 - val_mse: 3730.9971 - val_mae: 23.6399\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2294.6088 - mse: 2294.6089 - mae: 28.9891 - val_loss: 3730.0765 - val_mse: 3730.0769 - val_mae: 23.5719\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 1s 309us/step - loss: 2330.7654 - mse: 2330.7654 - mae: 29.3828 - val_loss: 3729.8092 - val_mse: 3729.8086 - val_mae: 23.7054\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2342.3127 - mse: 2342.3130 - mae: 29.2381 - val_loss: 3729.8007 - val_mse: 3729.8003 - val_mae: 23.3211\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2303.5345 - mse: 2303.5344 - mae: 29.0115 - val_loss: 3733.0902 - val_mse: 3733.0901 - val_mae: 23.9577\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2344.2416 - mse: 2344.2412 - mae: 28.9226 - val_loss: 3734.3835 - val_mse: 3734.3835 - val_mae: 24.0923\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 1s 296us/step - loss: 2306.1691 - mse: 2306.1689 - mae: 28.8377 - val_loss: 3732.1992 - val_mse: 3732.1987 - val_mae: 23.1881\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 1s 306us/step - loss: 2329.6401 - mse: 2329.6406 - mae: 29.3225 - val_loss: 3729.3448 - val_mse: 3729.3447 - val_mae: 23.4924\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 1s 342us/step - loss: 2315.2426 - mse: 2315.2427 - mae: 29.0570 - val_loss: 3729.9679 - val_mse: 3729.9673 - val_mae: 23.2693\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 1s 301us/step - loss: 2311.7233 - mse: 2311.7227 - mae: 28.8124 - val_loss: 3732.9330 - val_mse: 3732.9331 - val_mae: 23.9031\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2320.1366 - mse: 2320.1375 - mae: 29.0594 - val_loss: 3732.8146 - val_mse: 3732.8162 - val_mae: 23.7644\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 1s 292us/step - loss: 2308.3990 - mse: 2308.3992 - mae: 29.0532 - val_loss: 3735.1657 - val_mse: 3735.1653 - val_mae: 23.6697\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 284us/step - loss: 2295.0732 - mse: 2295.0735 - mae: 29.1635 - val_loss: 3735.7855 - val_mse: 3735.7854 - val_mae: 23.6308\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 333us/step - loss: 2293.4541 - mse: 2293.4546 - mae: 28.9291 - val_loss: 3733.2287 - val_mse: 3733.2285 - val_mae: 23.7312\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 1s 320us/step - loss: 2306.2297 - mse: 2306.2297 - mae: 29.2073 - val_loss: 3735.0263 - val_mse: 3735.0264 - val_mae: 23.6130\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 300us/step - loss: 2310.4212 - mse: 2310.4216 - mae: 29.2031 - val_loss: 3737.0661 - val_mse: 3737.0669 - val_mae: 23.6555\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 1s 327us/step - loss: 2279.6759 - mse: 2279.6758 - mae: 28.7323 - val_loss: 3731.9482 - val_mse: 3731.9470 - val_mae: 23.3940\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 270us/step - loss: 2337.8415 - mse: 2337.8413 - mae: 28.9327 - val_loss: 3732.2565 - val_mse: 3732.2556 - val_mae: 23.4739\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 1s 302us/step - loss: 2245.8822 - mse: 2245.8816 - mae: 28.8451 - val_loss: 3734.2398 - val_mse: 3734.2397 - val_mae: 23.5366\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 1s 311us/step - loss: 2290.7056 - mse: 2290.7056 - mae: 28.8635 - val_loss: 3735.8610 - val_mse: 3735.8611 - val_mae: 23.2561\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 1s 275us/step - loss: 2246.3822 - mse: 2246.3821 - mae: 28.6022 - val_loss: 3738.8121 - val_mse: 3738.8125 - val_mae: 23.1713\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 1s 257us/step - loss: 2346.6351 - mse: 2346.6357 - mae: 29.1865 - val_loss: 3736.0971 - val_mse: 3736.0972 - val_mae: 23.0528\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2335.5322 - mse: 2335.5320 - mae: 29.1602 - val_loss: 3739.2048 - val_mse: 3739.2056 - val_mae: 23.6977\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 1s 285us/step - loss: 2349.8034 - mse: 2349.8032 - mae: 29.1155 - val_loss: 3736.4144 - val_mse: 3736.4143 - val_mae: 23.4114\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 1s 325us/step - loss: 2326.5889 - mse: 2326.5886 - mae: 29.0501 - val_loss: 3736.9586 - val_mse: 3736.9583 - val_mae: 23.6307\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 1s 260us/step - loss: 2281.3788 - mse: 2281.3787 - mae: 28.7739 - val_loss: 3733.5184 - val_mse: 3733.5181 - val_mae: 23.3199\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 305us/step - loss: 2351.4650 - mse: 2351.4653 - mae: 29.4427 - val_loss: 3735.4731 - val_mse: 3735.4731 - val_mae: 23.4020\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 1s 248us/step - loss: 2323.0825 - mse: 2323.0823 - mae: 28.8944 - val_loss: 3736.8551 - val_mse: 3736.8538 - val_mae: 23.4456\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 1s 277us/step - loss: 2246.3656 - mse: 2246.3667 - mae: 28.5153 - val_loss: 3737.3920 - val_mse: 3737.3923 - val_mae: 23.4261\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 299us/step - loss: 2267.9026 - mse: 2267.9028 - mae: 28.9500 - val_loss: 3740.0064 - val_mse: 3740.0066 - val_mae: 23.6695\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 1s 332us/step - loss: 2276.6480 - mse: 2276.6482 - mae: 28.5843 - val_loss: 3741.9439 - val_mse: 3741.9441 - val_mae: 23.5434\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 1s 337us/step - loss: 2266.8251 - mse: 2266.8254 - mae: 28.6489 - val_loss: 3743.8805 - val_mse: 3743.8811 - val_mae: 23.8224\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 1s 235us/step - loss: 2320.0200 - mse: 2320.0193 - mae: 29.1037 - val_loss: 3739.2681 - val_mse: 3739.2681 - val_mae: 23.3106\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 1s 249us/step - loss: 2273.9216 - mse: 2273.9216 - mae: 28.9082 - val_loss: 3741.1941 - val_mse: 3741.1941 - val_mae: 23.4233\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 1s 267us/step - loss: 2672.6945 - mse: 2672.6943 - mae: 28.6099 - val_loss: 2053.5646 - val_mse: 2053.5645 - val_mae: 27.1064\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 1s 316us/step - loss: 2711.1959 - mse: 2711.1963 - mae: 28.5121 - val_loss: 2062.8480 - val_mse: 2062.8479 - val_mae: 26.9159\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 1s 327us/step - loss: 2705.2530 - mse: 2705.2522 - mae: 28.7083 - val_loss: 2072.2615 - val_mse: 2072.2617 - val_mae: 26.7953\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2727.4726 - mse: 2727.4727 - mae: 28.6367 - val_loss: 2078.0695 - val_mse: 2078.0693 - val_mae: 26.4400\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2726.8687 - mse: 2726.8694 - mae: 28.4231 - val_loss: 2074.5955 - val_mse: 2074.5952 - val_mae: 26.4067\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 1s 284us/step - loss: 2634.4373 - mse: 2634.4373 - mae: 28.0882 - val_loss: 2056.6050 - val_mse: 2056.6050 - val_mae: 27.0812\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2671.9472 - mse: 2671.9475 - mae: 28.6081 - val_loss: 2057.9738 - val_mse: 2057.9741 - val_mae: 27.1282\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 1s 282us/step - loss: 2698.0767 - mse: 2698.0771 - mae: 28.3682 - val_loss: 2066.3903 - val_mse: 2066.3906 - val_mae: 26.4961\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2669.7448 - mse: 2669.7446 - mae: 28.0560 - val_loss: 2060.0212 - val_mse: 2060.0210 - val_mae: 26.9678\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 1s 264us/step - loss: 2669.8015 - mse: 2669.8005 - mae: 28.0995 - val_loss: 2066.1202 - val_mse: 2066.1201 - val_mae: 26.7352\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2652.8011 - mse: 2652.8013 - mae: 28.4545 - val_loss: 2062.2289 - val_mse: 2062.2288 - val_mae: 27.4874\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2659.6302 - mse: 2659.6296 - mae: 28.1592 - val_loss: 2064.7197 - val_mse: 2064.7197 - val_mae: 26.8097\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2704.3777 - mse: 2704.3779 - mae: 28.5307 - val_loss: 2069.3431 - val_mse: 2069.3433 - val_mae: 26.8567\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 1s 313us/step - loss: 2726.3415 - mse: 2726.3423 - mae: 28.7614 - val_loss: 2085.3832 - val_mse: 2085.3828 - val_mae: 26.1004\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2705.5032 - mse: 2705.5044 - mae: 28.7398 - val_loss: 2074.8420 - val_mse: 2074.8418 - val_mae: 26.9621\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 1s 315us/step - loss: 2710.4595 - mse: 2710.4595 - mae: 28.7696 - val_loss: 2075.6063 - val_mse: 2075.6064 - val_mae: 26.8086\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 1s 328us/step - loss: 2665.2423 - mse: 2665.2424 - mae: 28.4901 - val_loss: 2073.5154 - val_mse: 2073.5159 - val_mae: 26.9676\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2666.6277 - mse: 2666.6267 - mae: 28.5900 - val_loss: 2084.0418 - val_mse: 2084.0417 - val_mae: 26.5163\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 1s 298us/step - loss: 2702.6784 - mse: 2702.6780 - mae: 28.6088 - val_loss: 2082.9373 - val_mse: 2082.9373 - val_mae: 26.5905\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 1s 327us/step - loss: 2676.7776 - mse: 2676.7773 - mae: 28.3751 - val_loss: 2084.8184 - val_mse: 2084.8186 - val_mae: 27.2215\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 1s 341us/step - loss: 2706.9093 - mse: 2706.9097 - mae: 28.5337 - val_loss: 2088.8678 - val_mse: 2088.8679 - val_mae: 26.6960\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 1s 319us/step - loss: 2706.9495 - mse: 2706.9490 - mae: 28.4791 - val_loss: 2084.8555 - val_mse: 2084.8555 - val_mae: 27.2613\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 1s 281us/step - loss: 2704.4410 - mse: 2704.4409 - mae: 28.5139 - val_loss: 2093.9645 - val_mse: 2093.9646 - val_mae: 26.9908\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 1s 267us/step - loss: 2712.2770 - mse: 2712.2769 - mae: 28.5492 - val_loss: 2093.3934 - val_mse: 2093.3933 - val_mae: 26.8299\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2697.7986 - mse: 2697.7993 - mae: 28.2880 - val_loss: 2090.7823 - val_mse: 2090.7822 - val_mae: 26.8522\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2695.4096 - mse: 2695.4104 - mae: 28.2094 - val_loss: 2094.7281 - val_mse: 2094.7283 - val_mae: 26.3879\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 1s 334us/step - loss: 2663.8939 - mse: 2663.8938 - mae: 27.6331 - val_loss: 2090.8380 - val_mse: 2090.8381 - val_mae: 26.8607\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 1s 345us/step - loss: 2723.1599 - mse: 2723.1594 - mae: 28.3354 - val_loss: 2086.9629 - val_mse: 2086.9629 - val_mae: 27.0668\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 1s 277us/step - loss: 2604.5622 - mse: 2604.5627 - mae: 28.1853 - val_loss: 2080.5838 - val_mse: 2080.5833 - val_mae: 27.3819\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 1s 290us/step - loss: 2639.1481 - mse: 2639.1479 - mae: 28.1995 - val_loss: 2081.8217 - val_mse: 2081.8218 - val_mae: 26.9050\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2717.5163 - mse: 2717.5168 - mae: 27.9689 - val_loss: 2081.6607 - val_mse: 2081.6609 - val_mae: 27.1690\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2660.9992 - mse: 2661.0002 - mae: 28.3661 - val_loss: 2081.4285 - val_mse: 2081.4282 - val_mae: 26.9759\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2709.7221 - mse: 2709.7217 - mae: 28.5592 - val_loss: 2079.3538 - val_mse: 2079.3538 - val_mae: 26.8193\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2685.2349 - mse: 2685.2344 - mae: 28.3245 - val_loss: 2079.2894 - val_mse: 2079.2893 - val_mae: 26.8891\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 1s 266us/step - loss: 2670.3845 - mse: 2670.3848 - mae: 28.2503 - val_loss: 2078.4915 - val_mse: 2078.4915 - val_mae: 26.6052\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2702.7812 - mse: 2702.7808 - mae: 28.5857 - val_loss: 2074.4044 - val_mse: 2074.4045 - val_mae: 26.9842\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 1s 322us/step - loss: 2663.6030 - mse: 2663.6033 - mae: 28.1663 - val_loss: 2080.2433 - val_mse: 2080.2434 - val_mae: 26.7627\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 1s 279us/step - loss: 2749.7380 - mse: 2749.7380 - mae: 28.8017 - val_loss: 2080.4128 - val_mse: 2080.4126 - val_mae: 26.8903\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 1s 283us/step - loss: 2616.9441 - mse: 2616.9431 - mae: 28.0454 - val_loss: 2077.0900 - val_mse: 2077.0903 - val_mae: 27.2356\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 1s 295us/step - loss: 2672.6257 - mse: 2672.6255 - mae: 28.2256 - val_loss: 2072.6547 - val_mse: 2072.6548 - val_mae: 27.3305\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2651.8855 - mse: 2651.8865 - mae: 28.3319 - val_loss: 2070.3089 - val_mse: 2070.3086 - val_mae: 27.2986\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 1s 323us/step - loss: 2671.6237 - mse: 2671.6221 - mae: 28.2787 - val_loss: 2072.2265 - val_mse: 2072.2263 - val_mae: 26.9416\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2665.5954 - mse: 2665.5957 - mae: 28.3711 - val_loss: 2070.2350 - val_mse: 2070.2346 - val_mae: 27.0441\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 1s 289us/step - loss: 2687.6329 - mse: 2687.6331 - mae: 28.1791 - val_loss: 2078.1882 - val_mse: 2078.1885 - val_mae: 26.6039\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 1s 333us/step - loss: 2620.3828 - mse: 2620.3816 - mae: 27.9317 - val_loss: 2072.0986 - val_mse: 2072.0986 - val_mae: 26.9279\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 1s 294us/step - loss: 2589.1929 - mse: 2589.1929 - mae: 28.2202 - val_loss: 2067.0253 - val_mse: 2067.0256 - val_mae: 26.9292\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 1s 252us/step - loss: 2653.0308 - mse: 2653.0310 - mae: 28.2450 - val_loss: 2066.8717 - val_mse: 2066.8718 - val_mae: 26.8392\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2665.0504 - mse: 2665.0491 - mae: 28.2998 - val_loss: 2072.4899 - val_mse: 2072.4897 - val_mae: 26.3877\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 1s 275us/step - loss: 2644.9589 - mse: 2644.9585 - mae: 28.1825 - val_loss: 2065.0636 - val_mse: 2065.0640 - val_mae: 26.9569\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 1s 264us/step - loss: 2635.9933 - mse: 2635.9929 - mae: 27.9192 - val_loss: 2059.1680 - val_mse: 2059.1682 - val_mae: 27.2499\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 1s 302us/step - loss: 2689.1410 - mse: 2689.1409 - mae: 28.6246 - val_loss: 2070.8733 - val_mse: 2070.8735 - val_mae: 26.8890\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - ETA: 0s - loss: 2728.2756 - mse: 2728.2756 - mae: 28.41 - 1s 285us/step - loss: 2658.8858 - mse: 2658.8855 - mae: 28.1525 - val_loss: 2068.8841 - val_mse: 2068.8845 - val_mae: 27.1034\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2675.1132 - mse: 2675.1145 - mae: 28.0360 - val_loss: 2068.1052 - val_mse: 2068.1052 - val_mae: 27.1993\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 1s 376us/step - loss: 2732.5361 - mse: 2732.5364 - mae: 28.4744 - val_loss: 2088.4373 - val_mse: 2088.4370 - val_mae: 26.5511\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 1s 303us/step - loss: 2676.8400 - mse: 2676.8394 - mae: 28.2986 - val_loss: 2080.1549 - val_mse: 2080.1548 - val_mae: 26.7737\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 1s 265us/step - loss: 2718.5936 - mse: 2718.5930 - mae: 28.3604 - val_loss: 2080.1135 - val_mse: 2080.1135 - val_mae: 26.6064\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 1s 317us/step - loss: 2664.3038 - mse: 2664.3035 - mae: 28.0952 - val_loss: 2078.7668 - val_mse: 2078.7671 - val_mae: 26.9027\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2663.4542 - mse: 2663.4539 - mae: 28.2876 - val_loss: 2074.7806 - val_mse: 2074.7803 - val_mae: 26.9394\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 1s 330us/step - loss: 2637.4085 - mse: 2637.4089 - mae: 28.1285 - val_loss: 2080.2464 - val_mse: 2080.2463 - val_mae: 26.9429\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 1s 292us/step - loss: 2645.5713 - mse: 2645.5713 - mae: 27.7900 - val_loss: 2076.2221 - val_mse: 2076.2222 - val_mae: 26.6201\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 1s 304us/step - loss: 2630.8307 - mse: 2630.8293 - mae: 28.0974 - val_loss: 2077.3954 - val_mse: 2077.3955 - val_mae: 27.1593\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 1s 287us/step - loss: 2620.3436 - mse: 2620.3435 - mae: 27.6599 - val_loss: 2068.2011 - val_mse: 2068.2012 - val_mae: 27.6280\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 1s 296us/step - loss: 2660.4095 - mse: 2660.4092 - mae: 28.3852 - val_loss: 2069.8113 - val_mse: 2069.8110 - val_mae: 26.8740\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2639.9839 - mse: 2639.9839 - mae: 27.9488 - val_loss: 2059.1665 - val_mse: 2059.1665 - val_mae: 27.4610\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2680.9926 - mse: 2680.9924 - mae: 28.4410 - val_loss: 2062.4076 - val_mse: 2062.4075 - val_mae: 27.2583\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 1s 325us/step - loss: 2605.9346 - mse: 2605.9348 - mae: 28.1397 - val_loss: 2060.2792 - val_mse: 2060.2795 - val_mae: 27.4103\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 1s 272us/step - loss: 2698.4954 - mse: 2698.4963 - mae: 28.5367 - val_loss: 2069.7019 - val_mse: 2069.7019 - val_mae: 27.0640\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 1s 305us/step - loss: 2643.0089 - mse: 2643.0088 - mae: 27.7332 - val_loss: 2067.8026 - val_mse: 2067.8025 - val_mae: 27.2762\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2680.7258 - mse: 2680.7256 - mae: 28.6348 - val_loss: 2073.3523 - val_mse: 2073.3523 - val_mae: 26.5615\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2615.2842 - mse: 2615.2837 - mae: 27.8024 - val_loss: 2071.4323 - val_mse: 2071.4326 - val_mae: 26.9791\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 1s 308us/step - loss: 2648.6259 - mse: 2648.6257 - mae: 28.0540 - val_loss: 2071.6139 - val_mse: 2071.6138 - val_mae: 26.9733\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 1s 324us/step - loss: 2601.9627 - mse: 2601.9617 - mae: 27.9796 - val_loss: 2078.4839 - val_mse: 2078.4839 - val_mae: 26.7528\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 1s 327us/step - loss: 2610.6600 - mse: 2610.6606 - mae: 27.5534 - val_loss: 2070.4906 - val_mse: 2070.4900 - val_mae: 27.0288\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 1s 318us/step - loss: 2594.1866 - mse: 2594.1870 - mae: 27.7887 - val_loss: 2063.3328 - val_mse: 2063.3330 - val_mae: 27.0081\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 1s 310us/step - loss: 2656.8657 - mse: 2656.8652 - mae: 28.2016 - val_loss: 2064.9710 - val_mse: 2064.9705 - val_mae: 26.6399\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 1s 301us/step - loss: 2654.7029 - mse: 2654.7036 - mae: 27.8313 - val_loss: 2056.7042 - val_mse: 2056.7041 - val_mae: 27.3271\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 306us/step - loss: 2656.6235 - mse: 2656.6233 - mae: 28.2375 - val_loss: 2061.3378 - val_mse: 2061.3381 - val_mae: 26.7966\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 271us/step - loss: 2597.7484 - mse: 2597.7476 - mae: 28.0516 - val_loss: 2058.0727 - val_mse: 2058.0730 - val_mae: 26.9635\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 321us/step - loss: 2627.5829 - mse: 2627.5825 - mae: 28.1971 - val_loss: 2057.9025 - val_mse: 2057.9026 - val_mae: 27.5656\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 291us/step - loss: 2626.6053 - mse: 2626.6055 - mae: 27.9683 - val_loss: 2065.1617 - val_mse: 2065.1619 - val_mae: 26.5066\n"
     ]
    }
   ],
   "source": [
    "# features list; order made according to Linear Regression FS\n",
    "features_list = ['APXP', \n",
    "                 'LOLP',  \n",
    "                 'In_gen',\n",
    "                 'Ren_R',\n",
    "                 'DA_imb_France', \n",
    "                 'Rene',\n",
    "                 'ratio_offers_vol',\n",
    "                 'DA_price_france',\n",
    "                 'TSDF',\n",
    "                 'dino_bin',\n",
    "                 'DA_margin',\n",
    "                 'Im_Pr']\n",
    "\n",
    "best_score = rmse_error\n",
    "\n",
    "for i in features_list: \n",
    "    \n",
    "   # X_recovery = X_.copy()\n",
    "    X_recovery = X_\n",
    "    \n",
    "    X_ = pd.concat([X_, X.loc[:,i]], axis = 1)\n",
    "                    \n",
    "    X_.fillna(method = 'ffill', inplace = True)\n",
    "    y.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "    X_ = X_.astype('float64')\n",
    "    X_ = X_.round(20)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_, y, test_size = 0.15, shuffle = False)\n",
    "    \n",
    "    sc_X = MinMaxScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "\n",
    "    # possible debug\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "\n",
    "    def regressor_tunning(n_hidden = 5, \n",
    "                          n_neurons = 40, \n",
    "                          kernel_initializer = \"he_normal\",\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = n_neurons, input_dim = len(X_.columns)))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.3))\n",
    "        for layer in range(n_hidden):\n",
    "            model.add(Dense(units = n_neurons))\n",
    "            model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(rate = 0.3))\n",
    "        model.add(Dense(units = 1, activation = 'linear'))\n",
    "        optimizer = optimizers.Adamax(lr = 0.001)\n",
    "        model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "        return model\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits = splits)    \n",
    "    regressor = regressor_tunning()\n",
    "\n",
    "    # train model\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "        y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "        regressor.fit(X_train_split, y_train_split,  \n",
    "                             shuffle = False, \n",
    "                             validation_split = 0.2,\n",
    "                             batch_size = 20, \n",
    "                             epochs = epochs)\n",
    "\n",
    "    # make predictions and evaluate for all regions\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRICS EVALUATION (1) for the whole test set\n",
    "    # =============================================================================\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    # calculate metrics\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "\n",
    "    # append to list\n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRICS EVALUATION (2) on spike regions\n",
    "    # =============================================================================\n",
    "\n",
    "    # download spike indication binary set\n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "    # append ot lists\n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRIC EVALUATION (3) on normal regions\n",
    "    # =============================================================================\n",
    "\n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "    # append to list\n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "\n",
    "    # condition of improvement for FS\n",
    "    if best_score < rmse_gen[-1]:\n",
    "        X_ = X_recovery\n",
    "    else:\n",
    "        X_ = X_\n",
    "        best_score = rmse_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PrevDay</th>\n",
       "      <td>30.869098</td>\n",
       "      <td>21.861864</td>\n",
       "      <td>43.221078</td>\n",
       "      <td>32.066380</td>\n",
       "      <td>28.550797</td>\n",
       "      <td>20.325845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APXP</th>\n",
       "      <td>30.299779</td>\n",
       "      <td>21.095644</td>\n",
       "      <td>41.852143</td>\n",
       "      <td>31.067724</td>\n",
       "      <td>28.153355</td>\n",
       "      <td>19.594611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOLP</th>\n",
       "      <td>30.296505</td>\n",
       "      <td>21.040219</td>\n",
       "      <td>41.849467</td>\n",
       "      <td>30.979879</td>\n",
       "      <td>28.149901</td>\n",
       "      <td>19.544066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In_gen</th>\n",
       "      <td>30.326971</td>\n",
       "      <td>21.207724</td>\n",
       "      <td>41.704116</td>\n",
       "      <td>30.869534</td>\n",
       "      <td>28.220027</td>\n",
       "      <td>19.753394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ren_R</th>\n",
       "      <td>30.688847</td>\n",
       "      <td>21.450446</td>\n",
       "      <td>41.143979</td>\n",
       "      <td>30.648447</td>\n",
       "      <td>28.788173</td>\n",
       "      <td>20.065931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_imb_France</th>\n",
       "      <td>30.420877</td>\n",
       "      <td>21.479184</td>\n",
       "      <td>41.842518</td>\n",
       "      <td>31.174677</td>\n",
       "      <td>28.305347</td>\n",
       "      <td>20.019783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rene</th>\n",
       "      <td>30.525093</td>\n",
       "      <td>21.410706</td>\n",
       "      <td>41.433856</td>\n",
       "      <td>30.701346</td>\n",
       "      <td>28.524075</td>\n",
       "      <td>20.012246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_offers_vol</th>\n",
       "      <td>31.158010</td>\n",
       "      <td>21.661340</td>\n",
       "      <td>42.488293</td>\n",
       "      <td>30.328342</td>\n",
       "      <td>29.072662</td>\n",
       "      <td>20.356752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_price_france</th>\n",
       "      <td>30.292605</td>\n",
       "      <td>20.753558</td>\n",
       "      <td>41.790493</td>\n",
       "      <td>30.461410</td>\n",
       "      <td>28.158258</td>\n",
       "      <td>19.292297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSDF</th>\n",
       "      <td>29.968841</td>\n",
       "      <td>20.369317</td>\n",
       "      <td>41.244563</td>\n",
       "      <td>30.062999</td>\n",
       "      <td>27.879448</td>\n",
       "      <td>18.910190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dino_bin</th>\n",
       "      <td>30.119080</td>\n",
       "      <td>21.038765</td>\n",
       "      <td>40.946278</td>\n",
       "      <td>30.428373</td>\n",
       "      <td>28.130771</td>\n",
       "      <td>19.625407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_margin</th>\n",
       "      <td>30.993487</td>\n",
       "      <td>22.278418</td>\n",
       "      <td>41.766462</td>\n",
       "      <td>31.479881</td>\n",
       "      <td>29.027739</td>\n",
       "      <td>20.893382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Im_Pr</th>\n",
       "      <td>29.736896</td>\n",
       "      <td>19.783511</td>\n",
       "      <td>41.171195</td>\n",
       "      <td>29.734352</td>\n",
       "      <td>27.608710</td>\n",
       "      <td>18.285675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rmse_general  mae_general  rmse_spike  mae_spike  \\\n",
       "PrevDay              30.869098    21.861864   43.221078  32.066380   \n",
       "APXP                 30.299779    21.095644   41.852143  31.067724   \n",
       "LOLP                 30.296505    21.040219   41.849467  30.979879   \n",
       "In_gen               30.326971    21.207724   41.704116  30.869534   \n",
       "Ren_R                30.688847    21.450446   41.143979  30.648447   \n",
       "DA_imb_France        30.420877    21.479184   41.842518  31.174677   \n",
       "Rene                 30.525093    21.410706   41.433856  30.701346   \n",
       "ratio_offers_vol     31.158010    21.661340   42.488293  30.328342   \n",
       "DA_price_france      30.292605    20.753558   41.790493  30.461410   \n",
       "TSDF                 29.968841    20.369317   41.244563  30.062999   \n",
       "dino_bin             30.119080    21.038765   40.946278  30.428373   \n",
       "DA_margin            30.993487    22.278418   41.766462  31.479881   \n",
       "Im_Pr                29.736896    19.783511   41.171195  29.734352   \n",
       "\n",
       "                  rmse_normal  mae_normal  \n",
       "PrevDay             28.550797   20.325845  \n",
       "APXP                28.153355   19.594611  \n",
       "LOLP                28.149901   19.544066  \n",
       "In_gen              28.220027   19.753394  \n",
       "Ren_R               28.788173   20.065931  \n",
       "DA_imb_France       28.305347   20.019783  \n",
       "Rene                28.524075   20.012246  \n",
       "ratio_offers_vol    29.072662   20.356752  \n",
       "DA_price_france     28.158258   19.292297  \n",
       "TSDF                27.879448   18.910190  \n",
       "dino_bin            28.130771   19.625407  \n",
       "DA_margin           29.027739   20.893382  \n",
       "Im_Pr               27.608710   18.285675  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list_index =   ['PrevDay',\n",
    "                         'APXP', \n",
    "                         'LOLP',  \n",
    "                         'In_gen',\n",
    "                         'Ren_R',\n",
    "                         'DA_imb_France', \n",
    "                         'Rene',\n",
    "                         'ratio_offers_vol',\n",
    "                         'DA_price_france',\n",
    "                         'TSDF',\n",
    "                         'dino_bin',\n",
    "                         'DA_margin',\n",
    "                         'Im_Pr']\n",
    "\n",
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,}, index = features_list_index)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PrevDay', 'APXP', 'LOLP', 'DA_price_france', 'TSDF', 'Im_Pr'], dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.736896274923556"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
