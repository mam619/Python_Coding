{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply FS in ANN\n",
    "Condition of improvement only if RMSE on spike regions improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (1.18.1)\n",
      "Requirement already satisfied: sklearn in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# empty list to append metric values\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data & treat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > 2018090000, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential # to initialise the NN\n",
    "from keras.layers import Dense # to create layers\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "\n",
    "# parameters for ANN\n",
    "splits = 7\n",
    "epochs = 80\n",
    "\n",
    "# divide into features and label\n",
    "X = data.iloc[:, 0:14]\n",
    "y = data.loc[:, 'Offers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do first prediciton with first feature in list from Linear Regression FS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/series.py:4523: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 13340.4484 - mse: 13340.4482 - mae: 109.9549 - val_loss: 34637.3145 - val_mse: 34637.3125 - val_mae: 132.8003\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 13219.1904 - mse: 13219.1904 - mae: 109.4139 - val_loss: 34394.4432 - val_mse: 34394.4453 - val_mae: 131.8861\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 12872.0937 - mse: 12872.0928 - mae: 107.8353 - val_loss: 33702.5513 - val_mse: 33702.5508 - val_mae: 129.2464\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 11855.7543 - mse: 11855.7568 - mae: 102.9922 - val_loss: 31842.5462 - val_mse: 31842.5469 - val_mae: 121.8672\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 9493.5852 - mse: 9493.5859 - mae: 90.5908 - val_loss: 27412.1698 - val_mse: 27412.1699 - val_mae: 102.1680\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 623us/step - loss: 5281.0796 - mse: 5281.0791 - mae: 61.6008 - val_loss: 20253.5624 - val_mse: 20253.5645 - val_mae: 57.5575\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 2567.5409 - mse: 2567.5410 - mae: 36.1210 - val_loss: 17537.1387 - val_mse: 17537.1387 - val_mae: 37.3816\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 519us/step - loss: 2685.6218 - mse: 2685.6223 - mae: 38.2363 - val_loss: 18124.6472 - val_mse: 18124.6484 - val_mae: 39.2030\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 2448.4098 - mse: 2448.4099 - mae: 35.0478 - val_loss: 17741.2525 - val_mse: 17741.2520 - val_mae: 37.2973\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2720.0816 - mse: 2720.0815 - mae: 37.7020 - val_loss: 18223.6282 - val_mse: 18223.6309 - val_mae: 39.8198\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 652us/step - loss: 2523.4280 - mse: 2523.4280 - mae: 36.4689 - val_loss: 17978.8358 - val_mse: 17978.8359 - val_mae: 38.3483\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 527us/step - loss: 2585.7824 - mse: 2585.7825 - mae: 35.4672 - val_loss: 18045.9181 - val_mse: 18045.9199 - val_mae: 38.7393\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 2513.0346 - mse: 2513.0342 - mae: 35.0928 - val_loss: 18153.3631 - val_mse: 18153.3613 - val_mae: 39.3690\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2318.4556 - mse: 2318.4558 - mae: 34.4166 - val_loss: 17821.2931 - val_mse: 17821.2930 - val_mae: 37.6048\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 2503.8724 - mse: 2503.8723 - mae: 35.9889 - val_loss: 17881.8244 - val_mse: 17881.8242 - val_mae: 37.8616\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2327.6667 - mse: 2327.6667 - mae: 35.6874 - val_loss: 17897.4612 - val_mse: 17897.4609 - val_mae: 37.9269\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2380.8431 - mse: 2380.8433 - mae: 35.0139 - val_loss: 18045.7838 - val_mse: 18045.7812 - val_mae: 38.7275\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 2351.7634 - mse: 2351.7629 - mae: 34.9755 - val_loss: 18005.2329 - val_mse: 18005.2324 - val_mae: 38.4849\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 2480.5132 - mse: 2480.5129 - mae: 34.6297 - val_loss: 17939.9694 - val_mse: 17939.9707 - val_mae: 38.1203\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 503us/step - loss: 2186.0412 - mse: 2186.0408 - mae: 33.2298 - val_loss: 17909.1228 - val_mse: 17909.1211 - val_mae: 37.9674\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 529us/step - loss: 2356.9035 - mse: 2356.9036 - mae: 35.2825 - val_loss: 17993.4640 - val_mse: 17993.4648 - val_mae: 38.4065\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2178.9888 - mse: 2178.9890 - mae: 33.5347 - val_loss: 17886.4000 - val_mse: 17886.4004 - val_mae: 37.8664\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 2214.0181 - mse: 2214.0181 - mae: 33.5639 - val_loss: 17726.3376 - val_mse: 17726.3359 - val_mae: 37.2615\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 506us/step - loss: 2275.8758 - mse: 2275.8760 - mae: 32.8681 - val_loss: 17685.2954 - val_mse: 17685.2969 - val_mae: 37.2196\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2418.1112 - mse: 2418.1113 - mae: 34.4795 - val_loss: 17935.0576 - val_mse: 17935.0586 - val_mae: 38.0866\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 418us/step - loss: 2361.0456 - mse: 2361.0457 - mae: 34.6042 - val_loss: 17769.0182 - val_mse: 17769.0176 - val_mae: 37.3724\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 417us/step - loss: 2209.0094 - mse: 2209.0093 - mae: 33.5237 - val_loss: 17931.4558 - val_mse: 17931.4531 - val_mae: 38.0663\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 407us/step - loss: 2162.8479 - mse: 2162.8477 - mae: 31.9800 - val_loss: 17783.5444 - val_mse: 17783.5449 - val_mae: 37.4272\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 497us/step - loss: 2111.4142 - mse: 2111.4143 - mae: 32.4023 - val_loss: 17978.8229 - val_mse: 17978.8242 - val_mae: 38.3156\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 2354.4770 - mse: 2354.4771 - mae: 34.3199 - val_loss: 17844.5770 - val_mse: 17844.5762 - val_mae: 37.6738\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 701us/step - loss: 2225.3226 - mse: 2225.3225 - mae: 33.1304 - val_loss: 17825.4604 - val_mse: 17825.4609 - val_mae: 37.5883\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 2182.9448 - mse: 2182.9448 - mae: 32.4535 - val_loss: 17946.7242 - val_mse: 17946.7246 - val_mae: 38.1253\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 531us/step - loss: 2095.7691 - mse: 2095.7693 - mae: 31.7131 - val_loss: 17658.2939 - val_mse: 17658.2949 - val_mae: 37.1881\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2109.4776 - mse: 2109.4775 - mae: 32.6570 - val_loss: 17645.2491 - val_mse: 17645.2500 - val_mae: 37.1877\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 594us/step - loss: 2046.9822 - mse: 2046.9823 - mae: 32.1408 - val_loss: 17834.0613 - val_mse: 17834.0605 - val_mae: 37.6153\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2096.6424 - mse: 2096.6426 - mae: 32.4595 - val_loss: 17894.2802 - val_mse: 17894.2793 - val_mae: 37.8745\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2184.4918 - mse: 2184.4917 - mae: 32.6596 - val_loss: 17779.5141 - val_mse: 17779.5156 - val_mae: 37.3911\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 510us/step - loss: 2165.0172 - mse: 2165.0173 - mae: 32.3709 - val_loss: 17803.3539 - val_mse: 17803.3535 - val_mae: 37.4794\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 523us/step - loss: 2232.2199 - mse: 2232.2197 - mae: 32.9293 - val_loss: 17850.2423 - val_mse: 17850.2402 - val_mae: 37.6744\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 508us/step - loss: 2147.7725 - mse: 2147.7725 - mae: 32.6470 - val_loss: 17856.2210 - val_mse: 17856.2227 - val_mae: 37.6959\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 537us/step - loss: 1966.4494 - mse: 1966.4493 - mae: 30.7052 - val_loss: 17652.4875 - val_mse: 17652.4883 - val_mae: 37.1807\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2118.1913 - mse: 2118.1909 - mae: 32.1510 - val_loss: 17983.7292 - val_mse: 17983.7305 - val_mae: 38.3003\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 449us/step - loss: 2033.8853 - mse: 2033.8851 - mae: 31.6076 - val_loss: 17815.8241 - val_mse: 17815.8242 - val_mae: 37.5232\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 490us/step - loss: 2057.3101 - mse: 2057.3103 - mae: 32.2416 - val_loss: 18008.7971 - val_mse: 18008.7988 - val_mae: 38.4409\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 507us/step - loss: 2116.7649 - mse: 2116.7649 - mae: 32.1458 - val_loss: 17732.9216 - val_mse: 17732.9219 - val_mae: 37.2357\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2024.4541 - mse: 2024.4541 - mae: 31.4076 - val_loss: 17891.2790 - val_mse: 17891.2793 - val_mae: 37.8351\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 2227.0455 - mse: 2227.0454 - mae: 32.4581 - val_loss: 17746.3629 - val_mse: 17746.3613 - val_mae: 37.2491\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 2032.1021 - mse: 2032.1019 - mae: 31.0334 - val_loss: 17729.9562 - val_mse: 17729.9551 - val_mae: 37.2271\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 1873.2135 - mse: 1873.2135 - mae: 30.5469 - val_loss: 17772.6118 - val_mse: 17772.6113 - val_mae: 37.3411\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 528us/step - loss: 2131.9989 - mse: 2131.9988 - mae: 32.0830 - val_loss: 17898.1867 - val_mse: 17898.1875 - val_mae: 37.8568\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 2098.9824 - mse: 2098.9824 - mae: 32.0419 - val_loss: 17835.7552 - val_mse: 17835.7559 - val_mae: 37.5869\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 2000.4416 - mse: 2000.4418 - mae: 32.0169 - val_loss: 17788.0014 - val_mse: 17788.0020 - val_mae: 37.3976\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 550us/step - loss: 1863.5745 - mse: 1863.5743 - mae: 30.5504 - val_loss: 17517.9026 - val_mse: 17517.9043 - val_mae: 37.4277\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 1989.8407 - mse: 1989.8408 - mae: 30.8143 - val_loss: 17690.6844 - val_mse: 17690.6836 - val_mae: 37.1800\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 448us/step - loss: 2084.4569 - mse: 2084.4568 - mae: 32.4523 - val_loss: 17807.5042 - val_mse: 17807.5020 - val_mae: 37.4668\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 523us/step - loss: 1874.0883 - mse: 1874.0883 - mae: 29.8428 - val_loss: 17756.1979 - val_mse: 17756.1973 - val_mae: 37.2682\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 563us/step - loss: 2239.1123 - mse: 2239.1121 - mae: 33.0073 - val_loss: 17769.7835 - val_mse: 17769.7832 - val_mae: 37.3179\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 1755.1607 - mse: 1755.1605 - mae: 28.5246 - val_loss: 17693.5157 - val_mse: 17693.5137 - val_mae: 37.1761\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2060.7870 - mse: 2060.7871 - mae: 32.6781 - val_loss: 17720.8802 - val_mse: 17720.8809 - val_mae: 37.2018\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 501us/step - loss: 1832.0254 - mse: 1832.0255 - mae: 29.9479 - val_loss: 17758.1157 - val_mse: 17758.1152 - val_mae: 37.2657\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 468us/step - loss: 2082.8929 - mse: 2082.8928 - mae: 31.1886 - val_loss: 17823.7079 - val_mse: 17823.7051 - val_mae: 37.5156\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 1929.6255 - mse: 1929.6252 - mae: 29.9468 - val_loss: 17732.4663 - val_mse: 17732.4688 - val_mae: 37.2122\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 586us/step - loss: 2100.5675 - mse: 2100.5671 - mae: 31.8223 - val_loss: 17855.6599 - val_mse: 17855.6602 - val_mae: 37.6497\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 1651.5583 - mse: 1651.5582 - mae: 28.7553 - val_loss: 17676.3313 - val_mse: 17676.3320 - val_mae: 37.1601\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 1994.6342 - mse: 1994.6340 - mae: 31.1346 - val_loss: 17767.2995 - val_mse: 17767.3008 - val_mae: 37.2962\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 1890.3949 - mse: 1890.3948 - mae: 30.0701 - val_loss: 17693.4476 - val_mse: 17693.4492 - val_mae: 37.1664\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 1989.3182 - mse: 1989.3184 - mae: 31.1515 - val_loss: 17868.6230 - val_mse: 17868.6230 - val_mae: 37.6960\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 654us/step - loss: 1830.4913 - mse: 1830.4912 - mae: 29.4904 - val_loss: 17584.9983 - val_mse: 17584.9980 - val_mae: 37.2480\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 652us/step - loss: 2055.8750 - mse: 2055.8752 - mae: 30.5862 - val_loss: 17667.6347 - val_mse: 17667.6348 - val_mae: 37.1553\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 1879.6926 - mse: 1879.6925 - mae: 30.5280 - val_loss: 17630.3017 - val_mse: 17630.3047 - val_mae: 37.1653\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 1895.0532 - mse: 1895.0531 - mae: 29.4039 - val_loss: 17655.8745 - val_mse: 17655.8730 - val_mae: 37.1541\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 1810.3992 - mse: 1810.3993 - mae: 29.3481 - val_loss: 17451.2020 - val_mse: 17451.2012 - val_mae: 37.6184\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 623us/step - loss: 1923.5031 - mse: 1923.5029 - mae: 29.9444 - val_loss: 17567.5151 - val_mse: 17567.5137 - val_mae: 37.2887\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 1924.8196 - mse: 1924.8197 - mae: 31.2497 - val_loss: 17697.4366 - val_mse: 17697.4355 - val_mae: 37.1619\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 1884.0981 - mse: 1884.0980 - mae: 30.0034 - val_loss: 17719.0320 - val_mse: 17719.0312 - val_mae: 37.1810\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 1950.2965 - mse: 1950.2965 - mae: 30.3250 - val_loss: 17908.3797 - val_mse: 17908.3828 - val_mae: 37.8499\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 1954.1636 - mse: 1954.1637 - mae: 30.1951 - val_loss: 17773.3240 - val_mse: 17773.3242 - val_mae: 37.3033\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 1696.1802 - mse: 1696.1804 - mae: 28.1699 - val_loss: 17557.3544 - val_mse: 17557.3535 - val_mae: 37.3156\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 637us/step - loss: 1896.9027 - mse: 1896.9027 - mae: 29.1477 - val_loss: 17610.5360 - val_mse: 17610.5352 - val_mae: 37.1858\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 405us/step - loss: 1999.0379 - mse: 1999.0378 - mae: 31.3664 - val_loss: 17673.8769 - val_mse: 17673.8789 - val_mae: 37.1475\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 451us/step - loss: 4010.8616 - mse: 4010.8616 - mae: 33.7483 - val_loss: 2184.3565 - val_mse: 2184.3564 - val_mae: 31.0455\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 671us/step - loss: 4061.3463 - mse: 4061.3464 - mae: 33.4940 - val_loss: 2331.8028 - val_mse: 2331.8027 - val_mae: 31.4548\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 669us/step - loss: 4234.4728 - mse: 4234.4731 - mae: 34.7681 - val_loss: 2294.9060 - val_mse: 2294.9058 - val_mae: 31.3597\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 4149.7873 - mse: 4149.7876 - mae: 34.3160 - val_loss: 2342.9253 - val_mse: 2342.9250 - val_mae: 31.4929\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4322.1883 - mse: 4322.1880 - mae: 35.0343 - val_loss: 2344.4539 - val_mse: 2344.4543 - val_mae: 31.5008\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4359.6509 - mse: 4359.6514 - mae: 35.1685 - val_loss: 2357.9413 - val_mse: 2357.9409 - val_mae: 31.5393\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 658us/step - loss: 3894.9222 - mse: 3894.9221 - mae: 33.9821 - val_loss: 2259.0844 - val_mse: 2259.0847 - val_mae: 31.2664\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4066.8218 - mse: 4066.8223 - mae: 33.9722 - val_loss: 2344.0747 - val_mse: 2344.0747 - val_mae: 31.5042\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4427.5611 - mse: 4427.5610 - mae: 35.2176 - val_loss: 2427.1218 - val_mse: 2427.1221 - val_mae: 31.7508\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4272.9325 - mse: 4272.9321 - mae: 34.7351 - val_loss: 2349.2391 - val_mse: 2349.2390 - val_mae: 31.5242\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4283.2767 - mse: 4283.2769 - mae: 35.5019 - val_loss: 2319.9461 - val_mse: 2319.9460 - val_mae: 31.4464\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4168.8452 - mse: 4168.8452 - mae: 33.9758 - val_loss: 2377.6003 - val_mse: 2377.6003 - val_mae: 31.6113\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 4006.9065 - mse: 4006.9070 - mae: 33.3935 - val_loss: 2247.9157 - val_mse: 2247.9158 - val_mae: 31.2499\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4142.0989 - mse: 4142.0986 - mae: 34.3283 - val_loss: 2335.0262 - val_mse: 2335.0264 - val_mae: 31.4932\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 4278.5123 - mse: 4278.5122 - mae: 35.1230 - val_loss: 2337.5665 - val_mse: 2337.5662 - val_mae: 31.5015\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4219.1917 - mse: 4219.1914 - mae: 34.7139 - val_loss: 2386.2125 - val_mse: 2386.2124 - val_mae: 31.6451\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4353.0254 - mse: 4353.0249 - mae: 35.6431 - val_loss: 2329.1498 - val_mse: 2329.1499 - val_mae: 31.4857\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 4264.8045 - mse: 4264.8042 - mae: 35.1958 - val_loss: 2299.0342 - val_mse: 2299.0342 - val_mae: 31.4100\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4200.2779 - mse: 4200.2773 - mae: 34.3458 - val_loss: 2323.6636 - val_mse: 2323.6636 - val_mae: 31.4799\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 547us/step - loss: 4161.5854 - mse: 4161.5859 - mae: 33.8459 - val_loss: 2324.4965 - val_mse: 2324.4963 - val_mae: 31.4862\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 3929.6511 - mse: 3929.6516 - mae: 33.5497 - val_loss: 2263.5188 - val_mse: 2263.5190 - val_mae: 31.3168\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4046.9564 - mse: 4046.9561 - mae: 33.8246 - val_loss: 2309.8600 - val_mse: 2309.8601 - val_mae: 31.4494\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 0s 473us/step - loss: 4267.7220 - mse: 4267.7217 - mae: 34.6557 - val_loss: 2334.9709 - val_mse: 2334.9709 - val_mae: 31.5172\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 3904.8262 - mse: 3904.8264 - mae: 33.5461 - val_loss: 2246.5301 - val_mse: 2246.5303 - val_mae: 31.2758\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 0s 435us/step - loss: 4042.9576 - mse: 4042.9578 - mae: 33.4857 - val_loss: 2342.6665 - val_mse: 2342.6663 - val_mae: 31.5397\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 0s 482us/step - loss: 4096.9747 - mse: 4096.9746 - mae: 33.8671 - val_loss: 2286.6562 - val_mse: 2286.6562 - val_mae: 31.3895\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4147.0295 - mse: 4147.0293 - mae: 34.0109 - val_loss: 2265.4554 - val_mse: 2265.4556 - val_mae: 31.3297\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 3960.2633 - mse: 3960.2629 - mae: 33.5271 - val_loss: 2287.8300 - val_mse: 2287.8301 - val_mae: 31.3940\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 431us/step - loss: 3927.2845 - mse: 3927.2844 - mae: 33.2154 - val_loss: 2269.3998 - val_mse: 2269.3997 - val_mae: 31.3418\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4151.3151 - mse: 4151.3145 - mae: 34.6781 - val_loss: 2317.6460 - val_mse: 2317.6458 - val_mae: 31.4807\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 499us/step - loss: 4042.8465 - mse: 4042.8459 - mae: 33.8106 - val_loss: 2364.3476 - val_mse: 2364.3474 - val_mae: 31.6062\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4098.1600 - mse: 4098.1602 - mae: 34.7576 - val_loss: 2281.8994 - val_mse: 2281.8994 - val_mae: 31.3831\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4196.0169 - mse: 4196.0171 - mae: 34.1903 - val_loss: 2273.3524 - val_mse: 2273.3525 - val_mae: 31.3601\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4125.6031 - mse: 4125.6030 - mae: 33.4305 - val_loss: 2282.6478 - val_mse: 2282.6477 - val_mae: 31.3881\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - ETA: 0s - loss: 4315.8750 - mse: 4315.8750 - mae: 33.58 - 1s 622us/step - loss: 4182.3743 - mse: 4182.3745 - mae: 34.0962 - val_loss: 2308.8748 - val_mse: 2308.8750 - val_mae: 31.4628\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 509us/step - loss: 3976.3083 - mse: 3976.3083 - mae: 33.2742 - val_loss: 2307.9381 - val_mse: 2307.9380 - val_mae: 31.4648\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4079.6793 - mse: 4079.6790 - mae: 32.8378 - val_loss: 2311.0143 - val_mse: 2311.0142 - val_mae: 31.4758\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 4054.5464 - mse: 4054.5461 - mae: 33.7730 - val_loss: 2283.4863 - val_mse: 2283.4863 - val_mae: 31.4014\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 3974.2535 - mse: 3974.2539 - mae: 32.8190 - val_loss: 2274.5282 - val_mse: 2274.5283 - val_mae: 31.3769\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 3947.4922 - mse: 3947.4924 - mae: 33.0277 - val_loss: 2297.7041 - val_mse: 2297.7041 - val_mae: 31.4447\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 516us/step - loss: 4125.9018 - mse: 4125.9019 - mae: 33.9345 - val_loss: 2255.0162 - val_mse: 2255.0164 - val_mae: 31.3298\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 488us/step - loss: 4260.1537 - mse: 4260.1528 - mae: 34.6939 - val_loss: 2372.2184 - val_mse: 2372.2183 - val_mae: 31.6506\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 0s 392us/step - loss: 4113.9487 - mse: 4113.9487 - mae: 33.0009 - val_loss: 2273.5844 - val_mse: 2273.5842 - val_mae: 31.3822\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 4097.0037 - mse: 4097.0039 - mae: 33.5476 - val_loss: 2244.3974 - val_mse: 2244.3972 - val_mae: 31.3074\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 0s 470us/step - loss: 4041.9215 - mse: 4041.9216 - mae: 34.3353 - val_loss: 2324.6788 - val_mse: 2324.6790 - val_mae: 31.5277\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 556us/step - loss: 4148.6114 - mse: 4148.6118 - mae: 33.3017 - val_loss: 2338.0346 - val_mse: 2338.0347 - val_mae: 31.5640\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 663us/step - loss: 4002.6473 - mse: 4002.6472 - mae: 32.5487 - val_loss: 2260.0427 - val_mse: 2260.0422 - val_mae: 31.3551\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 516us/step - loss: 4001.4613 - mse: 4001.4612 - mae: 33.7433 - val_loss: 2301.7385 - val_mse: 2301.7388 - val_mae: 31.4700\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4034.8068 - mse: 4034.8069 - mae: 33.5316 - val_loss: 2324.1769 - val_mse: 2324.1768 - val_mae: 31.5320\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 614us/step - loss: 4032.4299 - mse: 4032.4304 - mae: 33.8349 - val_loss: 2342.4578 - val_mse: 2342.4580 - val_mae: 31.5811\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 514us/step - loss: 4167.7008 - mse: 4167.7007 - mae: 34.0356 - val_loss: 2332.4706 - val_mse: 2332.4705 - val_mae: 31.5574\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4048.3024 - mse: 4048.3025 - mae: 32.8857 - val_loss: 2300.4347 - val_mse: 2300.4351 - val_mae: 31.4733\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 506us/step - loss: 4009.1693 - mse: 4009.1689 - mae: 33.1220 - val_loss: 2219.2302 - val_mse: 2219.2302 - val_mae: 31.2572\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 0s 471us/step - loss: 4044.7746 - mse: 4044.7751 - mae: 34.5725 - val_loss: 2362.7730 - val_mse: 2362.7732 - val_mae: 31.6437\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 0s 443us/step - loss: 4082.3548 - mse: 4082.3540 - mae: 33.9689 - val_loss: 2272.8453 - val_mse: 2272.8452 - val_mae: 31.4001\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 535us/step - loss: 4111.6188 - mse: 4111.6196 - mae: 35.1599 - val_loss: 2340.5387 - val_mse: 2340.5388 - val_mae: 31.5883\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 533us/step - loss: 4118.0271 - mse: 4118.0269 - mae: 33.7063 - val_loss: 2315.7536 - val_mse: 2315.7534 - val_mae: 31.5236\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 4087.7638 - mse: 4087.7632 - mae: 33.4698 - val_loss: 2316.7310 - val_mse: 2316.7310 - val_mae: 31.5268\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 3861.9331 - mse: 3861.9329 - mae: 32.7539 - val_loss: 2354.6398 - val_mse: 2354.6396 - val_mae: 31.6367\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 502us/step - loss: 3991.7677 - mse: 3991.7681 - mae: 32.9283 - val_loss: 2319.3007 - val_mse: 2319.3008 - val_mae: 31.5373\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4027.0398 - mse: 4027.0398 - mae: 33.3561 - val_loss: 2321.8107 - val_mse: 2321.8108 - val_mae: 31.5450\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4013.2428 - mse: 4013.2424 - mae: 34.1737 - val_loss: 2310.7061 - val_mse: 2310.7061 - val_mae: 31.5157\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 522us/step - loss: 4223.5738 - mse: 4223.5732 - mae: 33.9784 - val_loss: 2291.8381 - val_mse: 2291.8381 - val_mae: 31.4636\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 3943.7310 - mse: 3943.7310 - mae: 32.9664 - val_loss: 2303.1255 - val_mse: 2303.1255 - val_mae: 31.4967\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4074.3071 - mse: 4074.3064 - mae: 33.2607 - val_loss: 2318.5488 - val_mse: 2318.5491 - val_mae: 31.5417\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4040.5744 - mse: 4040.5747 - mae: 32.9203 - val_loss: 2274.4588 - val_mse: 2274.4587 - val_mae: 31.4209\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4135.2989 - mse: 4135.2983 - mae: 34.0944 - val_loss: 2282.4726 - val_mse: 2282.4724 - val_mae: 31.4432\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 4100.0489 - mse: 4100.0493 - mae: 33.3180 - val_loss: 2318.5631 - val_mse: 2318.5632 - val_mae: 31.5499\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 529us/step - loss: 4066.2083 - mse: 4066.2087 - mae: 32.9017 - val_loss: 2303.9172 - val_mse: 2303.9172 - val_mae: 31.5058\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 515us/step - loss: 3936.0794 - mse: 3936.0798 - mae: 32.1216 - val_loss: 2301.9574 - val_mse: 2301.9573 - val_mae: 31.5011\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4061.3762 - mse: 4061.3757 - mae: 33.5461 - val_loss: 2322.9963 - val_mse: 2322.9963 - val_mae: 31.5666\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 3933.4483 - mse: 3933.4485 - mae: 32.0369 - val_loss: 2257.3045 - val_mse: 2257.3044 - val_mae: 31.3849\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4148.0478 - mse: 4148.0479 - mae: 33.9630 - val_loss: 2311.2511 - val_mse: 2311.2512 - val_mae: 31.5375\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 0s 489us/step - loss: 4107.6611 - mse: 4107.6616 - mae: 33.9127 - val_loss: 2302.7646 - val_mse: 2302.7644 - val_mae: 31.5144\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4239.9825 - mse: 4239.9824 - mae: 34.5385 - val_loss: 2289.1408 - val_mse: 2289.1404 - val_mae: 31.4726\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 3953.6674 - mse: 3953.6677 - mae: 33.1722 - val_loss: 2247.5443 - val_mse: 2247.5439 - val_mae: 31.3648\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4039.9555 - mse: 4039.9548 - mae: 33.8816 - val_loss: 2321.0298 - val_mse: 2321.0298 - val_mae: 31.5733\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 548us/step - loss: 3948.7416 - mse: 3948.7415 - mae: 32.5432 - val_loss: 2302.4276 - val_mse: 2302.4275 - val_mae: 31.5173\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 506us/step - loss: 3963.4288 - mse: 3963.4285 - mae: 33.2801 - val_loss: 2281.7467 - val_mse: 2281.7466 - val_mae: 31.4579\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4096.7161 - mse: 4096.7158 - mae: 34.3202 - val_loss: 2304.6363 - val_mse: 2304.6362 - val_mae: 31.5283\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3363.6776 - mse: 3363.6780 - mae: 32.7788 - val_loss: 1457.1643 - val_mse: 1457.1643 - val_mae: 25.5574\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3468.9118 - mse: 3468.9124 - mae: 32.9403 - val_loss: 1457.3617 - val_mse: 1457.3616 - val_mae: 25.5744\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3429.4472 - mse: 3429.4465 - mae: 32.6153 - val_loss: 1457.9119 - val_mse: 1457.9120 - val_mae: 25.7875\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3340.2650 - mse: 3340.2659 - mae: 32.5081 - val_loss: 1459.8666 - val_mse: 1459.8665 - val_mae: 26.1643\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3342.7665 - mse: 3342.7666 - mae: 32.6408 - val_loss: 1457.9261 - val_mse: 1457.9261 - val_mae: 25.8085\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 526us/step - loss: 3375.9955 - mse: 3375.9954 - mae: 32.8712 - val_loss: 1459.5065 - val_mse: 1459.5065 - val_mae: 26.0857\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3373.4569 - mse: 3373.4563 - mae: 32.6437 - val_loss: 1459.3221 - val_mse: 1459.3220 - val_mae: 26.0483\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 494us/step - loss: 3330.3045 - mse: 3330.3054 - mae: 32.6649 - val_loss: 1458.1895 - val_mse: 1458.1895 - val_mae: 25.7168\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3317.1515 - mse: 3317.1511 - mae: 32.0620 - val_loss: 1458.0998 - val_mse: 1458.0997 - val_mae: 25.4095\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3428.2216 - mse: 3428.2214 - mae: 33.2152 - val_loss: 1458.7368 - val_mse: 1458.7368 - val_mae: 25.8835\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3502.6896 - mse: 3502.6897 - mae: 32.9378 - val_loss: 1458.3737 - val_mse: 1458.3737 - val_mae: 25.3592\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3440.2591 - mse: 3440.2595 - mae: 33.4040 - val_loss: 1459.1221 - val_mse: 1459.1219 - val_mae: 25.2212\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3473.3978 - mse: 3473.3979 - mae: 33.0517 - val_loss: 1458.5579 - val_mse: 1458.5580 - val_mae: 25.4386\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3373.3060 - mse: 3373.3059 - mae: 32.3348 - val_loss: 1461.2515 - val_mse: 1461.2513 - val_mae: 26.2424\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3454.7161 - mse: 3454.7158 - mae: 32.9775 - val_loss: 1458.5937 - val_mse: 1458.5938 - val_mae: 25.5896\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3292.4549 - mse: 3292.4558 - mae: 31.9312 - val_loss: 1460.7399 - val_mse: 1460.7400 - val_mae: 26.1512\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 659us/step - loss: 3436.0534 - mse: 3436.0535 - mae: 32.7759 - val_loss: 1458.8820 - val_mse: 1458.8820 - val_mae: 25.5945\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3293.6996 - mse: 3293.6997 - mae: 32.1144 - val_loss: 1462.1069 - val_mse: 1462.1068 - val_mae: 26.3053\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3364.9703 - mse: 3364.9702 - mae: 31.9766 - val_loss: 1459.2292 - val_mse: 1459.2291 - val_mae: 25.7293\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3356.1135 - mse: 3356.1130 - mae: 31.7483 - val_loss: 1460.2397 - val_mse: 1460.2396 - val_mae: 26.0095\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 510us/step - loss: 3298.4520 - mse: 3298.4519 - mae: 31.7189 - val_loss: 1459.0064 - val_mse: 1459.0065 - val_mae: 25.6745\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 469us/step - loss: 3234.7430 - mse: 3234.7429 - mae: 32.9428 - val_loss: 1460.2456 - val_mse: 1460.2456 - val_mae: 26.0648\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 488us/step - loss: 3276.5622 - mse: 3276.5625 - mae: 31.8099 - val_loss: 1465.3351 - val_mse: 1465.3352 - val_mae: 26.6741\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 480us/step - loss: 3284.9681 - mse: 3284.9683 - mae: 32.0314 - val_loss: 1459.7331 - val_mse: 1459.7332 - val_mae: 25.9598\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 447us/step - loss: 3202.7065 - mse: 3202.7061 - mae: 31.5116 - val_loss: 1461.5637 - val_mse: 1461.5638 - val_mae: 26.2712\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 518us/step - loss: 3328.9264 - mse: 3328.9258 - mae: 32.8467 - val_loss: 1458.9679 - val_mse: 1458.9679 - val_mae: 25.8249\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 523us/step - loss: 3425.4909 - mse: 3425.4907 - mae: 32.3202 - val_loss: 1459.4231 - val_mse: 1459.4231 - val_mae: 25.2825\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3400.2475 - mse: 3400.2471 - mae: 32.5907 - val_loss: 1459.6520 - val_mse: 1459.6521 - val_mae: 25.9312\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3305.5031 - mse: 3305.5020 - mae: 32.4502 - val_loss: 1458.8813 - val_mse: 1458.8812 - val_mae: 25.6652\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3289.2767 - mse: 3289.2759 - mae: 31.1704 - val_loss: 1460.0146 - val_mse: 1460.0145 - val_mae: 26.0159\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3356.6083 - mse: 3356.6074 - mae: 31.9516 - val_loss: 1459.7245 - val_mse: 1459.7245 - val_mae: 25.8668\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3344.8693 - mse: 3344.8689 - mae: 32.7845 - val_loss: 1459.0580 - val_mse: 1459.0580 - val_mae: 25.7281\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3236.2656 - mse: 3236.2651 - mae: 31.9116 - val_loss: 1459.5358 - val_mse: 1459.5359 - val_mae: 25.9265\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3310.1592 - mse: 3310.1599 - mae: 31.8276 - val_loss: 1459.2296 - val_mse: 1459.2296 - val_mae: 25.8415\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3376.9641 - mse: 3376.9646 - mae: 32.4963 - val_loss: 1459.4483 - val_mse: 1459.4482 - val_mae: 25.9204\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3363.5867 - mse: 3363.5869 - mae: 31.9344 - val_loss: 1458.6977 - val_mse: 1458.6975 - val_mae: 25.6906\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3362.7182 - mse: 3362.7188 - mae: 32.6939 - val_loss: 1459.9135 - val_mse: 1459.9136 - val_mae: 26.0477\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 516us/step - loss: 3341.3246 - mse: 3341.3247 - mae: 32.1456 - val_loss: 1458.2955 - val_mse: 1458.2954 - val_mae: 25.5111\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 451us/step - loss: 3291.9707 - mse: 3291.9709 - mae: 32.3189 - val_loss: 1458.3910 - val_mse: 1458.3910 - val_mae: 25.7194\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 508us/step - loss: 3263.8767 - mse: 3263.8767 - mae: 32.1026 - val_loss: 1459.2200 - val_mse: 1459.2200 - val_mae: 26.0063\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3370.6902 - mse: 3370.6899 - mae: 32.6349 - val_loss: 1458.1009 - val_mse: 1458.1010 - val_mae: 25.3926\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3296.1948 - mse: 3296.1951 - mae: 32.5327 - val_loss: 1460.8813 - val_mse: 1460.8812 - val_mae: 26.2668\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 518us/step - loss: 3267.4342 - mse: 3267.4338 - mae: 32.2878 - val_loss: 1458.7619 - val_mse: 1458.7621 - val_mae: 25.8792\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 477us/step - loss: 3406.1233 - mse: 3406.1238 - mae: 33.2104 - val_loss: 1458.2439 - val_mse: 1458.2440 - val_mae: 25.5898\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 504us/step - loss: 3243.3767 - mse: 3243.3767 - mae: 31.5147 - val_loss: 1461.1486 - val_mse: 1461.1486 - val_mae: 26.2146\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3263.7652 - mse: 3263.7656 - mae: 31.8497 - val_loss: 1459.0939 - val_mse: 1459.0938 - val_mae: 25.7294\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3362.9691 - mse: 3362.9690 - mae: 32.4225 - val_loss: 1460.0183 - val_mse: 1460.0184 - val_mae: 25.9639\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3363.6036 - mse: 3363.6042 - mae: 32.4757 - val_loss: 1459.9002 - val_mse: 1459.9003 - val_mae: 25.2745\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 651us/step - loss: 3387.1892 - mse: 3387.1890 - mae: 32.1831 - val_loss: 1459.8541 - val_mse: 1459.8541 - val_mae: 25.8390\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3393.1128 - mse: 3393.1123 - mae: 32.6896 - val_loss: 1459.3136 - val_mse: 1459.3135 - val_mae: 25.6838\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 427us/step - loss: 3109.2594 - mse: 3109.2605 - mae: 30.9029 - val_loss: 1461.8609 - val_mse: 1461.8610 - val_mae: 26.2584\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3273.9989 - mse: 3273.9993 - mae: 31.5827 - val_loss: 1460.7663 - val_mse: 1460.7662 - val_mae: 26.0928\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3247.3140 - mse: 3247.3145 - mae: 32.1475 - val_loss: 1462.3877 - val_mse: 1462.3876 - val_mae: 26.3560\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3455.6167 - mse: 3455.6167 - mae: 32.7072 - val_loss: 1459.5430 - val_mse: 1459.5430 - val_mae: 25.2791\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 535us/step - loss: 3419.1560 - mse: 3419.1562 - mae: 32.0866 - val_loss: 1461.4543 - val_mse: 1461.4543 - val_mae: 25.0171\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3287.6695 - mse: 3287.6699 - mae: 31.8476 - val_loss: 1459.3333 - val_mse: 1459.3334 - val_mae: 25.3025\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 505us/step - loss: 3259.6057 - mse: 3259.6064 - mae: 31.5868 - val_loss: 1461.1434 - val_mse: 1461.1434 - val_mae: 26.1864\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 449us/step - loss: 3261.8593 - mse: 3261.8591 - mae: 31.2637 - val_loss: 1460.3764 - val_mse: 1460.3765 - val_mae: 26.0537\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 521us/step - loss: 3320.7416 - mse: 3320.7415 - mae: 31.4797 - val_loss: 1459.3805 - val_mse: 1459.3807 - val_mae: 25.4667\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3284.4662 - mse: 3284.4666 - mae: 32.1648 - val_loss: 1459.1796 - val_mse: 1459.1797 - val_mae: 25.5453\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3243.9676 - mse: 3243.9678 - mae: 31.5874 - val_loss: 1460.3549 - val_mse: 1460.3550 - val_mae: 26.0240\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 492us/step - loss: 3238.7531 - mse: 3238.7532 - mae: 31.6544 - val_loss: 1460.1863 - val_mse: 1460.1862 - val_mae: 25.9937\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3207.6434 - mse: 3207.6428 - mae: 30.5987 - val_loss: 1459.7015 - val_mse: 1459.7015 - val_mae: 25.8711\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3279.4691 - mse: 3279.4695 - mae: 31.7614 - val_loss: 1460.4615 - val_mse: 1460.4614 - val_mae: 26.0507\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3250.9032 - mse: 3250.9036 - mae: 32.0870 - val_loss: 1460.8079 - val_mse: 1460.8079 - val_mae: 26.0998\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 532us/step - loss: 3267.2505 - mse: 3267.2512 - mae: 31.6083 - val_loss: 1459.2051 - val_mse: 1459.2051 - val_mae: 25.7096\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 514us/step - loss: 3211.6334 - mse: 3211.6340 - mae: 31.1674 - val_loss: 1460.4483 - val_mse: 1460.4484 - val_mae: 26.0360\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3198.7313 - mse: 3198.7307 - mae: 30.93 - 1s 578us/step - loss: 3186.7140 - mse: 3186.7134 - mae: 30.9491 - val_loss: 1460.4904 - val_mse: 1460.4905 - val_mae: 26.0376\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3331.5002 - mse: 3331.5002 - mae: 32.2971 - val_loss: 1459.7634 - val_mse: 1459.7634 - val_mae: 25.9103\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3181.0983 - mse: 3181.0986 - mae: 31.7143 - val_loss: 1461.4784 - val_mse: 1461.4784 - val_mae: 26.2527\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 492us/step - loss: 3356.9537 - mse: 3356.9539 - mae: 32.0216 - val_loss: 1459.4471 - val_mse: 1459.4469 - val_mae: 25.8605\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3266.4621 - mse: 3266.4614 - mae: 31.3877 - val_loss: 1459.1544 - val_mse: 1459.1544 - val_mae: 25.6643\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 526us/step - loss: 3365.0980 - mse: 3365.0986 - mae: 32.2417 - val_loss: 1459.6562 - val_mse: 1459.6562 - val_mae: 25.8862\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 448us/step - loss: 3329.6661 - mse: 3329.6663 - mae: 32.1674 - val_loss: 1459.1568 - val_mse: 1459.1569 - val_mae: 25.8062\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 419us/step - loss: 3327.1462 - mse: 3327.1462 - mae: 32.4396 - val_loss: 1459.1342 - val_mse: 1459.1342 - val_mae: 25.8203\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 485us/step - loss: 3339.4480 - mse: 3339.4485 - mae: 31.6986 - val_loss: 1459.2022 - val_mse: 1459.2021 - val_mae: 25.3424\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 520us/step - loss: 3200.5906 - mse: 3200.5896 - mae: 30.8727 - val_loss: 1461.7116 - val_mse: 1461.7115 - val_mae: 26.3121\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3262.6927 - mse: 3262.6929 - mae: 31.8895 - val_loss: 1458.4514 - val_mse: 1458.4512 - val_mae: 25.5746\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3300.3398 - mse: 3300.3398 - mae: 31.8472 - val_loss: 1458.6653 - val_mse: 1458.6654 - val_mae: 25.7621\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3412.6712 - mse: 3412.6709 - mae: 31.9427 - val_loss: 1458.8979 - val_mse: 1458.8979 - val_mae: 25.8607\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 671us/step - loss: 2959.3810 - mse: 2959.3809 - mae: 31.2680 - val_loss: 1067.9230 - val_mse: 1067.9231 - val_mae: 23.6967\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2963.6678 - mse: 2963.6670 - mae: 31.6612 - val_loss: 1068.8311 - val_mse: 1068.8312 - val_mae: 23.6219\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2922.3282 - mse: 2922.3284 - mae: 31.3032 - val_loss: 1063.5063 - val_mse: 1063.5062 - val_mae: 24.1874\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2917.4040 - mse: 2917.4036 - mae: 30.8437 - val_loss: 1068.1532 - val_mse: 1068.1533 - val_mae: 23.6706\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2963.3385 - mse: 2963.3389 - mae: 31.3819 - val_loss: 1064.3326 - val_mse: 1064.3325 - val_mae: 23.9872\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 548us/step - loss: 2861.9732 - mse: 2861.9724 - mae: 30.6727 - val_loss: 1062.9497 - val_mse: 1062.9497 - val_mae: 24.2999\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2920.7835 - mse: 2920.7834 - mae: 30.6949 - val_loss: 1063.1059 - val_mse: 1063.1058 - val_mae: 24.1593\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 509us/step - loss: 2978.3177 - mse: 2978.3176 - mae: 31.6063 - val_loss: 1064.4994 - val_mse: 1064.4994 - val_mae: 23.9820\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2954.7823 - mse: 2954.7827 - mae: 31.5675 - val_loss: 1065.4994 - val_mse: 1065.4994 - val_mae: 23.8429\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2951.2415 - mse: 2951.2412 - mae: 30.9631 - val_loss: 1063.5920 - val_mse: 1063.5920 - val_mae: 24.1026\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2919.5391 - mse: 2919.5383 - mae: 31.4456 - val_loss: 1064.2305 - val_mse: 1064.2306 - val_mae: 23.9685\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2923.0464 - mse: 2923.0466 - mae: 31.4685 - val_loss: 1068.8890 - val_mse: 1068.8890 - val_mae: 23.6000\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2986.6834 - mse: 2986.6829 - mae: 31.2348 - val_loss: 1065.7850 - val_mse: 1065.7849 - val_mae: 23.8158\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2939.7787 - mse: 2939.7781 - mae: 31.2950 - val_loss: 1064.3010 - val_mse: 1064.3010 - val_mae: 24.0020\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2895.3214 - mse: 2895.3208 - mae: 30.9321 - val_loss: 1064.1603 - val_mse: 1064.1603 - val_mae: 24.0270\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 510us/step - loss: 2903.2397 - mse: 2903.2402 - mae: 31.1378 - val_loss: 1062.7689 - val_mse: 1062.7688 - val_mae: 24.2587\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 495us/step - loss: 2974.8766 - mse: 2974.8772 - mae: 31.3956 - val_loss: 1064.0177 - val_mse: 1064.0177 - val_mae: 23.9558\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 536us/step - loss: 2856.2544 - mse: 2856.2537 - mae: 30.7650 - val_loss: 1062.7293 - val_mse: 1062.7292 - val_mae: 24.3006\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 526us/step - loss: 2927.2813 - mse: 2927.2822 - mae: 31.1341 - val_loss: 1063.1359 - val_mse: 1063.1359 - val_mae: 24.0489\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2961.3026 - mse: 2961.3025 - mae: 31.7729 - val_loss: 1066.1534 - val_mse: 1066.1533 - val_mae: 23.7665\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2831.5622 - mse: 2831.5618 - mae: 30.7757 - val_loss: 1062.3403 - val_mse: 1062.3403 - val_mae: 24.3715\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2886.6887 - mse: 2886.6877 - mae: 30.9669 - val_loss: 1063.7794 - val_mse: 1063.7793 - val_mae: 23.9120\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2889.2996 - mse: 2889.2988 - mae: 30.8384 - val_loss: 1062.9040 - val_mse: 1062.9039 - val_mae: 24.0356\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 528us/step - loss: 2922.0935 - mse: 2922.0945 - mae: 31.0617 - val_loss: 1061.7517 - val_mse: 1061.7518 - val_mae: 24.2115\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 477us/step - loss: 2912.5416 - mse: 2912.5420 - mae: 31.0985 - val_loss: 1062.9633 - val_mse: 1062.9633 - val_mae: 24.0150\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 461us/step - loss: 2850.9126 - mse: 2850.9126 - mae: 30.8137 - val_loss: 1062.3344 - val_mse: 1062.3345 - val_mae: 24.5157\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 484us/step - loss: 2873.3829 - mse: 2873.3833 - mae: 30.6818 - val_loss: 1063.3752 - val_mse: 1063.3754 - val_mae: 23.9085\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 513us/step - loss: 2905.2928 - mse: 2905.2927 - mae: 30.2973 - val_loss: 1062.8418 - val_mse: 1062.8420 - val_mae: 24.0150\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 528us/step - loss: 2891.9384 - mse: 2891.9385 - mae: 30.3387 - val_loss: 1064.6112 - val_mse: 1064.6111 - val_mae: 23.8096\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2949.2121 - mse: 2949.2119 - mae: 31.2673 - val_loss: 1065.3411 - val_mse: 1065.3412 - val_mae: 23.8428\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2841.4566 - mse: 2841.4568 - mae: 30.5509 - val_loss: 1063.8993 - val_mse: 1063.8994 - val_mae: 23.9350\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2878.5011 - mse: 2878.5010 - mae: 30.7773 - val_loss: 1062.2151 - val_mse: 1062.2151 - val_mae: 24.3657\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2896.6384 - mse: 2896.6382 - mae: 30.3741 - val_loss: 1062.5156 - val_mse: 1062.5156 - val_mae: 24.1333\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 483us/step - loss: 2923.9402 - mse: 2923.9412 - mae: 30.5586 - val_loss: 1062.1144 - val_mse: 1062.1145 - val_mae: 24.2264\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 517us/step - loss: 2963.9964 - mse: 2963.9966 - mae: 31.6984 - val_loss: 1064.8610 - val_mse: 1064.8610 - val_mae: 23.7490\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 512us/step - loss: 2859.8693 - mse: 2859.8687 - mae: 30.6124 - val_loss: 1061.0848 - val_mse: 1061.0850 - val_mae: 24.4466\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 483us/step - loss: 2865.9372 - mse: 2865.9370 - mae: 30.2669 - val_loss: 1060.8889 - val_mse: 1060.8890 - val_mae: 24.1859\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 496us/step - loss: 2836.3825 - mse: 2836.3823 - mae: 30.5320 - val_loss: 1061.8772 - val_mse: 1061.8772 - val_mae: 23.9990\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2890.4081 - mse: 2890.4087 - mae: 30.9233 - val_loss: 1061.8770 - val_mse: 1061.8768 - val_mae: 23.9654\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2920.2151 - mse: 2920.2156 - mae: 30.9299 - val_loss: 1061.7726 - val_mse: 1061.7728 - val_mae: 23.9572\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2928.8665 - mse: 2928.8660 - mae: 31.2754 - val_loss: 1065.6781 - val_mse: 1065.6781 - val_mae: 23.6605\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2803.4161 - mse: 2803.4165 - mae: 29.9424 - val_loss: 1060.5114 - val_mse: 1060.5112 - val_mae: 24.2796\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2924.8604 - mse: 2924.8608 - mae: 31.1176 - val_loss: 1064.5183 - val_mse: 1064.5184 - val_mae: 23.7389\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2935.7758 - mse: 2935.7761 - mae: 30.8415 - val_loss: 1064.2945 - val_mse: 1064.2943 - val_mae: 23.7253\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2898.3554 - mse: 2898.3550 - mae: 30.8483 - val_loss: 1060.1049 - val_mse: 1060.1050 - val_mae: 24.2299\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 520us/step - loss: 2874.7023 - mse: 2874.7034 - mae: 30.8952 - val_loss: 1064.3815 - val_mse: 1064.3816 - val_mae: 23.7083\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 527us/step - loss: 2870.7984 - mse: 2870.7979 - mae: 30.3514 - val_loss: 1060.9546 - val_mse: 1060.9546 - val_mae: 24.0130\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2898.3251 - mse: 2898.3252 - mae: 31.2204 - val_loss: 1060.0292 - val_mse: 1060.0291 - val_mae: 24.2481\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2867.8734 - mse: 2867.8728 - mae: 30.6541 - val_loss: 1060.8389 - val_mse: 1060.8387 - val_mae: 23.9453\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 535us/step - loss: 2913.6040 - mse: 2913.6033 - mae: 30.9661 - val_loss: 1061.7083 - val_mse: 1061.7081 - val_mae: 23.8002\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 543us/step - loss: 2873.0050 - mse: 2873.0051 - mae: 30.5300 - val_loss: 1058.7174 - val_mse: 1058.7174 - val_mae: 24.1547\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2761.5900 - mse: 2761.5906 - mae: 30.3918 - val_loss: 1059.4802 - val_mse: 1059.4802 - val_mae: 23.9579\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2824.5296 - mse: 2824.5295 - mae: 30.0733 - val_loss: 1058.0294 - val_mse: 1058.0295 - val_mae: 24.1837\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 528us/step - loss: 2798.2945 - mse: 2798.2944 - mae: 30.6096 - val_loss: 1057.3138 - val_mse: 1057.3137 - val_mae: 24.1216\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2972.4943 - mse: 2972.4939 - mae: 31.4566 - val_loss: 1058.0585 - val_mse: 1058.0583 - val_mae: 23.9695\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2838.7490 - mse: 2838.7488 - mae: 30.7395 - val_loss: 1056.3659 - val_mse: 1056.3658 - val_mae: 24.3175\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2891.2170 - mse: 2891.2168 - mae: 31.4325 - val_loss: 1056.3194 - val_mse: 1056.3195 - val_mae: 24.0516\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2780.4044 - mse: 2780.4048 - mae: 30.3468 - val_loss: 1055.5256 - val_mse: 1055.5255 - val_mae: 24.1726\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2948.6891 - mse: 2948.6895 - mae: 30.7894 - val_loss: 1055.5787 - val_mse: 1055.5787 - val_mae: 24.1588\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2859.7379 - mse: 2859.7380 - mae: 30.7511 - val_loss: 1055.7187 - val_mse: 1055.7189 - val_mae: 23.9982\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 510us/step - loss: 2913.8897 - mse: 2913.8894 - mae: 31.0838 - val_loss: 1054.8300 - val_mse: 1054.8301 - val_mae: 24.0204\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 519us/step - loss: 2799.9752 - mse: 2799.9756 - mae: 29.9915 - val_loss: 1053.7454 - val_mse: 1053.7455 - val_mae: 24.0763\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 659us/step - loss: 2817.4495 - mse: 2817.4502 - mae: 30.7712 - val_loss: 1053.3020 - val_mse: 1053.3021 - val_mae: 24.4284\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2879.9438 - mse: 2879.9436 - mae: 30.7463 - val_loss: 1053.9305 - val_mse: 1053.9307 - val_mae: 23.9439\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 535us/step - loss: 2859.1463 - mse: 2859.1465 - mae: 30.4790 - val_loss: 1053.1677 - val_mse: 1053.1676 - val_mae: 24.1023\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 554us/step - loss: 2856.4497 - mse: 2856.4492 - mae: 30.5964 - val_loss: 1054.4734 - val_mse: 1054.4733 - val_mae: 23.9456\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 526us/step - loss: 2836.3318 - mse: 2836.3323 - mae: 30.2426 - val_loss: 1053.5976 - val_mse: 1053.5974 - val_mae: 23.9857\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 527us/step - loss: 2795.7093 - mse: 2795.7092 - mae: 30.2588 - val_loss: 1052.9924 - val_mse: 1052.9922 - val_mae: 24.2132\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2892.3708 - mse: 2892.3713 - mae: 30.6790 - val_loss: 1053.6852 - val_mse: 1053.6854 - val_mae: 24.0067\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 521us/step - loss: 2949.1605 - mse: 2949.1599 - mae: 31.3842 - val_loss: 1054.6104 - val_mse: 1054.6104 - val_mae: 23.8980\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2870.6266 - mse: 2870.6267 - mae: 30.2675 - val_loss: 1053.5083 - val_mse: 1053.5084 - val_mae: 24.1977\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2822.3328 - mse: 2822.3325 - mae: 30.4680 - val_loss: 1053.2122 - val_mse: 1053.2123 - val_mae: 24.2970\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2785.0209 - mse: 2785.0210 - mae: 29.8936 - val_loss: 1052.6513 - val_mse: 1052.6512 - val_mae: 24.2115\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2920.3182 - mse: 2920.3184 - mae: 31.2324 - val_loss: 1055.4745 - val_mse: 1055.4745 - val_mae: 23.7810\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2835.0026 - mse: 2835.0027 - mae: 30.5753 - val_loss: 1053.1933 - val_mse: 1053.1932 - val_mae: 24.0381\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2789.0515 - mse: 2789.0508 - mae: 30.0524 - val_loss: 1053.3949 - val_mse: 1053.3949 - val_mae: 24.5664\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2860.2777 - mse: 2860.2776 - mae: 30.7944 - val_loss: 1052.1137 - val_mse: 1052.1139 - val_mae: 24.2483\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 482us/step - loss: 2880.1737 - mse: 2880.1743 - mae: 30.3656 - val_loss: 1052.5750 - val_mse: 1052.5750 - val_mae: 24.1371\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 536us/step - loss: 2890.9461 - mse: 2890.9463 - mae: 30.9397 - val_loss: 1053.8881 - val_mse: 1053.8882 - val_mae: 23.8726\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 551us/step - loss: 2853.1893 - mse: 2853.1897 - mae: 30.4285 - val_loss: 1053.1399 - val_mse: 1053.1400 - val_mae: 24.4671\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2613.4094 - mse: 2613.4102 - mae: 30.2446 - val_loss: 1545.2974 - val_mse: 1545.2975 - val_mae: 27.9095\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 471us/step - loss: 2582.3338 - mse: 2582.3335 - mae: 30.1993 - val_loss: 1541.2270 - val_mse: 1541.2268 - val_mae: 28.0726\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 463us/step - loss: 2527.5652 - mse: 2527.5659 - mae: 29.8681 - val_loss: 1549.5561 - val_mse: 1549.5560 - val_mae: 27.7294\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2520.7463 - mse: 2520.7463 - mae: 29.5047 - val_loss: 1544.0102 - val_mse: 1544.0101 - val_mae: 27.9408\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2628.7681 - mse: 2628.7678 - mae: 30.2067 - val_loss: 1546.2579 - val_mse: 1546.2581 - val_mae: 27.8289\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 481us/step - loss: 2532.0676 - mse: 2532.0671 - mae: 29.7898 - val_loss: 1548.7609 - val_mse: 1548.7609 - val_mae: 27.7549\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 473us/step - loss: 2602.1320 - mse: 2602.1328 - mae: 29.9922 - val_loss: 1552.3038 - val_mse: 1552.3038 - val_mae: 27.6126\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 521us/step - loss: 2499.2780 - mse: 2499.2781 - mae: 29.4348 - val_loss: 1535.5188 - val_mse: 1535.5188 - val_mae: 28.2964\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2598.0807 - mse: 2598.0813 - mae: 30.0737 - val_loss: 1560.7167 - val_mse: 1560.7166 - val_mae: 27.3363\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2556.0834 - mse: 2556.0825 - mae: 29.7669 - val_loss: 1550.6578 - val_mse: 1550.6575 - val_mae: 27.6481\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 504us/step - loss: 2553.5992 - mse: 2553.5996 - mae: 30.0728 - val_loss: 1548.1583 - val_mse: 1548.1584 - val_mae: 27.7485\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 515us/step - loss: 2560.4105 - mse: 2560.4102 - mae: 29.7332 - val_loss: 1534.9404 - val_mse: 1534.9402 - val_mae: 28.3181\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 498us/step - loss: 2575.2467 - mse: 2575.2461 - mae: 30.0586 - val_loss: 1545.9954 - val_mse: 1545.9954 - val_mae: 27.7394\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2527.0009 - mse: 2527.0017 - mae: 29.5750 - val_loss: 1548.1742 - val_mse: 1548.1743 - val_mae: 27.6438\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 531us/step - loss: 2524.7273 - mse: 2524.7278 - mae: 29.6379 - val_loss: 1540.4361 - val_mse: 1540.4360 - val_mae: 27.9382\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2516.0742 - mse: 2516.0745 - mae: 29.3817 - val_loss: 1539.6454 - val_mse: 1539.6455 - val_mae: 27.9608\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 514us/step - loss: 2541.0178 - mse: 2541.0186 - mae: 29.7306 - val_loss: 1551.3145 - val_mse: 1551.3146 - val_mae: 27.5418\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 541us/step - loss: 2570.3381 - mse: 2570.3381 - mae: 30.5905 - val_loss: 1552.4228 - val_mse: 1552.4229 - val_mae: 27.4727\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 514us/step - loss: 2450.8750 - mse: 2450.8748 - mae: 29.1904 - val_loss: 1545.9481 - val_mse: 1545.9480 - val_mae: 27.7121\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2504.0829 - mse: 2504.0835 - mae: 29.5261 - val_loss: 1552.5195 - val_mse: 1552.5194 - val_mae: 27.4942\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2499.2739 - mse: 2499.2732 - mae: 29.1656 - val_loss: 1543.1448 - val_mse: 1543.1450 - val_mae: 27.8274\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2516.8345 - mse: 2516.8342 - mae: 30.0051 - val_loss: 1543.5109 - val_mse: 1543.5110 - val_mae: 27.7778\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2516.1211 - mse: 2516.1201 - mae: 29.6049 - val_loss: 1542.3508 - val_mse: 1542.3511 - val_mae: 27.8013\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2639.0219 - mse: 2639.0222 - mae: 29.8918 - val_loss: 1547.1313 - val_mse: 1547.1315 - val_mae: 27.6158\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2590.3894 - mse: 2590.3899 - mae: 30.0967 - val_loss: 1558.0117 - val_mse: 1558.0118 - val_mae: 27.2723\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2565.5147 - mse: 2565.5144 - mae: 29.7197 - val_loss: 1538.8902 - val_mse: 1538.8901 - val_mae: 27.8997\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2599.0113 - mse: 2599.0107 - mae: 30.2095 - val_loss: 1542.0761 - val_mse: 1542.0759 - val_mae: 27.7669\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 697us/step - loss: 2428.9807 - mse: 2428.9800 - mae: 29.2136 - val_loss: 1535.4712 - val_mse: 1535.4711 - val_mae: 28.0916\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2627.2120 - mse: 2627.2122 - mae: 30.3309 - val_loss: 1538.4787 - val_mse: 1538.4788 - val_mae: 27.9324\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2538.0696 - mse: 2538.0693 - mae: 29.8430 - val_loss: 1546.0559 - val_mse: 1546.0559 - val_mae: 27.6331\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2604.0182 - mse: 2604.0186 - mae: 29.9108 - val_loss: 1541.6102 - val_mse: 1541.6102 - val_mae: 27.7997\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 530us/step - loss: 2535.4312 - mse: 2535.4316 - mae: 29.9108 - val_loss: 1537.5939 - val_mse: 1537.5939 - val_mae: 27.9872\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2480.7372 - mse: 2480.7371 - mae: 29.3469 - val_loss: 1539.6051 - val_mse: 1539.6050 - val_mae: 27.8795\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2614.5708 - mse: 2614.5710 - mae: 30.2129 - val_loss: 1541.5239 - val_mse: 1541.5238 - val_mae: 27.7819\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2578.3511 - mse: 2578.3513 - mae: 29.6075 - val_loss: 1535.9125 - val_mse: 1535.9124 - val_mae: 28.0172\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2542.4736 - mse: 2542.4741 - mae: 29.7469 - val_loss: 1546.9796 - val_mse: 1546.9796 - val_mae: 27.5401\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2580.3053 - mse: 2580.3044 - mae: 29.8347 - val_loss: 1540.8031 - val_mse: 1540.8031 - val_mae: 27.7582\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 691us/step - loss: 2528.3219 - mse: 2528.3218 - mae: 29.2109 - val_loss: 1535.5860 - val_mse: 1535.5862 - val_mae: 27.9917\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2573.2697 - mse: 2573.2695 - mae: 29.5987 - val_loss: 1542.2193 - val_mse: 1542.2194 - val_mae: 27.6887\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2515.8262 - mse: 2515.8262 - mae: 29.5667 - val_loss: 1542.8874 - val_mse: 1542.8871 - val_mae: 27.6616\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2527.2861 - mse: 2527.2854 - mae: 29.4078 - val_loss: 1537.9531 - val_mse: 1537.9530 - val_mae: 27.8948\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 548us/step - loss: 2502.4292 - mse: 2502.4294 - mae: 29.1291 - val_loss: 1550.5086 - val_mse: 1550.5085 - val_mae: 27.4280\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2466.5921 - mse: 2466.5928 - mae: 29.2035 - val_loss: 1542.9651 - val_mse: 1542.9652 - val_mae: 27.7075\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2474.6278 - mse: 2474.6277 - mae: 28.9707 - val_loss: 1532.5678 - val_mse: 1532.5679 - val_mae: 28.1979\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2554.8761 - mse: 2554.8765 - mae: 29.8699 - val_loss: 1539.3238 - val_mse: 1539.3236 - val_mae: 27.8321\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2557.3019 - mse: 2557.3018 - mae: 29.8789 - val_loss: 1543.3971 - val_mse: 1543.3970 - val_mae: 27.6836\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2557.3136 - mse: 2557.3142 - mae: 30.1238 - val_loss: 1545.2578 - val_mse: 1545.2576 - val_mae: 27.6134\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2487.1941 - mse: 2487.1943 - mae: 29.0601 - val_loss: 1536.0425 - val_mse: 1536.0425 - val_mae: 27.9850\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 535us/step - loss: 2526.7142 - mse: 2526.7146 - mae: 29.6372 - val_loss: 1538.9764 - val_mse: 1538.9767 - val_mae: 27.8606\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2497.1190 - mse: 2497.1189 - mae: 29.6742 - val_loss: 1544.4910 - val_mse: 1544.4910 - val_mae: 27.6374\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2517.4633 - mse: 2517.4634 - mae: 29.6561 - val_loss: 1533.7498 - val_mse: 1533.7498 - val_mae: 28.1266\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 547us/step - loss: 2516.0957 - mse: 2516.0967 - mae: 29.2348 - val_loss: 1533.7664 - val_mse: 1533.7665 - val_mae: 28.1238\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 533us/step - loss: 2442.2902 - mse: 2442.2915 - mae: 29.1373 - val_loss: 1532.3362 - val_mse: 1532.3363 - val_mae: 28.1937\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2533.9303 - mse: 2533.9304 - mae: 29.6378 - val_loss: 1535.6735 - val_mse: 1535.6735 - val_mae: 28.0392\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2502.0253 - mse: 2502.0259 - mae: 29.3412 - val_loss: 1540.2735 - val_mse: 1540.2736 - val_mae: 27.8221\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2554.3738 - mse: 2554.3743 - mae: 29.7565 - val_loss: 1546.2170 - val_mse: 1546.2172 - val_mae: 27.5934\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2509.3150 - mse: 2509.3157 - mae: 29.3044 - val_loss: 1537.7649 - val_mse: 1537.7649 - val_mae: 27.9227\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 501us/step - loss: 2555.7294 - mse: 2555.7302 - mae: 29.8289 - val_loss: 1543.4629 - val_mse: 1543.4629 - val_mae: 27.6734\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 479us/step - loss: 2499.4329 - mse: 2499.4329 - mae: 29.0627 - val_loss: 1541.1305 - val_mse: 1541.1304 - val_mae: 27.7520\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 453us/step - loss: 2509.3981 - mse: 2509.3977 - mae: 29.4530 - val_loss: 1539.3625 - val_mse: 1539.3625 - val_mae: 27.8019\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 492us/step - loss: 2468.6120 - mse: 2468.6123 - mae: 29.4339 - val_loss: 1537.2586 - val_mse: 1537.2585 - val_mae: 27.8922\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2493.3047 - mse: 2493.3049 - mae: 29.0119 - val_loss: 1531.0437 - val_mse: 1531.0436 - val_mae: 28.2482\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2586.3235 - mse: 2586.3235 - mae: 29.6980 - val_loss: 1536.4943 - val_mse: 1536.4944 - val_mae: 27.9489\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2450.8451 - mse: 2450.8455 - mae: 29.5452 - val_loss: 1535.1621 - val_mse: 1535.1619 - val_mae: 27.9927\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2518.3640 - mse: 2518.3638 - mae: 29.4713 - val_loss: 1539.9526 - val_mse: 1539.9525 - val_mae: 27.7998\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2466.6882 - mse: 2466.6882 - mae: 29.5708 - val_loss: 1541.9353 - val_mse: 1541.9355 - val_mae: 27.6709\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2529.3314 - mse: 2529.3315 - mae: 29.4788 - val_loss: 1533.1418 - val_mse: 1533.1416 - val_mae: 28.0631\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2527.8894 - mse: 2527.8892 - mae: 30.1421 - val_loss: 1539.1646 - val_mse: 1539.1646 - val_mae: 27.7424\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2492.6913 - mse: 2492.6912 - mae: 29.5476 - val_loss: 1541.0287 - val_mse: 1541.0286 - val_mae: 27.6494\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2508.1423 - mse: 2508.1423 - mae: 29.3246 - val_loss: 1537.5530 - val_mse: 1537.5530 - val_mae: 27.7859\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2484.4656 - mse: 2484.4656 - mae: 29.5627 - val_loss: 1535.3229 - val_mse: 1535.3229 - val_mae: 27.9153\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2489.9942 - mse: 2489.9949 - mae: 29.0591 - val_loss: 1532.1716 - val_mse: 1532.1716 - val_mae: 28.0828\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 511us/step - loss: 2502.7706 - mse: 2502.7708 - mae: 29.2078 - val_loss: 1537.8628 - val_mse: 1537.8629 - val_mae: 27.7892\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2441.6228 - mse: 2441.6230 - mae: 29.2589 - val_loss: 1540.0037 - val_mse: 1540.0035 - val_mae: 27.7235\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 534us/step - loss: 2549.2366 - mse: 2549.2363 - mae: 29.9595 - val_loss: 1538.9631 - val_mse: 1538.9630 - val_mae: 27.7677\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 548us/step - loss: 2463.3414 - mse: 2463.3413 - mae: 29.0772 - val_loss: 1538.1503 - val_mse: 1538.1505 - val_mae: 27.8252\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2500.6552 - mse: 2500.6555 - mae: 29.1317 - val_loss: 1535.1985 - val_mse: 1535.1985 - val_mae: 27.9508\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 547us/step - loss: 2574.2540 - mse: 2574.2544 - mae: 29.7769 - val_loss: 1537.6259 - val_mse: 1537.6260 - val_mae: 27.8630\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 528us/step - loss: 2517.1972 - mse: 2517.1975 - mae: 29.4503 - val_loss: 1538.9917 - val_mse: 1538.9918 - val_mae: 27.7973\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2473.2271 - mse: 2473.2271 - mae: 29.1090 - val_loss: 1534.7395 - val_mse: 1534.7395 - val_mae: 27.9879\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 526us/step - loss: 2420.7147 - mse: 2420.7144 - mae: 29.8913 - val_loss: 3702.6161 - val_mse: 3702.6157 - val_mae: 24.8987\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 533us/step - loss: 2393.9959 - mse: 2393.9958 - mae: 29.8872 - val_loss: 3700.0367 - val_mse: 3700.0369 - val_mae: 24.3240\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2475.9211 - mse: 2475.9207 - mae: 30.2968 - val_loss: 3700.4197 - val_mse: 3700.4207 - val_mae: 24.5000\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 515us/step - loss: 2471.7259 - mse: 2471.7263 - mae: 30.2874 - val_loss: 3700.6321 - val_mse: 3700.6321 - val_mae: 24.5223\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2431.0260 - mse: 2431.0254 - mae: 29.7380 - val_loss: 3701.9510 - val_mse: 3701.9514 - val_mae: 24.7938\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2422.0850 - mse: 2422.0847 - mae: 29.9313 - val_loss: 3701.9083 - val_mse: 3701.9089 - val_mae: 24.7738\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 504us/step - loss: 2338.6801 - mse: 2338.6802 - mae: 29.1918 - val_loss: 3706.5281 - val_mse: 3706.5273 - val_mae: 25.3440\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 531us/step - loss: 2461.3281 - mse: 2461.3279 - mae: 30.5061 - val_loss: 3699.9405 - val_mse: 3699.9397 - val_mae: 24.3058\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2431.7103 - mse: 2431.7100 - mae: 30.2289 - val_loss: 3703.8131 - val_mse: 3703.8130 - val_mae: 25.0585\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - ETA: 0s - loss: 2387.6381 - mse: 2387.6377 - mae: 29.78 - 2s 541us/step - loss: 2387.8320 - mse: 2387.8315 - mae: 29.8292 - val_loss: 3703.2538 - val_mse: 3703.2544 - val_mae: 24.9136\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 537us/step - loss: 2429.3677 - mse: 2429.3679 - mae: 29.8329 - val_loss: 3701.3664 - val_mse: 3701.3652 - val_mae: 24.5477\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 539us/step - loss: 2412.3174 - mse: 2412.3171 - mae: 30.2099 - val_loss: 3701.8050 - val_mse: 3701.8057 - val_mae: 24.7248\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2393.6120 - mse: 2393.6121 - mae: 29.5845 - val_loss: 3701.3834 - val_mse: 3701.3838 - val_mae: 24.6772\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2424.7153 - mse: 2424.7163 - mae: 29.9344 - val_loss: 3701.1363 - val_mse: 3701.1360 - val_mae: 24.6064\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 558us/step - loss: 2451.6741 - mse: 2451.6741 - mae: 29.7927 - val_loss: 3700.8055 - val_mse: 3700.8062 - val_mae: 24.5643\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2411.0155 - mse: 2411.0168 - mae: 29.4729 - val_loss: 3700.5489 - val_mse: 3700.5486 - val_mae: 24.5717\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2375.9826 - mse: 2375.9817 - mae: 29.7280 - val_loss: 3700.4966 - val_mse: 3700.4973 - val_mae: 24.5654\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2340.1820 - mse: 2340.1831 - mae: 29.3808 - val_loss: 3700.9549 - val_mse: 3700.9556 - val_mae: 24.6521\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2426.3199 - mse: 2426.3203 - mae: 29.7310 - val_loss: 3701.9601 - val_mse: 3701.9602 - val_mae: 24.8505\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2485.3201 - mse: 2485.3210 - mae: 30.2609 - val_loss: 3698.7552 - val_mse: 3698.7551 - val_mae: 24.2137\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2315.7410 - mse: 2315.7402 - mae: 29.0142 - val_loss: 3700.5638 - val_mse: 3700.5640 - val_mae: 24.7013\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2365.6615 - mse: 2365.6611 - mae: 29.6460 - val_loss: 3701.3950 - val_mse: 3701.3950 - val_mae: 24.7846\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2400.1570 - mse: 2400.1567 - mae: 29.8007 - val_loss: 3702.4671 - val_mse: 3702.4673 - val_mae: 25.0268\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 518us/step - loss: 2392.7713 - mse: 2392.7708 - mae: 29.5608 - val_loss: 3699.9769 - val_mse: 3699.9775 - val_mae: 24.4140\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2376.6540 - mse: 2376.6536 - mae: 29.5787 - val_loss: 3704.2831 - val_mse: 3704.2837 - val_mae: 25.1979\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 514us/step - loss: 2334.0750 - mse: 2334.0757 - mae: 29.2128 - val_loss: 3704.7154 - val_mse: 3704.7151 - val_mae: 25.2304\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 1s 491us/step - loss: 2367.1697 - mse: 2367.1685 - mae: 29.4355 - val_loss: 3707.7445 - val_mse: 3707.7454 - val_mae: 25.5880\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2407.9167 - mse: 2407.9163 - mae: 29.7941 - val_loss: 3700.0303 - val_mse: 3700.0305 - val_mae: 24.5472\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2370.6762 - mse: 2370.6758 - mae: 29.7165 - val_loss: 3705.8763 - val_mse: 3705.8755 - val_mae: 25.4953\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 1s 488us/step - loss: 2340.9711 - mse: 2340.9709 - mae: 29.6825 - val_loss: 3700.0437 - val_mse: 3700.0439 - val_mae: 24.6871\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 540us/step - loss: 2361.2593 - mse: 2361.2600 - mae: 29.4523 - val_loss: 3703.4723 - val_mse: 3703.4727 - val_mae: 25.1450\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 538us/step - loss: 2366.9750 - mse: 2366.9753 - mae: 29.3690 - val_loss: 3702.9343 - val_mse: 3702.9343 - val_mae: 25.1712\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2387.3031 - mse: 2387.3030 - mae: 29.8780 - val_loss: 3699.7207 - val_mse: 3699.7205 - val_mae: 24.6424\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2374.8611 - mse: 2374.8604 - mae: 29.3866 - val_loss: 3704.1357 - val_mse: 3704.1355 - val_mae: 25.2561\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2444.3481 - mse: 2444.3481 - mae: 29.9269 - val_loss: 3701.0639 - val_mse: 3701.0640 - val_mae: 24.7973\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2344.7577 - mse: 2344.7578 - mae: 29.5169 - val_loss: 3700.2905 - val_mse: 3700.2908 - val_mae: 24.6781\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 514us/step - loss: 2414.5848 - mse: 2414.5845 - mae: 29.6168 - val_loss: 3701.3055 - val_mse: 3701.3052 - val_mae: 24.9906\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 1s 490us/step - loss: 2378.6563 - mse: 2378.6565 - mae: 29.5895 - val_loss: 3700.0978 - val_mse: 3700.0977 - val_mae: 24.6827\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2408.9107 - mse: 2408.9109 - mae: 29.6819 - val_loss: 3699.6903 - val_mse: 3699.6904 - val_mae: 24.6247\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 1s 486us/step - loss: 2345.3436 - mse: 2345.3438 - mae: 29.4391 - val_loss: 3701.4836 - val_mse: 3701.4836 - val_mae: 24.9380\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 552us/step - loss: 2337.9750 - mse: 2337.9749 - mae: 29.2507 - val_loss: 3702.6949 - val_mse: 3702.6948 - val_mae: 25.1913\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 1s 468us/step - loss: 2355.5905 - mse: 2355.5903 - mae: 29.6280 - val_loss: 3698.9982 - val_mse: 3698.9983 - val_mae: 24.7010\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 513us/step - loss: 2400.6660 - mse: 2400.6663 - mae: 29.3765 - val_loss: 3698.3258 - val_mse: 3698.3259 - val_mae: 24.6698\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 1s 490us/step - loss: 2399.6989 - mse: 2399.6987 - mae: 29.5090 - val_loss: 3699.0200 - val_mse: 3699.0200 - val_mae: 24.8611\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2372.1716 - mse: 2372.1709 - mae: 29.5407 - val_loss: 3700.6963 - val_mse: 3700.6960 - val_mae: 25.1603\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 517us/step - loss: 2401.3345 - mse: 2401.3354 - mae: 29.9738 - val_loss: 3696.2300 - val_mse: 3696.2292 - val_mae: 24.4250\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2358.3217 - mse: 2358.3215 - mae: 29.7928 - val_loss: 3697.0762 - val_mse: 3697.0759 - val_mae: 24.4751\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2420.4207 - mse: 2420.4211 - mae: 29.9051 - val_loss: 3699.4297 - val_mse: 3699.4297 - val_mae: 25.0044\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2363.8863 - mse: 2363.8862 - mae: 29.4933 - val_loss: 3698.3018 - val_mse: 3698.3022 - val_mae: 24.8178\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 1s 447us/step - loss: 2303.9522 - mse: 2303.9519 - mae: 29.2111 - val_loss: 3698.9573 - val_mse: 3698.9568 - val_mae: 24.8569\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2415.2895 - mse: 2415.2886 - mae: 29.9603 - val_loss: 3699.3796 - val_mse: 3699.3796 - val_mae: 24.8264\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 534us/step - loss: 2341.4896 - mse: 2341.4897 - mae: 29.5332 - val_loss: 3700.3524 - val_mse: 3700.3528 - val_mae: 24.9307\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 526us/step - loss: 2349.6653 - mse: 2349.6650 - mae: 29.4054 - val_loss: 3702.9650 - val_mse: 3702.9639 - val_mae: 25.1752\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 530us/step - loss: 2392.2713 - mse: 2392.2720 - mae: 29.6941 - val_loss: 3698.7448 - val_mse: 3698.7444 - val_mae: 24.5582\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2376.5295 - mse: 2376.5291 - mae: 29.4241 - val_loss: 3700.3263 - val_mse: 3700.3252 - val_mae: 24.8694\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2360.1164 - mse: 2360.1165 - mae: 29.2140 - val_loss: 3701.4376 - val_mse: 3701.4377 - val_mae: 25.0671\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2392.9796 - mse: 2392.9795 - mae: 29.7404 - val_loss: 3700.7302 - val_mse: 3700.7295 - val_mae: 25.1175\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2360.2350 - mse: 2360.2349 - mae: 29.9267 - val_loss: 3698.1032 - val_mse: 3698.1035 - val_mae: 24.5546\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 1s 430us/step - loss: 2404.7276 - mse: 2404.7278 - mae: 29.8087 - val_loss: 3696.6956 - val_mse: 3696.6956 - val_mae: 24.4584\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 1s 448us/step - loss: 2419.2628 - mse: 2419.2625 - mae: 29.9432 - val_loss: 3698.7585 - val_mse: 3698.7583 - val_mae: 24.8519\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 530us/step - loss: 2431.8294 - mse: 2431.8296 - mae: 30.1997 - val_loss: 3698.0201 - val_mse: 3698.0205 - val_mae: 24.6595\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 463us/step - loss: 2398.8747 - mse: 2398.8748 - mae: 29.8502 - val_loss: 3698.8406 - val_mse: 3698.8411 - val_mae: 24.8947\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2389.2710 - mse: 2389.2710 - mae: 29.8737 - val_loss: 3700.4312 - val_mse: 3700.4312 - val_mae: 25.0346\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2338.5124 - mse: 2338.5127 - mae: 29.3662 - val_loss: 3701.2701 - val_mse: 3701.2688 - val_mae: 25.0888\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 538us/step - loss: 2412.6859 - mse: 2412.6860 - mae: 29.6311 - val_loss: 3698.9014 - val_mse: 3698.9016 - val_mae: 24.5395\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2437.8363 - mse: 2437.8369 - mae: 30.0445 - val_loss: 3698.2731 - val_mse: 3698.2734 - val_mae: 24.6679\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 512us/step - loss: 2394.1642 - mse: 2394.1646 - mae: 29.3642 - val_loss: 3700.7862 - val_mse: 3700.7864 - val_mae: 25.0595\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2407.1745 - mse: 2407.1750 - mae: 29.8140 - val_loss: 3698.9470 - val_mse: 3698.9470 - val_mae: 24.5680\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 485us/step - loss: 2357.5523 - mse: 2357.5530 - mae: 29.4164 - val_loss: 3702.8955 - val_mse: 3702.8960 - val_mae: 25.3275\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2358.3833 - mse: 2358.3838 - mae: 29.1203 - val_loss: 3702.4231 - val_mse: 3702.4231 - val_mae: 25.2788\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2343.0336 - mse: 2343.0337 - mae: 29.3433 - val_loss: 3700.7028 - val_mse: 3700.7031 - val_mae: 24.9880\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2337.5211 - mse: 2337.5205 - mae: 29.4746 - val_loss: 3701.0582 - val_mse: 3701.0588 - val_mae: 25.0049\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 1s 474us/step - loss: 2342.6375 - mse: 2342.6379 - mae: 29.3425 - val_loss: 3704.4231 - val_mse: 3704.4231 - val_mae: 25.4731\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2350.5479 - mse: 2350.5481 - mae: 29.5078 - val_loss: 3701.4548 - val_mse: 3701.4551 - val_mae: 25.2032\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2379.7893 - mse: 2379.7896 - mae: 29.8530 - val_loss: 3700.4136 - val_mse: 3700.4131 - val_mae: 24.9934\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2344.8947 - mse: 2344.8950 - mae: 29.4039 - val_loss: 3701.0087 - val_mse: 3701.0090 - val_mae: 25.0123\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2373.0755 - mse: 2373.0752 - mae: 29.2175 - val_loss: 3705.1622 - val_mse: 3705.1624 - val_mae: 25.5812\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2327.6921 - mse: 2327.6929 - mae: 29.1842 - val_loss: 3703.0560 - val_mse: 3703.0559 - val_mae: 25.1837\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 527us/step - loss: 2383.5564 - mse: 2383.5562 - mae: 29.4532 - val_loss: 3703.1812 - val_mse: 3703.1812 - val_mae: 25.2340\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 514us/step - loss: 2342.9230 - mse: 2342.9233 - mae: 29.5246 - val_loss: 3704.3102 - val_mse: 3704.3103 - val_mae: 25.4040\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 507us/step - loss: 2744.2815 - mse: 2744.2815 - mae: 29.2477 - val_loss: 2504.3555 - val_mse: 2504.3555 - val_mae: 26.9027\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 544us/step - loss: 2743.2294 - mse: 2743.2290 - mae: 28.6029 - val_loss: 2503.3525 - val_mse: 2503.3525 - val_mae: 27.2782\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 552us/step - loss: 2736.4788 - mse: 2736.4792 - mae: 28.9114 - val_loss: 2503.6697 - val_mse: 2503.6694 - val_mae: 27.4906\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 539us/step - loss: 2731.3358 - mse: 2731.3367 - mae: 28.9748 - val_loss: 2494.0985 - val_mse: 2494.0984 - val_mae: 27.6235\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2705.5718 - mse: 2705.5718 - mae: 29.0477 - val_loss: 2496.8139 - val_mse: 2496.8142 - val_mae: 27.6553\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2769.4100 - mse: 2769.4102 - mae: 29.3397 - val_loss: 2510.7608 - val_mse: 2510.7615 - val_mae: 27.3017\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 540us/step - loss: 2747.6385 - mse: 2747.6389 - mae: 29.0428 - val_loss: 2509.1774 - val_mse: 2509.1775 - val_mae: 27.3828\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 545us/step - loss: 2718.9409 - mse: 2718.9407 - mae: 29.0428 - val_loss: 2497.2215 - val_mse: 2497.2214 - val_mae: 27.9705\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2730.6697 - mse: 2730.6697 - mae: 29.1696 - val_loss: 2501.3317 - val_mse: 2501.3318 - val_mae: 27.7318\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2735.3417 - mse: 2735.3416 - mae: 29.1476 - val_loss: 2498.1873 - val_mse: 2498.1870 - val_mae: 27.5904\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2735.9148 - mse: 2735.9146 - mae: 28.8093 - val_loss: 2508.1280 - val_mse: 2508.1282 - val_mae: 27.1994\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 514us/step - loss: 2747.2642 - mse: 2747.2639 - mae: 28.7337 - val_loss: 2505.4339 - val_mse: 2505.4336 - val_mae: 27.4110\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 552us/step - loss: 2711.9508 - mse: 2711.9512 - mae: 28.8133 - val_loss: 2497.4543 - val_mse: 2497.4543 - val_mae: 27.8693\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2787.0185 - mse: 2787.0176 - mae: 29.1667 - val_loss: 2506.8829 - val_mse: 2506.8828 - val_mae: 27.5823\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2793.0483 - mse: 2793.0486 - mae: 29.0553 - val_loss: 2518.7941 - val_mse: 2518.7944 - val_mae: 27.3135\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2775.7490 - mse: 2775.7498 - mae: 29.0003 - val_loss: 2524.2861 - val_mse: 2524.2856 - val_mae: 27.2544\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 459us/step - loss: 2762.0000 - mse: 2761.9990 - mae: 29.3567 - val_loss: 2517.6198 - val_mse: 2517.6194 - val_mae: 27.6427\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 554us/step - loss: 2756.2214 - mse: 2756.2214 - mae: 28.9585 - val_loss: 2516.1595 - val_mse: 2516.1597 - val_mae: 27.7451\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2724.9833 - mse: 2724.9832 - mae: 28.9925 - val_loss: 2509.0868 - val_mse: 2509.0862 - val_mae: 27.7296\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 514us/step - loss: 2710.2558 - mse: 2710.2563 - mae: 29.0045 - val_loss: 2504.6680 - val_mse: 2504.6677 - val_mae: 27.6265\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 475us/step - loss: 2714.2920 - mse: 2714.2920 - mae: 28.7840 - val_loss: 2515.2116 - val_mse: 2515.2117 - val_mae: 27.3916\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2700.6853 - mse: 2700.6855 - mae: 28.6028 - val_loss: 2507.0311 - val_mse: 2507.0315 - val_mae: 27.7435\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2765.2063 - mse: 2765.2061 - mae: 28.8468 - val_loss: 2516.9354 - val_mse: 2516.9355 - val_mae: 27.2350\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2770.2597 - mse: 2770.2588 - mae: 28.9514 - val_loss: 2518.6295 - val_mse: 2518.6294 - val_mae: 27.2302\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 513us/step - loss: 2735.3175 - mse: 2735.3174 - mae: 28.6651 - val_loss: 2510.0497 - val_mse: 2510.0503 - val_mae: 27.6159\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2774.2309 - mse: 2774.2314 - mae: 29.3981 - val_loss: 2510.0873 - val_mse: 2510.0872 - val_mae: 27.6083\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 552us/step - loss: 2755.3142 - mse: 2755.3152 - mae: 29.0424 - val_loss: 2515.3963 - val_mse: 2515.3962 - val_mae: 27.7252\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2726.0892 - mse: 2726.0894 - mae: 28.9567 - val_loss: 2513.2761 - val_mse: 2513.2761 - val_mae: 27.9886\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2721.1200 - mse: 2721.1199 - mae: 28.7221 - val_loss: 2517.8424 - val_mse: 2517.8428 - val_mae: 27.5486\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2734.6467 - mse: 2734.6470 - mae: 28.6712 - val_loss: 2516.1733 - val_mse: 2516.1738 - val_mae: 27.6918\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2806.5440 - mse: 2806.5435 - mae: 29.2261 - val_loss: 2520.1996 - val_mse: 2520.1995 - val_mae: 27.7490\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2815.3759 - mse: 2815.3755 - mae: 29.5252 - val_loss: 2525.7642 - val_mse: 2525.7644 - val_mae: 27.8072\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 496us/step - loss: 2742.0227 - mse: 2742.0217 - mae: 28.9363 - val_loss: 2535.7960 - val_mse: 2535.7961 - val_mae: 27.4174\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 457us/step - loss: 2748.4959 - mse: 2748.4956 - mae: 28.9330 - val_loss: 2524.8169 - val_mse: 2524.8169 - val_mae: 27.6858\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 537us/step - loss: 2765.4263 - mse: 2765.4270 - mae: 28.9636 - val_loss: 2523.3729 - val_mse: 2523.3730 - val_mae: 27.6444\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2735.0544 - mse: 2735.0540 - mae: 29.1328 - val_loss: 2525.1854 - val_mse: 2525.1855 - val_mae: 27.4383\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2734.0295 - mse: 2734.0293 - mae: 28.5505 - val_loss: 2515.3801 - val_mse: 2515.3796 - val_mae: 27.8790\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 551us/step - loss: 2748.4091 - mse: 2748.4087 - mae: 29.0898 - val_loss: 2511.5618 - val_mse: 2511.5620 - val_mae: 27.7614\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2752.6348 - mse: 2752.6340 - mae: 28.6844 - val_loss: 2518.6030 - val_mse: 2518.6033 - val_mae: 27.7358\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2750.5445 - mse: 2750.5447 - mae: 28.7822 - val_loss: 2516.8057 - val_mse: 2516.8059 - val_mae: 27.8200\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2745.1671 - mse: 2745.1672 - mae: 28.8117 - val_loss: 2520.6850 - val_mse: 2520.6846 - val_mae: 27.7340\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2719.3931 - mse: 2719.3931 - mae: 28.8491 - val_loss: 2521.2688 - val_mse: 2521.2690 - val_mae: 27.8084\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 512us/step - loss: 2730.4399 - mse: 2730.4397 - mae: 28.8269 - val_loss: 2523.7472 - val_mse: 2523.7473 - val_mae: 27.5997\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2705.8090 - mse: 2705.8091 - mae: 28.7813 - val_loss: 2512.4777 - val_mse: 2512.4773 - val_mae: 27.8137\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2761.6595 - mse: 2761.6594 - mae: 29.1833 - val_loss: 2519.4704 - val_mse: 2519.4705 - val_mae: 27.6126\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2756.2054 - mse: 2756.2056 - mae: 29.0958 - val_loss: 2520.9304 - val_mse: 2520.9307 - val_mae: 27.5742\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2699.0087 - mse: 2699.0078 - mae: 28.5890 - val_loss: 2508.3901 - val_mse: 2508.3904 - val_mae: 27.8147\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 491us/step - loss: 2773.2718 - mse: 2773.2720 - mae: 29.2774 - val_loss: 2516.4841 - val_mse: 2516.4839 - val_mae: 27.6287\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2706.9315 - mse: 2706.9319 - mae: 28.6414 - val_loss: 2524.2802 - val_mse: 2524.2803 - val_mae: 27.4153\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2713.8948 - mse: 2713.8950 - mae: 28.6057 - val_loss: 2518.7493 - val_mse: 2518.7488 - val_mae: 27.7828\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 519us/step - loss: 2723.8629 - mse: 2723.8623 - mae: 28.9672 - val_loss: 2518.5978 - val_mse: 2518.5974 - val_mae: 28.0292\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 496us/step - loss: 2708.4566 - mse: 2708.4561 - mae: 28.8205 - val_loss: 2518.1776 - val_mse: 2518.1772 - val_mae: 27.8392\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 489us/step - loss: 2722.5741 - mse: 2722.5740 - mae: 29.0252 - val_loss: 2514.1772 - val_mse: 2514.1777 - val_mae: 27.8664\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 465us/step - loss: 2758.7357 - mse: 2758.7351 - mae: 28.9738 - val_loss: 2520.6054 - val_mse: 2520.6052 - val_mae: 27.4922\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 481us/step - loss: 2749.3035 - mse: 2749.3035 - mae: 28.8064 - val_loss: 2516.8062 - val_mse: 2516.8059 - val_mae: 27.8134\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2705.4114 - mse: 2705.4114 - mae: 29.0892 - val_loss: 2515.6825 - val_mse: 2515.6826 - val_mae: 27.8982\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 535us/step - loss: 2720.9988 - mse: 2720.9990 - mae: 28.7828 - val_loss: 2520.8380 - val_mse: 2520.8381 - val_mae: 27.4647\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 515us/step - loss: 2701.9574 - mse: 2701.9573 - mae: 28.4600 - val_loss: 2515.8181 - val_mse: 2515.8179 - val_mae: 27.6350\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 555us/step - loss: 2702.9333 - mse: 2702.9338 - mae: 28.5846 - val_loss: 2515.3667 - val_mse: 2515.3667 - val_mae: 27.8462\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2691.1068 - mse: 2691.1057 - mae: 28.7225 - val_loss: 2517.3380 - val_mse: 2517.3376 - val_mae: 27.7682\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 525us/step - loss: 2704.5434 - mse: 2704.5427 - mae: 28.7030 - val_loss: 2520.0275 - val_mse: 2520.0281 - val_mae: 27.5648\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2735.0385 - mse: 2735.0388 - mae: 28.6517 - val_loss: 2521.7925 - val_mse: 2521.7927 - val_mae: 27.3884\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 546us/step - loss: 2731.2175 - mse: 2731.2173 - mae: 28.7600 - val_loss: 2510.6813 - val_mse: 2510.6812 - val_mae: 27.7476\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2757.5033 - mse: 2757.5039 - mae: 28.8538 - val_loss: 2519.4188 - val_mse: 2519.4180 - val_mae: 27.5738\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2733.0039 - mse: 2733.0044 - mae: 28.7409 - val_loss: 2517.3416 - val_mse: 2517.3420 - val_mae: 27.5903\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 511us/step - loss: 2746.6805 - mse: 2746.6809 - mae: 28.9069 - val_loss: 2518.3001 - val_mse: 2518.3005 - val_mae: 27.6496\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 481us/step - loss: 2649.2763 - mse: 2649.2778 - mae: 28.8119 - val_loss: 2507.9270 - val_mse: 2507.9270 - val_mae: 27.9460\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 506us/step - loss: 2715.6918 - mse: 2715.6912 - mae: 28.8470 - val_loss: 2516.2790 - val_mse: 2516.2791 - val_mae: 27.5295\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 546us/step - loss: 2714.4077 - mse: 2714.4075 - mae: 28.8448 - val_loss: 2509.1490 - val_mse: 2509.1492 - val_mae: 27.8773\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 469us/step - loss: 2703.5947 - mse: 2703.5952 - mae: 28.5929 - val_loss: 2523.6598 - val_mse: 2523.6599 - val_mae: 27.4331\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2701.6586 - mse: 2701.6589 - mae: 28.5972 - val_loss: 2515.8051 - val_mse: 2515.8054 - val_mae: 27.6846\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 551us/step - loss: 2749.7899 - mse: 2749.7896 - mae: 28.7746 - val_loss: 2520.0730 - val_mse: 2520.0728 - val_mae: 27.7740\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 500us/step - loss: 2745.4911 - mse: 2745.4922 - mae: 28.9919 - val_loss: 2520.0770 - val_mse: 2520.0774 - val_mae: 27.6500\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 474us/step - loss: 2731.4226 - mse: 2731.4216 - mae: 28.5291 - val_loss: 2515.2364 - val_mse: 2515.2366 - val_mae: 27.9555\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2731.3922 - mse: 2731.3926 - mae: 28.9322 - val_loss: 2516.7262 - val_mse: 2516.7258 - val_mae: 27.9504\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2766.5499 - mse: 2766.5496 - mae: 28.8469 - val_loss: 2520.7520 - val_mse: 2520.7520 - val_mae: 27.6836\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 508us/step - loss: 2736.5771 - mse: 2736.5764 - mae: 28.9385 - val_loss: 2526.0232 - val_mse: 2526.0234 - val_mae: 27.4882\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2712.5004 - mse: 2712.5002 - mae: 28.5870 - val_loss: 2524.8419 - val_mse: 2524.8418 - val_mae: 27.5302\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2677.3229 - mse: 2677.3228 - mae: 28.5467 - val_loss: 2508.0162 - val_mse: 2508.0161 - val_mae: 28.1576\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2678.7912 - mse: 2678.7917 - mae: 28.4930 - val_loss: 2513.2942 - val_mse: 2513.2939 - val_mae: 27.8921\n"
     ]
    }
   ],
   "source": [
    "# data set to append here (start with first feature added)\n",
    "X_ = X.loc[:,'PrevDay']\n",
    "\n",
    "# do first prediction\n",
    "X_.fillna(method = 'ffill', inplace = True)\n",
    "y.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "X_ = X_.astype('float64')\n",
    "X_ = X_.round(20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_, y, test_size = 0.15, shuffle = False)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# possible debug\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "def regressor_tunning(n_hidden = 5, \n",
    "                      n_neurons = 40, \n",
    "                      kernel_initializer = \"he_normal\",\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = n_neurons, input_dim = 1))\n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dropout(rate = 0.3))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(units = n_neurons))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.3))\n",
    "    model.add(Dense(units = 1, activation = 'linear'))\n",
    "    optimizer = optimizers.Adamax(lr = 0.001)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = splits)    \n",
    "regressor = regressor_tunning()\n",
    "\n",
    "# train model\n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "    y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "    regressor.fit(X_train_split, y_train_split,  \n",
    "                         shuffle = False, \n",
    "                         validation_split = 0.2,\n",
    "                         batch_size = 20, \n",
    "                         epochs = epochs)\n",
    "\n",
    "# make predictions and evaluate for all regions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS EVALUATION (1) for the whole test set\n",
    "# =============================================================================\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "# calculate metrics\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mae_error = mae(y_test, y_pred)\n",
    "\n",
    "# append to list\n",
    "rmse_gen.append(rmse_error)\n",
    "mae_gen.append(mae_error)\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS EVALUATION (2) on spike regions\n",
    "# =============================================================================\n",
    "\n",
    "# download spike indication binary set\n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "# append ot lists\n",
    "rmse_spi.append(rmse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "\n",
    "# =============================================================================\n",
    "# METRIC EVALUATION (3) on normal regions\n",
    "# =============================================================================\n",
    "\n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "# append to list\n",
    "rmse_nor.append(rmse_normal)\n",
    "mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply loop for the rest of the list of features with condition of improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 13306.2625 - mse: 13306.2617 - mae: 109.8038 - val_loss: 34546.6887 - val_mse: 34546.6836 - val_mae: 132.4607\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 517us/step - loss: 13054.2635 - mse: 13054.2617 - mae: 108.6513 - val_loss: 34028.3438 - val_mse: 34028.3438 - val_mae: 130.4986\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 12282.1594 - mse: 12282.1592 - mae: 105.0098 - val_loss: 32508.2935 - val_mse: 32508.2949 - val_mae: 124.5681\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 10174.6887 - mse: 10174.6904 - mae: 94.2229 - val_loss: 28468.0892 - val_mse: 28468.0898 - val_mae: 107.2245\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 637us/step - loss: 5962.1595 - mse: 5962.1597 - mae: 65.7649 - val_loss: 20759.1571 - val_mse: 20759.1562 - val_mae: 61.8548\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2677.0298 - mse: 2677.0300 - mae: 37.7610 - val_loss: 17480.9920 - val_mse: 17480.9922 - val_mae: 34.8415\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2761.3600 - mse: 2761.3604 - mae: 36.8491 - val_loss: 17884.4081 - val_mse: 17884.4082 - val_mae: 35.7169\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2445.0828 - mse: 2445.0833 - mae: 35.6536 - val_loss: 18175.3785 - val_mse: 18175.3809 - val_mae: 37.6881\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 750us/step - loss: 2559.3901 - mse: 2559.3901 - mae: 36.4291 - val_loss: 17891.1414 - val_mse: 17891.1426 - val_mae: 35.7587\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 550us/step - loss: 2659.9863 - mse: 2659.9863 - mae: 36.5244 - val_loss: 18136.5826 - val_mse: 18136.5840 - val_mae: 37.3997\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 537us/step - loss: 2522.3178 - mse: 2522.3181 - mae: 36.0642 - val_loss: 17964.3026 - val_mse: 17964.3027 - val_mae: 36.2072\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 2280.0575 - mse: 2280.0576 - mae: 34.7580 - val_loss: 17780.9223 - val_mse: 17780.9238 - val_mae: 35.1862\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2650.4986 - mse: 2650.4985 - mae: 36.8027 - val_loss: 17910.8430 - val_mse: 17910.8418 - val_mae: 35.8814\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 476us/step - loss: 2528.8416 - mse: 2528.8418 - mae: 36.3866 - val_loss: 17863.9152 - val_mse: 17863.9160 - val_mae: 35.5996\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2485.9072 - mse: 2485.9072 - mae: 35.7475 - val_loss: 17864.9369 - val_mse: 17864.9355 - val_mae: 35.6070\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 431us/step - loss: 2377.2851 - mse: 2377.2852 - mae: 35.7704 - val_loss: 17877.1121 - val_mse: 17877.1113 - val_mae: 35.6820\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 463us/step - loss: 2392.7422 - mse: 2392.7417 - mae: 35.5253 - val_loss: 17743.5841 - val_mse: 17743.5859 - val_mae: 35.1303\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 2514.2501 - mse: 2514.2500 - mae: 36.0630 - val_loss: 18110.9533 - val_mse: 18110.9531 - val_mae: 37.2123\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2179.5690 - mse: 2179.5691 - mae: 32.3005 - val_loss: 17679.5308 - val_mse: 17679.5312 - val_mae: 35.0564\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2301.3054 - mse: 2301.3052 - mae: 34.7905 - val_loss: 17821.2286 - val_mse: 17821.2305 - val_mae: 35.3690\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 2370.8083 - mse: 2370.8081 - mae: 34.2630 - val_loss: 17765.8051 - val_mse: 17765.8066 - val_mae: 35.2100\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 716us/step - loss: 2436.7839 - mse: 2436.7839 - mae: 34.4097 - val_loss: 17775.4864 - val_mse: 17775.4863 - val_mae: 35.2382\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2361.0731 - mse: 2361.0730 - mae: 33.9578 - val_loss: 17858.2713 - val_mse: 17858.2715 - val_mae: 35.5780\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 722us/step - loss: 2381.0220 - mse: 2381.0222 - mae: 33.9876 - val_loss: 17923.4807 - val_mse: 17923.4805 - val_mae: 35.9665\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2324.8667 - mse: 2324.8665 - mae: 34.0222 - val_loss: 17835.6688 - val_mse: 17835.6680 - val_mae: 35.4651\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 663us/step - loss: 2485.6500 - mse: 2485.6499 - mae: 35.6844 - val_loss: 17795.7181 - val_mse: 17795.7168 - val_mae: 35.3222\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 663us/step - loss: 2215.3070 - mse: 2215.3069 - mae: 33.1435 - val_loss: 17851.4981 - val_mse: 17851.5000 - val_mae: 35.5535\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 700us/step - loss: 2178.6391 - mse: 2178.6389 - mae: 34.1698 - val_loss: 17745.1042 - val_mse: 17745.1055 - val_mae: 35.2368\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 2273.8762 - mse: 2273.8762 - mae: 34.6558 - val_loss: 17802.4520 - val_mse: 17802.4512 - val_mae: 35.3642\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 652us/step - loss: 2423.2794 - mse: 2423.2798 - mae: 34.0998 - val_loss: 17647.7922 - val_mse: 17647.7930 - val_mae: 35.1449\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2348.3555 - mse: 2348.3557 - mae: 34.2661 - val_loss: 17863.3849 - val_mse: 17863.3848 - val_mae: 35.6360\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 2129.2099 - mse: 2129.2100 - mae: 32.0121 - val_loss: 17725.7793 - val_mse: 17725.7773 - val_mae: 35.2412\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 2306.6284 - mse: 2306.6282 - mae: 34.0939 - val_loss: 17874.4065 - val_mse: 17874.4062 - val_mae: 35.6990\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 605us/step - loss: 2346.1779 - mse: 2346.1780 - mae: 34.9301 - val_loss: 17638.5032 - val_mse: 17638.5020 - val_mae: 35.1781\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2182.8461 - mse: 2182.8462 - mae: 32.0340 - val_loss: 17648.6331 - val_mse: 17648.6328 - val_mae: 35.1976\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2245.0301 - mse: 2245.0300 - mae: 33.0738 - val_loss: 17808.2280 - val_mse: 17808.2285 - val_mae: 35.4241\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 512us/step - loss: 2220.0329 - mse: 2220.0332 - mae: 32.7468 - val_loss: 17842.0467 - val_mse: 17842.0469 - val_mae: 35.5607\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 474us/step - loss: 2323.0296 - mse: 2323.0300 - mae: 32.8928 - val_loss: 17648.9646 - val_mse: 17648.9648 - val_mae: 35.2259\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 692us/step - loss: 2305.6004 - mse: 2305.6003 - mae: 33.4026 - val_loss: 17839.3980 - val_mse: 17839.3984 - val_mae: 35.5579\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2042.4416 - mse: 2042.4417 - mae: 32.0433 - val_loss: 17679.9466 - val_mse: 17679.9453 - val_mae: 35.2705\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2208.8653 - mse: 2208.8655 - mae: 32.7366 - val_loss: 17698.9971 - val_mse: 17698.9980 - val_mae: 35.2990\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2091.7406 - mse: 2091.7407 - mae: 31.4370 - val_loss: 17631.0209 - val_mse: 17631.0195 - val_mae: 35.2643\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2084.7757 - mse: 2084.7754 - mae: 31.2799 - val_loss: 17596.3657 - val_mse: 17596.3652 - val_mae: 35.2631\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2249.6047 - mse: 2249.6047 - mae: 33.7923 - val_loss: 17905.4487 - val_mse: 17905.4453 - val_mae: 35.9053\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 731us/step - loss: 2219.9973 - mse: 2219.9973 - mae: 32.3967 - val_loss: 17802.4960 - val_mse: 17802.4961 - val_mae: 35.4865\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 2130.4797 - mse: 2130.4797 - mae: 32.2754 - val_loss: 17677.7320 - val_mse: 17677.7324 - val_mae: 35.3367\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 2138.4013 - mse: 2138.4009 - mae: 32.5224 - val_loss: 17691.6545 - val_mse: 17691.6543 - val_mae: 35.3574\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 1901.9165 - mse: 1901.9166 - mae: 30.5933 - val_loss: 17720.9293 - val_mse: 17720.9277 - val_mae: 35.3904\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 458us/step - loss: 2169.7374 - mse: 2169.7375 - mae: 31.9556 - val_loss: 17690.4282 - val_mse: 17690.4277 - val_mae: 35.3759\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 367us/step - loss: 2074.9644 - mse: 2074.9646 - mae: 31.2665 - val_loss: 17701.9384 - val_mse: 17701.9395 - val_mae: 35.3959\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 516us/step - loss: 2073.3601 - mse: 2073.3601 - mae: 30.8441 - val_loss: 17561.7638 - val_mse: 17561.7637 - val_mae: 35.3917\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 516us/step - loss: 1998.1154 - mse: 1998.1154 - mae: 30.9329 - val_loss: 17700.3677 - val_mse: 17700.3672 - val_mae: 35.4204\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 504us/step - loss: 2135.7526 - mse: 2135.7527 - mae: 31.2103 - val_loss: 17810.2532 - val_mse: 17810.2539 - val_mae: 35.5957\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 472us/step - loss: 1921.5893 - mse: 1921.5894 - mae: 31.4775 - val_loss: 17554.2551 - val_mse: 17554.2539 - val_mae: 35.4446\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 350us/step - loss: 2381.6535 - mse: 2381.6536 - mae: 33.4430 - val_loss: 17864.9868 - val_mse: 17864.9863 - val_mae: 35.7980\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 519us/step - loss: 1933.7823 - mse: 1933.7823 - mae: 31.2568 - val_loss: 17584.4704 - val_mse: 17584.4707 - val_mae: 35.4398\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 405us/step - loss: 2062.9123 - mse: 2062.9124 - mae: 31.2932 - val_loss: 17894.8962 - val_mse: 17894.8984 - val_mae: 35.9332\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 347us/step - loss: 1983.6684 - mse: 1983.6685 - mae: 29.8778 - val_loss: 17661.4376 - val_mse: 17661.4375 - val_mae: 35.4591\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 285us/step - loss: 2149.6790 - mse: 2149.6787 - mae: 31.4879 - val_loss: 17735.8438 - val_mse: 17735.8438 - val_mae: 35.5225\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 443us/step - loss: 2203.8239 - mse: 2203.8240 - mae: 31.9765 - val_loss: 17989.3913 - val_mse: 17989.3926 - val_mae: 36.4033\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 454us/step - loss: 2150.9265 - mse: 2150.9265 - mae: 32.3930 - val_loss: 17682.7496 - val_mse: 17682.7500 - val_mae: 35.5061\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 451us/step - loss: 2014.6519 - mse: 2014.6517 - mae: 30.4865 - val_loss: 17663.6529 - val_mse: 17663.6523 - val_mae: 35.5093\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 462us/step - loss: 2198.6706 - mse: 2198.6704 - mae: 33.3314 - val_loss: 17855.6565 - val_mse: 17855.6562 - val_mae: 35.8312\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 386us/step - loss: 2060.6946 - mse: 2060.6948 - mae: 30.5674 - val_loss: 17827.7572 - val_mse: 17827.7578 - val_mae: 35.7545\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 413us/step - loss: 2119.0219 - mse: 2119.0220 - mae: 31.5552 - val_loss: 17787.0201 - val_mse: 17787.0195 - val_mae: 35.6536\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 476us/step - loss: 1929.1510 - mse: 1929.1509 - mae: 29.7694 - val_loss: 17663.6891 - val_mse: 17663.6875 - val_mae: 35.5665\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 455us/step - loss: 1900.5500 - mse: 1900.5500 - mae: 30.2888 - val_loss: 17752.4039 - val_mse: 17752.4023 - val_mae: 35.6230\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 476us/step - loss: 1904.1081 - mse: 1904.1080 - mae: 30.0314 - val_loss: 17522.4344 - val_mse: 17522.4336 - val_mae: 35.7181\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 465us/step - loss: 2120.7012 - mse: 2120.7009 - mae: 32.2003 - val_loss: 17721.8564 - val_mse: 17721.8555 - val_mae: 35.6156\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 539us/step - loss: 2011.1834 - mse: 2011.1835 - mae: 30.5254 - val_loss: 17638.7441 - val_mse: 17638.7461 - val_mae: 35.6233\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 441us/step - loss: 2068.8181 - mse: 2068.8181 - mae: 30.6084 - val_loss: 17807.3683 - val_mse: 17807.3691 - val_mae: 35.7594\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 447us/step - loss: 2055.1370 - mse: 2055.1372 - mae: 30.6105 - val_loss: 17800.3550 - val_mse: 17800.3574 - val_mae: 35.7496\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 379us/step - loss: 1893.8369 - mse: 1893.8368 - mae: 29.8514 - val_loss: 17605.1034 - val_mse: 17605.1035 - val_mae: 35.6795\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 397us/step - loss: 1847.4425 - mse: 1847.4425 - mae: 29.4606 - val_loss: 17741.3177 - val_mse: 17741.3184 - val_mae: 35.6820\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 518us/step - loss: 1791.4148 - mse: 1791.4149 - mae: 29.1332 - val_loss: 17689.3046 - val_mse: 17689.3047 - val_mae: 35.6838\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 1861.6945 - mse: 1861.6943 - mae: 28.5757 - val_loss: 17679.1842 - val_mse: 17679.1816 - val_mae: 35.6973\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 475us/step - loss: 1942.1429 - mse: 1942.1428 - mae: 30.1788 - val_loss: 17650.2880 - val_mse: 17650.2871 - val_mae: 35.7183\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 498us/step - loss: 1880.2241 - mse: 1880.2241 - mae: 29.3610 - val_loss: 17732.4883 - val_mse: 17732.4883 - val_mae: 35.7334\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 511us/step - loss: 2007.2641 - mse: 2007.2643 - mae: 30.7135 - val_loss: 17753.8864 - val_mse: 17753.8887 - val_mae: 35.7537\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 2014.7231 - mse: 2014.7230 - mae: 29.8769 - val_loss: 17662.4984 - val_mse: 17662.4980 - val_mae: 35.7578\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 509us/step - loss: 4376.7898 - mse: 4376.7900 - mae: 35.1037 - val_loss: 2108.1463 - val_mse: 2108.1460 - val_mae: 31.4556\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4440.5505 - mse: 4440.5508 - mae: 35.4537 - val_loss: 2153.7496 - val_mse: 2153.7495 - val_mae: 31.5689\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4307.9536 - mse: 4307.9546 - mae: 35.0270 - val_loss: 2191.4082 - val_mse: 2191.4080 - val_mae: 31.6830\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4312.5426 - mse: 4312.5425 - mae: 35.0942 - val_loss: 2300.1865 - val_mse: 2300.1865 - val_mae: 32.0076\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4366.4202 - mse: 4366.4214 - mae: 34.6188 - val_loss: 2190.3674 - val_mse: 2190.3672 - val_mae: 31.6733\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4148.9686 - mse: 4148.9688 - mae: 34.4699 - val_loss: 2219.4961 - val_mse: 2219.4963 - val_mae: 31.7605\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 568us/step - loss: 4262.2212 - mse: 4262.2217 - mae: 35.7463 - val_loss: 2249.1320 - val_mse: 2249.1321 - val_mae: 31.8436\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4314.2855 - mse: 4314.2852 - mae: 34.5243 - val_loss: 2288.7649 - val_mse: 2288.7646 - val_mae: 31.9460\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4094.8869 - mse: 4094.8865 - mae: 34.1028 - val_loss: 2274.9514 - val_mse: 2274.9514 - val_mae: 31.9030\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4254.9147 - mse: 4254.9146 - mae: 35.0934 - val_loss: 2275.1024 - val_mse: 2275.1023 - val_mae: 31.8996\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4271.7148 - mse: 4271.7148 - mae: 35.4167 - val_loss: 2320.8212 - val_mse: 2320.8213 - val_mae: 32.0365\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4185.9797 - mse: 4185.9795 - mae: 34.6663 - val_loss: 2224.8128 - val_mse: 2224.8127 - val_mae: 31.7589\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4271.9110 - mse: 4271.9111 - mae: 34.7145 - val_loss: 2239.2970 - val_mse: 2239.2971 - val_mae: 31.7964\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4251.8365 - mse: 4251.8364 - mae: 34.6859 - val_loss: 2246.8734 - val_mse: 2246.8733 - val_mae: 31.8136\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4186.9604 - mse: 4186.9600 - mae: 34.7216 - val_loss: 2297.4601 - val_mse: 2297.4597 - val_mae: 31.9489\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 693us/step - loss: 4173.6495 - mse: 4173.6499 - mae: 34.8209 - val_loss: 2259.6295 - val_mse: 2259.6294 - val_mae: 31.8406\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 4093.1129 - mse: 4093.1133 - mae: 34.8895 - val_loss: 2235.0917 - val_mse: 2235.0918 - val_mae: 31.7741\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 4243.3496 - mse: 4243.3501 - mae: 34.4756 - val_loss: 2269.2633 - val_mse: 2269.2629 - val_mae: 31.8620\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4261.1838 - mse: 4261.1841 - mae: 34.9305 - val_loss: 2235.0175 - val_mse: 2235.0173 - val_mae: 31.7729\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 4162.5800 - mse: 4162.5796 - mae: 34.4711 - val_loss: 2269.4196 - val_mse: 2269.4192 - val_mae: 31.8604\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4089.7759 - mse: 4089.7764 - mae: 34.3191 - val_loss: 2219.1092 - val_mse: 2219.1091 - val_mae: 31.7266\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 503us/step - loss: 4357.0718 - mse: 4357.0718 - mae: 34.3146 - val_loss: 2316.5673 - val_mse: 2316.5671 - val_mae: 31.9933\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4019.1018 - mse: 4019.1018 - mae: 34.4215 - val_loss: 2284.2105 - val_mse: 2284.2104 - val_mae: 31.8969\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 4218.8301 - mse: 4218.8301 - mae: 34.7361 - val_loss: 2299.7673 - val_mse: 2299.7676 - val_mae: 31.9366\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4232.1951 - mse: 4232.1948 - mae: 34.4949 - val_loss: 2314.6743 - val_mse: 2314.6743 - val_mae: 31.9773\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4174.7451 - mse: 4174.7451 - mae: 33.9808 - val_loss: 2257.4473 - val_mse: 2257.4475 - val_mae: 31.8222\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 4094.2956 - mse: 4094.2957 - mae: 33.5259 - val_loss: 2249.8361 - val_mse: 2249.8359 - val_mae: 31.8003\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4183.1930 - mse: 4183.1929 - mae: 34.2866 - val_loss: 2285.8186 - val_mse: 2285.8188 - val_mae: 31.8957\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 3922.8165 - mse: 3922.8162 - mae: 33.5881 - val_loss: 2261.0402 - val_mse: 2261.0403 - val_mae: 31.8280\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4064.6184 - mse: 4064.6189 - mae: 33.7579 - val_loss: 2269.1749 - val_mse: 2269.1748 - val_mae: 31.8487\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 682us/step - loss: 4294.1223 - mse: 4294.1226 - mae: 34.9154 - val_loss: 2291.8728 - val_mse: 2291.8728 - val_mae: 31.9105\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4233.4961 - mse: 4233.4961 - mae: 34.0562 - val_loss: 2290.1608 - val_mse: 2290.1606 - val_mae: 31.9048\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 569us/step - loss: 4208.6718 - mse: 4208.6704 - mae: 34.1890 - val_loss: 2288.6725 - val_mse: 2288.6726 - val_mae: 31.9006\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4193.4348 - mse: 4193.4351 - mae: 34.8781 - val_loss: 2284.8836 - val_mse: 2284.8838 - val_mae: 31.8889\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4124.8203 - mse: 4124.8203 - mae: 33.9951 - val_loss: 2205.3908 - val_mse: 2205.3909 - val_mae: 31.6677\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 649us/step - loss: 3936.2241 - mse: 3936.2239 - mae: 33.2810 - val_loss: 2185.6100 - val_mse: 2185.6101 - val_mae: 31.6119\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 622us/step - loss: 4120.9326 - mse: 4120.9321 - mae: 34.6087 - val_loss: 2287.4766 - val_mse: 2287.4766 - val_mae: 31.8897\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4174.7245 - mse: 4174.7241 - mae: 34.8475 - val_loss: 2271.6646 - val_mse: 2271.6648 - val_mae: 31.8436\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4034.9005 - mse: 4034.9004 - mae: 33.5038 - val_loss: 2282.7126 - val_mse: 2282.7126 - val_mae: 31.8713\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 3985.7940 - mse: 3985.7939 - mae: 33.6310 - val_loss: 2226.1598 - val_mse: 2226.1599 - val_mae: 31.7132\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 552us/step - loss: 3964.8454 - mse: 3964.8459 - mae: 33.4016 - val_loss: 2236.4986 - val_mse: 2236.4988 - val_mae: 31.7409\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4152.4068 - mse: 4152.4067 - mae: 33.5988 - val_loss: 2290.1234 - val_mse: 2290.1235 - val_mae: 31.8921\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4176.8833 - mse: 4176.8833 - mae: 33.1735 - val_loss: 2258.8602 - val_mse: 2258.8604 - val_mae: 31.8029\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 3929.5222 - mse: 3929.5222 - mae: 32.9488 - val_loss: 2252.9667 - val_mse: 2252.9670 - val_mae: 31.7849\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4147.2154 - mse: 4147.2148 - mae: 33.5085 - val_loss: 2231.9738 - val_mse: 2231.9736 - val_mae: 31.7238\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4124.0505 - mse: 4124.0503 - mae: 34.1196 - val_loss: 2283.5810 - val_mse: 2283.5811 - val_mae: 31.8678\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4094.7662 - mse: 4094.7654 - mae: 33.8957 - val_loss: 2296.1240 - val_mse: 2296.1240 - val_mae: 31.9019\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4103.3968 - mse: 4103.3965 - mae: 33.3323 - val_loss: 2285.8816 - val_mse: 2285.8818 - val_mae: 31.8727\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4109.7897 - mse: 4109.7900 - mae: 34.3367 - val_loss: 2305.1383 - val_mse: 2305.1382 - val_mae: 31.9267\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 524us/step - loss: 4034.8578 - mse: 4034.8574 - mae: 33.5307 - val_loss: 2277.4035 - val_mse: 2277.4033 - val_mae: 31.8477\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 517us/step - loss: 3946.7527 - mse: 3946.7524 - mae: 33.1502 - val_loss: 2241.1919 - val_mse: 2241.1919 - val_mae: 31.7414\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4043.5588 - mse: 4043.5588 - mae: 33.5211 - val_loss: 2270.2160 - val_mse: 2270.2161 - val_mae: 31.8257\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4092.4761 - mse: 4092.4763 - mae: 33.5511 - val_loss: 2290.1077 - val_mse: 2290.1074 - val_mae: 31.8819\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4132.6582 - mse: 4132.6582 - mae: 34.7801 - val_loss: 2347.2998 - val_mse: 2347.2998 - val_mae: 32.0401\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4068.7721 - mse: 4068.7727 - mae: 33.0494 - val_loss: 2302.6256 - val_mse: 2302.6255 - val_mae: 31.9172\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4051.7629 - mse: 4051.7632 - mae: 32.8501 - val_loss: 2317.8545 - val_mse: 2317.8545 - val_mae: 31.9552\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4082.7585 - mse: 4082.7585 - mae: 33.1002 - val_loss: 2271.3902 - val_mse: 2271.3901 - val_mae: 31.8269\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 475us/step - loss: 4211.7816 - mse: 4211.7822 - mae: 33.8493 - val_loss: 2243.7323 - val_mse: 2243.7322 - val_mae: 31.7431\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4048.8244 - mse: 4048.8247 - mae: 34.2264 - val_loss: 2329.6236 - val_mse: 2329.6235 - val_mae: 31.9835\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4034.6374 - mse: 4034.6372 - mae: 32.9583 - val_loss: 2312.3817 - val_mse: 2312.3818 - val_mae: 31.9367\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 3979.0801 - mse: 3979.0798 - mae: 33.2915 - val_loss: 2272.0393 - val_mse: 2272.0396 - val_mae: 31.8230\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 544us/step - loss: 4078.3751 - mse: 4078.3752 - mae: 33.3560 - val_loss: 2331.4498 - val_mse: 2331.4497 - val_mae: 31.9834\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 3962.5012 - mse: 3962.5012 - mae: 33.0372 - val_loss: 2293.7412 - val_mse: 2293.7412 - val_mae: 31.8795\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4095.0416 - mse: 4095.0415 - mae: 33.1150 - val_loss: 2334.0003 - val_mse: 2334.0005 - val_mae: 31.9868\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 515us/step - loss: 4128.9737 - mse: 4128.9741 - mae: 34.0481 - val_loss: 2331.5238 - val_mse: 2331.5239 - val_mae: 31.9785\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 3990.4814 - mse: 3990.4824 - mae: 32.9717 - val_loss: 2238.7023 - val_mse: 2238.7021 - val_mae: 31.7160\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 516us/step - loss: 4207.3334 - mse: 4207.3340 - mae: 33.7996 - val_loss: 2295.6127 - val_mse: 2295.6125 - val_mae: 31.8778\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 3898.1461 - mse: 3898.1460 - mae: 32.1533 - val_loss: 2308.5682 - val_mse: 2308.5681 - val_mae: 31.9136\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 518us/step - loss: 4000.5886 - mse: 4000.5889 - mae: 34.0127 - val_loss: 2250.0340 - val_mse: 2250.0339 - val_mae: 31.7452\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4049.5971 - mse: 4049.5977 - mae: 33.1761 - val_loss: 2299.3923 - val_mse: 2299.3926 - val_mae: 31.8861\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4112.4714 - mse: 4112.4712 - mae: 33.2808 - val_loss: 2326.9385 - val_mse: 2326.9387 - val_mae: 31.9575\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 0s 498us/step - loss: 4021.9700 - mse: 4021.9700 - mae: 33.3783 - val_loss: 2303.3372 - val_mse: 2303.3369 - val_mae: 31.8915\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 3911.4051 - mse: 3911.4053 - mae: 32.6166 - val_loss: 2256.9471 - val_mse: 2256.9473 - val_mae: 31.7575\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4190.4700 - mse: 4190.4697 - mae: 33.6641 - val_loss: 2298.8157 - val_mse: 2298.8154 - val_mae: 31.8766\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 535us/step - loss: 3969.0633 - mse: 3969.0637 - mae: 32.8171 - val_loss: 2285.5781 - val_mse: 2285.5779 - val_mae: 31.8370\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 684us/step - loss: 4019.7186 - mse: 4019.7185 - mae: 32.1811 - val_loss: 2275.4299 - val_mse: 2275.4297 - val_mae: 31.8050\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4032.7582 - mse: 4032.7585 - mae: 33.6411 - val_loss: 2278.8233 - val_mse: 2278.8230 - val_mae: 31.8108\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4039.8233 - mse: 4039.8232 - mae: 32.8586 - val_loss: 2333.0494 - val_mse: 2333.0496 - val_mae: 31.9595\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 653us/step - loss: 4110.2040 - mse: 4110.2041 - mae: 32.4712 - val_loss: 2294.3928 - val_mse: 2294.3928 - val_mae: 31.8525\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 680us/step - loss: 3987.3127 - mse: 3987.3130 - mae: 33.1467 - val_loss: 2294.4796 - val_mse: 2294.4797 - val_mae: 31.8524\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3316.8058 - mse: 3316.8049 - mae: 32.6659 - val_loss: 1456.0699 - val_mse: 1456.0701 - val_mae: 25.6290\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3365.4978 - mse: 3365.4976 - mae: 32.7332 - val_loss: 1458.1764 - val_mse: 1458.1765 - val_mae: 25.0035\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 536us/step - loss: 3413.8474 - mse: 3413.8472 - mae: 33.0367 - val_loss: 1457.1720 - val_mse: 1457.1720 - val_mae: 25.7370\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3340.7144 - mse: 3340.7141 - mae: 32.4360 - val_loss: 1460.4070 - val_mse: 1460.4067 - val_mae: 26.2725\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3418.4295 - mse: 3418.4307 - mae: 33.2875 - val_loss: 1458.5560 - val_mse: 1458.5562 - val_mae: 25.8775\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3343.5558 - mse: 3343.5559 - mae: 32.5667 - val_loss: 1458.8555 - val_mse: 1458.8556 - val_mae: 25.2254\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3292.4823 - mse: 3292.4817 - mae: 32.2471 - val_loss: 1460.3231 - val_mse: 1460.3232 - val_mae: 26.1539\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3330.2664 - mse: 3330.2668 - mae: 32.5753 - val_loss: 1460.0397 - val_mse: 1460.0398 - val_mae: 26.1017\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3596.8233 - mse: 3596.8225 - mae: 33.5538 - val_loss: 1458.5809 - val_mse: 1458.5807 - val_mae: 25.5345\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3376.9714 - mse: 3376.9709 - mae: 32.7262 - val_loss: 1459.2600 - val_mse: 1459.2600 - val_mae: 25.8505\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 673us/step - loss: 3421.8304 - mse: 3421.8303 - mae: 32.7517 - val_loss: 1465.4909 - val_mse: 1465.4908 - val_mae: 26.7229\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3358.4570 - mse: 3358.4568 - mae: 32.6917 - val_loss: 1459.4044 - val_mse: 1459.4043 - val_mae: 25.6019\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 701us/step - loss: 3454.8867 - mse: 3454.8870 - mae: 33.2703 - val_loss: 1460.8697 - val_mse: 1460.8696 - val_mae: 25.2058\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3437.4229 - mse: 3437.4229 - mae: 32.6028 - val_loss: 1459.9256 - val_mse: 1459.9257 - val_mae: 25.6703\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3431.6134 - mse: 3431.6130 - mae: 32.4236 - val_loss: 1461.5485 - val_mse: 1461.5485 - val_mae: 26.0626\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3454.4071 - mse: 3454.4067 - mae: 33.4135 - val_loss: 1462.8843 - val_mse: 1462.8844 - val_mae: 25.0070\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 533us/step - loss: 3389.5528 - mse: 3389.5527 - mae: 32.1838 - val_loss: 1461.0115 - val_mse: 1461.0117 - val_mae: 25.4283\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3435.3732 - mse: 3435.3733 - mae: 32.6734 - val_loss: 1460.8525 - val_mse: 1460.8524 - val_mae: 25.5555\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3399.1695 - mse: 3399.1697 - mae: 32.6735 - val_loss: 1461.7082 - val_mse: 1461.7080 - val_mae: 25.2949\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 733us/step - loss: 3404.4874 - mse: 3404.4863 - mae: 32.2209 - val_loss: 1461.4520 - val_mse: 1461.4520 - val_mae: 25.6885\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3421.9934 - mse: 3421.9929 - mae: 32.6852 - val_loss: 1461.4987 - val_mse: 1461.4988 - val_mae: 25.6309\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3230.3774 - mse: 3230.3772 - mae: 32.3824 - val_loss: 1460.7969 - val_mse: 1460.7968 - val_mae: 25.6580\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3175.8233 - mse: 3175.8235 - mae: 31.1519 - val_loss: 1460.8249 - val_mse: 1460.8248 - val_mae: 25.7258\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3343.1448 - mse: 3343.1462 - mae: 31.8128 - val_loss: 1463.0215 - val_mse: 1463.0216 - val_mae: 26.1286\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3266.6850 - mse: 3266.6853 - mae: 32.3228 - val_loss: 1467.0104 - val_mse: 1467.0105 - val_mae: 26.5715\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3465.1077 - mse: 3465.1077 - mae: 32.6454 - val_loss: 1463.2875 - val_mse: 1463.2875 - val_mae: 26.0779\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3262.5060 - mse: 3262.5054 - mae: 31.4188 - val_loss: 1463.1249 - val_mse: 1463.1250 - val_mae: 26.0548\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 534us/step - loss: 3316.2149 - mse: 3316.2151 - mae: 31.7732 - val_loss: 1463.4657 - val_mse: 1463.4657 - val_mae: 26.0016\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3238.7063 - mse: 3238.7061 - mae: 31.2198 - val_loss: 1464.6354 - val_mse: 1464.6354 - val_mae: 26.1820\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 527us/step - loss: 3277.1800 - mse: 3277.1807 - mae: 31.9549 - val_loss: 1468.0261 - val_mse: 1468.0261 - val_mae: 26.5700\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 516us/step - loss: 3365.8670 - mse: 3365.8665 - mae: 32.4559 - val_loss: 1463.5853 - val_mse: 1463.5851 - val_mae: 25.8268\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3315.2325 - mse: 3315.2322 - mae: 32.1304 - val_loss: 1463.3755 - val_mse: 1463.3755 - val_mae: 25.6926\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 530us/step - loss: 3252.1067 - mse: 3252.1077 - mae: 31.6382 - val_loss: 1465.7935 - val_mse: 1465.7938 - val_mae: 26.3508\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 502us/step - loss: 3396.5309 - mse: 3396.5310 - mae: 32.2010 - val_loss: 1463.7288 - val_mse: 1463.7288 - val_mae: 25.7211\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3308.1732 - mse: 3308.1738 - mae: 32.4328 - val_loss: 1465.7722 - val_mse: 1465.7721 - val_mae: 26.0648\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 544us/step - loss: 3275.0439 - mse: 3275.0435 - mae: 32.7193 - val_loss: 1465.9489 - val_mse: 1465.9490 - val_mae: 25.4917\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3367.3431 - mse: 3367.3435 - mae: 32.3856 - val_loss: 1467.8548 - val_mse: 1467.8547 - val_mae: 26.1626\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 508us/step - loss: 3139.4957 - mse: 3139.4954 - mae: 31.6257 - val_loss: 1469.0450 - val_mse: 1469.0449 - val_mae: 26.3037\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3334.0638 - mse: 3334.0642 - mae: 32.5924 - val_loss: 1467.0059 - val_mse: 1467.0060 - val_mae: 25.5673\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3329.2919 - mse: 3329.2930 - mae: 32.0131 - val_loss: 1468.1533 - val_mse: 1468.1532 - val_mae: 26.0532\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3350.0810 - mse: 3350.0806 - mae: 32.2247 - val_loss: 1469.9732 - val_mse: 1469.9733 - val_mae: 26.2603\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3347.2415 - mse: 3347.2415 - mae: 32.3625 - val_loss: 1469.3420 - val_mse: 1469.3419 - val_mae: 26.1294\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3416.0812 - mse: 3416.0813 - mae: 32.0955 - val_loss: 1468.1049 - val_mse: 1468.1047 - val_mae: 25.8809\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3291.3095 - mse: 3291.3098 - mae: 32.1133 - val_loss: 1468.2468 - val_mse: 1468.2468 - val_mae: 25.8524\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 515us/step - loss: 3297.5198 - mse: 3297.5200 - mae: 31.9412 - val_loss: 1472.3818 - val_mse: 1472.3818 - val_mae: 26.5410\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3399.0403 - mse: 3399.0396 - mae: 32.4860 - val_loss: 1468.3211 - val_mse: 1468.3210 - val_mae: 25.6953\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 660us/step - loss: 3194.4935 - mse: 3194.4946 - mae: 31.6550 - val_loss: 1470.8313 - val_mse: 1470.8314 - val_mae: 26.4111\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3361.3958 - mse: 3361.3960 - mae: 32.6170 - val_loss: 1468.1042 - val_mse: 1468.1042 - val_mae: 25.8630\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3266.1183 - mse: 3266.1182 - mae: 32.0688 - val_loss: 1467.7624 - val_mse: 1467.7626 - val_mae: 25.8296\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3286.9988 - mse: 3286.9990 - mae: 31.3372 - val_loss: 1468.4916 - val_mse: 1468.4917 - val_mae: 26.0631\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3238.7493 - mse: 3238.7502 - mae: 32.0302 - val_loss: 1468.2781 - val_mse: 1468.2781 - val_mae: 26.0279\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3330.4035 - mse: 3330.4041 - mae: 32.4092 - val_loss: 1467.9661 - val_mse: 1467.9661 - val_mae: 25.7857\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3291.2832 - mse: 3291.2837 - mae: 32.1754 - val_loss: 1468.6347 - val_mse: 1468.6346 - val_mae: 25.9437\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3329.9549 - mse: 3329.9548 - mae: 32.3761 - val_loss: 1468.7356 - val_mse: 1468.7356 - val_mae: 25.9152\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3212.0228 - mse: 3212.0217 - mae: 31.1408 - val_loss: 1469.7813 - val_mse: 1469.7812 - val_mae: 26.1913\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3252.6154 - mse: 3252.6155 - mae: 31.7009 - val_loss: 1467.8968 - val_mse: 1467.8969 - val_mae: 25.8337\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 456us/step - loss: 3208.0474 - mse: 3208.0471 - mae: 31.2075 - val_loss: 1468.5998 - val_mse: 1468.5997 - val_mae: 26.1558\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3337.6590 - mse: 3337.6592 - mae: 32.2169 - val_loss: 1468.1551 - val_mse: 1468.1550 - val_mae: 25.8869\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3308.8993 - mse: 3308.8989 - mae: 31.6689 - val_loss: 1468.8744 - val_mse: 1468.8744 - val_mae: 25.8404\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3379.4625 - mse: 3379.4626 - mae: 31.8648 - val_loss: 1468.2116 - val_mse: 1468.2115 - val_mae: 25.7626\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3259.6771 - mse: 3259.6770 - mae: 31.2909 - val_loss: 1471.9157 - val_mse: 1471.9158 - val_mae: 26.4704\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3342.5909 - mse: 3342.5906 - mae: 32.6505 - val_loss: 1468.9219 - val_mse: 1468.9220 - val_mae: 25.5953\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3272.2418 - mse: 3272.2424 - mae: 32.3936 - val_loss: 1469.9469 - val_mse: 1469.9469 - val_mae: 26.0967\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3320.8454 - mse: 3320.8455 - mae: 31.7061 - val_loss: 1470.2717 - val_mse: 1470.2717 - val_mae: 26.0545\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3358.2726 - mse: 3358.2722 - mae: 32.5883 - val_loss: 1469.8364 - val_mse: 1469.8364 - val_mae: 25.9193\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3348.9961 - mse: 3348.9963 - mae: 32.1661 - val_loss: 1470.8548 - val_mse: 1470.8546 - val_mae: 26.2066\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3235.8139 - mse: 3235.8135 - mae: 31.3902 - val_loss: 1471.6666 - val_mse: 1471.6665 - val_mae: 26.3604\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3214.7657 - mse: 3214.7659 - mae: 31.6180 - val_loss: 1473.1687 - val_mse: 1473.1689 - val_mae: 26.4907\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3325.4316 - mse: 3325.4314 - mae: 32.4844 - val_loss: 1469.9879 - val_mse: 1469.9878 - val_mae: 25.6694\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3217.0287 - mse: 3217.0291 - mae: 31.4096 - val_loss: 1469.5412 - val_mse: 1469.5413 - val_mae: 25.5588\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3213.7528 - mse: 3213.7529 - mae: 30.4273 - val_loss: 1472.9591 - val_mse: 1472.9591 - val_mae: 26.4278\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3238.7475 - mse: 3238.7478 - mae: 31.7780 - val_loss: 1470.7779 - val_mse: 1470.7780 - val_mae: 26.0982\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3267.4745 - mse: 3267.4746 - mae: 31.6139 - val_loss: 1473.1205 - val_mse: 1473.1206 - val_mae: 26.4471\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 517us/step - loss: 3325.5450 - mse: 3325.5449 - mae: 32.0623 - val_loss: 1471.3383 - val_mse: 1471.3383 - val_mae: 26.1528\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3304.2819 - mse: 3304.2815 - mae: 31.6461 - val_loss: 1470.3274 - val_mse: 1470.3274 - val_mae: 25.9997\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 528us/step - loss: 3289.3783 - mse: 3289.3779 - mae: 31.9984 - val_loss: 1472.4429 - val_mse: 1472.4429 - val_mae: 26.3198\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3305.5309 - mse: 3305.5317 - mae: 31.3778 - val_loss: 1472.2878 - val_mse: 1472.2877 - val_mae: 26.3085\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3231.0658 - mse: 3231.0652 - mae: 31.4103 - val_loss: 1472.3569 - val_mse: 1472.3568 - val_mae: 26.1920\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3232.5934 - mse: 3232.5938 - mae: 32.2320 - val_loss: 1471.6361 - val_mse: 1471.6360 - val_mae: 25.7485\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3269.3996 - mse: 3269.3999 - mae: 31.5479 - val_loss: 1472.1024 - val_mse: 1472.1024 - val_mae: 26.0153\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 417us/step - loss: 2919.1773 - mse: 2919.1765 - mae: 31.1979 - val_loss: 1075.7803 - val_mse: 1075.7803 - val_mae: 24.0170\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2872.5143 - mse: 2872.5146 - mae: 30.5614 - val_loss: 1076.1830 - val_mse: 1076.1830 - val_mae: 23.8555\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2864.4235 - mse: 2864.4236 - mae: 31.1831 - val_loss: 1075.5072 - val_mse: 1075.5073 - val_mae: 23.7860\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2940.8807 - mse: 2940.8809 - mae: 30.6820 - val_loss: 1071.6749 - val_mse: 1071.6749 - val_mae: 24.1110\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2949.2162 - mse: 2949.2158 - mae: 31.5694 - val_loss: 1075.7742 - val_mse: 1075.7743 - val_mae: 23.5988\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2935.8153 - mse: 2935.8152 - mae: 30.9046 - val_loss: 1072.4973 - val_mse: 1072.4972 - val_mae: 23.7610\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 649us/step - loss: 2970.9113 - mse: 2970.9116 - mae: 31.1516 - val_loss: 1070.1808 - val_mse: 1070.1809 - val_mae: 23.9056\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 523us/step - loss: 2827.3039 - mse: 2827.3040 - mae: 30.3649 - val_loss: 1072.2498 - val_mse: 1072.2499 - val_mae: 23.6667\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2955.7464 - mse: 2955.7461 - mae: 30.9708 - val_loss: 1069.3331 - val_mse: 1069.3331 - val_mae: 23.7912\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2896.4306 - mse: 2896.4304 - mae: 31.1049 - val_loss: 1068.5701 - val_mse: 1068.5702 - val_mae: 23.9109\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2930.1873 - mse: 2930.1875 - mae: 30.8925 - val_loss: 1067.6850 - val_mse: 1067.6848 - val_mae: 23.9784\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2955.1392 - mse: 2955.1394 - mae: 31.0420 - val_loss: 1069.6016 - val_mse: 1069.6016 - val_mae: 23.7577\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2884.0833 - mse: 2884.0828 - mae: 30.8410 - val_loss: 1069.4008 - val_mse: 1069.4008 - val_mae: 23.7569\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2873.7883 - mse: 2873.7881 - mae: 31.1475 - val_loss: 1070.5963 - val_mse: 1070.5964 - val_mae: 23.6400\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2826.0771 - mse: 2826.0781 - mae: 30.0296 - val_loss: 1066.2148 - val_mse: 1066.2147 - val_mae: 24.4039\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 2873.7254 - mse: 2873.7253 - mae: 31.0303 - val_loss: 1065.0209 - val_mse: 1065.0209 - val_mae: 24.2476\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2935.5698 - mse: 2935.5688 - mae: 31.1674 - val_loss: 1069.1291 - val_mse: 1069.1292 - val_mae: 23.6153\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 522us/step - loss: 2829.3175 - mse: 2829.3191 - mae: 30.6911 - val_loss: 1064.6763 - val_mse: 1064.6765 - val_mae: 23.9812\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 554us/step - loss: 2940.0046 - mse: 2940.0049 - mae: 31.1525 - val_loss: 1065.4043 - val_mse: 1065.4043 - val_mae: 23.8079\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2860.6832 - mse: 2860.6843 - mae: 30.5458 - val_loss: 1063.3828 - val_mse: 1063.3829 - val_mae: 24.0193\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2927.7360 - mse: 2927.7358 - mae: 31.1318 - val_loss: 1064.4419 - val_mse: 1064.4420 - val_mae: 23.7806\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 634us/step - loss: 2926.8287 - mse: 2926.8291 - mae: 31.0869 - val_loss: 1066.6054 - val_mse: 1066.6053 - val_mae: 23.5571\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2896.3182 - mse: 2896.3191 - mae: 30.7397 - val_loss: 1065.5485 - val_mse: 1065.5483 - val_mae: 23.6353\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2891.2923 - mse: 2891.2927 - mae: 30.1694 - val_loss: 1065.2402 - val_mse: 1065.2401 - val_mae: 23.6902\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2930.7128 - mse: 2930.7122 - mae: 31.5165 - val_loss: 1062.8073 - val_mse: 1062.8074 - val_mae: 23.9003\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2915.3975 - mse: 2915.3970 - mae: 31.0341 - val_loss: 1063.5681 - val_mse: 1063.5681 - val_mae: 23.7492\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2891.5382 - mse: 2891.5376 - mae: 31.1790 - val_loss: 1061.4330 - val_mse: 1061.4331 - val_mae: 24.0755\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2980.8152 - mse: 2980.8147 - mae: 31.0843 - val_loss: 1061.0876 - val_mse: 1061.0875 - val_mae: 23.9972\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2835.0397 - mse: 2835.0391 - mae: 30.6325 - val_loss: 1060.2637 - val_mse: 1060.2638 - val_mae: 24.1417\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2906.9393 - mse: 2906.9387 - mae: 31.1412 - val_loss: 1060.5400 - val_mse: 1060.5399 - val_mae: 24.0130\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2914.7533 - mse: 2914.7537 - mae: 30.9880 - val_loss: 1068.4158 - val_mse: 1068.4156 - val_mae: 23.3410\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2823.3199 - mse: 2823.3203 - mae: 30.1036 - val_loss: 1060.7922 - val_mse: 1060.7922 - val_mae: 23.7972\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2807.3580 - mse: 2807.3577 - mae: 30.2220 - val_loss: 1058.3151 - val_mse: 1058.3152 - val_mae: 24.1877\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 491us/step - loss: 2863.1669 - mse: 2863.1672 - mae: 30.7573 - val_loss: 1061.6013 - val_mse: 1061.6013 - val_mae: 23.6184\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 468us/step - loss: 3019.7123 - mse: 3019.7126 - mae: 31.5777 - val_loss: 1060.7110 - val_mse: 1060.7111 - val_mae: 23.6830\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 2964.6491 - mse: 2964.6492 - mae: 30.9915 - val_loss: 1058.9260 - val_mse: 1058.9260 - val_mae: 23.8125\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2905.1830 - mse: 2905.1838 - mae: 30.7630 - val_loss: 1057.5954 - val_mse: 1057.5955 - val_mae: 23.8176\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2939.2723 - mse: 2939.2720 - mae: 30.9583 - val_loss: 1058.2568 - val_mse: 1058.2566 - val_mae: 23.6457\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2858.9114 - mse: 2858.9109 - mae: 30.6742 - val_loss: 1056.4847 - val_mse: 1056.4847 - val_mae: 23.7378\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2864.7236 - mse: 2864.7227 - mae: 30.9904 - val_loss: 1054.4965 - val_mse: 1054.4963 - val_mae: 24.1042\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2889.8749 - mse: 2889.8750 - mae: 30.7455 - val_loss: 1054.7350 - val_mse: 1054.7351 - val_mae: 23.8911\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2940.1769 - mse: 2940.1765 - mae: 31.1457 - val_loss: 1054.2513 - val_mse: 1054.2512 - val_mae: 23.8057\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2789.5946 - mse: 2789.5947 - mae: 30.1333 - val_loss: 1054.6732 - val_mse: 1054.6732 - val_mae: 23.8231\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2961.0968 - mse: 2961.0964 - mae: 30.7787 - val_loss: 1054.5786 - val_mse: 1054.5786 - val_mae: 23.6525\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2911.5984 - mse: 2911.5979 - mae: 30.6641 - val_loss: 1055.4763 - val_mse: 1055.4763 - val_mae: 23.5543\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2903.4826 - mse: 2903.4834 - mae: 30.4335 - val_loss: 1056.2172 - val_mse: 1056.2172 - val_mae: 23.4491\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2798.7170 - mse: 2798.7163 - mae: 30.3169 - val_loss: 1051.1170 - val_mse: 1051.1169 - val_mae: 24.0244\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2880.5251 - mse: 2880.5254 - mae: 30.2676 - val_loss: 1050.6368 - val_mse: 1050.6368 - val_mae: 23.8259\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2786.1808 - mse: 2786.1807 - mae: 29.9146 - val_loss: 1048.7317 - val_mse: 1048.7317 - val_mae: 24.0425\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2830.8422 - mse: 2830.8418 - mae: 30.1171 - val_loss: 1049.0348 - val_mse: 1049.0347 - val_mae: 23.8420\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2847.2473 - mse: 2847.2476 - mae: 29.8967 - val_loss: 1051.3413 - val_mse: 1051.3412 - val_mae: 23.5669\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2882.0979 - mse: 2882.0986 - mae: 30.6541 - val_loss: 1049.1305 - val_mse: 1049.1305 - val_mae: 24.1628\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2948.3332 - mse: 2948.3330 - mae: 31.3768 - val_loss: 1051.1161 - val_mse: 1051.1162 - val_mae: 23.6238\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2829.3453 - mse: 2829.3452 - mae: 30.3706 - val_loss: 1047.8444 - val_mse: 1047.8444 - val_mae: 23.9158\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2843.4658 - mse: 2843.4656 - mae: 30.5815 - val_loss: 1046.7159 - val_mse: 1046.7158 - val_mae: 24.2166\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 451us/step - loss: 2936.0739 - mse: 2936.0742 - mae: 30.6176 - val_loss: 1046.3920 - val_mse: 1046.3920 - val_mae: 23.8846\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2813.9026 - mse: 2813.9026 - mae: 30.2812 - val_loss: 1046.5687 - val_mse: 1046.5686 - val_mae: 23.7065\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 649us/step - loss: 2894.1652 - mse: 2894.1655 - mae: 30.6440 - val_loss: 1044.0477 - val_mse: 1044.0476 - val_mae: 24.0622\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2878.4803 - mse: 2878.4805 - mae: 30.7187 - val_loss: 1043.2632 - val_mse: 1043.2632 - val_mae: 23.8219\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2854.8719 - mse: 2854.8713 - mae: 30.4840 - val_loss: 1044.4301 - val_mse: 1044.4301 - val_mae: 23.6096\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 550us/step - loss: 2829.4853 - mse: 2829.4856 - mae: 29.9946 - val_loss: 1042.0257 - val_mse: 1042.0258 - val_mae: 23.7633\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2824.8555 - mse: 2824.8562 - mae: 30.7427 - val_loss: 1039.6191 - val_mse: 1039.6191 - val_mae: 24.0737\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2814.3522 - mse: 2814.3521 - mae: 30.2063 - val_loss: 1041.2432 - val_mse: 1041.2432 - val_mae: 23.6609\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 512us/step - loss: 2924.8530 - mse: 2924.8530 - mae: 30.9122 - val_loss: 1041.7371 - val_mse: 1041.7372 - val_mae: 23.6524\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2937.4534 - mse: 2937.4539 - mae: 30.7647 - val_loss: 1041.5342 - val_mse: 1041.5343 - val_mae: 23.7077\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2838.2362 - mse: 2838.2363 - mae: 30.1498 - val_loss: 1038.1680 - val_mse: 1038.1678 - val_mae: 24.1194\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2752.5771 - mse: 2752.5771 - mae: 29.7948 - val_loss: 1038.3464 - val_mse: 1038.3464 - val_mae: 24.3963\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2882.0890 - mse: 2882.0896 - mae: 30.6030 - val_loss: 1039.2176 - val_mse: 1039.2175 - val_mae: 23.6494\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 512us/step - loss: 2784.1348 - mse: 2784.1350 - mae: 30.3537 - val_loss: 1038.6467 - val_mse: 1038.6467 - val_mae: 23.6746\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2840.2790 - mse: 2840.2793 - mae: 30.5261 - val_loss: 1036.4894 - val_mse: 1036.4894 - val_mae: 23.7747\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2852.7455 - mse: 2852.7458 - mae: 30.7381 - val_loss: 1034.9547 - val_mse: 1034.9547 - val_mae: 23.7258\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 659us/step - loss: 2820.8866 - mse: 2820.8865 - mae: 30.4043 - val_loss: 1038.9934 - val_mse: 1038.9934 - val_mae: 23.3132\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2795.2636 - mse: 2795.2642 - mae: 30.6428 - val_loss: 1035.5249 - val_mse: 1035.5249 - val_mae: 23.5455\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2799.2957 - mse: 2799.2954 - mae: 30.0765 - val_loss: 1033.8244 - val_mse: 1033.8245 - val_mae: 23.5950\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2796.1010 - mse: 2796.1013 - mae: 30.1858 - val_loss: 1033.8018 - val_mse: 1033.8018 - val_mae: 23.5442\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2803.1211 - mse: 2803.1204 - mae: 29.9884 - val_loss: 1035.9147 - val_mse: 1035.9146 - val_mae: 23.2724\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2813.2074 - mse: 2813.2083 - mae: 29.9298 - val_loss: 1030.8704 - val_mse: 1030.8706 - val_mae: 23.5707\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2798.0527 - mse: 2798.0525 - mae: 30.4526 - val_loss: 1028.7288 - val_mse: 1028.7288 - val_mae: 23.7093\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 499us/step - loss: 2780.4000 - mse: 2780.4001 - mae: 29.7842 - val_loss: 1033.3105 - val_mse: 1033.3104 - val_mae: 23.3640\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 543us/step - loss: 2830.1048 - mse: 2830.1052 - mae: 30.1948 - val_loss: 1028.2495 - val_mse: 1028.2494 - val_mae: 23.8497\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2560.1005 - mse: 2560.1001 - mae: 29.6264 - val_loss: 1509.1040 - val_mse: 1509.1041 - val_mae: 27.4631\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2494.7143 - mse: 2494.7144 - mae: 29.3089 - val_loss: 1513.2439 - val_mse: 1513.2438 - val_mae: 27.2305\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2575.1142 - mse: 2575.1128 - mae: 29.8650 - val_loss: 1509.1915 - val_mse: 1509.1915 - val_mae: 27.3061\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2493.5851 - mse: 2493.5852 - mae: 29.3810 - val_loss: 1516.1548 - val_mse: 1516.1552 - val_mae: 27.0805\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2562.2810 - mse: 2562.2812 - mae: 29.4273 - val_loss: 1506.2040 - val_mse: 1506.2040 - val_mae: 27.3523\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2578.3134 - mse: 2578.3140 - mae: 29.7324 - val_loss: 1515.0180 - val_mse: 1515.0181 - val_mae: 27.0353\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 657us/step - loss: 2542.2090 - mse: 2542.2090 - mae: 29.7481 - val_loss: 1502.5375 - val_mse: 1502.5375 - val_mae: 27.3285\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 561us/step - loss: 2493.4691 - mse: 2493.4695 - mae: 29.5878 - val_loss: 1497.8908 - val_mse: 1497.8909 - val_mae: 27.3967\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 669us/step - loss: 2495.2099 - mse: 2495.2102 - mae: 29.1920 - val_loss: 1504.6660 - val_mse: 1504.6660 - val_mae: 27.1210\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2464.7987 - mse: 2464.7998 - mae: 29.7411 - val_loss: 1512.3155 - val_mse: 1512.3154 - val_mae: 26.8570\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2567.5804 - mse: 2567.5806 - mae: 29.6923 - val_loss: 1505.4170 - val_mse: 1505.4170 - val_mae: 27.0285\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2532.7044 - mse: 2532.7048 - mae: 29.7167 - val_loss: 1503.0456 - val_mse: 1503.0455 - val_mae: 27.0527\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2536.2561 - mse: 2536.2556 - mae: 29.5241 - val_loss: 1505.7663 - val_mse: 1505.7660 - val_mae: 26.9164\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2529.3664 - mse: 2529.3660 - mae: 29.5320 - val_loss: 1500.0040 - val_mse: 1500.0040 - val_mae: 27.0102\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2500.6436 - mse: 2500.6438 - mae: 29.4416 - val_loss: 1497.6674 - val_mse: 1497.6675 - val_mae: 27.0561\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2516.0429 - mse: 2516.0425 - mae: 29.4715 - val_loss: 1499.3616 - val_mse: 1499.3613 - val_mae: 26.9685\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2507.7593 - mse: 2507.7595 - mae: 29.1709 - val_loss: 1494.7180 - val_mse: 1494.7178 - val_mae: 27.1065\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2541.4484 - mse: 2541.4487 - mae: 29.3126 - val_loss: 1499.6715 - val_mse: 1499.6714 - val_mae: 26.9115\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2507.7945 - mse: 2507.7942 - mae: 29.1396 - val_loss: 1492.9212 - val_mse: 1492.9210 - val_mae: 27.0690\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 549us/step - loss: 2536.5424 - mse: 2536.5425 - mae: 30.0088 - val_loss: 1504.4244 - val_mse: 1504.4243 - val_mae: 26.7269\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2482.8867 - mse: 2482.8879 - mae: 29.2582 - val_loss: 1494.4719 - val_mse: 1494.4720 - val_mae: 26.9344\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2495.7806 - mse: 2495.7800 - mae: 29.3793 - val_loss: 1488.3366 - val_mse: 1488.3364 - val_mae: 27.1156\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2518.0808 - mse: 2518.0803 - mae: 29.5125 - val_loss: 1502.2495 - val_mse: 1502.2494 - val_mae: 26.7108\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2505.4334 - mse: 2505.4338 - mae: 29.6105 - val_loss: 1485.1337 - val_mse: 1485.1337 - val_mae: 27.1944\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 534us/step - loss: 2497.7844 - mse: 2497.7847 - mae: 29.5073 - val_loss: 1489.4394 - val_mse: 1489.4391 - val_mae: 27.0186\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2520.1791 - mse: 2520.1792 - mae: 29.4123 - val_loss: 1493.9715 - val_mse: 1493.9716 - val_mae: 26.8790\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2472.8758 - mse: 2472.8755 - mae: 28.9831 - val_loss: 1484.5449 - val_mse: 1484.5450 - val_mae: 27.1124\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 498us/step - loss: 2483.5144 - mse: 2483.5142 - mae: 29.5026 - val_loss: 1489.4902 - val_mse: 1489.4901 - val_mae: 26.9056\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2489.2426 - mse: 2489.2432 - mae: 29.1184 - val_loss: 1493.4931 - val_mse: 1493.4932 - val_mae: 26.7552\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2451.5204 - mse: 2451.5198 - mae: 28.6697 - val_loss: 1483.1596 - val_mse: 1483.1598 - val_mae: 27.0184\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 536us/step - loss: 2437.6007 - mse: 2437.6011 - mae: 29.0052 - val_loss: 1492.3852 - val_mse: 1492.3853 - val_mae: 26.7431\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 517us/step - loss: 2447.6167 - mse: 2447.6172 - mae: 28.5133 - val_loss: 1494.5021 - val_mse: 1494.5021 - val_mae: 26.7165\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2465.9966 - mse: 2465.9961 - mae: 29.1394 - val_loss: 1481.0514 - val_mse: 1481.0514 - val_mae: 27.0110\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2484.6818 - mse: 2484.6821 - mae: 29.2036 - val_loss: 1476.6910 - val_mse: 1476.6910 - val_mae: 27.1387\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2505.6959 - mse: 2505.6953 - mae: 29.2540 - val_loss: 1486.3212 - val_mse: 1486.3212 - val_mae: 26.8416\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2477.4560 - mse: 2477.4558 - mae: 28.7758 - val_loss: 1477.3278 - val_mse: 1477.3279 - val_mae: 27.1311\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2457.7191 - mse: 2457.7197 - mae: 29.1997 - val_loss: 1484.8159 - val_mse: 1484.8159 - val_mae: 26.8864\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 554us/step - loss: 2501.4375 - mse: 2501.4377 - mae: 29.6617 - val_loss: 1479.7767 - val_mse: 1479.7767 - val_mae: 27.0146\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2543.6433 - mse: 2543.6433 - mae: 29.4617 - val_loss: 1501.0767 - val_mse: 1501.0768 - val_mae: 26.4643\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2501.0103 - mse: 2501.0107 - mae: 29.0674 - val_loss: 1475.7071 - val_mse: 1475.7072 - val_mae: 27.0869\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2533.2324 - mse: 2533.2329 - mae: 29.4916 - val_loss: 1491.1465 - val_mse: 1491.1465 - val_mae: 26.6174\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2500.6820 - mse: 2500.6812 - mae: 29.2462 - val_loss: 1484.5457 - val_mse: 1484.5458 - val_mae: 26.7557\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2468.0618 - mse: 2468.0615 - mae: 29.0339 - val_loss: 1477.6821 - val_mse: 1477.6821 - val_mae: 26.9452\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 549us/step - loss: 2425.2339 - mse: 2425.2334 - mae: 28.7396 - val_loss: 1472.6416 - val_mse: 1472.6416 - val_mae: 27.0532\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2499.8398 - mse: 2499.8403 - mae: 29.4585 - val_loss: 1479.1652 - val_mse: 1479.1653 - val_mae: 26.8688\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2562.0187 - mse: 2562.0193 - mae: 29.3043 - val_loss: 1477.3400 - val_mse: 1477.3400 - val_mae: 26.9413\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - ETA: 0s - loss: 2548.2300 - mse: 2548.2295 - mae: 29.50 - 1s 596us/step - loss: 2511.0909 - mse: 2511.0906 - mae: 29.2901 - val_loss: 1481.4651 - val_mse: 1481.4651 - val_mae: 26.7336\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2493.7365 - mse: 2493.7368 - mae: 29.3342 - val_loss: 1473.7496 - val_mse: 1473.7498 - val_mae: 26.9626\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2492.7936 - mse: 2492.7939 - mae: 29.0331 - val_loss: 1475.3575 - val_mse: 1475.3575 - val_mae: 26.9219\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2468.4809 - mse: 2468.4802 - mae: 28.8394 - val_loss: 1482.5597 - val_mse: 1482.5598 - val_mae: 26.7073\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2502.2687 - mse: 2502.2681 - mae: 29.2621 - val_loss: 1482.0800 - val_mse: 1482.0800 - val_mae: 26.7414\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2492.1792 - mse: 2492.1787 - mae: 29.1057 - val_loss: 1479.9568 - val_mse: 1479.9567 - val_mae: 26.8490\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2549.4465 - mse: 2549.4460 - mae: 29.5133 - val_loss: 1479.0396 - val_mse: 1479.0396 - val_mae: 26.8125\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2487.1335 - mse: 2487.1328 - mae: 29.4720 - val_loss: 1481.4485 - val_mse: 1481.4486 - val_mae: 26.7252\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2491.5234 - mse: 2491.5237 - mae: 29.3693 - val_loss: 1485.2672 - val_mse: 1485.2672 - val_mae: 26.6530\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2479.0809 - mse: 2479.0811 - mae: 29.2840 - val_loss: 1477.3799 - val_mse: 1477.3800 - val_mae: 26.8571\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 552us/step - loss: 2475.8185 - mse: 2475.8174 - mae: 29.2356 - val_loss: 1476.2656 - val_mse: 1476.2657 - val_mae: 26.8933\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2492.5083 - mse: 2492.5093 - mae: 29.4555 - val_loss: 1479.1951 - val_mse: 1479.1952 - val_mae: 26.7742\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2553.0459 - mse: 2553.0461 - mae: 29.2844 - val_loss: 1487.6347 - val_mse: 1487.6348 - val_mae: 26.5491\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2482.1772 - mse: 2482.1770 - mae: 29.1024 - val_loss: 1471.8986 - val_mse: 1471.8986 - val_mae: 26.9802\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 561us/step - loss: 2471.0073 - mse: 2471.0076 - mae: 29.1195 - val_loss: 1475.4877 - val_mse: 1475.4879 - val_mae: 26.8978\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2539.8926 - mse: 2539.8936 - mae: 29.2264 - val_loss: 1467.9931 - val_mse: 1467.9929 - val_mae: 27.2051\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2421.1994 - mse: 2421.2002 - mae: 29.0764 - val_loss: 1483.2458 - val_mse: 1483.2458 - val_mae: 26.7128\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2473.2429 - mse: 2473.2437 - mae: 29.3598 - val_loss: 1475.9561 - val_mse: 1475.9561 - val_mae: 26.9217\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 500us/step - loss: 2414.2869 - mse: 2414.2866 - mae: 28.7312 - val_loss: 1477.3366 - val_mse: 1477.3365 - val_mae: 26.9160\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2538.6344 - mse: 2538.6345 - mae: 29.1878 - val_loss: 1474.2288 - val_mse: 1474.2289 - val_mae: 27.0180\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2500.0912 - mse: 2500.0903 - mae: 29.0705 - val_loss: 1480.5981 - val_mse: 1480.5983 - val_mae: 26.7889\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 538us/step - loss: 2493.6914 - mse: 2493.6917 - mae: 29.3126 - val_loss: 1480.6067 - val_mse: 1480.6067 - val_mae: 26.7232\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2436.1587 - mse: 2436.1589 - mae: 28.6885 - val_loss: 1474.2101 - val_mse: 1474.2102 - val_mae: 26.8688\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2522.9744 - mse: 2522.9741 - mae: 29.7794 - val_loss: 1475.9633 - val_mse: 1475.9631 - val_mae: 26.7678\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2471.5686 - mse: 2471.5684 - mae: 29.0011 - val_loss: 1478.6251 - val_mse: 1478.6250 - val_mae: 26.6473\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 530us/step - loss: 2459.1490 - mse: 2459.1499 - mae: 28.9842 - val_loss: 1471.4043 - val_mse: 1471.4044 - val_mae: 26.8649\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2475.5196 - mse: 2475.5198 - mae: 28.8826 - val_loss: 1476.5996 - val_mse: 1476.5994 - val_mae: 26.7053\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2521.6290 - mse: 2521.6289 - mae: 29.2192 - val_loss: 1471.4066 - val_mse: 1471.4066 - val_mae: 26.8600\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2470.5646 - mse: 2470.5645 - mae: 28.9015 - val_loss: 1480.0846 - val_mse: 1480.0845 - val_mae: 26.6593\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 550us/step - loss: 2454.6562 - mse: 2454.6560 - mae: 29.0992 - val_loss: 1467.0117 - val_mse: 1467.0116 - val_mae: 27.0493\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2514.0221 - mse: 2514.0220 - mae: 29.9389 - val_loss: 1474.8311 - val_mse: 1474.8313 - val_mae: 26.7874\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2475.8806 - mse: 2475.8806 - mae: 29.0622 - val_loss: 1471.7768 - val_mse: 1471.7767 - val_mae: 26.9050\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2510.8198 - mse: 2510.8201 - mae: 29.5988 - val_loss: 1477.1491 - val_mse: 1477.1490 - val_mae: 26.6823\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2479.0877 - mse: 2479.0879 - mae: 28.6841 - val_loss: 1476.7174 - val_mse: 1476.7173 - val_mae: 26.6818\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2385.7794 - mse: 2385.7786 - mae: 29.2308 - val_loss: 3703.0169 - val_mse: 3703.0161 - val_mae: 23.8466\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2338.7148 - mse: 2338.7146 - mae: 29.6290 - val_loss: 3703.7506 - val_mse: 3703.7502 - val_mae: 23.6699\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2383.2463 - mse: 2383.2468 - mae: 29.1873 - val_loss: 3708.2218 - val_mse: 3708.2227 - val_mae: 24.7151\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2396.7954 - mse: 2396.7961 - mae: 29.7621 - val_loss: 3706.9263 - val_mse: 3706.9263 - val_mae: 23.8178\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2371.5030 - mse: 2371.5029 - mae: 29.6226 - val_loss: 3707.7662 - val_mse: 3707.7671 - val_mae: 23.6170\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2349.2071 - mse: 2349.2075 - mae: 29.3338 - val_loss: 3708.1312 - val_mse: 3708.1309 - val_mae: 23.9466\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2420.8725 - mse: 2420.8721 - mae: 29.2608 - val_loss: 3708.8072 - val_mse: 3708.8071 - val_mae: 24.4557\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 536us/step - loss: 2373.2254 - mse: 2373.2253 - mae: 29.7847 - val_loss: 3708.2969 - val_mse: 3708.2969 - val_mae: 23.8171\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2363.0963 - mse: 2363.0955 - mae: 29.7581 - val_loss: 3708.0462 - val_mse: 3708.0461 - val_mae: 24.1883\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2300.8527 - mse: 2300.8523 - mae: 28.9067 - val_loss: 3708.1347 - val_mse: 3708.1338 - val_mae: 24.0228\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2406.5953 - mse: 2406.5950 - mae: 29.7905 - val_loss: 3708.2929 - val_mse: 3708.2927 - val_mae: 23.8138\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2379.7472 - mse: 2379.7468 - mae: 29.0652 - val_loss: 3710.0179 - val_mse: 3710.0178 - val_mae: 23.9676\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2363.3257 - mse: 2363.3259 - mae: 29.3318 - val_loss: 3709.8442 - val_mse: 3709.8438 - val_mae: 24.2155\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2339.2743 - mse: 2339.2734 - mae: 29.2986 - val_loss: 3708.7164 - val_mse: 3708.7163 - val_mae: 23.9775\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 645us/step - loss: 2355.8675 - mse: 2355.8679 - mae: 28.9556 - val_loss: 3708.3383 - val_mse: 3708.3384 - val_mae: 24.2594\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2383.2305 - mse: 2383.2307 - mae: 29.4554 - val_loss: 3707.4593 - val_mse: 3707.4592 - val_mae: 23.7542\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2365.7608 - mse: 2365.7603 - mae: 29.3430 - val_loss: 3706.5739 - val_mse: 3706.5740 - val_mae: 23.8623\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2404.9919 - mse: 2404.9917 - mae: 29.4102 - val_loss: 3706.3583 - val_mse: 3706.3584 - val_mae: 23.7128\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2356.9501 - mse: 2356.9507 - mae: 29.4035 - val_loss: 3708.2986 - val_mse: 3708.2986 - val_mae: 24.3774\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2327.8959 - mse: 2327.8965 - mae: 29.0444 - val_loss: 3710.0387 - val_mse: 3710.0378 - val_mae: 24.4399\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2361.4109 - mse: 2361.4104 - mae: 29.5652 - val_loss: 3708.8974 - val_mse: 3708.8977 - val_mae: 23.6774\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2399.6295 - mse: 2399.6296 - mae: 29.5636 - val_loss: 3708.4997 - val_mse: 3708.4993 - val_mae: 23.9004\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2335.1403 - mse: 2335.1404 - mae: 29.3681 - val_loss: 3709.6077 - val_mse: 3709.6074 - val_mae: 23.9769\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2423.3230 - mse: 2423.3232 - mae: 29.3590 - val_loss: 3710.8977 - val_mse: 3710.8987 - val_mae: 24.1206\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2352.7059 - mse: 2352.7053 - mae: 28.8744 - val_loss: 3713.6871 - val_mse: 3713.6875 - val_mae: 24.5676\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2339.2797 - mse: 2339.2798 - mae: 29.1981 - val_loss: 3712.3517 - val_mse: 3712.3521 - val_mae: 24.2623\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2362.4204 - mse: 2362.4207 - mae: 29.7662 - val_loss: 3712.7717 - val_mse: 3712.7717 - val_mae: 24.0881\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2328.5035 - mse: 2328.5032 - mae: 29.1194 - val_loss: 3712.9112 - val_mse: 3712.9114 - val_mae: 24.0010\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2349.8921 - mse: 2349.8928 - mae: 29.3627 - val_loss: 3715.8552 - val_mse: 3715.8557 - val_mae: 24.7194\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2361.6292 - mse: 2361.6296 - mae: 29.2698 - val_loss: 3714.7627 - val_mse: 3714.7634 - val_mae: 24.5675\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2348.0044 - mse: 2348.0039 - mae: 29.4013 - val_loss: 3711.5230 - val_mse: 3711.5234 - val_mae: 24.1158\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2343.6171 - mse: 2343.6169 - mae: 29.0868 - val_loss: 3710.0280 - val_mse: 3710.0281 - val_mae: 23.9194\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2338.3676 - mse: 2338.3677 - mae: 29.3348 - val_loss: 3711.8133 - val_mse: 3711.8130 - val_mae: 24.6071\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2369.7846 - mse: 2369.7852 - mae: 29.6078 - val_loss: 3707.8354 - val_mse: 3707.8354 - val_mae: 24.1412\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 540us/step - loss: 2319.4715 - mse: 2319.4712 - mae: 29.0787 - val_loss: 3707.0770 - val_mse: 3707.0764 - val_mae: 23.8946\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2334.2876 - mse: 2334.2871 - mae: 29.1866 - val_loss: 3709.2377 - val_mse: 3709.2380 - val_mae: 24.4357\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2329.2763 - mse: 2329.2761 - mae: 29.0345 - val_loss: 3708.5382 - val_mse: 3708.5378 - val_mae: 24.3995\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2351.5356 - mse: 2351.5356 - mae: 29.3940 - val_loss: 3706.7720 - val_mse: 3706.7720 - val_mae: 24.1212\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2322.9991 - mse: 2322.9988 - mae: 29.1018 - val_loss: 3708.4234 - val_mse: 3708.4226 - val_mae: 23.8236\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2290.6403 - mse: 2290.6404 - mae: 28.6093 - val_loss: 3709.8361 - val_mse: 3709.8369 - val_mae: 24.3685\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2383.5324 - mse: 2383.5317 - mae: 29.5711 - val_loss: 3707.9994 - val_mse: 3707.9988 - val_mae: 23.9782\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2332.9206 - mse: 2332.9211 - mae: 28.9641 - val_loss: 3708.3958 - val_mse: 3708.3962 - val_mae: 24.3390\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2353.5711 - mse: 2353.5718 - mae: 29.4060 - val_loss: 3711.8844 - val_mse: 3711.8843 - val_mae: 24.7911\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2377.0385 - mse: 2377.0378 - mae: 29.2810 - val_loss: 3709.9337 - val_mse: 3709.9338 - val_mae: 24.2491\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2339.1436 - mse: 2339.1436 - mae: 29.2670 - val_loss: 3708.0838 - val_mse: 3708.0837 - val_mae: 23.7837\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2342.1899 - mse: 2342.1897 - mae: 28.8776 - val_loss: 3708.3322 - val_mse: 3708.3320 - val_mae: 23.9555\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2308.2929 - mse: 2308.2925 - mae: 28.9042 - val_loss: 3710.3054 - val_mse: 3710.3052 - val_mae: 24.5337\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2367.9793 - mse: 2367.9792 - mae: 29.4848 - val_loss: 3707.6131 - val_mse: 3707.6147 - val_mae: 24.1192\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2321.6352 - mse: 2321.6355 - mae: 29.0404 - val_loss: 3707.0251 - val_mse: 3707.0259 - val_mae: 23.5710\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2316.1719 - mse: 2316.1721 - mae: 29.1321 - val_loss: 3708.6411 - val_mse: 3708.6411 - val_mae: 24.0139\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2363.3505 - mse: 2363.3508 - mae: 29.3989 - val_loss: 3707.4131 - val_mse: 3707.4116 - val_mae: 23.7962\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2298.8788 - mse: 2298.8789 - mae: 29.2435 - val_loss: 3708.7280 - val_mse: 3708.7280 - val_mae: 24.1085\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2322.6293 - mse: 2322.6296 - mae: 28.9021 - val_loss: 3707.9408 - val_mse: 3707.9404 - val_mae: 24.0472\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2366.9074 - mse: 2366.9070 - mae: 29.1537 - val_loss: 3708.9963 - val_mse: 3708.9961 - val_mae: 24.0884\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2306.9621 - mse: 2306.9614 - mae: 29.1434 - val_loss: 3707.1971 - val_mse: 3707.1975 - val_mae: 23.9696\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2332.4794 - mse: 2332.4795 - mae: 29.1024 - val_loss: 3707.4519 - val_mse: 3707.4521 - val_mae: 24.0347\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2346.9746 - mse: 2346.9758 - mae: 29.5338 - val_loss: 3708.8579 - val_mse: 3708.8582 - val_mae: 23.9163\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2308.9017 - mse: 2308.9014 - mae: 29.0677 - val_loss: 3709.3711 - val_mse: 3709.3716 - val_mae: 24.1099\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2307.7611 - mse: 2307.7612 - mae: 29.0228 - val_loss: 3709.3489 - val_mse: 3709.3486 - val_mae: 24.6055\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2322.2171 - mse: 2322.2170 - mae: 29.0919 - val_loss: 3706.8641 - val_mse: 3706.8638 - val_mae: 23.9263\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2300.4411 - mse: 2300.4412 - mae: 28.8210 - val_loss: 3706.8654 - val_mse: 3706.8657 - val_mae: 24.3310\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2343.9715 - mse: 2343.9714 - mae: 28.8384 - val_loss: 3709.1073 - val_mse: 3709.1062 - val_mae: 24.7183\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 654us/step - loss: 2307.1976 - mse: 2307.1978 - mae: 28.7658 - val_loss: 3709.1659 - val_mse: 3709.1646 - val_mae: 24.5548\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2315.5732 - mse: 2315.5728 - mae: 29.0795 - val_loss: 3708.8871 - val_mse: 3708.8870 - val_mae: 24.4447\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2313.7983 - mse: 2313.7981 - mae: 29.3377 - val_loss: 3705.8870 - val_mse: 3705.8862 - val_mae: 23.6341\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2271.1267 - mse: 2271.1265 - mae: 28.6304 - val_loss: 3706.9479 - val_mse: 3706.9482 - val_mae: 24.3533\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2280.3618 - mse: 2280.3621 - mae: 28.8851 - val_loss: 3705.5316 - val_mse: 3705.5317 - val_mae: 23.8201\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 691us/step - loss: 2408.8217 - mse: 2408.8218 - mae: 29.3498 - val_loss: 3704.2902 - val_mse: 3704.2913 - val_mae: 23.9727\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2327.5661 - mse: 2327.5667 - mae: 28.9714 - val_loss: 3703.7947 - val_mse: 3703.7949 - val_mae: 23.9727\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2373.4067 - mse: 2373.4077 - mae: 29.2842 - val_loss: 3704.9002 - val_mse: 3704.9001 - val_mae: 24.1932\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2313.6142 - mse: 2313.6140 - mae: 29.0957 - val_loss: 3703.5704 - val_mse: 3703.5696 - val_mae: 23.9452\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2399.8016 - mse: 2399.8008 - mae: 29.3996 - val_loss: 3704.0668 - val_mse: 3704.0667 - val_mae: 23.8183\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2317.3308 - mse: 2317.3306 - mae: 29.3362 - val_loss: 3703.5562 - val_mse: 3703.5562 - val_mae: 23.5781\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2251.8686 - mse: 2251.8694 - mae: 28.6321 - val_loss: 3706.3851 - val_mse: 3706.3855 - val_mae: 24.4342\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2299.0113 - mse: 2299.0110 - mae: 28.7659 - val_loss: 3706.7953 - val_mse: 3706.7952 - val_mae: 24.2334\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2340.1285 - mse: 2340.1284 - mae: 29.0677 - val_loss: 3706.8886 - val_mse: 3706.8879 - val_mae: 24.0949\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2329.8002 - mse: 2329.8008 - mae: 28.9891 - val_loss: 3708.5992 - val_mse: 3708.5991 - val_mae: 24.4225\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2406.7809 - mse: 2406.7805 - mae: 29.2996 - val_loss: 3707.0040 - val_mse: 3707.0037 - val_mae: 24.1451\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2301.5204 - mse: 2301.5205 - mae: 29.0153 - val_loss: 3705.4887 - val_mse: 3705.4883 - val_mae: 23.9850\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2310.3642 - mse: 2310.3650 - mae: 29.0477 - val_loss: 3707.3479 - val_mse: 3707.3481 - val_mae: 24.1456\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2706.5808 - mse: 2706.5811 - mae: 28.6017 - val_loss: 2152.3082 - val_mse: 2152.3081 - val_mae: 26.5607\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2762.7231 - mse: 2762.7224 - mae: 28.7182 - val_loss: 2160.3688 - val_mse: 2160.3689 - val_mae: 26.5848\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2768.4842 - mse: 2768.4844 - mae: 28.9052 - val_loss: 2189.9580 - val_mse: 2189.9580 - val_mae: 26.2936\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2747.6993 - mse: 2747.7000 - mae: 28.5394 - val_loss: 2178.7030 - val_mse: 2178.7029 - val_mae: 26.8434\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2723.5007 - mse: 2723.4998 - mae: 28.7108 - val_loss: 2201.5651 - val_mse: 2201.5654 - val_mae: 26.4327\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2687.7315 - mse: 2687.7314 - mae: 28.6255 - val_loss: 2208.8502 - val_mse: 2208.8503 - val_mae: 26.5433\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2691.3559 - mse: 2691.3552 - mae: 28.6649 - val_loss: 2211.1222 - val_mse: 2211.1223 - val_mae: 26.5362\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2689.2540 - mse: 2689.2539 - mae: 28.3586 - val_loss: 2186.6329 - val_mse: 2186.6326 - val_mae: 26.9628\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2731.2645 - mse: 2731.2639 - mae: 28.5337 - val_loss: 2207.7958 - val_mse: 2207.7957 - val_mae: 26.3929\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2742.8543 - mse: 2742.8542 - mae: 28.4641 - val_loss: 2205.8234 - val_mse: 2205.8232 - val_mae: 26.4085\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2720.8431 - mse: 2720.8430 - mae: 28.6708 - val_loss: 2202.4796 - val_mse: 2202.4797 - val_mae: 26.6994\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2746.0066 - mse: 2746.0068 - mae: 28.9085 - val_loss: 2207.6328 - val_mse: 2207.6331 - val_mae: 26.5121\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2742.8025 - mse: 2742.8022 - mae: 28.8099 - val_loss: 2209.4159 - val_mse: 2209.4155 - val_mae: 26.7633\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2705.4789 - mse: 2705.4792 - mae: 28.0562 - val_loss: 2219.6143 - val_mse: 2219.6143 - val_mae: 26.3093\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2675.4450 - mse: 2675.4451 - mae: 28.0992 - val_loss: 2193.9857 - val_mse: 2193.9856 - val_mae: 27.0539\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2743.4012 - mse: 2743.4014 - mae: 28.6897 - val_loss: 2214.6480 - val_mse: 2214.6475 - val_mae: 26.5357\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2691.4463 - mse: 2691.4468 - mae: 28.4552 - val_loss: 2207.8537 - val_mse: 2207.8533 - val_mae: 26.5222\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2709.5683 - mse: 2709.5686 - mae: 28.6484 - val_loss: 2207.6986 - val_mse: 2207.6985 - val_mae: 26.6432\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2672.6463 - mse: 2672.6462 - mae: 28.6490 - val_loss: 2206.7294 - val_mse: 2206.7290 - val_mae: 26.6351\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 545us/step - loss: 2709.7737 - mse: 2709.7732 - mae: 28.5872 - val_loss: 2200.1748 - val_mse: 2200.1750 - val_mae: 26.9685\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 537us/step - loss: 2717.6891 - mse: 2717.6892 - mae: 28.4947 - val_loss: 2199.2062 - val_mse: 2199.2061 - val_mae: 26.9382\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2706.6300 - mse: 2706.6311 - mae: 28.3931 - val_loss: 2226.0365 - val_mse: 2226.0361 - val_mae: 26.4238\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 538us/step - loss: 2710.0380 - mse: 2710.0371 - mae: 28.2077 - val_loss: 2221.2701 - val_mse: 2221.2700 - val_mae: 26.6788\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 674us/step - loss: 2691.9303 - mse: 2691.9307 - mae: 28.3270 - val_loss: 2201.0436 - val_mse: 2201.0432 - val_mae: 26.8802\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2716.7769 - mse: 2716.7773 - mae: 28.7058 - val_loss: 2205.5280 - val_mse: 2205.5286 - val_mae: 26.8631\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2693.8609 - mse: 2693.8604 - mae: 28.3088 - val_loss: 2214.8315 - val_mse: 2214.8313 - val_mae: 26.4147\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2699.4924 - mse: 2699.4932 - mae: 28.7262 - val_loss: 2207.9382 - val_mse: 2207.9385 - val_mae: 26.5593\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 468us/step - loss: 2678.8929 - mse: 2678.8931 - mae: 28.6498 - val_loss: 2202.7720 - val_mse: 2202.7720 - val_mae: 26.5332\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 524us/step - loss: 2680.8103 - mse: 2680.8096 - mae: 28.6451 - val_loss: 2195.6751 - val_mse: 2195.6750 - val_mae: 26.7352\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2710.5681 - mse: 2710.5684 - mae: 28.8219 - val_loss: 2196.4190 - val_mse: 2196.4189 - val_mae: 26.6455\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2718.2365 - mse: 2718.2358 - mae: 28.4807 - val_loss: 2206.7318 - val_mse: 2206.7319 - val_mae: 26.3827\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 518us/step - loss: 2734.5413 - mse: 2734.5417 - mae: 28.6984 - val_loss: 2198.8513 - val_mse: 2198.8516 - val_mae: 26.7220\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2684.7693 - mse: 2684.7690 - mae: 28.6236 - val_loss: 2197.8604 - val_mse: 2197.8604 - val_mae: 26.7814\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2709.1321 - mse: 2709.1331 - mae: 28.2180 - val_loss: 2197.3050 - val_mse: 2197.3047 - val_mae: 26.8423\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2660.9335 - mse: 2660.9331 - mae: 28.2412 - val_loss: 2201.6826 - val_mse: 2201.6829 - val_mae: 26.6702\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2715.7951 - mse: 2715.7959 - mae: 28.6405 - val_loss: 2195.0930 - val_mse: 2195.0928 - val_mae: 26.6474\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2741.5852 - mse: 2741.5850 - mae: 28.6553 - val_loss: 2189.2264 - val_mse: 2189.2266 - val_mae: 26.4906\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2747.1461 - mse: 2747.1470 - mae: 28.6460 - val_loss: 2209.5841 - val_mse: 2209.5840 - val_mae: 26.3369\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2693.0083 - mse: 2693.0085 - mae: 28.3028 - val_loss: 2188.1402 - val_mse: 2188.1399 - val_mae: 26.9069\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2715.2641 - mse: 2715.2646 - mae: 28.3258 - val_loss: 2202.3108 - val_mse: 2202.3108 - val_mae: 26.6005\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2671.9234 - mse: 2671.9229 - mae: 28.1520 - val_loss: 2196.9694 - val_mse: 2196.9695 - val_mae: 26.2066\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2620.2834 - mse: 2620.2832 - mae: 27.9695 - val_loss: 2193.3836 - val_mse: 2193.3838 - val_mae: 26.8295\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2701.2120 - mse: 2701.2124 - mae: 28.4672 - val_loss: 2201.4662 - val_mse: 2201.4666 - val_mae: 26.2392\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2761.5285 - mse: 2761.5291 - mae: 28.5621 - val_loss: 2207.1723 - val_mse: 2207.1724 - val_mae: 26.3629\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2700.2124 - mse: 2700.2122 - mae: 28.3915 - val_loss: 2194.3290 - val_mse: 2194.3291 - val_mae: 26.6557\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2721.3315 - mse: 2721.3320 - mae: 28.6142 - val_loss: 2190.7426 - val_mse: 2190.7424 - val_mae: 26.7880\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2703.5853 - mse: 2703.5854 - mae: 28.5542 - val_loss: 2192.1843 - val_mse: 2192.1841 - val_mae: 26.8271\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 652us/step - loss: 2655.0892 - mse: 2655.0886 - mae: 28.0611 - val_loss: 2211.4157 - val_mse: 2211.4158 - val_mae: 26.5854\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2608.8780 - mse: 2608.8784 - mae: 28.1196 - val_loss: 2194.2590 - val_mse: 2194.2593 - val_mae: 27.2680\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2704.3323 - mse: 2704.3325 - mae: 28.4258 - val_loss: 2203.0727 - val_mse: 2203.0725 - val_mae: 26.6561\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2705.4026 - mse: 2705.4031 - mae: 28.4630 - val_loss: 2177.7383 - val_mse: 2177.7383 - val_mae: 26.8565\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 477us/step - loss: 2741.6145 - mse: 2741.6150 - mae: 28.3155 - val_loss: 2189.6116 - val_mse: 2189.6121 - val_mae: 26.8176\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2673.7838 - mse: 2673.7830 - mae: 28.4087 - val_loss: 2187.4216 - val_mse: 2187.4216 - val_mae: 26.9586\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 505us/step - loss: 2647.5548 - mse: 2647.5544 - mae: 27.9180 - val_loss: 2181.9349 - val_mse: 2181.9351 - val_mae: 26.8260\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2703.3791 - mse: 2703.3794 - mae: 28.2843 - val_loss: 2192.9441 - val_mse: 2192.9443 - val_mae: 26.4785\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2730.6485 - mse: 2730.6479 - mae: 28.4629 - val_loss: 2200.3419 - val_mse: 2200.3418 - val_mae: 26.9300\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 671us/step - loss: 2662.2876 - mse: 2662.2871 - mae: 27.9743 - val_loss: 2207.8481 - val_mse: 2207.8481 - val_mae: 26.5360\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2668.7978 - mse: 2668.7976 - mae: 28.1634 - val_loss: 2192.8148 - val_mse: 2192.8147 - val_mae: 26.9298\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2713.0968 - mse: 2713.0964 - mae: 28.5482 - val_loss: 2199.5708 - val_mse: 2199.5710 - val_mae: 26.7240\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2673.2180 - mse: 2673.2178 - mae: 28.3192 - val_loss: 2200.0441 - val_mse: 2200.0442 - val_mae: 27.0881\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2712.0839 - mse: 2712.0840 - mae: 28.6117 - val_loss: 2212.4478 - val_mse: 2212.4478 - val_mae: 26.4580\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2670.7977 - mse: 2670.7976 - mae: 28.2325 - val_loss: 2203.5254 - val_mse: 2203.5251 - val_mae: 26.4825\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2718.0476 - mse: 2718.0486 - mae: 28.4705 - val_loss: 2199.0021 - val_mse: 2199.0020 - val_mae: 26.5613\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2644.0681 - mse: 2644.0684 - mae: 28.0678 - val_loss: 2201.5490 - val_mse: 2201.5491 - val_mae: 26.6968\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 555us/step - loss: 2710.4970 - mse: 2710.4958 - mae: 28.2356 - val_loss: 2208.1628 - val_mse: 2208.1624 - val_mae: 26.4957\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2662.2828 - mse: 2662.2827 - mae: 28.3621 - val_loss: 2185.2046 - val_mse: 2185.2046 - val_mae: 27.0371\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2679.8267 - mse: 2679.8271 - mae: 28.0836 - val_loss: 2194.4966 - val_mse: 2194.4963 - val_mae: 26.7291\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2713.0840 - mse: 2713.0835 - mae: 28.2941 - val_loss: 2192.2840 - val_mse: 2192.2839 - val_mae: 26.5179\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2677.2764 - mse: 2677.2764 - mae: 28.1724 - val_loss: 2191.3234 - val_mse: 2191.3235 - val_mae: 26.7480\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2688.3172 - mse: 2688.3179 - mae: 28.0394 - val_loss: 2198.2426 - val_mse: 2198.2422 - val_mae: 26.9149\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2688.7588 - mse: 2688.7581 - mae: 28.1318 - val_loss: 2205.1669 - val_mse: 2205.1667 - val_mae: 26.6015\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2688.5446 - mse: 2688.5452 - mae: 28.3219 - val_loss: 2199.9019 - val_mse: 2199.9019 - val_mae: 26.5581\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2655.6501 - mse: 2655.6504 - mae: 28.0996 - val_loss: 2196.6480 - val_mse: 2196.6482 - val_mae: 26.5777\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2696.0696 - mse: 2696.0703 - mae: 28.2983 - val_loss: 2193.3509 - val_mse: 2193.3508 - val_mae: 26.7624\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2632.8471 - mse: 2632.8467 - mae: 27.9011 - val_loss: 2197.0991 - val_mse: 2197.0989 - val_mae: 26.8961\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2738.0382 - mse: 2738.0378 - mae: 28.3306 - val_loss: 2220.0816 - val_mse: 2220.0811 - val_mae: 26.4174\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 655us/step - loss: 2702.0149 - mse: 2702.0151 - mae: 28.0376 - val_loss: 2208.4648 - val_mse: 2208.4651 - val_mae: 26.9461\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 536us/step - loss: 2730.0730 - mse: 2730.0730 - mae: 28.6587 - val_loss: 2200.4903 - val_mse: 2200.4905 - val_mae: 26.9795\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2725.9125 - mse: 2725.9119 - mae: 28.8025 - val_loss: 2204.8642 - val_mse: 2204.8640 - val_mae: 26.7441\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2667.8865 - mse: 2667.8857 - mae: 28.2698 - val_loss: 2202.8924 - val_mse: 2202.8921 - val_mae: 26.6606\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 13322.2756 - mse: 13322.2773 - mae: 109.8729 - val_loss: 34600.3309 - val_mse: 34600.3320 - val_mae: 132.6614\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 13142.2052 - mse: 13142.2061 - mae: 109.0528 - val_loss: 34247.8629 - val_mse: 34247.8633 - val_mae: 131.3316\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 629us/step - loss: 12615.8068 - mse: 12615.8047 - mae: 106.6259 - val_loss: 33245.5775 - val_mse: 33245.5742 - val_mae: 127.4771\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 495us/step - loss: 11291.1844 - mse: 11291.1846 - mae: 100.2583 - val_loss: 30481.4545 - val_mse: 30481.4531 - val_mae: 116.1909\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 8043.3112 - mse: 8043.3101 - mae: 81.8825 - val_loss: 24508.1804 - val_mse: 24508.1816 - val_mae: 86.9482\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 501us/step - loss: 3666.7174 - mse: 3666.7173 - mae: 47.0596 - val_loss: 17933.2139 - val_mse: 17933.2148 - val_mae: 36.3310\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 565us/step - loss: 2331.7997 - mse: 2331.7998 - mae: 35.7804 - val_loss: 17799.0507 - val_mse: 17799.0508 - val_mae: 35.6688\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2477.0540 - mse: 2477.0540 - mae: 35.3839 - val_loss: 17992.1184 - val_mse: 17992.1172 - val_mae: 36.6693\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2741.2389 - mse: 2741.2388 - mae: 37.2155 - val_loss: 18093.9373 - val_mse: 18093.9355 - val_mae: 37.3709\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 524us/step - loss: 2602.1474 - mse: 2602.1477 - mae: 36.1925 - val_loss: 17823.2177 - val_mse: 17823.2168 - val_mae: 35.7864\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2316.0047 - mse: 2316.0044 - mae: 34.4391 - val_loss: 17796.4724 - val_mse: 17796.4746 - val_mae: 35.6849\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 2356.9977 - mse: 2356.9976 - mae: 35.2211 - val_loss: 17840.1163 - val_mse: 17840.1152 - val_mae: 35.8717\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 2374.3921 - mse: 2374.3918 - mae: 35.4307 - val_loss: 17622.8810 - val_mse: 17622.8809 - val_mae: 35.4161\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 527us/step - loss: 2251.4543 - mse: 2251.4543 - mae: 33.8386 - val_loss: 17647.4893 - val_mse: 17647.4883 - val_mae: 35.4446\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2313.5026 - mse: 2313.5024 - mae: 34.1021 - val_loss: 17848.2726 - val_mse: 17848.2715 - val_mae: 35.9292\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2302.2951 - mse: 2302.2949 - mae: 34.4964 - val_loss: 17908.4945 - val_mse: 17908.4941 - val_mae: 36.2111\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2398.6525 - mse: 2398.6528 - mae: 34.4646 - val_loss: 17838.7625 - val_mse: 17838.7617 - val_mae: 35.9014\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 2310.0094 - mse: 2310.0093 - mae: 33.5629 - val_loss: 17772.1570 - val_mse: 17772.1562 - val_mae: 35.6719\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2225.0821 - mse: 2225.0820 - mae: 32.9433 - val_loss: 17874.2036 - val_mse: 17874.2031 - val_mae: 36.0687\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2352.7578 - mse: 2352.7576 - mae: 34.2047 - val_loss: 17591.0033 - val_mse: 17591.0039 - val_mae: 35.4938\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2386.5207 - mse: 2386.5208 - mae: 34.3477 - val_loss: 17740.6026 - val_mse: 17740.6016 - val_mae: 35.6354\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 676us/step - loss: 2170.0057 - mse: 2170.0059 - mae: 31.4807 - val_loss: 17809.8735 - val_mse: 17809.8730 - val_mae: 35.8254\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 2317.9218 - mse: 2317.9214 - mae: 33.3163 - val_loss: 17658.0103 - val_mse: 17658.0098 - val_mae: 35.5572\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 2315.7402 - mse: 2315.7397 - mae: 32.9448 - val_loss: 17925.5083 - val_mse: 17925.5078 - val_mae: 36.3249\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2418.5723 - mse: 2418.5720 - mae: 34.1329 - val_loss: 17903.0035 - val_mse: 17903.0039 - val_mae: 36.2240\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 633us/step - loss: 2239.5992 - mse: 2239.5996 - mae: 32.6991 - val_loss: 17634.2565 - val_mse: 17634.2559 - val_mae: 35.5733\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 2362.1335 - mse: 2362.1335 - mae: 33.6714 - val_loss: 17796.5572 - val_mse: 17796.5566 - val_mae: 35.8160\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 1958.3607 - mse: 1958.3606 - mae: 31.9974 - val_loss: 17827.0298 - val_mse: 17827.0273 - val_mae: 35.9234\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2073.5405 - mse: 2073.5405 - mae: 31.0548 - val_loss: 17655.2828 - val_mse: 17655.2812 - val_mae: 35.6101\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 633us/step - loss: 2225.7353 - mse: 2225.7351 - mae: 33.7883 - val_loss: 17784.4915 - val_mse: 17784.4922 - val_mae: 35.8015\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 2156.4066 - mse: 2156.4067 - mae: 32.3713 - val_loss: 17715.3244 - val_mse: 17715.3242 - val_mae: 35.6817\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 709us/step - loss: 2217.4331 - mse: 2217.4333 - mae: 31.9909 - val_loss: 17855.3251 - val_mse: 17855.3242 - val_mae: 36.0533\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 797us/step - loss: 2179.4231 - mse: 2179.4236 - mae: 32.8001 - val_loss: 17622.4235 - val_mse: 17622.4238 - val_mae: 35.6601\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 2158.1697 - mse: 2158.1697 - mae: 32.2589 - val_loss: 17721.7837 - val_mse: 17721.7812 - val_mae: 35.7214\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 513us/step - loss: 2097.5660 - mse: 2097.5659 - mae: 32.6101 - val_loss: 17822.7226 - val_mse: 17822.7227 - val_mae: 35.9598\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2175.6008 - mse: 2175.6008 - mae: 31.4726 - val_loss: 17736.2966 - val_mse: 17736.2969 - val_mae: 35.7528\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 531us/step - loss: 2079.4184 - mse: 2079.4182 - mae: 31.7723 - val_loss: 17792.3560 - val_mse: 17792.3555 - val_mae: 35.8854\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2155.5924 - mse: 2155.5925 - mae: 32.0676 - val_loss: 17660.3689 - val_mse: 17660.3691 - val_mae: 35.7320\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2171.6349 - mse: 2171.6350 - mae: 32.4071 - val_loss: 17758.3280 - val_mse: 17758.3262 - val_mae: 35.8225\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2104.0169 - mse: 2104.0171 - mae: 32.1359 - val_loss: 17644.0388 - val_mse: 17644.0391 - val_mae: 35.7562\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2169.2781 - mse: 2169.2783 - mae: 31.6813 - val_loss: 17548.4619 - val_mse: 17548.4629 - val_mae: 35.8352\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 571us/step - loss: 1822.4311 - mse: 1822.4310 - mae: 29.2557 - val_loss: 17633.8241 - val_mse: 17633.8223 - val_mae: 35.7856\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2173.5660 - mse: 2173.5659 - mae: 31.5615 - val_loss: 17781.1471 - val_mse: 17781.1465 - val_mae: 35.9122\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2139.1145 - mse: 2139.1143 - mae: 32.4301 - val_loss: 17858.2407 - val_mse: 17858.2422 - val_mae: 36.1418\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 1962.7452 - mse: 1962.7451 - mae: 29.6413 - val_loss: 17774.6114 - val_mse: 17774.6094 - val_mae: 35.9098\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 1860.5817 - mse: 1860.5819 - mae: 30.0215 - val_loss: 17727.1785 - val_mse: 17727.1777 - val_mae: 35.8465\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 2069.7882 - mse: 2069.7881 - mae: 31.1861 - val_loss: 17644.7405 - val_mse: 17644.7402 - val_mae: 35.8457\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 489us/step - loss: 2152.5101 - mse: 2152.5103 - mae: 31.8398 - val_loss: 17814.9419 - val_mse: 17814.9414 - val_mae: 36.0355\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 485us/step - loss: 1962.7041 - mse: 1962.7042 - mae: 30.9913 - val_loss: 17722.8570 - val_mse: 17722.8574 - val_mae: 35.8802\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 528us/step - loss: 1808.9251 - mse: 1808.9254 - mae: 29.6201 - val_loss: 17663.0121 - val_mse: 17663.0117 - val_mae: 35.8772\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 510us/step - loss: 2004.4382 - mse: 2004.4382 - mae: 31.7741 - val_loss: 17610.3056 - val_mse: 17610.3066 - val_mae: 35.9099\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 471us/step - loss: 1912.9552 - mse: 1912.9552 - mae: 30.7716 - val_loss: 17861.1555 - val_mse: 17861.1562 - val_mae: 36.1970\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 348us/step - loss: 2155.0818 - mse: 2155.0818 - mae: 30.9404 - val_loss: 17777.6247 - val_mse: 17777.6230 - val_mae: 35.9786\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 1875.9718 - mse: 1875.9716 - mae: 29.1777 - val_loss: 17697.1827 - val_mse: 17697.1816 - val_mae: 35.9224\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2071.0433 - mse: 2071.0432 - mae: 31.2475 - val_loss: 17540.9475 - val_mse: 17540.9473 - val_mae: 36.0348\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 415us/step - loss: 1977.5254 - mse: 1977.5254 - mae: 30.9427 - val_loss: 17776.2261 - val_mse: 17776.2266 - val_mae: 35.9995\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 420us/step - loss: 2053.2542 - mse: 2053.2542 - mae: 31.2383 - val_loss: 17620.4139 - val_mse: 17620.4121 - val_mae: 35.9603\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 410us/step - loss: 2075.1853 - mse: 2075.1851 - mae: 32.2975 - val_loss: 17724.2512 - val_mse: 17724.2500 - val_mae: 35.9644\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 418us/step - loss: 1855.9169 - mse: 1855.9167 - mae: 29.9966 - val_loss: 17632.6521 - val_mse: 17632.6523 - val_mae: 35.9762\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 465us/step - loss: 2139.2549 - mse: 2139.2546 - mae: 32.0414 - val_loss: 17745.6266 - val_mse: 17745.6270 - val_mae: 36.0006\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 462us/step - loss: 1916.0072 - mse: 1916.0071 - mae: 29.8498 - val_loss: 17565.7982 - val_mse: 17565.7988 - val_mae: 36.0773\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 444us/step - loss: 1968.1988 - mse: 1968.1986 - mae: 30.4328 - val_loss: 17710.2898 - val_mse: 17710.2891 - val_mae: 36.0079\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 1928.7270 - mse: 1928.7268 - mae: 30.5280 - val_loss: 17680.5199 - val_mse: 17680.5195 - val_mae: 36.0094\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 495us/step - loss: 1857.7361 - mse: 1857.7362 - mae: 29.1807 - val_loss: 17724.6988 - val_mse: 17724.6992 - val_mae: 36.0334\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 477us/step - loss: 1946.9760 - mse: 1946.9760 - mae: 30.0208 - val_loss: 17602.3682 - val_mse: 17602.3672 - val_mae: 36.0751\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 2000.9021 - mse: 2000.9021 - mae: 31.2456 - val_loss: 17790.3226 - val_mse: 17790.3223 - val_mae: 36.1294\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 1981.5436 - mse: 1981.5437 - mae: 30.3313 - val_loss: 17635.0537 - val_mse: 17635.0547 - val_mae: 36.0760\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 629us/step - loss: 1869.3116 - mse: 1869.3116 - mae: 29.3892 - val_loss: 17690.2063 - val_mse: 17690.2070 - val_mae: 36.0665\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 442us/step - loss: 1944.7846 - mse: 1944.7848 - mae: 30.3315 - val_loss: 17654.5175 - val_mse: 17654.5195 - val_mae: 36.0887\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 430us/step - loss: 1918.5941 - mse: 1918.5941 - mae: 30.1688 - val_loss: 17580.4170 - val_mse: 17580.4180 - val_mae: 36.1735\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 1878.5118 - mse: 1878.5120 - mae: 30.0950 - val_loss: 17614.9380 - val_mse: 17614.9375 - val_mae: 36.1361\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 620us/step - loss: 1864.5801 - mse: 1864.5798 - mae: 29.6294 - val_loss: 17652.2238 - val_mse: 17652.2246 - val_mae: 36.1229\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 665us/step - loss: 1956.4498 - mse: 1956.4498 - mae: 29.5694 - val_loss: 17813.7587 - val_mse: 17813.7598 - val_mae: 36.2428\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 487us/step - loss: 1847.2239 - mse: 1847.2239 - mae: 29.7159 - val_loss: 17610.9824 - val_mse: 17610.9824 - val_mae: 36.1772\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 513us/step - loss: 1985.2589 - mse: 1985.2589 - mae: 30.1757 - val_loss: 17856.6989 - val_mse: 17856.6992 - val_mae: 36.3407\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 1990.2165 - mse: 1990.2166 - mae: 29.3763 - val_loss: 17511.5388 - val_mse: 17511.5391 - val_mae: 36.3988\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 1944.6210 - mse: 1944.6208 - mae: 31.3870 - val_loss: 17626.1863 - val_mse: 17626.1875 - val_mae: 36.1905\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 559us/step - loss: 1903.8535 - mse: 1903.8533 - mae: 30.0684 - val_loss: 17709.8407 - val_mse: 17709.8398 - val_mae: 36.1592\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 1704.8012 - mse: 1704.8010 - mae: 28.2327 - val_loss: 17683.3697 - val_mse: 17683.3691 - val_mae: 36.1664\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 523us/step - loss: 1956.0103 - mse: 1956.0103 - mae: 29.8766 - val_loss: 17624.1110 - val_mse: 17624.1113 - val_mae: 36.2157\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 518us/step - loss: 4004.8422 - mse: 4004.8428 - mae: 33.0962 - val_loss: 2133.8003 - val_mse: 2133.8003 - val_mae: 31.3491\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4314.1988 - mse: 4314.1987 - mae: 35.8258 - val_loss: 2271.1327 - val_mse: 2271.1326 - val_mae: 31.6892\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4233.7471 - mse: 4233.7476 - mae: 34.9466 - val_loss: 2190.4680 - val_mse: 2190.4680 - val_mae: 31.4871\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4157.3223 - mse: 4157.3223 - mae: 34.1372 - val_loss: 2268.4161 - val_mse: 2268.4163 - val_mae: 31.6764\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4355.1598 - mse: 4355.1597 - mae: 35.0688 - val_loss: 2241.8860 - val_mse: 2241.8860 - val_mae: 31.6061\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 4301.7427 - mse: 4301.7422 - mae: 33.9806 - val_loss: 2260.4790 - val_mse: 2260.4790 - val_mae: 31.6561\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4137.4878 - mse: 4137.4878 - mae: 34.2507 - val_loss: 2276.4425 - val_mse: 2276.4424 - val_mae: 31.6988\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4073.2484 - mse: 4073.2485 - mae: 34.2455 - val_loss: 2285.8322 - val_mse: 2285.8323 - val_mae: 31.7235\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4146.6748 - mse: 4146.6743 - mae: 33.5898 - val_loss: 2249.1197 - val_mse: 2249.1196 - val_mae: 31.6226\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 4184.2485 - mse: 4184.2485 - mae: 34.1447 - val_loss: 2257.5493 - val_mse: 2257.5493 - val_mae: 31.6433\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4179.6758 - mse: 4179.6758 - mae: 33.7573 - val_loss: 2213.4381 - val_mse: 2213.4382 - val_mae: 31.5236\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4063.1058 - mse: 4063.1057 - mae: 32.8781 - val_loss: 2224.6772 - val_mse: 2224.6770 - val_mae: 31.5518\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4110.5173 - mse: 4110.5171 - mae: 34.1279 - val_loss: 2202.2281 - val_mse: 2202.2280 - val_mae: 31.4938\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4251.1765 - mse: 4251.1772 - mae: 34.8613 - val_loss: 2314.4586 - val_mse: 2314.4585 - val_mae: 31.8008\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4105.9965 - mse: 4105.9971 - mae: 33.3199 - val_loss: 2200.5805 - val_mse: 2200.5806 - val_mae: 31.4872\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4161.8718 - mse: 4161.8721 - mae: 34.1515 - val_loss: 2234.9663 - val_mse: 2234.9666 - val_mae: 31.5803\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 4178.9366 - mse: 4178.9365 - mae: 33.6475 - val_loss: 2260.6135 - val_mse: 2260.6135 - val_mae: 31.6484\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4267.0778 - mse: 4267.0771 - mae: 34.7305 - val_loss: 2325.4284 - val_mse: 2325.4282 - val_mae: 31.8330\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4153.8297 - mse: 4153.8296 - mae: 33.7238 - val_loss: 2208.4810 - val_mse: 2208.4810 - val_mae: 31.5099\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 4265.0955 - mse: 4265.0952 - mae: 35.1434 - val_loss: 2283.6574 - val_mse: 2283.6575 - val_mae: 31.7122\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 4124.6504 - mse: 4124.6504 - mae: 34.3127 - val_loss: 2306.5361 - val_mse: 2306.5361 - val_mae: 31.7800\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 700us/step - loss: 4071.3177 - mse: 4071.3171 - mae: 33.0026 - val_loss: 2251.1086 - val_mse: 2251.1084 - val_mae: 31.6287\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 697us/step - loss: 4115.9641 - mse: 4115.9644 - mae: 33.3469 - val_loss: 2268.7342 - val_mse: 2268.7346 - val_mae: 31.6749\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4124.9693 - mse: 4124.9692 - mae: 34.7038 - val_loss: 2319.7692 - val_mse: 2319.7695 - val_mae: 31.8192\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4034.4400 - mse: 4034.4395 - mae: 33.8457 - val_loss: 2263.4453 - val_mse: 2263.4453 - val_mae: 31.6625\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4036.8580 - mse: 4036.8574 - mae: 33.4601 - val_loss: 2281.1362 - val_mse: 2281.1360 - val_mae: 31.7091\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 530us/step - loss: 4243.4481 - mse: 4243.4478 - mae: 34.1778 - val_loss: 2292.0080 - val_mse: 2292.0081 - val_mae: 31.7409\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 669us/step - loss: 4076.8043 - mse: 4076.8049 - mae: 33.2844 - val_loss: 2294.1868 - val_mse: 2294.1868 - val_mae: 31.7470\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 726us/step - loss: 4049.9691 - mse: 4049.9690 - mae: 33.3624 - val_loss: 2273.6166 - val_mse: 2273.6165 - val_mae: 31.6930\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 523us/step - loss: 4142.1926 - mse: 4142.1929 - mae: 34.0423 - val_loss: 2273.8211 - val_mse: 2273.8213 - val_mae: 31.6906\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4113.1444 - mse: 4113.1440 - mae: 33.7002 - val_loss: 2252.6625 - val_mse: 2252.6624 - val_mae: 31.6318\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4056.0546 - mse: 4056.0547 - mae: 33.4098 - val_loss: 2293.5918 - val_mse: 2293.5918 - val_mae: 31.7394\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4123.3710 - mse: 4123.3706 - mae: 33.3569 - val_loss: 2254.8107 - val_mse: 2254.8110 - val_mae: 31.6393\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 527us/step - loss: 4102.9096 - mse: 4102.9092 - mae: 33.5842 - val_loss: 2289.9676 - val_mse: 2289.9678 - val_mae: 31.7336\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 501us/step - loss: 4252.8944 - mse: 4252.8940 - mae: 33.9880 - val_loss: 2287.7331 - val_mse: 2287.7332 - val_mae: 31.7304\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 534us/step - loss: 4058.4962 - mse: 4058.4961 - mae: 32.9726 - val_loss: 2252.9892 - val_mse: 2252.9890 - val_mae: 31.6384\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4142.6397 - mse: 4142.6401 - mae: 34.8190 - val_loss: 2319.0065 - val_mse: 2319.0066 - val_mae: 31.8203\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4157.5164 - mse: 4157.5166 - mae: 33.9988 - val_loss: 2342.1496 - val_mse: 2342.1497 - val_mae: 31.8899\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 545us/step - loss: 4034.3709 - mse: 4034.3708 - mae: 33.0023 - val_loss: 2262.8218 - val_mse: 2262.8218 - val_mae: 31.6689\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 708us/step - loss: 4149.5316 - mse: 4149.5308 - mae: 33.6047 - val_loss: 2325.4678 - val_mse: 2325.4680 - val_mae: 31.8429\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4132.3482 - mse: 4132.3481 - mae: 33.3409 - val_loss: 2286.5185 - val_mse: 2286.5186 - val_mae: 31.7297\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 3813.5327 - mse: 3813.5322 - mae: 32.0138 - val_loss: 2219.2523 - val_mse: 2219.2522 - val_mae: 31.5458\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 558us/step - loss: 4159.8416 - mse: 4159.8418 - mae: 34.2051 - val_loss: 2316.4580 - val_mse: 2316.4580 - val_mae: 31.8143\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4194.8050 - mse: 4194.8052 - mae: 34.0641 - val_loss: 2306.5918 - val_mse: 2306.5918 - val_mae: 31.7823\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 4089.0681 - mse: 4089.0681 - mae: 33.5223 - val_loss: 2251.0223 - val_mse: 2251.0222 - val_mae: 31.6337\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 516us/step - loss: 4154.7170 - mse: 4154.7163 - mae: 33.4383 - val_loss: 2313.4798 - val_mse: 2313.4800 - val_mae: 31.8046\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4070.7483 - mse: 4070.7478 - mae: 33.7065 - val_loss: 2282.4936 - val_mse: 2282.4937 - val_mae: 31.7174\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 3939.7484 - mse: 3939.7483 - mae: 31.9348 - val_loss: 2291.1068 - val_mse: 2291.1069 - val_mae: 31.7387\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 0s 454us/step - loss: 4151.4790 - mse: 4151.4795 - mae: 34.8744 - val_loss: 2334.6223 - val_mse: 2334.6223 - val_mae: 31.8630\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 556us/step - loss: 3999.1440 - mse: 3999.1445 - mae: 33.0808 - val_loss: 2263.8313 - val_mse: 2263.8311 - val_mae: 31.6663\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 3955.8875 - mse: 3955.8872 - mae: 33.0384 - val_loss: 2263.5751 - val_mse: 2263.5750 - val_mae: 31.6646\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 3965.1385 - mse: 3965.1379 - mae: 32.7996 - val_loss: 2266.1561 - val_mse: 2266.1560 - val_mae: 31.6703\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 4079.4944 - mse: 4079.4941 - mae: 33.6036 - val_loss: 2316.3875 - val_mse: 2316.3875 - val_mae: 31.8043\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 520us/step - loss: 4191.9169 - mse: 4191.9170 - mae: 34.1650 - val_loss: 2348.0811 - val_mse: 2348.0811 - val_mae: 31.8942\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4076.7861 - mse: 4076.7861 - mae: 33.6608 - val_loss: 2277.7871 - val_mse: 2277.7871 - val_mae: 31.6990\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 3952.2552 - mse: 3952.2549 - mae: 32.6652 - val_loss: 2264.2877 - val_mse: 2264.2878 - val_mae: 31.6631\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 3961.6933 - mse: 3961.6934 - mae: 33.4519 - val_loss: 2298.1384 - val_mse: 2298.1379 - val_mae: 31.7519\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 3973.5578 - mse: 3973.5574 - mae: 33.2047 - val_loss: 2297.4803 - val_mse: 2297.4800 - val_mae: 31.7492\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4011.3914 - mse: 4011.3911 - mae: 33.9412 - val_loss: 2295.2115 - val_mse: 2295.2114 - val_mae: 31.7431\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 511us/step - loss: 4168.0929 - mse: 4168.0928 - mae: 33.1584 - val_loss: 2315.5344 - val_mse: 2315.5344 - val_mae: 31.7964\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 553us/step - loss: 3825.2085 - mse: 3825.2085 - mae: 32.1658 - val_loss: 2296.9868 - val_mse: 2296.9868 - val_mae: 31.7454\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 3930.2757 - mse: 3930.2749 - mae: 32.8312 - val_loss: 2331.9263 - val_mse: 2331.9263 - val_mae: 31.8444\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 3991.8102 - mse: 3991.8098 - mae: 32.3546 - val_loss: 2241.5908 - val_mse: 2241.5908 - val_mae: 31.5964\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4011.1719 - mse: 4011.1726 - mae: 33.5848 - val_loss: 2290.0542 - val_mse: 2290.0542 - val_mae: 31.7270\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 3848.7877 - mse: 3848.7876 - mae: 32.6257 - val_loss: 2281.3111 - val_mse: 2281.3113 - val_mae: 31.7029\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4090.2771 - mse: 4090.2778 - mae: 33.8615 - val_loss: 2295.5376 - val_mse: 2295.5378 - val_mae: 31.7395\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 0s 443us/step - loss: 3962.8378 - mse: 3962.8374 - mae: 32.7822 - val_loss: 2276.4630 - val_mse: 2276.4626 - val_mae: 31.6915\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 4041.7735 - mse: 4041.7739 - mae: 33.0673 - val_loss: 2301.0809 - val_mse: 2301.0811 - val_mae: 31.7558\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 3981.8036 - mse: 3981.8042 - mae: 33.2720 - val_loss: 2301.3901 - val_mse: 2301.3899 - val_mae: 31.7573\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4033.2433 - mse: 4033.2437 - mae: 32.3583 - val_loss: 2308.9118 - val_mse: 2308.9119 - val_mae: 31.7769\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 695us/step - loss: 4142.8302 - mse: 4142.8306 - mae: 33.5974 - val_loss: 2311.6859 - val_mse: 2311.6858 - val_mae: 31.7848\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4020.3357 - mse: 4020.3359 - mae: 32.5008 - val_loss: 2327.2134 - val_mse: 2327.2131 - val_mae: 31.8303\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4204.0945 - mse: 4204.0947 - mae: 33.2170 - val_loss: 2312.1411 - val_mse: 2312.1414 - val_mae: 31.7862\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 3965.4187 - mse: 3965.4194 - mae: 32.6639 - val_loss: 2254.4952 - val_mse: 2254.4954 - val_mae: 31.6338\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 521us/step - loss: 3979.2584 - mse: 3979.2581 - mae: 32.9259 - val_loss: 2321.4728 - val_mse: 2321.4727 - val_mae: 31.8128\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 0s 452us/step - loss: 4095.1593 - mse: 4095.1589 - mae: 33.8136 - val_loss: 2309.1650 - val_mse: 2309.1650 - val_mae: 31.7759\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 0s 488us/step - loss: 3991.4933 - mse: 3991.4932 - mae: 32.7544 - val_loss: 2319.5772 - val_mse: 2319.5774 - val_mae: 31.8077\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4006.6476 - mse: 4006.6479 - mae: 33.3751 - val_loss: 2310.9843 - val_mse: 2310.9841 - val_mae: 31.7821\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4123.8067 - mse: 4123.8071 - mae: 32.9800 - val_loss: 2351.5680 - val_mse: 2351.5681 - val_mae: 31.8983\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 3791.7585 - mse: 3791.7583 - mae: 32.1717 - val_loss: 2298.3343 - val_mse: 2298.3347 - val_mae: 31.7522\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3487.6423 - mse: 3487.6421 - mae: 32.8735 - val_loss: 1461.7142 - val_mse: 1461.7141 - val_mae: 25.6208\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3429.6433 - mse: 3429.6436 - mae: 32.7871 - val_loss: 1461.7387 - val_mse: 1461.7386 - val_mae: 25.7083\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3434.6913 - mse: 3434.6909 - mae: 33.1648 - val_loss: 1461.9884 - val_mse: 1461.9885 - val_mae: 25.6974\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3261.1082 - mse: 3261.1079 - mae: 32.3966 - val_loss: 1462.1307 - val_mse: 1462.1307 - val_mae: 25.9329\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3477.3092 - mse: 3477.3086 - mae: 33.1151 - val_loss: 1461.8815 - val_mse: 1461.8815 - val_mae: 25.9552\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3281.6281 - mse: 3281.6277 - mae: 31.6042 - val_loss: 1462.6830 - val_mse: 1462.6830 - val_mae: 26.1332\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3389.4269 - mse: 3389.4265 - mae: 32.4012 - val_loss: 1463.9314 - val_mse: 1463.9313 - val_mae: 26.2366\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3380.4708 - mse: 3380.4709 - mae: 31.9589 - val_loss: 1461.6699 - val_mse: 1461.6698 - val_mae: 25.7057\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3298.7915 - mse: 3298.7913 - mae: 31.9882 - val_loss: 1462.2630 - val_mse: 1462.2631 - val_mae: 25.8655\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3417.7076 - mse: 3417.7078 - mae: 32.2949 - val_loss: 1463.2268 - val_mse: 1463.2268 - val_mae: 25.8449\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3410.7126 - mse: 3410.7134 - mae: 32.8882 - val_loss: 1464.1244 - val_mse: 1464.1244 - val_mae: 25.9670\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3324.9257 - mse: 3324.9258 - mae: 31.6737 - val_loss: 1464.6500 - val_mse: 1464.6500 - val_mae: 26.1579\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3221.2234 - mse: 3221.2234 - mae: 32.4184 - val_loss: 1462.8864 - val_mse: 1462.8864 - val_mae: 25.7131\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3326.9068 - mse: 3326.9062 - mae: 31.2290 - val_loss: 1463.2113 - val_mse: 1463.2114 - val_mae: 25.6143\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3168.5817 - mse: 3168.5815 - mae: 32.3689 - val_loss: 1463.4722 - val_mse: 1463.4723 - val_mae: 25.7643\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3297.6204 - mse: 3297.6208 - mae: 32.0882 - val_loss: 1467.2447 - val_mse: 1467.2446 - val_mae: 26.3552\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3438.8757 - mse: 3438.8755 - mae: 32.3522 - val_loss: 1464.7214 - val_mse: 1464.7214 - val_mae: 25.6089\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3341.8014 - mse: 3341.8018 - mae: 31.9929 - val_loss: 1465.1599 - val_mse: 1465.1599 - val_mae: 25.4784\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3282.3158 - mse: 3282.3154 - mae: 31.65 - 1s 630us/step - loss: 3268.0955 - mse: 3268.0952 - mae: 31.6563 - val_loss: 1465.8492 - val_mse: 1465.8495 - val_mae: 25.9924\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3315.5715 - mse: 3315.5720 - mae: 31.7407 - val_loss: 1469.4335 - val_mse: 1469.4335 - val_mae: 26.5343\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3298.3379 - mse: 3298.3379 - mae: 32.0193 - val_loss: 1465.1534 - val_mse: 1465.1532 - val_mae: 25.6930\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3278.5692 - mse: 3278.5693 - mae: 31.9589 - val_loss: 1466.5334 - val_mse: 1466.5336 - val_mae: 26.1783\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3237.8926 - mse: 3237.8923 - mae: 32.5334 - val_loss: 1468.9914 - val_mse: 1468.9915 - val_mae: 26.5642\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3351.3710 - mse: 3351.3706 - mae: 32.4684 - val_loss: 1465.5608 - val_mse: 1465.5607 - val_mae: 25.1959\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3353.8652 - mse: 3353.8652 - mae: 32.1146 - val_loss: 1464.7987 - val_mse: 1464.7988 - val_mae: 25.7235\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3165.0097 - mse: 3165.0088 - mae: 31.3058 - val_loss: 1465.6141 - val_mse: 1465.6140 - val_mae: 26.1090\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3384.0682 - mse: 3384.0681 - mae: 32.6802 - val_loss: 1464.4055 - val_mse: 1464.4056 - val_mae: 25.8864\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3297.2224 - mse: 3297.2217 - mae: 31.5831 - val_loss: 1463.7677 - val_mse: 1463.7677 - val_mae: 25.8633\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3374.0062 - mse: 3374.0066 - mae: 32.5892 - val_loss: 1463.6717 - val_mse: 1463.6718 - val_mae: 25.6180\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3323.0145 - mse: 3323.0142 - mae: 32.1590 - val_loss: 1464.1138 - val_mse: 1464.1138 - val_mae: 25.7268\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3331.3421 - mse: 3331.3420 - mae: 31.8368 - val_loss: 1465.6911 - val_mse: 1465.6912 - val_mae: 26.0373\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3310.3049 - mse: 3310.3044 - mae: 31.6821 - val_loss: 1467.9469 - val_mse: 1467.9470 - val_mae: 26.3591\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3355.4881 - mse: 3355.4873 - mae: 32.1219 - val_loss: 1464.1726 - val_mse: 1464.1729 - val_mae: 25.7249\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3337.3247 - mse: 3337.3252 - mae: 32.0160 - val_loss: 1463.6334 - val_mse: 1463.6335 - val_mae: 25.6521\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 491us/step - loss: 3440.1346 - mse: 3440.1350 - mae: 32.8236 - val_loss: 1464.0151 - val_mse: 1464.0153 - val_mae: 25.5555\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 535us/step - loss: 3290.3913 - mse: 3290.3916 - mae: 31.5454 - val_loss: 1465.2272 - val_mse: 1465.2271 - val_mae: 26.0242\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3368.1933 - mse: 3368.1941 - mae: 32.0767 - val_loss: 1466.2506 - val_mse: 1466.2506 - val_mae: 26.1586\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3308.1389 - mse: 3308.1394 - mae: 31.9822 - val_loss: 1471.1914 - val_mse: 1471.1915 - val_mae: 26.6456\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3366.5581 - mse: 3366.5591 - mae: 32.1822 - val_loss: 1468.4258 - val_mse: 1468.4260 - val_mae: 26.4055\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3341.8077 - mse: 3341.8071 - mae: 32.0814 - val_loss: 1466.2953 - val_mse: 1466.2954 - val_mae: 26.0318\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 495us/step - loss: 3397.5889 - mse: 3397.5884 - mae: 32.0665 - val_loss: 1468.9691 - val_mse: 1468.9689 - val_mae: 26.4165\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 686us/step - loss: 3316.1957 - mse: 3316.1956 - mae: 32.1599 - val_loss: 1470.0327 - val_mse: 1470.0327 - val_mae: 26.4987\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3223.5817 - mse: 3223.5825 - mae: 31.0184 - val_loss: 1468.1063 - val_mse: 1468.1063 - val_mae: 26.2908\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3276.4646 - mse: 3276.4641 - mae: 32.0715 - val_loss: 1467.7148 - val_mse: 1467.7147 - val_mae: 26.2834\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3342.4236 - mse: 3342.4241 - mae: 32.6333 - val_loss: 1465.0958 - val_mse: 1465.0957 - val_mae: 25.5721\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3333.3471 - mse: 3333.3472 - mae: 31.5610 - val_loss: 1466.6590 - val_mse: 1466.6589 - val_mae: 26.1936\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 526us/step - loss: 3298.1860 - mse: 3298.1855 - mae: 32.3236 - val_loss: 1466.2128 - val_mse: 1466.2128 - val_mae: 26.1161\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3317.1956 - mse: 3317.1953 - mae: 31.8285 - val_loss: 1467.9623 - val_mse: 1467.9623 - val_mae: 26.3493\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3202.7028 - mse: 3202.7019 - mae: 31.5728 - val_loss: 1465.9316 - val_mse: 1465.9316 - val_mae: 25.9119\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3192.1008 - mse: 3192.1001 - mae: 31.2351 - val_loss: 1468.7800 - val_mse: 1468.7800 - val_mae: 26.3324\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3137.9911 - mse: 3137.9912 - mae: 31.3580 - val_loss: 1471.7259 - val_mse: 1471.7260 - val_mae: 26.6907\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3338.1704 - mse: 3338.1697 - mae: 32.0126 - val_loss: 1465.9080 - val_mse: 1465.9080 - val_mae: 25.4774\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3243.8448 - mse: 3243.8447 - mae: 31.0020 - val_loss: 1467.5778 - val_mse: 1467.5778 - val_mae: 26.0926\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3230.9564 - mse: 3230.9565 - mae: 32.1706 - val_loss: 1467.8175 - val_mse: 1467.8175 - val_mae: 26.1843\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3213.0584 - mse: 3213.0588 - mae: 31.3370 - val_loss: 1467.0276 - val_mse: 1467.0276 - val_mae: 26.1367\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3243.2174 - mse: 3243.2173 - mae: 31.3621 - val_loss: 1466.2769 - val_mse: 1466.2767 - val_mae: 25.9083\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3154.1105 - mse: 3154.1106 - mae: 31.4948 - val_loss: 1466.1025 - val_mse: 1466.1024 - val_mae: 25.9434\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3267.3001 - mse: 3267.3003 - mae: 32.2890 - val_loss: 1465.6117 - val_mse: 1465.6118 - val_mae: 25.7351\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3293.6265 - mse: 3293.6265 - mae: 31.8878 - val_loss: 1468.8437 - val_mse: 1468.8438 - val_mae: 26.4023\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 528us/step - loss: 3301.9763 - mse: 3301.9766 - mae: 32.2573 - val_loss: 1466.4724 - val_mse: 1466.4724 - val_mae: 25.6605\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3395.6660 - mse: 3395.6658 - mae: 32.2110 - val_loss: 1466.1998 - val_mse: 1466.1997 - val_mae: 25.4468\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 482us/step - loss: 3329.1648 - mse: 3329.1643 - mae: 31.2401 - val_loss: 1466.2184 - val_mse: 1466.2184 - val_mae: 25.8937\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 498us/step - loss: 3266.5881 - mse: 3266.5884 - mae: 32.0669 - val_loss: 1465.9024 - val_mse: 1465.9025 - val_mae: 25.8082\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 527us/step - loss: 3212.6605 - mse: 3212.6606 - mae: 31.0030 - val_loss: 1471.1086 - val_mse: 1471.1084 - val_mae: 26.6162\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3254.2192 - mse: 3254.2195 - mae: 32.3122 - val_loss: 1467.6470 - val_mse: 1467.6471 - val_mae: 26.2031\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3283.5748 - mse: 3283.5745 - mae: 31.9467 - val_loss: 1466.4152 - val_mse: 1466.4153 - val_mae: 25.7430\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 494us/step - loss: 3256.9271 - mse: 3256.9280 - mae: 31.9941 - val_loss: 1467.0629 - val_mse: 1467.0629 - val_mae: 26.0560\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3241.4114 - mse: 3241.4121 - mae: 31.2004 - val_loss: 1465.9332 - val_mse: 1465.9331 - val_mae: 25.7857\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3194.7570 - mse: 3194.7571 - mae: 31.5647 - val_loss: 1467.9900 - val_mse: 1467.9900 - val_mae: 26.2833\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3207.6854 - mse: 3207.6846 - mae: 31.0428 - val_loss: 1465.6681 - val_mse: 1465.6681 - val_mae: 25.8324\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3262.1223 - mse: 3262.1223 - mae: 31.4123 - val_loss: 1468.1478 - val_mse: 1468.1477 - val_mae: 26.2914\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3276.7698 - mse: 3276.7700 - mae: 31.6478 - val_loss: 1469.4361 - val_mse: 1469.4362 - val_mae: 26.4922\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3353.3546 - mse: 3353.3547 - mae: 32.1992 - val_loss: 1468.3312 - val_mse: 1468.3311 - val_mae: 26.2666\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3279.5246 - mse: 3279.5244 - mae: 32.3125 - val_loss: 1467.5637 - val_mse: 1467.5635 - val_mae: 26.1608\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3275.6109 - mse: 3275.6106 - mae: 31.4338 - val_loss: 1466.0770 - val_mse: 1466.0770 - val_mae: 25.8858\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3146.7231 - mse: 3146.7236 - mae: 31.2097 - val_loss: 1466.3167 - val_mse: 1466.3169 - val_mae: 25.9208\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3350.5692 - mse: 3350.5688 - mae: 31.9705 - val_loss: 1466.3046 - val_mse: 1466.3047 - val_mae: 25.3733\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 677us/step - loss: 3213.2043 - mse: 3213.2043 - mae: 31.1213 - val_loss: 1466.2635 - val_mse: 1466.2635 - val_mae: 25.9392\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3277.2812 - mse: 3277.2812 - mae: 31.5474 - val_loss: 1465.7993 - val_mse: 1465.7993 - val_mae: 25.6415\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3195.6333 - mse: 3195.6328 - mae: 31.0705 - val_loss: 1467.9525 - val_mse: 1467.9524 - val_mae: 26.2531\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2943.5600 - mse: 2943.5605 - mae: 31.4281 - val_loss: 1070.3317 - val_mse: 1070.3317 - val_mae: 23.7863\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2895.8198 - mse: 2895.8196 - mae: 30.7067 - val_loss: 1067.7253 - val_mse: 1067.7253 - val_mae: 24.0314\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2922.5200 - mse: 2922.5200 - mae: 31.3539 - val_loss: 1069.9465 - val_mse: 1069.9464 - val_mae: 23.7162\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2896.4833 - mse: 2896.4827 - mae: 31.1521 - val_loss: 1066.2422 - val_mse: 1066.2423 - val_mae: 24.0148\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2907.3062 - mse: 2907.3071 - mae: 30.4833 - val_loss: 1065.4078 - val_mse: 1065.4078 - val_mae: 24.1378\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2821.2687 - mse: 2821.2690 - mae: 30.5202 - val_loss: 1064.9547 - val_mse: 1064.9548 - val_mae: 24.0793\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2916.9639 - mse: 2916.9631 - mae: 31.1960 - val_loss: 1069.3916 - val_mse: 1069.3915 - val_mae: 23.5715\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2902.7483 - mse: 2902.7476 - mae: 30.6007 - val_loss: 1066.6041 - val_mse: 1066.6041 - val_mae: 23.6929\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2850.2483 - mse: 2850.2488 - mae: 30.5340 - val_loss: 1063.3408 - val_mse: 1063.3408 - val_mae: 24.1066\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2826.4844 - mse: 2826.4844 - mae: 30.8582 - val_loss: 1062.5025 - val_mse: 1062.5026 - val_mae: 24.1095\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2864.2443 - mse: 2864.2437 - mae: 31.0462 - val_loss: 1063.2167 - val_mse: 1063.2167 - val_mae: 23.7962\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2863.2882 - mse: 2863.2874 - mae: 30.5486 - val_loss: 1061.1605 - val_mse: 1061.1604 - val_mae: 24.1819\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2949.1266 - mse: 2949.1270 - mae: 31.3539 - val_loss: 1063.2698 - val_mse: 1063.2697 - val_mae: 23.7137\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2803.8465 - mse: 2803.8459 - mae: 29.9534 - val_loss: 1060.4668 - val_mse: 1060.4667 - val_mae: 24.1533\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2943.0293 - mse: 2943.0298 - mae: 31.2050 - val_loss: 1062.9309 - val_mse: 1062.9309 - val_mae: 23.7015\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2884.3849 - mse: 2884.3843 - mae: 30.1880 - val_loss: 1059.9483 - val_mse: 1059.9482 - val_mae: 24.3784\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2903.7730 - mse: 2903.7732 - mae: 30.8468 - val_loss: 1060.1750 - val_mse: 1060.1749 - val_mae: 23.9262\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2891.6386 - mse: 2891.6375 - mae: 30.8237 - val_loss: 1059.5197 - val_mse: 1059.5199 - val_mae: 23.9949\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2939.1806 - mse: 2939.1804 - mae: 31.7206 - val_loss: 1060.9538 - val_mse: 1060.9537 - val_mae: 23.7279\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2857.4378 - mse: 2857.4380 - mae: 30.3992 - val_loss: 1058.5106 - val_mse: 1058.5105 - val_mae: 24.1008\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2859.6869 - mse: 2859.6870 - mae: 30.9866 - val_loss: 1062.4020 - val_mse: 1062.4020 - val_mae: 23.5991\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 516us/step - loss: 2843.8893 - mse: 2843.8894 - mae: 30.2391 - val_loss: 1058.1816 - val_mse: 1058.1816 - val_mae: 24.5600\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2901.6147 - mse: 2901.6150 - mae: 31.3208 - val_loss: 1057.3598 - val_mse: 1057.3599 - val_mae: 24.0815\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2874.0339 - mse: 2874.0330 - mae: 30.5314 - val_loss: 1056.0645 - val_mse: 1056.0645 - val_mae: 24.1922\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2862.4048 - mse: 2862.4045 - mae: 30.6065 - val_loss: 1060.9840 - val_mse: 1060.9840 - val_mae: 23.4875\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 689us/step - loss: 2903.0116 - mse: 2903.0117 - mae: 30.8640 - val_loss: 1058.5103 - val_mse: 1058.5103 - val_mae: 23.6140\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2811.4720 - mse: 2811.4729 - mae: 30.2507 - val_loss: 1054.5563 - val_mse: 1054.5563 - val_mae: 24.1783\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2957.3719 - mse: 2957.3716 - mae: 30.8137 - val_loss: 1055.7140 - val_mse: 1055.7139 - val_mae: 23.7940\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2876.4485 - mse: 2876.4482 - mae: 30.6574 - val_loss: 1053.9139 - val_mse: 1053.9138 - val_mae: 24.1245\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 513us/step - loss: 2853.4595 - mse: 2853.4600 - mae: 30.5561 - val_loss: 1054.4297 - val_mse: 1054.4296 - val_mae: 23.8749\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2894.2163 - mse: 2894.2156 - mae: 30.5703 - val_loss: 1054.2588 - val_mse: 1054.2587 - val_mae: 23.7378\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2919.4800 - mse: 2919.4797 - mae: 30.6190 - val_loss: 1054.1980 - val_mse: 1054.1980 - val_mae: 23.7711\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2901.7860 - mse: 2901.7849 - mae: 30.9230 - val_loss: 1056.4206 - val_mse: 1056.4205 - val_mae: 23.5196\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2874.9676 - mse: 2874.9675 - mae: 30.5228 - val_loss: 1052.0947 - val_mse: 1052.0948 - val_mae: 24.1842\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2818.6203 - mse: 2818.6208 - mae: 30.9553 - val_loss: 1053.8907 - val_mse: 1053.8906 - val_mae: 23.6617\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 525us/step - loss: 2806.2821 - mse: 2806.2825 - mae: 30.4594 - val_loss: 1049.5031 - val_mse: 1049.5032 - val_mae: 24.2746\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2871.1055 - mse: 2871.1052 - mae: 31.1031 - val_loss: 1051.8296 - val_mse: 1051.8296 - val_mae: 23.5540\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 634us/step - loss: 2876.6082 - mse: 2876.6079 - mae: 30.6565 - val_loss: 1047.9667 - val_mse: 1047.9666 - val_mae: 23.8242\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2842.4203 - mse: 2842.4207 - mae: 30.5630 - val_loss: 1047.7694 - val_mse: 1047.7694 - val_mae: 23.7327\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 666us/step - loss: 2882.3550 - mse: 2882.3557 - mae: 30.7645 - val_loss: 1045.7333 - val_mse: 1045.7333 - val_mae: 24.0795\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2768.3367 - mse: 2768.3367 - mae: 30.3098 - val_loss: 1045.2037 - val_mse: 1045.2036 - val_mae: 23.7983\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2891.8405 - mse: 2891.8406 - mae: 30.5819 - val_loss: 1047.2709 - val_mse: 1047.2708 - val_mae: 23.5165\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2831.2620 - mse: 2831.2615 - mae: 30.0127 - val_loss: 1044.3220 - val_mse: 1044.3219 - val_mae: 23.8324\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2789.0912 - mse: 2789.0913 - mae: 30.3061 - val_loss: 1042.3570 - val_mse: 1042.3569 - val_mae: 23.9155\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2845.6395 - mse: 2845.6396 - mae: 30.6392 - val_loss: 1042.8050 - val_mse: 1042.8049 - val_mae: 23.7581\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2886.2965 - mse: 2886.2966 - mae: 30.2673 - val_loss: 1041.9181 - val_mse: 1041.9180 - val_mae: 23.7106\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2899.3280 - mse: 2899.3279 - mae: 30.5270 - val_loss: 1041.8760 - val_mse: 1041.8760 - val_mae: 23.7063\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 664us/step - loss: 2904.9680 - mse: 2904.9678 - mae: 30.6123 - val_loss: 1041.7516 - val_mse: 1041.7517 - val_mae: 23.6900\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2774.2248 - mse: 2774.2246 - mae: 30.0873 - val_loss: 1040.9728 - val_mse: 1040.9728 - val_mae: 23.9075\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 530us/step - loss: 2899.0157 - mse: 2899.0154 - mae: 30.5166 - val_loss: 1039.6670 - val_mse: 1039.6671 - val_mae: 24.0882\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 479us/step - loss: 2863.3643 - mse: 2863.3640 - mae: 30.6367 - val_loss: 1039.4167 - val_mse: 1039.4167 - val_mae: 23.7080\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2877.9559 - mse: 2877.9558 - mae: 30.6429 - val_loss: 1040.6559 - val_mse: 1040.6560 - val_mae: 23.5470\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2842.5385 - mse: 2842.5388 - mae: 30.5246 - val_loss: 1038.3170 - val_mse: 1038.3169 - val_mae: 23.8294\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2841.4511 - mse: 2841.4512 - mae: 30.0630 - val_loss: 1041.6284 - val_mse: 1041.6284 - val_mae: 23.3908\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 529us/step - loss: 2814.3692 - mse: 2814.3689 - mae: 29.5084 - val_loss: 1035.4485 - val_mse: 1035.4485 - val_mae: 23.8184\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2819.3788 - mse: 2819.3777 - mae: 30.2571 - val_loss: 1033.9797 - val_mse: 1033.9796 - val_mae: 23.8337\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2857.5376 - mse: 2857.5376 - mae: 30.1966 - val_loss: 1034.6076 - val_mse: 1034.6074 - val_mae: 23.5341\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2758.0904 - mse: 2758.0916 - mae: 30.0272 - val_loss: 1032.3815 - val_mse: 1032.3813 - val_mae: 23.6585\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 634us/step - loss: 2817.4129 - mse: 2817.4133 - mae: 30.4314 - val_loss: 1032.0124 - val_mse: 1032.0123 - val_mae: 23.6211\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2850.1927 - mse: 2850.1931 - mae: 29.9225 - val_loss: 1029.9718 - val_mse: 1029.9718 - val_mae: 23.8711\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2870.2150 - mse: 2870.2148 - mae: 30.4375 - val_loss: 1029.4271 - val_mse: 1029.4271 - val_mae: 24.0494\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2797.4737 - mse: 2797.4744 - mae: 30.2928 - val_loss: 1029.0691 - val_mse: 1029.0691 - val_mae: 23.6807\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 553us/step - loss: 2829.0009 - mse: 2829.0000 - mae: 30.2226 - val_loss: 1027.5906 - val_mse: 1027.5906 - val_mae: 23.7122\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2889.8855 - mse: 2889.8857 - mae: 30.5588 - val_loss: 1028.4566 - val_mse: 1028.4567 - val_mae: 23.6294\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2813.1647 - mse: 2813.1646 - mae: 30.1310 - val_loss: 1026.9710 - val_mse: 1026.9709 - val_mae: 23.7797\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2778.9800 - mse: 2778.9800 - mae: 30.4719 - val_loss: 1026.8397 - val_mse: 1026.8395 - val_mae: 23.9082\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2726.8934 - mse: 2726.8938 - mae: 29.7899 - val_loss: 1026.5159 - val_mse: 1026.5159 - val_mae: 24.0295\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2792.3800 - mse: 2792.3811 - mae: 30.2399 - val_loss: 1026.9794 - val_mse: 1026.9794 - val_mae: 24.0524\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2773.2840 - mse: 2773.2844 - mae: 30.0654 - val_loss: 1025.8682 - val_mse: 1025.8683 - val_mae: 23.6545\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2783.3656 - mse: 2783.3665 - mae: 30.1423 - val_loss: 1026.3235 - val_mse: 1026.3235 - val_mae: 23.6109\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2823.7320 - mse: 2823.7317 - mae: 30.0845 - val_loss: 1027.5941 - val_mse: 1027.5941 - val_mae: 23.5402\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2812.7599 - mse: 2812.7590 - mae: 29.9209 - val_loss: 1024.9424 - val_mse: 1024.9425 - val_mae: 23.7759\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2779.2703 - mse: 2779.2708 - mae: 29.9644 - val_loss: 1026.2968 - val_mse: 1026.2970 - val_mae: 23.5548\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - ETA: 0s - loss: 2875.2409 - mse: 2875.2419 - mae: 30.41 - 1s 556us/step - loss: 2860.4485 - mse: 2860.4495 - mae: 30.4527 - val_loss: 1023.4844 - val_mse: 1023.4843 - val_mae: 23.7986\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2753.9386 - mse: 2753.9382 - mae: 29.5931 - val_loss: 1023.0671 - val_mse: 1023.0671 - val_mae: 23.7593\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2820.8594 - mse: 2820.8596 - mae: 30.2641 - val_loss: 1032.7100 - val_mse: 1032.7100 - val_mae: 22.9758\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 665us/step - loss: 2817.7820 - mse: 2817.7827 - mae: 29.6356 - val_loss: 1022.6287 - val_mse: 1022.6287 - val_mae: 23.7647\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2879.2519 - mse: 2879.2512 - mae: 30.4328 - val_loss: 1022.9111 - val_mse: 1022.9111 - val_mae: 23.6239\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2787.8607 - mse: 2787.8601 - mae: 29.8880 - val_loss: 1022.5305 - val_mse: 1022.5305 - val_mae: 23.6379\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2877.7651 - mse: 2877.7654 - mae: 30.4371 - val_loss: 1023.1989 - val_mse: 1023.1989 - val_mae: 23.4469\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2531.4454 - mse: 2531.4458 - mae: 29.8151 - val_loss: 1506.7903 - val_mse: 1506.7903 - val_mae: 27.1663\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2478.4397 - mse: 2478.4399 - mae: 29.3340 - val_loss: 1502.4490 - val_mse: 1502.4490 - val_mae: 27.2013\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2503.3023 - mse: 2503.3025 - mae: 29.0219 - val_loss: 1501.2745 - val_mse: 1501.2742 - val_mae: 27.1703\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2453.9002 - mse: 2453.9001 - mae: 28.9576 - val_loss: 1496.0632 - val_mse: 1496.0632 - val_mae: 27.2844\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 531us/step - loss: 2416.6376 - mse: 2416.6387 - mae: 29.0154 - val_loss: 1491.1996 - val_mse: 1491.1996 - val_mae: 27.3506\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2568.5907 - mse: 2568.5911 - mae: 29.8253 - val_loss: 1494.3591 - val_mse: 1494.3595 - val_mae: 27.1944\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2577.1096 - mse: 2577.1084 - mae: 29.4397 - val_loss: 1502.0977 - val_mse: 1502.0978 - val_mae: 26.9395\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2551.4697 - mse: 2551.4697 - mae: 29.7682 - val_loss: 1507.4712 - val_mse: 1507.4712 - val_mae: 26.7883\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 462us/step - loss: 2559.5971 - mse: 2559.5967 - mae: 29.4176 - val_loss: 1506.2324 - val_mse: 1506.2324 - val_mae: 26.7869\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2538.7592 - mse: 2538.7585 - mae: 29.4613 - val_loss: 1501.3538 - val_mse: 1501.3538 - val_mae: 26.8816\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2488.1752 - mse: 2488.1748 - mae: 29.2512 - val_loss: 1481.0764 - val_mse: 1481.0764 - val_mae: 27.4787\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 667us/step - loss: 2466.2926 - mse: 2466.2927 - mae: 29.3124 - val_loss: 1486.5713 - val_mse: 1486.5712 - val_mae: 27.2333\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2552.2562 - mse: 2552.2563 - mae: 29.3734 - val_loss: 1489.4269 - val_mse: 1489.4270 - val_mae: 27.0787\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2528.5306 - mse: 2528.5308 - mae: 29.5505 - val_loss: 1498.3272 - val_mse: 1498.3274 - val_mae: 26.8307\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2515.6897 - mse: 2515.6899 - mae: 29.4605 - val_loss: 1483.0930 - val_mse: 1483.0931 - val_mae: 27.2407\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 549us/step - loss: 2526.3804 - mse: 2526.3801 - mae: 29.5771 - val_loss: 1489.4423 - val_mse: 1489.4423 - val_mae: 26.9913\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 496us/step - loss: 2498.4774 - mse: 2498.4780 - mae: 29.1629 - val_loss: 1488.0178 - val_mse: 1488.0177 - val_mae: 26.9840\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2475.7367 - mse: 2475.7361 - mae: 29.2575 - val_loss: 1491.6835 - val_mse: 1491.6832 - val_mae: 26.8735\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2494.2894 - mse: 2494.2893 - mae: 29.3895 - val_loss: 1484.4096 - val_mse: 1484.4099 - val_mae: 27.0223\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2453.9004 - mse: 2453.9004 - mae: 29.0371 - val_loss: 1482.2265 - val_mse: 1482.2266 - val_mae: 27.0504\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2550.6951 - mse: 2550.6941 - mae: 29.4993 - val_loss: 1491.5010 - val_mse: 1491.5009 - val_mae: 26.7754\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2486.7805 - mse: 2486.7810 - mae: 29.7125 - val_loss: 1488.3962 - val_mse: 1488.3962 - val_mae: 26.8235\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2495.6994 - mse: 2495.6987 - mae: 29.1007 - val_loss: 1481.4192 - val_mse: 1481.4192 - val_mae: 27.0305\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2482.1802 - mse: 2482.1794 - mae: 29.4267 - val_loss: 1482.9450 - val_mse: 1482.9451 - val_mae: 27.0008\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2499.2055 - mse: 2499.2053 - mae: 29.0797 - val_loss: 1485.2644 - val_mse: 1485.2644 - val_mae: 26.8932\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2513.1953 - mse: 2513.1953 - mae: 29.2473 - val_loss: 1479.6763 - val_mse: 1479.6761 - val_mae: 27.0742\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2460.2779 - mse: 2460.2791 - mae: 29.2896 - val_loss: 1485.9042 - val_mse: 1485.9044 - val_mae: 26.8470\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2502.0312 - mse: 2502.0310 - mae: 29.4075 - val_loss: 1491.0614 - val_mse: 1491.0613 - val_mae: 26.7145\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2579.3072 - mse: 2579.3064 - mae: 29.5864 - val_loss: 1488.4776 - val_mse: 1488.4775 - val_mae: 26.8352\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 500us/step - loss: 2491.4139 - mse: 2491.4143 - mae: 29.1236 - val_loss: 1472.3867 - val_mse: 1472.3867 - val_mae: 27.3190\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2515.4720 - mse: 2515.4717 - mae: 29.2488 - val_loss: 1491.6322 - val_mse: 1491.6323 - val_mae: 26.6885\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2450.9455 - mse: 2450.9456 - mae: 28.8024 - val_loss: 1476.3840 - val_mse: 1476.3838 - val_mae: 27.1427\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2523.0126 - mse: 2523.0125 - mae: 29.1347 - val_loss: 1479.7799 - val_mse: 1479.7798 - val_mae: 27.0470\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 663us/step - loss: 2479.9222 - mse: 2479.9226 - mae: 29.4627 - val_loss: 1476.4719 - val_mse: 1476.4717 - val_mae: 27.1530\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2520.6215 - mse: 2520.6216 - mae: 29.6940 - val_loss: 1479.7548 - val_mse: 1479.7549 - val_mae: 27.0275\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2509.6897 - mse: 2509.6892 - mae: 29.2270 - val_loss: 1482.4945 - val_mse: 1482.4946 - val_mae: 26.9028\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2503.6048 - mse: 2503.6057 - mae: 29.3371 - val_loss: 1484.9836 - val_mse: 1484.9836 - val_mae: 26.8463\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2505.6447 - mse: 2505.6450 - mae: 29.1809 - val_loss: 1472.4662 - val_mse: 1472.4663 - val_mae: 27.2671\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2497.1600 - mse: 2497.1604 - mae: 29.3084 - val_loss: 1486.6697 - val_mse: 1486.6697 - val_mae: 26.7427\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 677us/step - loss: 2501.6393 - mse: 2501.6396 - mae: 29.0710 - val_loss: 1482.8327 - val_mse: 1482.8329 - val_mae: 26.8411\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 561us/step - loss: 2447.0667 - mse: 2447.0669 - mae: 29.2431 - val_loss: 1481.6312 - val_mse: 1481.6313 - val_mae: 26.9040\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 544us/step - loss: 2512.4157 - mse: 2512.4158 - mae: 29.3763 - val_loss: 1474.2558 - val_mse: 1474.2559 - val_mae: 27.1390\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2549.0216 - mse: 2549.0215 - mae: 29.1288 - val_loss: 1478.6486 - val_mse: 1478.6484 - val_mae: 27.0163\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 521us/step - loss: 2481.9893 - mse: 2481.9885 - mae: 29.4457 - val_loss: 1482.8439 - val_mse: 1482.8439 - val_mae: 26.8874\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 499us/step - loss: 2452.3735 - mse: 2452.3738 - mae: 28.9660 - val_loss: 1477.6129 - val_mse: 1477.6129 - val_mae: 27.0364\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2464.7869 - mse: 2464.7866 - mae: 29.0324 - val_loss: 1471.4184 - val_mse: 1471.4186 - val_mae: 27.2343\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 693us/step - loss: 2457.9505 - mse: 2457.9507 - mae: 28.9079 - val_loss: 1477.6742 - val_mse: 1477.6741 - val_mae: 26.9612\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2499.9475 - mse: 2499.9487 - mae: 29.3665 - val_loss: 1467.2265 - val_mse: 1467.2266 - val_mae: 27.3053\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 531us/step - loss: 2529.6165 - mse: 2529.6165 - mae: 29.0528 - val_loss: 1479.7819 - val_mse: 1479.7819 - val_mae: 26.8037\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2546.2039 - mse: 2546.2046 - mae: 29.0617 - val_loss: 1474.8229 - val_mse: 1474.8229 - val_mae: 27.0243\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2517.1638 - mse: 2517.1643 - mae: 29.4394 - val_loss: 1478.0308 - val_mse: 1478.0308 - val_mae: 26.9052\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2481.2522 - mse: 2481.2529 - mae: 29.0825 - val_loss: 1490.0125 - val_mse: 1490.0125 - val_mae: 26.6037\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2492.5885 - mse: 2492.5884 - mae: 28.9891 - val_loss: 1475.7508 - val_mse: 1475.7507 - val_mae: 27.0096\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2517.3632 - mse: 2517.3628 - mae: 29.4431 - val_loss: 1483.0033 - val_mse: 1483.0033 - val_mae: 26.7407\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2493.2776 - mse: 2493.2776 - mae: 28.7692 - val_loss: 1470.1244 - val_mse: 1470.1243 - val_mae: 27.1329\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 519us/step - loss: 2509.9166 - mse: 2509.9160 - mae: 29.5804 - val_loss: 1474.6111 - val_mse: 1474.6112 - val_mae: 26.9528\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2483.6587 - mse: 2483.6582 - mae: 29.2291 - val_loss: 1473.2663 - val_mse: 1473.2662 - val_mae: 26.9871\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2494.6497 - mse: 2494.6506 - mae: 28.6834 - val_loss: 1481.1347 - val_mse: 1481.1346 - val_mae: 26.7236\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2473.9256 - mse: 2473.9258 - mae: 29.0717 - val_loss: 1482.0749 - val_mse: 1482.0747 - val_mae: 26.6936\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2434.3985 - mse: 2434.3989 - mae: 28.8163 - val_loss: 1472.0626 - val_mse: 1472.0625 - val_mae: 26.9515\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2513.9272 - mse: 2513.9272 - mae: 28.8377 - val_loss: 1478.7770 - val_mse: 1478.7770 - val_mae: 26.8038\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2478.8055 - mse: 2478.8062 - mae: 29.0045 - val_loss: 1475.6480 - val_mse: 1475.6481 - val_mae: 26.9057\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2511.3379 - mse: 2511.3379 - mae: 29.0405 - val_loss: 1480.2550 - val_mse: 1480.2550 - val_mae: 26.7110\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2432.8740 - mse: 2432.8748 - mae: 28.9179 - val_loss: 1464.7671 - val_mse: 1464.7672 - val_mae: 27.2197\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2502.5939 - mse: 2502.5945 - mae: 29.3348 - val_loss: 1482.0946 - val_mse: 1482.0947 - val_mae: 26.6324\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2439.4397 - mse: 2439.4390 - mae: 28.7123 - val_loss: 1470.3686 - val_mse: 1470.3685 - val_mae: 26.9191\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2472.8773 - mse: 2472.8770 - mae: 29.0994 - val_loss: 1482.4076 - val_mse: 1482.4073 - val_mae: 26.5958\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2470.2535 - mse: 2470.2537 - mae: 29.1171 - val_loss: 1471.8340 - val_mse: 1471.8339 - val_mae: 26.9123\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2507.3311 - mse: 2507.3320 - mae: 29.4253 - val_loss: 1472.9414 - val_mse: 1472.9413 - val_mae: 26.8034\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 666us/step - loss: 2458.2571 - mse: 2458.2563 - mae: 28.8379 - val_loss: 1473.1817 - val_mse: 1473.1816 - val_mae: 26.8379\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2501.5587 - mse: 2501.5593 - mae: 28.7728 - val_loss: 1480.9773 - val_mse: 1480.9773 - val_mae: 26.6240\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2422.5699 - mse: 2422.5701 - mae: 28.3495 - val_loss: 1467.6449 - val_mse: 1467.6449 - val_mae: 27.0188\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2422.4862 - mse: 2422.4871 - mae: 28.5501 - val_loss: 1474.0464 - val_mse: 1474.0463 - val_mae: 26.7526\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2428.3322 - mse: 2428.3325 - mae: 28.6866 - val_loss: 1476.8965 - val_mse: 1476.8967 - val_mae: 26.6997\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2431.2029 - mse: 2431.2029 - mae: 28.5707 - val_loss: 1475.6788 - val_mse: 1475.6790 - val_mae: 26.7416\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2424.8250 - mse: 2424.8252 - mae: 28.7653 - val_loss: 1472.5628 - val_mse: 1472.5627 - val_mae: 26.8894\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2368.3600 - mse: 2368.3608 - mae: 28.7256 - val_loss: 1468.6468 - val_mse: 1468.6470 - val_mae: 26.9845\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2479.1071 - mse: 2479.1069 - mae: 28.7203 - val_loss: 1472.1415 - val_mse: 1472.1412 - val_mae: 26.8536\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2470.8970 - mse: 2470.8970 - mae: 29.1787 - val_loss: 1476.9860 - val_mse: 1476.9860 - val_mae: 26.7704\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2431.8527 - mse: 2431.8528 - mae: 28.8404 - val_loss: 1474.6425 - val_mse: 1474.6427 - val_mae: 26.7817\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2316.8632 - mse: 2316.8625 - mae: 29.0157 - val_loss: 3689.2981 - val_mse: 3689.2986 - val_mae: 23.7908\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2408.0404 - mse: 2408.0403 - mae: 29.6144 - val_loss: 3688.9685 - val_mse: 3688.9683 - val_mae: 24.0409\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2353.8682 - mse: 2353.8679 - mae: 29.5896 - val_loss: 3689.5139 - val_mse: 3689.5134 - val_mae: 23.9167\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2343.1733 - mse: 2343.1726 - mae: 29.5003 - val_loss: 3688.6872 - val_mse: 3688.6877 - val_mae: 24.0774\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2312.2053 - mse: 2312.2058 - mae: 28.8559 - val_loss: 3689.5271 - val_mse: 3689.5264 - val_mae: 24.3384\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2301.3882 - mse: 2301.3879 - mae: 29.3306 - val_loss: 3689.8158 - val_mse: 3689.8157 - val_mae: 24.0597\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2370.3940 - mse: 2370.3938 - mae: 29.1668 - val_loss: 3690.4253 - val_mse: 3690.4243 - val_mae: 23.9546\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - ETA: 0s - loss: 2325.8815 - mse: 2325.8818 - mae: 29.00 - 2s 599us/step - loss: 2336.5056 - mse: 2336.5059 - mae: 29.1978 - val_loss: 3691.5060 - val_mse: 3691.5056 - val_mae: 24.1170\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 502us/step - loss: 2410.4656 - mse: 2410.4658 - mae: 29.5033 - val_loss: 3689.3155 - val_mse: 3689.3159 - val_mae: 23.7521\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2441.8510 - mse: 2441.8513 - mae: 29.5975 - val_loss: 3689.2454 - val_mse: 3689.2444 - val_mae: 23.9008\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2372.6973 - mse: 2372.6963 - mae: 29.0781 - val_loss: 3690.9250 - val_mse: 3690.9248 - val_mae: 24.1402\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2342.7894 - mse: 2342.7896 - mae: 29.2061 - val_loss: 3690.8132 - val_mse: 3690.8132 - val_mae: 23.7402\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2397.5372 - mse: 2397.5366 - mae: 29.3657 - val_loss: 3691.0321 - val_mse: 3691.0322 - val_mae: 23.7306\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2350.2126 - mse: 2350.2126 - mae: 28.9370 - val_loss: 3692.7408 - val_mse: 3692.7410 - val_mae: 24.1491\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2371.7154 - mse: 2371.7156 - mae: 29.1657 - val_loss: 3691.8990 - val_mse: 3691.8979 - val_mae: 23.5134\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2347.7751 - mse: 2347.7747 - mae: 28.9869 - val_loss: 3692.7823 - val_mse: 3692.7827 - val_mae: 23.3157\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2338.3659 - mse: 2338.3662 - mae: 29.1204 - val_loss: 3693.4758 - val_mse: 3693.4756 - val_mae: 24.2519\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2299.2018 - mse: 2299.2021 - mae: 29.0889 - val_loss: 3693.7572 - val_mse: 3693.7571 - val_mae: 24.1856\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2365.8781 - mse: 2365.8779 - mae: 29.4417 - val_loss: 3692.5605 - val_mse: 3692.5598 - val_mae: 23.9136\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2379.4499 - mse: 2379.4500 - mae: 29.6810 - val_loss: 3691.0603 - val_mse: 3691.0603 - val_mae: 23.7801\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2308.5225 - mse: 2308.5222 - mae: 28.8679 - val_loss: 3690.6144 - val_mse: 3690.6138 - val_mae: 23.8543\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2344.4902 - mse: 2344.4900 - mae: 29.2444 - val_loss: 3694.2819 - val_mse: 3694.2822 - val_mae: 24.4424\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2342.8369 - mse: 2342.8369 - mae: 29.4049 - val_loss: 3694.6918 - val_mse: 3694.6924 - val_mae: 24.2184\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2304.8329 - mse: 2304.8335 - mae: 28.9529 - val_loss: 3693.1344 - val_mse: 3693.1348 - val_mae: 24.0929\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2234.4073 - mse: 2234.4072 - mae: 28.9182 - val_loss: 3692.5262 - val_mse: 3692.5269 - val_mae: 24.2904\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2351.0333 - mse: 2351.0332 - mae: 29.2676 - val_loss: 3691.3218 - val_mse: 3691.3220 - val_mae: 23.9191\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 529us/step - loss: 2309.3132 - mse: 2309.3125 - mae: 28.8644 - val_loss: 3692.1542 - val_mse: 3692.1541 - val_mae: 24.1418\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2377.2292 - mse: 2377.2290 - mae: 29.4742 - val_loss: 3689.7713 - val_mse: 3689.7725 - val_mae: 23.4580\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2337.4703 - mse: 2337.4700 - mae: 29.1290 - val_loss: 3690.3329 - val_mse: 3690.3333 - val_mae: 23.9269\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2329.1553 - mse: 2329.1548 - mae: 29.2013 - val_loss: 3690.4680 - val_mse: 3690.4673 - val_mae: 23.9145\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2338.2119 - mse: 2338.2114 - mae: 29.3566 - val_loss: 3690.2872 - val_mse: 3690.2878 - val_mae: 23.9646\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2341.2603 - mse: 2341.2603 - mae: 29.2450 - val_loss: 3690.9391 - val_mse: 3690.9392 - val_mae: 23.8976\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2380.5412 - mse: 2380.5408 - mae: 29.4767 - val_loss: 3689.0991 - val_mse: 3689.0994 - val_mae: 23.6553\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2355.1496 - mse: 2355.1492 - mae: 29.0202 - val_loss: 3688.7273 - val_mse: 3688.7275 - val_mae: 23.8034\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2348.9629 - mse: 2348.9636 - mae: 29.4024 - val_loss: 3689.4756 - val_mse: 3689.4766 - val_mae: 23.5949\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2306.0611 - mse: 2306.0608 - mae: 28.8497 - val_loss: 3690.2600 - val_mse: 3690.2600 - val_mae: 23.5974\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2353.0013 - mse: 2353.0012 - mae: 29.1486 - val_loss: 3689.6611 - val_mse: 3689.6616 - val_mae: 23.6357\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2349.7765 - mse: 2349.7761 - mae: 28.9146 - val_loss: 3689.0207 - val_mse: 3689.0205 - val_mae: 23.6074\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2361.5706 - mse: 2361.5708 - mae: 28.9545 - val_loss: 3688.8616 - val_mse: 3688.8625 - val_mae: 23.4406\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2348.6192 - mse: 2348.6196 - mae: 29.2030 - val_loss: 3689.2703 - val_mse: 3689.2705 - val_mae: 23.9397\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2338.5004 - mse: 2338.5012 - mae: 29.0251 - val_loss: 3689.0538 - val_mse: 3689.0525 - val_mae: 24.0296\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2370.1943 - mse: 2370.1951 - mae: 29.1798 - val_loss: 3687.4195 - val_mse: 3687.4197 - val_mae: 23.5295\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2278.2588 - mse: 2278.2593 - mae: 28.4466 - val_loss: 3688.0430 - val_mse: 3688.0432 - val_mae: 24.2897\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2344.5336 - mse: 2344.5332 - mae: 29.1058 - val_loss: 3686.1548 - val_mse: 3686.1550 - val_mae: 23.7845\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2296.2734 - mse: 2296.2739 - mae: 28.6245 - val_loss: 3686.1929 - val_mse: 3686.1931 - val_mae: 23.7313\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2323.2311 - mse: 2323.2312 - mae: 29.1441 - val_loss: 3686.3833 - val_mse: 3686.3833 - val_mae: 23.5241\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2333.4165 - mse: 2333.4167 - mae: 28.7131 - val_loss: 3685.4222 - val_mse: 3685.4221 - val_mae: 23.6517\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2295.3656 - mse: 2295.3660 - mae: 28.6408 - val_loss: 3687.8138 - val_mse: 3687.8135 - val_mae: 23.9316\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2342.3261 - mse: 2342.3264 - mae: 29.3787 - val_loss: 3686.2713 - val_mse: 3686.2708 - val_mae: 23.8898\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2289.6594 - mse: 2289.6592 - mae: 28.6791 - val_loss: 3686.0557 - val_mse: 3686.0559 - val_mae: 23.9604\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 534us/step - loss: 2290.4564 - mse: 2290.4570 - mae: 28.8152 - val_loss: 3684.5060 - val_mse: 3684.5071 - val_mae: 23.8210\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 681us/step - loss: 2351.7385 - mse: 2351.7385 - mae: 29.4127 - val_loss: 3683.1018 - val_mse: 3683.1028 - val_mae: 23.6602\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2368.7487 - mse: 2368.7483 - mae: 29.1203 - val_loss: 3684.1266 - val_mse: 3684.1270 - val_mae: 24.0662\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2294.4383 - mse: 2294.4380 - mae: 28.9409 - val_loss: 3682.3671 - val_mse: 3682.3667 - val_mae: 23.6473\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2284.1385 - mse: 2284.1379 - mae: 28.6576 - val_loss: 3683.1855 - val_mse: 3683.1865 - val_mae: 23.9909\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2338.2843 - mse: 2338.2842 - mae: 28.7821 - val_loss: 3685.1374 - val_mse: 3685.1370 - val_mae: 24.1424\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2309.6005 - mse: 2309.6018 - mae: 28.9419 - val_loss: 3686.0554 - val_mse: 3686.0552 - val_mae: 23.6232\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2327.3531 - mse: 2327.3530 - mae: 28.8411 - val_loss: 3687.5859 - val_mse: 3687.5864 - val_mae: 23.5922\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2303.6860 - mse: 2303.6848 - mae: 28.9637 - val_loss: 3687.8310 - val_mse: 3687.8311 - val_mae: 24.0775\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 685us/step - loss: 2353.2107 - mse: 2353.2104 - mae: 29.3698 - val_loss: 3688.2817 - val_mse: 3688.2810 - val_mae: 24.2000\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2305.6365 - mse: 2305.6365 - mae: 29.0115 - val_loss: 3690.5506 - val_mse: 3690.5505 - val_mae: 24.4266\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2292.5750 - mse: 2292.5750 - mae: 28.9832 - val_loss: 3690.4201 - val_mse: 3690.4204 - val_mae: 24.4823\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2329.5148 - mse: 2329.5146 - mae: 28.9414 - val_loss: 3687.4627 - val_mse: 3687.4622 - val_mae: 24.0200\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2323.1646 - mse: 2323.1646 - mae: 28.9078 - val_loss: 3688.8661 - val_mse: 3688.8652 - val_mae: 24.1242\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2356.5605 - mse: 2356.5620 - mae: 29.0616 - val_loss: 3687.4891 - val_mse: 3687.4893 - val_mae: 23.8362\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2267.4339 - mse: 2267.4343 - mae: 28.5212 - val_loss: 3689.1252 - val_mse: 3689.1257 - val_mae: 24.1129\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2336.7578 - mse: 2336.7578 - mae: 29.0475 - val_loss: 3690.0052 - val_mse: 3690.0054 - val_mae: 24.2814\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2319.0123 - mse: 2319.0120 - mae: 29.0828 - val_loss: 3686.3158 - val_mse: 3686.3157 - val_mae: 23.9244\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2323.7762 - mse: 2323.7759 - mae: 28.9480 - val_loss: 3684.8730 - val_mse: 3684.8726 - val_mae: 23.8705\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2356.4438 - mse: 2356.4443 - mae: 29.2451 - val_loss: 3685.2553 - val_mse: 3685.2554 - val_mae: 24.1088\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2305.1114 - mse: 2305.1113 - mae: 28.8912 - val_loss: 3684.3975 - val_mse: 3684.3982 - val_mae: 23.9655\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 522us/step - loss: 2278.8997 - mse: 2278.8999 - mae: 28.8801 - val_loss: 3685.6437 - val_mse: 3685.6438 - val_mae: 23.7853\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 564us/step - loss: 2303.5100 - mse: 2303.5103 - mae: 28.6238 - val_loss: 3688.8755 - val_mse: 3688.8755 - val_mae: 24.4040\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2309.7458 - mse: 2309.7454 - mae: 29.0138 - val_loss: 3686.3804 - val_mse: 3686.3806 - val_mae: 23.5891\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2333.0540 - mse: 2333.0532 - mae: 28.9171 - val_loss: 3685.5129 - val_mse: 3685.5137 - val_mae: 24.1394\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2313.7711 - mse: 2313.7705 - mae: 28.9721 - val_loss: 3686.3240 - val_mse: 3686.3237 - val_mae: 24.0370\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2343.5906 - mse: 2343.5903 - mae: 28.8737 - val_loss: 3685.9052 - val_mse: 3685.9060 - val_mae: 24.0369\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2303.9185 - mse: 2303.9189 - mae: 29.1628 - val_loss: 3684.2394 - val_mse: 3684.2400 - val_mae: 23.8944\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2300.3145 - mse: 2300.3145 - mae: 28.8574 - val_loss: 3684.4375 - val_mse: 3684.4368 - val_mae: 23.8044\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2360.2005 - mse: 2360.1997 - mae: 29.2946 - val_loss: 3685.3652 - val_mse: 3685.3647 - val_mae: 24.2755\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2724.4299 - mse: 2724.4299 - mae: 28.4630 - val_loss: 2212.9636 - val_mse: 2212.9634 - val_mae: 26.3808\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2731.7464 - mse: 2731.7466 - mae: 28.3709 - val_loss: 2209.6602 - val_mse: 2209.6602 - val_mae: 26.3772\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2725.1891 - mse: 2725.1892 - mae: 28.3230 - val_loss: 2201.3959 - val_mse: 2201.3960 - val_mae: 26.4880\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2630.7650 - mse: 2630.7646 - mae: 28.1155 - val_loss: 2194.0669 - val_mse: 2194.0669 - val_mae: 26.7225\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2726.6757 - mse: 2726.6760 - mae: 28.5666 - val_loss: 2217.0317 - val_mse: 2217.0317 - val_mae: 26.5345\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 531us/step - loss: 2694.9927 - mse: 2694.9932 - mae: 28.6493 - val_loss: 2214.8570 - val_mse: 2214.8569 - val_mae: 26.6134\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2742.6562 - mse: 2742.6560 - mae: 28.7132 - val_loss: 2226.0523 - val_mse: 2226.0525 - val_mae: 26.4278\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2672.9965 - mse: 2672.9961 - mae: 28.1504 - val_loss: 2208.0107 - val_mse: 2208.0105 - val_mae: 26.9170\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2715.0097 - mse: 2715.0098 - mae: 28.3984 - val_loss: 2222.4457 - val_mse: 2222.4456 - val_mae: 26.3514\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2723.8946 - mse: 2723.8953 - mae: 28.4504 - val_loss: 2206.5305 - val_mse: 2206.5303 - val_mae: 26.9987\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2710.9962 - mse: 2710.9963 - mae: 28.6608 - val_loss: 2217.0071 - val_mse: 2217.0076 - val_mae: 26.7724\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2667.1354 - mse: 2667.1355 - mae: 28.2192 - val_loss: 2201.3934 - val_mse: 2201.3933 - val_mae: 27.0625\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2715.7509 - mse: 2715.7507 - mae: 28.2664 - val_loss: 2226.3217 - val_mse: 2226.3215 - val_mae: 26.4516\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2708.0335 - mse: 2708.0332 - mae: 28.3615 - val_loss: 2221.0546 - val_mse: 2221.0547 - val_mae: 26.6649\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2670.9202 - mse: 2670.9199 - mae: 28.0193 - val_loss: 2208.8186 - val_mse: 2208.8184 - val_mae: 26.9696\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 665us/step - loss: 2697.8671 - mse: 2697.8669 - mae: 28.4476 - val_loss: 2215.4764 - val_mse: 2215.4763 - val_mae: 26.8966\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 671us/step - loss: 2655.6581 - mse: 2655.6582 - mae: 28.1921 - val_loss: 2208.2816 - val_mse: 2208.2815 - val_mae: 26.9003\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2701.7667 - mse: 2701.7666 - mae: 28.4282 - val_loss: 2218.7750 - val_mse: 2218.7749 - val_mae: 26.5283\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2625.0683 - mse: 2625.0691 - mae: 27.9961 - val_loss: 2208.4170 - val_mse: 2208.4170 - val_mae: 26.8184\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2672.5853 - mse: 2672.5842 - mae: 28.4921 - val_loss: 2195.2271 - val_mse: 2195.2273 - val_mae: 26.9769\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2716.7851 - mse: 2716.7852 - mae: 28.3914 - val_loss: 2212.1176 - val_mse: 2212.1179 - val_mae: 26.6649\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2689.8090 - mse: 2689.8098 - mae: 28.2364 - val_loss: 2221.4846 - val_mse: 2221.4846 - val_mae: 26.6029\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2696.1293 - mse: 2696.1306 - mae: 28.5171 - val_loss: 2207.9287 - val_mse: 2207.9290 - val_mae: 27.1665\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2709.4243 - mse: 2709.4243 - mae: 28.4283 - val_loss: 2221.7137 - val_mse: 2221.7139 - val_mae: 26.5213\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2681.9083 - mse: 2681.9082 - mae: 28.1121 - val_loss: 2209.8275 - val_mse: 2209.8276 - val_mae: 27.1703\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2694.9944 - mse: 2694.9934 - mae: 28.5468 - val_loss: 2216.9401 - val_mse: 2216.9402 - val_mae: 26.7519\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2691.6037 - mse: 2691.6033 - mae: 28.2248 - val_loss: 2225.4031 - val_mse: 2225.4028 - val_mae: 26.6113\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2709.3013 - mse: 2709.3000 - mae: 28.3347 - val_loss: 2232.0703 - val_mse: 2232.0706 - val_mae: 26.3413\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2668.3391 - mse: 2668.3398 - mae: 28.1352 - val_loss: 2208.8284 - val_mse: 2208.8289 - val_mae: 27.0996\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2674.0227 - mse: 2674.0239 - mae: 28.2010 - val_loss: 2210.2080 - val_mse: 2210.2083 - val_mae: 26.7163\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 554us/step - loss: 2685.9796 - mse: 2685.9802 - mae: 28.3297 - val_loss: 2218.9024 - val_mse: 2218.9026 - val_mae: 26.5524\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2658.3377 - mse: 2658.3369 - mae: 28.4222 - val_loss: 2198.1062 - val_mse: 2198.1060 - val_mae: 26.9767\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2699.4045 - mse: 2699.4053 - mae: 28.3191 - val_loss: 2212.7161 - val_mse: 2212.7166 - val_mae: 26.3412\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2676.5304 - mse: 2676.5303 - mae: 28.0167 - val_loss: 2212.7687 - val_mse: 2212.7690 - val_mae: 26.6739\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2685.9036 - mse: 2685.9033 - mae: 28.1705 - val_loss: 2225.4147 - val_mse: 2225.4150 - val_mae: 26.5147\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2649.4961 - mse: 2649.4958 - mae: 28.2690 - val_loss: 2204.5366 - val_mse: 2204.5364 - val_mae: 26.9407\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2655.5510 - mse: 2655.5515 - mae: 28.4979 - val_loss: 2206.2344 - val_mse: 2206.2346 - val_mae: 26.6865\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2675.5025 - mse: 2675.5015 - mae: 28.1669 - val_loss: 2205.9458 - val_mse: 2205.9458 - val_mae: 27.2042\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 552us/step - loss: 2696.9942 - mse: 2696.9939 - mae: 28.4453 - val_loss: 2225.5580 - val_mse: 2225.5583 - val_mae: 26.6256\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2654.2768 - mse: 2654.2769 - mae: 27.7930 - val_loss: 2226.2468 - val_mse: 2226.2466 - val_mae: 26.6179\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2672.7884 - mse: 2672.7878 - mae: 28.2077 - val_loss: 2219.4142 - val_mse: 2219.4143 - val_mae: 26.5662\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2651.3188 - mse: 2651.3206 - mae: 28.0224 - val_loss: 2212.3074 - val_mse: 2212.3074 - val_mae: 26.7170\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 512us/step - loss: 2662.3445 - mse: 2662.3447 - mae: 28.0124 - val_loss: 2217.4348 - val_mse: 2217.4348 - val_mae: 26.4768\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2693.5216 - mse: 2693.5220 - mae: 28.3734 - val_loss: 2209.3511 - val_mse: 2209.3508 - val_mae: 26.8699\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2744.2810 - mse: 2744.2810 - mae: 28.4422 - val_loss: 2232.4835 - val_mse: 2232.4834 - val_mae: 26.2477\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 472us/step - loss: 2705.1104 - mse: 2705.1104 - mae: 28.1234 - val_loss: 2207.9965 - val_mse: 2207.9966 - val_mae: 26.6055\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 522us/step - loss: 2682.8346 - mse: 2682.8345 - mae: 28.4227 - val_loss: 2220.1152 - val_mse: 2220.1150 - val_mae: 26.4703\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2711.7635 - mse: 2711.7637 - mae: 28.4602 - val_loss: 2222.3063 - val_mse: 2222.3069 - val_mae: 26.9844\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2661.7807 - mse: 2661.7803 - mae: 27.9472 - val_loss: 2208.2059 - val_mse: 2208.2061 - val_mae: 27.1075\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2674.1328 - mse: 2674.1326 - mae: 28.4502 - val_loss: 2214.1391 - val_mse: 2214.1394 - val_mae: 26.7042\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 511us/step - loss: 2682.5577 - mse: 2682.5581 - mae: 28.0705 - val_loss: 2234.0566 - val_mse: 2234.0566 - val_mae: 26.2949\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2676.6675 - mse: 2676.6667 - mae: 28.1122 - val_loss: 2228.8665 - val_mse: 2228.8667 - val_mae: 26.5377\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2699.8212 - mse: 2699.8210 - mae: 28.1546 - val_loss: 2221.9806 - val_mse: 2221.9800 - val_mae: 26.8245\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2720.6152 - mse: 2720.6160 - mae: 28.3949 - val_loss: 2224.1451 - val_mse: 2224.1453 - val_mae: 26.6601\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2708.4944 - mse: 2708.4944 - mae: 28.4656 - val_loss: 2222.4740 - val_mse: 2222.4741 - val_mae: 26.7973\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2678.0296 - mse: 2678.0305 - mae: 28.1127 - val_loss: 2227.1114 - val_mse: 2227.1113 - val_mae: 26.7081\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2664.6216 - mse: 2664.6208 - mae: 28.2897 - val_loss: 2224.5805 - val_mse: 2224.5803 - val_mae: 26.8840\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2629.8640 - mse: 2629.8635 - mae: 28.1405 - val_loss: 2208.4804 - val_mse: 2208.4805 - val_mae: 27.1813\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2694.2516 - mse: 2694.2517 - mae: 28.4759 - val_loss: 2211.7949 - val_mse: 2211.7949 - val_mae: 26.8811\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2661.9708 - mse: 2661.9709 - mae: 28.1887 - val_loss: 2222.6069 - val_mse: 2222.6067 - val_mae: 26.8088\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2643.5351 - mse: 2643.5352 - mae: 28.0780 - val_loss: 2220.7640 - val_mse: 2220.7639 - val_mae: 26.4443\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2699.7733 - mse: 2699.7732 - mae: 28.1688 - val_loss: 2230.6194 - val_mse: 2230.6191 - val_mae: 26.8584\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2683.0695 - mse: 2683.0691 - mae: 28.3284 - val_loss: 2216.0518 - val_mse: 2216.0515 - val_mae: 26.8011\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2676.0570 - mse: 2676.0559 - mae: 27.9965 - val_loss: 2210.8590 - val_mse: 2210.8596 - val_mae: 27.1201\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2718.4129 - mse: 2718.4128 - mae: 28.1790 - val_loss: 2226.4356 - val_mse: 2226.4355 - val_mae: 26.8570\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2710.3765 - mse: 2710.3765 - mae: 28.1807 - val_loss: 2240.8594 - val_mse: 2240.8591 - val_mae: 26.5636\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 517us/step - loss: 2651.3819 - mse: 2651.3813 - mae: 27.7493 - val_loss: 2244.0728 - val_mse: 2244.0730 - val_mae: 26.7378\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2663.8303 - mse: 2663.8308 - mae: 28.1215 - val_loss: 2235.7324 - val_mse: 2235.7324 - val_mae: 26.6580\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2635.6410 - mse: 2635.6406 - mae: 27.9409 - val_loss: 2228.2728 - val_mse: 2228.2727 - val_mae: 27.0115\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2644.0423 - mse: 2644.0422 - mae: 28.1327 - val_loss: 2236.3080 - val_mse: 2236.3083 - val_mae: 26.6609\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2618.9624 - mse: 2618.9624 - mae: 27.7110 - val_loss: 2204.2886 - val_mse: 2204.2883 - val_mae: 27.0920\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2641.5981 - mse: 2641.5979 - mae: 27.7538 - val_loss: 2206.5685 - val_mse: 2206.5684 - val_mae: 26.7682\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2657.0460 - mse: 2657.0461 - mae: 28.0964 - val_loss: 2212.2013 - val_mse: 2212.2014 - val_mae: 26.7981\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2644.8511 - mse: 2644.8513 - mae: 28.0556 - val_loss: 2228.2551 - val_mse: 2228.2549 - val_mae: 26.5254\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 547us/step - loss: 2698.4052 - mse: 2698.4045 - mae: 28.2119 - val_loss: 2224.0416 - val_mse: 2224.0417 - val_mae: 26.7594\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 508us/step - loss: 2652.2871 - mse: 2652.2883 - mae: 28.0196 - val_loss: 2223.1595 - val_mse: 2223.1597 - val_mae: 26.4340\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 506us/step - loss: 2685.4674 - mse: 2685.4680 - mae: 28.3895 - val_loss: 2223.7918 - val_mse: 2223.7920 - val_mae: 26.8430\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2629.7436 - mse: 2629.7437 - mae: 28.2470 - val_loss: 2206.1395 - val_mse: 2206.1394 - val_mae: 27.4216\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2650.0100 - mse: 2650.0098 - mae: 28.3287 - val_loss: 2223.1530 - val_mse: 2223.1531 - val_mae: 26.6394\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2666.0919 - mse: 2666.0928 - mae: 27.9641 - val_loss: 2247.8134 - val_mse: 2247.8132 - val_mae: 26.0910\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 13326.1571 - mse: 13326.1572 - mae: 109.8912 - val_loss: 34601.5007 - val_mse: 34601.5000 - val_mae: 132.6661\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 444us/step - loss: 13139.3521 - mse: 13139.3506 - mae: 109.0552 - val_loss: 34264.0628 - val_mse: 34264.0625 - val_mae: 131.3942\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 443us/step - loss: 12640.8662 - mse: 12640.8662 - mae: 106.7435 - val_loss: 33259.9975 - val_mse: 33259.9961 - val_mae: 127.5361\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 460us/step - loss: 11122.4533 - mse: 11122.4541 - mae: 99.3770 - val_loss: 30630.1368 - val_mse: 30630.1367 - val_mae: 116.8258\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 467us/step - loss: 7782.0273 - mse: 7782.0278 - mae: 80.1322 - val_loss: 24881.6537 - val_mse: 24881.6523 - val_mae: 88.9976\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 704us/step - loss: 3493.2300 - mse: 3493.2305 - mae: 45.9593 - val_loss: 18557.1079 - val_mse: 18557.1055 - val_mae: 44.2718\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2732.8106 - mse: 2732.8108 - mae: 38.3847 - val_loss: 18325.0817 - val_mse: 18325.0820 - val_mae: 42.8196\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 700us/step - loss: 2728.9741 - mse: 2728.9744 - mae: 37.7152 - val_loss: 18612.6570 - val_mse: 18612.6562 - val_mae: 44.5951\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 530us/step - loss: 2856.0694 - mse: 2856.0691 - mae: 38.6557 - val_loss: 18622.4201 - val_mse: 18622.4199 - val_mae: 44.6385\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2643.5320 - mse: 2643.5322 - mae: 37.2311 - val_loss: 18547.3041 - val_mse: 18547.3047 - val_mae: 44.0867\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2659.0785 - mse: 2659.0786 - mae: 37.3978 - val_loss: 18526.2444 - val_mse: 18526.2441 - val_mae: 43.9034\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 751us/step - loss: 2496.7264 - mse: 2496.7266 - mae: 35.7329 - val_loss: 18612.7663 - val_mse: 18612.7656 - val_mae: 44.4755\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2601.9298 - mse: 2601.9299 - mae: 37.3176 - val_loss: 18427.2976 - val_mse: 18427.2988 - val_mae: 43.1776\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 733us/step - loss: 2419.6620 - mse: 2419.6621 - mae: 36.5914 - val_loss: 18169.5047 - val_mse: 18169.5059 - val_mae: 41.7460\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2551.2489 - mse: 2551.2493 - mae: 37.0335 - val_loss: 18607.1658 - val_mse: 18607.1660 - val_mae: 44.3526\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2577.0024 - mse: 2577.0027 - mae: 36.1997 - val_loss: 18627.7141 - val_mse: 18627.7148 - val_mae: 44.4717\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 737us/step - loss: 2518.9404 - mse: 2518.9402 - mae: 36.4309 - val_loss: 18555.2416 - val_mse: 18555.2422 - val_mae: 43.9157\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 666us/step - loss: 2402.4251 - mse: 2402.4255 - mae: 35.2794 - val_loss: 18285.4659 - val_mse: 18285.4668 - val_mae: 42.1527\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2724.1950 - mse: 2724.1948 - mae: 37.3358 - val_loss: 18423.4131 - val_mse: 18423.4141 - val_mae: 42.9633\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2146.0287 - mse: 2146.0288 - mae: 34.2665 - val_loss: 18352.7652 - val_mse: 18352.7656 - val_mae: 42.4799\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2490.5182 - mse: 2490.5181 - mae: 35.9743 - val_loss: 18396.5702 - val_mse: 18396.5703 - val_mae: 42.7354\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 628us/step - loss: 2546.9452 - mse: 2546.9453 - mae: 36.2850 - val_loss: 18461.7324 - val_mse: 18461.7344 - val_mae: 43.1424\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 472us/step - loss: 2508.8336 - mse: 2508.8337 - mae: 36.5468 - val_loss: 18326.3724 - val_mse: 18326.3711 - val_mae: 42.2260\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2313.2802 - mse: 2313.2805 - mae: 33.3764 - val_loss: 18158.0989 - val_mse: 18158.0977 - val_mae: 41.2851\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 454us/step - loss: 2537.9054 - mse: 2537.9055 - mae: 36.2559 - val_loss: 18451.8929 - val_mse: 18451.8945 - val_mae: 42.9821\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 501us/step - loss: 2324.4365 - mse: 2324.4363 - mae: 35.0435 - val_loss: 18260.2651 - val_mse: 18260.2656 - val_mae: 41.7423\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 465us/step - loss: 2410.3443 - mse: 2410.3440 - mae: 34.8571 - val_loss: 18332.8699 - val_mse: 18332.8711 - val_mae: 42.1480\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 2452.6652 - mse: 2452.6653 - mae: 34.6244 - val_loss: 18291.9421 - val_mse: 18291.9434 - val_mae: 41.8607\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 527us/step - loss: 2450.9493 - mse: 2450.9495 - mae: 36.0748 - val_loss: 18262.7323 - val_mse: 18262.7324 - val_mae: 41.6565\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 511us/step - loss: 2433.3444 - mse: 2433.3445 - mae: 35.2900 - val_loss: 18309.8988 - val_mse: 18309.9004 - val_mae: 41.8949\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 487us/step - loss: 2470.8615 - mse: 2470.8616 - mae: 36.0234 - val_loss: 18501.7369 - val_mse: 18501.7383 - val_mae: 43.1607\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 445us/step - loss: 2326.0050 - mse: 2326.0046 - mae: 34.5745 - val_loss: 18248.5612 - val_mse: 18248.5605 - val_mae: 41.4527\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 514us/step - loss: 2438.7618 - mse: 2438.7617 - mae: 35.9228 - val_loss: 18565.0225 - val_mse: 18565.0234 - val_mae: 43.5863\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2299.1088 - mse: 2299.1086 - mae: 33.2739 - val_loss: 17983.7625 - val_mse: 17983.7617 - val_mae: 40.1763\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2528.6987 - mse: 2528.6985 - mae: 35.7010 - val_loss: 18331.4114 - val_mse: 18331.4121 - val_mae: 41.8545\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 511us/step - loss: 2236.8628 - mse: 2236.8628 - mae: 33.7416 - val_loss: 18319.8354 - val_mse: 18319.8359 - val_mae: 41.7519\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 449us/step - loss: 2360.0812 - mse: 2360.0813 - mae: 35.1952 - val_loss: 18289.5062 - val_mse: 18289.5059 - val_mae: 41.5232\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 550us/step - loss: 2472.2252 - mse: 2472.2253 - mae: 35.3918 - val_loss: 18320.3969 - val_mse: 18320.3965 - val_mae: 41.6975\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2400.9285 - mse: 2400.9285 - mae: 34.6007 - val_loss: 18288.8462 - val_mse: 18288.8457 - val_mae: 41.4669\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 534us/step - loss: 2441.2128 - mse: 2441.2126 - mae: 34.0782 - val_loss: 18276.7795 - val_mse: 18276.7793 - val_mae: 41.3519\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 2172.0895 - mse: 2172.0896 - mae: 32.5284 - val_loss: 18148.2598 - val_mse: 18148.2578 - val_mae: 40.5477\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 522us/step - loss: 2201.4576 - mse: 2201.4575 - mae: 34.1936 - val_loss: 18170.8850 - val_mse: 18170.8848 - val_mae: 40.6389\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 418us/step - loss: 2090.0277 - mse: 2090.0278 - mae: 33.1353 - val_loss: 17973.3422 - val_mse: 17973.3418 - val_mae: 39.7399\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 2314.6077 - mse: 2314.6074 - mae: 33.4896 - val_loss: 18150.7096 - val_mse: 18150.7109 - val_mae: 40.4380\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 707us/step - loss: 2229.9726 - mse: 2229.9727 - mae: 34.2592 - val_loss: 18210.5679 - val_mse: 18210.5684 - val_mae: 40.7345\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2308.4628 - mse: 2308.4626 - mae: 33.6142 - val_loss: 18262.5818 - val_mse: 18262.5820 - val_mae: 41.0199\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 706us/step - loss: 2178.3560 - mse: 2178.3564 - mae: 33.6244 - val_loss: 18095.8216 - val_mse: 18095.8203 - val_mae: 40.0035\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 2351.0559 - mse: 2351.0557 - mae: 34.2106 - val_loss: 18144.3443 - val_mse: 18144.3438 - val_mae: 40.2062\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 732us/step - loss: 2220.4530 - mse: 2220.4534 - mae: 33.0913 - val_loss: 18168.3331 - val_mse: 18168.3320 - val_mae: 40.2916\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2195.1037 - mse: 2195.1038 - mae: 32.4946 - val_loss: 18157.6893 - val_mse: 18157.6895 - val_mae: 40.1824\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2186.5116 - mse: 2186.5112 - mae: 33.4021 - val_loss: 18018.5331 - val_mse: 18018.5332 - val_mae: 39.4524\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2139.1773 - mse: 2139.1775 - mae: 32.1517 - val_loss: 18240.5070 - val_mse: 18240.5059 - val_mae: 40.6140\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2226.2809 - mse: 2226.2808 - mae: 32.7371 - val_loss: 18015.4755 - val_mse: 18015.4766 - val_mae: 39.3293\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 2030.2820 - mse: 2030.2820 - mae: 32.3263 - val_loss: 18048.5754 - val_mse: 18048.5762 - val_mae: 39.4362\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 638us/step - loss: 2340.9773 - mse: 2340.9773 - mae: 33.9891 - val_loss: 18192.4682 - val_mse: 18192.4688 - val_mae: 40.1722\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 543us/step - loss: 1944.4205 - mse: 1944.4208 - mae: 30.3869 - val_loss: 17783.4251 - val_mse: 17783.4258 - val_mae: 38.4946\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 528us/step - loss: 2089.3628 - mse: 2089.3630 - mae: 31.6860 - val_loss: 18095.5025 - val_mse: 18095.5020 - val_mae: 39.5056\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2102.6480 - mse: 2102.6479 - mae: 31.9217 - val_loss: 17902.6718 - val_mse: 17902.6719 - val_mae: 38.6696\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 542us/step - loss: 2194.6998 - mse: 2194.6997 - mae: 32.6417 - val_loss: 18076.3710 - val_mse: 18076.3691 - val_mae: 39.3101\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 1972.5070 - mse: 1972.5068 - mae: 31.0580 - val_loss: 17907.3605 - val_mse: 17907.3594 - val_mae: 38.5787\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2059.1910 - mse: 2059.1909 - mae: 31.6070 - val_loss: 18009.2643 - val_mse: 18009.2637 - val_mae: 38.8861\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 668us/step - loss: 2131.1318 - mse: 2131.1318 - mae: 32.5187 - val_loss: 18051.2787 - val_mse: 18051.2773 - val_mae: 39.0224\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 643us/step - loss: 2110.7552 - mse: 2110.7554 - mae: 30.8085 - val_loss: 17810.0580 - val_mse: 17810.0605 - val_mae: 38.1411\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 632us/step - loss: 2048.1181 - mse: 2048.1179 - mae: 32.4850 - val_loss: 18138.7613 - val_mse: 18138.7598 - val_mae: 39.4443\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2049.3283 - mse: 2049.3284 - mae: 30.9963 - val_loss: 17838.9276 - val_mse: 17838.9277 - val_mae: 38.1245\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 2089.6269 - mse: 2089.6270 - mae: 31.7635 - val_loss: 17897.3086 - val_mse: 17897.3105 - val_mae: 38.2660\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 2084.8392 - mse: 2084.8391 - mae: 31.4859 - val_loss: 18059.1726 - val_mse: 18059.1719 - val_mae: 38.8930\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 1939.2619 - mse: 1939.2621 - mae: 31.1991 - val_loss: 17874.8994 - val_mse: 17874.8984 - val_mae: 38.1026\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 1916.4478 - mse: 1916.4479 - mae: 30.2667 - val_loss: 17971.4677 - val_mse: 17971.4668 - val_mae: 38.4108\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 1859.2250 - mse: 1859.2250 - mae: 30.4840 - val_loss: 17798.4867 - val_mse: 17798.4883 - val_mae: 37.8117\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 513us/step - loss: 1980.8682 - mse: 1980.8682 - mae: 30.8072 - val_loss: 17949.1545 - val_mse: 17949.1543 - val_mae: 38.2189\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 2049.9826 - mse: 2049.9827 - mae: 32.1752 - val_loss: 17899.6128 - val_mse: 17899.6133 - val_mae: 37.9778\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 1968.0806 - mse: 1968.0806 - mae: 31.1609 - val_loss: 17839.0060 - val_mse: 17839.0059 - val_mae: 37.7299\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 308us/step - loss: 2035.0263 - mse: 2035.0261 - mae: 31.7397 - val_loss: 18063.9616 - val_mse: 18063.9629 - val_mae: 38.6028\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 467us/step - loss: 1990.1585 - mse: 1990.1584 - mae: 30.3312 - val_loss: 17766.4702 - val_mse: 17766.4707 - val_mae: 37.4605\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 482us/step - loss: 2005.0282 - mse: 2005.0281 - mae: 30.3579 - val_loss: 17866.4319 - val_mse: 17866.4297 - val_mae: 37.6426\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 506us/step - loss: 2030.4543 - mse: 2030.4545 - mae: 30.6801 - val_loss: 17877.2119 - val_mse: 17877.2129 - val_mae: 37.6423\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 422us/step - loss: 1887.0537 - mse: 1887.0536 - mae: 31.1475 - val_loss: 17829.1616 - val_mse: 17829.1602 - val_mae: 37.4575\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 472us/step - loss: 1974.9075 - mse: 1974.9076 - mae: 30.6877 - val_loss: 17816.4607 - val_mse: 17816.4609 - val_mae: 37.3793\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 500us/step - loss: 2031.2818 - mse: 2031.2816 - mae: 30.4936 - val_loss: 17769.0536 - val_mse: 17769.0547 - val_mae: 37.2732\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 0s 458us/step - loss: 4193.6809 - mse: 4193.6807 - mae: 34.7031 - val_loss: 2162.8504 - val_mse: 2162.8503 - val_mae: 31.7007\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 329us/step - loss: 4508.8391 - mse: 4508.8384 - mae: 36.3537 - val_loss: 2271.5428 - val_mse: 2271.5427 - val_mae: 31.9667\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 454us/step - loss: 4356.9066 - mse: 4356.9062 - mae: 35.6162 - val_loss: 2223.5817 - val_mse: 2223.5815 - val_mae: 31.8359\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 485us/step - loss: 4240.4466 - mse: 4240.4468 - mae: 35.2041 - val_loss: 2217.6939 - val_mse: 2217.6938 - val_mae: 31.8096\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 530us/step - loss: 4508.6971 - mse: 4508.6973 - mae: 36.5299 - val_loss: 2318.1577 - val_mse: 2318.1577 - val_mae: 32.0605\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 509us/step - loss: 4122.1395 - mse: 4122.1396 - mae: 35.1039 - val_loss: 2241.0741 - val_mse: 2241.0740 - val_mae: 31.8510\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 0s 470us/step - loss: 4135.1589 - mse: 4135.1592 - mae: 35.0725 - val_loss: 2264.8525 - val_mse: 2264.8525 - val_mae: 31.9129\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4324.8849 - mse: 4324.8853 - mae: 34.9352 - val_loss: 2334.5551 - val_mse: 2334.5549 - val_mae: 32.0915\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 539us/step - loss: 4398.8062 - mse: 4398.8066 - mae: 36.1209 - val_loss: 2310.5587 - val_mse: 2310.5586 - val_mae: 32.0181\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4335.8854 - mse: 4335.8857 - mae: 34.5815 - val_loss: 2262.5148 - val_mse: 2262.5146 - val_mae: 31.8860\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 719us/step - loss: 4226.2164 - mse: 4226.2168 - mae: 35.2962 - val_loss: 2292.7292 - val_mse: 2292.7292 - val_mae: 31.9576\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 693us/step - loss: 4318.7703 - mse: 4318.7705 - mae: 35.4605 - val_loss: 2333.6585 - val_mse: 2333.6584 - val_mae: 32.0638\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4349.0308 - mse: 4349.0308 - mae: 35.8255 - val_loss: 2301.1252 - val_mse: 2301.1252 - val_mae: 31.9675\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4367.1489 - mse: 4367.1494 - mae: 35.5699 - val_loss: 2254.4055 - val_mse: 2254.4055 - val_mae: 31.8407\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4162.5872 - mse: 4162.5869 - mae: 34.6993 - val_loss: 2249.4400 - val_mse: 2249.4397 - val_mae: 31.8180\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4073.9606 - mse: 4073.9602 - mae: 34.1445 - val_loss: 2310.1530 - val_mse: 2310.1533 - val_mae: 31.9718\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4351.2150 - mse: 4351.2144 - mae: 35.3716 - val_loss: 2280.4716 - val_mse: 2280.4717 - val_mae: 31.8847\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 0s 501us/step - loss: 4167.3166 - mse: 4167.3174 - mae: 34.5189 - val_loss: 2244.4425 - val_mse: 2244.4424 - val_mae: 31.7815\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 512us/step - loss: 4477.6211 - mse: 4477.6216 - mae: 35.5165 - val_loss: 2298.0405 - val_mse: 2298.0405 - val_mae: 31.9257\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4203.5429 - mse: 4203.5430 - mae: 35.0914 - val_loss: 2286.0739 - val_mse: 2286.0737 - val_mae: 31.8871\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4111.2896 - mse: 4111.2896 - mae: 33.9618 - val_loss: 2249.7281 - val_mse: 2249.7280 - val_mae: 31.7847\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 639us/step - loss: 4097.9973 - mse: 4097.9966 - mae: 34.4265 - val_loss: 2279.4677 - val_mse: 2279.4678 - val_mae: 31.8588\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 700us/step - loss: 4168.8579 - mse: 4168.8579 - mae: 35.1085 - val_loss: 2265.5301 - val_mse: 2265.5300 - val_mae: 31.8170\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 669us/step - loss: 4296.9435 - mse: 4296.9438 - mae: 34.3663 - val_loss: 2285.1609 - val_mse: 2285.1611 - val_mae: 31.8710\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4359.5493 - mse: 4359.5498 - mae: 35.9903 - val_loss: 2271.1132 - val_mse: 2271.1133 - val_mae: 31.8269\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 723us/step - loss: 4188.3428 - mse: 4188.3423 - mae: 34.6788 - val_loss: 2229.7446 - val_mse: 2229.7446 - val_mae: 31.7112\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 0s 467us/step - loss: 4259.9317 - mse: 4259.9316 - mae: 34.6781 - val_loss: 2332.1606 - val_mse: 2332.1602 - val_mae: 31.9949\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 455us/step - loss: 4183.0628 - mse: 4183.0630 - mae: 33.4769 - val_loss: 2245.0284 - val_mse: 2245.0283 - val_mae: 31.7475\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4133.5476 - mse: 4133.5479 - mae: 34.4259 - val_loss: 2332.3273 - val_mse: 2332.3271 - val_mae: 31.9842\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4174.3051 - mse: 4174.3052 - mae: 33.1160 - val_loss: 2237.6248 - val_mse: 2237.6248 - val_mae: 31.7155\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4215.1020 - mse: 4215.1016 - mae: 34.4962 - val_loss: 2315.1822 - val_mse: 2315.1821 - val_mae: 31.9268\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - ETA: 0s - loss: 4162.5314 - mse: 4162.5317 - mae: 33.52 - 1s 551us/step - loss: 4050.5808 - mse: 4050.5813 - mae: 34.2987 - val_loss: 2253.5354 - val_mse: 2253.5352 - val_mae: 31.7517\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4255.7841 - mse: 4255.7847 - mae: 33.3312 - val_loss: 2349.5086 - val_mse: 2349.5085 - val_mae: 32.0200\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4252.4662 - mse: 4252.4658 - mae: 34.8923 - val_loss: 2290.8076 - val_mse: 2290.8076 - val_mae: 31.8425\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4050.7809 - mse: 4050.7805 - mae: 34.3042 - val_loss: 2282.4671 - val_mse: 2282.4673 - val_mae: 31.8164\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 3872.1243 - mse: 3872.1240 - mae: 32.8339 - val_loss: 2185.1707 - val_mse: 2185.1704 - val_mae: 31.5510\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 4121.9198 - mse: 4121.9194 - mae: 34.3364 - val_loss: 2303.5097 - val_mse: 2303.5095 - val_mae: 31.8724\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 568us/step - loss: 4297.3935 - mse: 4297.3936 - mae: 33.8222 - val_loss: 2276.8558 - val_mse: 2276.8555 - val_mae: 31.7971\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4381.4990 - mse: 4381.4995 - mae: 34.8952 - val_loss: 2324.1281 - val_mse: 2324.1279 - val_mae: 31.9262\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4059.0065 - mse: 4059.0063 - mae: 33.2575 - val_loss: 2257.0757 - val_mse: 2257.0757 - val_mae: 31.7299\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4295.4302 - mse: 4295.4297 - mae: 34.3923 - val_loss: 2322.4471 - val_mse: 2322.4470 - val_mae: 31.9093\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4027.8770 - mse: 4027.8770 - mae: 33.8351 - val_loss: 2235.0582 - val_mse: 2235.0583 - val_mae: 31.6601\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 4115.0209 - mse: 4115.0220 - mae: 33.5034 - val_loss: 2283.1480 - val_mse: 2283.1477 - val_mae: 31.7864\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 636us/step - loss: 4068.0920 - mse: 4068.0918 - mae: 33.2662 - val_loss: 2267.9888 - val_mse: 2267.9890 - val_mae: 31.7415\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4163.6506 - mse: 4163.6509 - mae: 34.6177 - val_loss: 2301.1508 - val_mse: 2301.1506 - val_mae: 31.8322\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4111.5097 - mse: 4111.5088 - mae: 34.1974 - val_loss: 2319.4899 - val_mse: 2319.4900 - val_mae: 31.8791\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4199.5076 - mse: 4199.5078 - mae: 34.1629 - val_loss: 2311.4588 - val_mse: 2311.4587 - val_mae: 31.8482\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4125.3949 - mse: 4125.3950 - mae: 33.3960 - val_loss: 2259.5658 - val_mse: 2259.5657 - val_mae: 31.6931\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 3845.0593 - mse: 3845.0598 - mae: 32.0931 - val_loss: 2209.5471 - val_mse: 2209.5471 - val_mae: 31.5500\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4175.8403 - mse: 4175.8403 - mae: 33.4835 - val_loss: 2324.6410 - val_mse: 2324.6411 - val_mae: 31.8726\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 3983.8596 - mse: 3983.8599 - mae: 33.1995 - val_loss: 2283.7133 - val_mse: 2283.7129 - val_mae: 31.7576\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4324.6621 - mse: 4324.6621 - mae: 34.4479 - val_loss: 2319.9074 - val_mse: 2319.9072 - val_mae: 31.8509\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 521us/step - loss: 4052.1716 - mse: 4052.1711 - mae: 33.4252 - val_loss: 2260.9349 - val_mse: 2260.9351 - val_mae: 31.6796\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4080.9837 - mse: 4080.9839 - mae: 33.4527 - val_loss: 2333.1435 - val_mse: 2333.1436 - val_mae: 31.8804\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4061.6816 - mse: 4061.6821 - mae: 33.8969 - val_loss: 2308.6896 - val_mse: 2308.6895 - val_mae: 31.8034\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 639us/step - loss: 4000.0248 - mse: 4000.0247 - mae: 33.8516 - val_loss: 2290.8904 - val_mse: 2290.8904 - val_mae: 31.7456\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 653us/step - loss: 3996.2689 - mse: 3996.2686 - mae: 32.6965 - val_loss: 2289.8414 - val_mse: 2289.8413 - val_mae: 31.7391\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4015.5864 - mse: 4015.5862 - mae: 33.0727 - val_loss: 2317.8815 - val_mse: 2317.8816 - val_mae: 31.8165\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 682us/step - loss: 4091.7411 - mse: 4091.7410 - mae: 33.1618 - val_loss: 2313.3178 - val_mse: 2313.3181 - val_mae: 31.7952\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 720us/step - loss: 4133.2779 - mse: 4133.2778 - mae: 33.9321 - val_loss: 2320.2370 - val_mse: 2320.2368 - val_mae: 31.8102\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 725us/step - loss: 3995.7308 - mse: 3995.7310 - mae: 34.0128 - val_loss: 2304.6728 - val_mse: 2304.6729 - val_mae: 31.7622\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4019.3644 - mse: 4019.3635 - mae: 32.3871 - val_loss: 2275.5063 - val_mse: 2275.5063 - val_mae: 31.6727\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 749us/step - loss: 4060.9762 - mse: 4060.9758 - mae: 33.5007 - val_loss: 2284.8180 - val_mse: 2284.8179 - val_mae: 31.6936\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 3923.3814 - mse: 3923.3809 - mae: 32.3761 - val_loss: 2307.8881 - val_mse: 2307.8879 - val_mae: 31.7556\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 3926.0069 - mse: 3926.0071 - mae: 32.9429 - val_loss: 2304.9009 - val_mse: 2304.9006 - val_mae: 31.7456\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4031.8066 - mse: 4031.8071 - mae: 32.6588 - val_loss: 2353.2835 - val_mse: 2353.2834 - val_mae: 31.8888\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4085.4386 - mse: 4085.4380 - mae: 33.5758 - val_loss: 2363.1734 - val_mse: 2363.1733 - val_mae: 31.9171\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4179.6495 - mse: 4179.6499 - mae: 33.3893 - val_loss: 2355.0376 - val_mse: 2355.0378 - val_mae: 31.8888\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 3901.3643 - mse: 3901.3643 - mae: 32.8860 - val_loss: 2325.9803 - val_mse: 2325.9802 - val_mae: 31.7959\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4025.7625 - mse: 4025.7622 - mae: 33.4042 - val_loss: 2351.3573 - val_mse: 2351.3572 - val_mae: 31.8803\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 702us/step - loss: 4080.9822 - mse: 4080.9827 - mae: 33.0192 - val_loss: 2279.3933 - val_mse: 2279.3933 - val_mae: 31.6635\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 679us/step - loss: 4101.3427 - mse: 4101.3433 - mae: 33.1613 - val_loss: 2402.5913 - val_mse: 2402.5911 - val_mae: 32.0307\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 4081.1518 - mse: 4081.1519 - mae: 33.0292 - val_loss: 2309.7483 - val_mse: 2309.7485 - val_mae: 31.7478\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 680us/step - loss: 3988.6777 - mse: 3988.6777 - mae: 34.0594 - val_loss: 2332.8329 - val_mse: 2332.8328 - val_mae: 31.8157\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4134.8393 - mse: 4134.8394 - mae: 33.6538 - val_loss: 2340.5220 - val_mse: 2340.5220 - val_mae: 31.8376\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 670us/step - loss: 3901.6489 - mse: 3901.6492 - mae: 33.1516 - val_loss: 2317.8755 - val_mse: 2317.8755 - val_mae: 31.7700\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 3908.0039 - mse: 3908.0042 - mae: 33.3365 - val_loss: 2291.6108 - val_mse: 2291.6108 - val_mae: 31.6942\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4075.3706 - mse: 4075.3704 - mae: 32.8950 - val_loss: 2338.6942 - val_mse: 2338.6941 - val_mae: 31.8339\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4098.5940 - mse: 4098.5942 - mae: 33.0903 - val_loss: 2344.1806 - val_mse: 2344.1804 - val_mae: 31.8476\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4146.3900 - mse: 4146.3901 - mae: 33.3116 - val_loss: 2395.5603 - val_mse: 2395.5605 - val_mae: 32.0048\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3293.2880 - mse: 3293.2891 - mae: 32.0081 - val_loss: 1445.9233 - val_mse: 1445.9233 - val_mae: 25.5851\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3544.2500 - mse: 3544.2502 - mae: 34.4057 - val_loss: 1447.2202 - val_mse: 1447.2201 - val_mae: 24.8031\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3431.1152 - mse: 3431.1150 - mae: 32.7963 - val_loss: 1444.8618 - val_mse: 1444.8619 - val_mae: 25.3351\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3423.2161 - mse: 3423.2161 - mae: 32.9134 - val_loss: 1445.8611 - val_mse: 1445.8611 - val_mae: 25.3492\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3529.4365 - mse: 3529.4370 - mae: 33.6657 - val_loss: 1446.2043 - val_mse: 1446.2041 - val_mae: 25.5146\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 656us/step - loss: 3443.1282 - mse: 3443.1284 - mae: 32.6084 - val_loss: 1447.2008 - val_mse: 1447.2007 - val_mae: 25.7733\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 678us/step - loss: 3407.8653 - mse: 3407.8650 - mae: 32.7390 - val_loss: 1445.9746 - val_mse: 1445.9745 - val_mae: 25.5180\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3448.5965 - mse: 3448.5964 - mae: 32.3222 - val_loss: 1446.3178 - val_mse: 1446.3179 - val_mae: 25.9156\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3403.3432 - mse: 3403.3438 - mae: 32.4384 - val_loss: 1445.0537 - val_mse: 1445.0536 - val_mae: 25.4785\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3343.7248 - mse: 3343.7249 - mae: 32.5931 - val_loss: 1444.8409 - val_mse: 1444.8407 - val_mae: 25.5867\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3196.7480 - mse: 3196.7478 - mae: 31.9932 - val_loss: 1446.3007 - val_mse: 1446.3007 - val_mae: 25.9369\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3369.4371 - mse: 3369.4382 - mae: 32.4996 - val_loss: 1447.4035 - val_mse: 1447.4034 - val_mae: 24.8575\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3364.2169 - mse: 3364.2170 - mae: 32.1630 - val_loss: 1446.2256 - val_mse: 1446.2256 - val_mae: 25.0275\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3562.7886 - mse: 3562.7883 - mae: 33.4835 - val_loss: 1445.3278 - val_mse: 1445.3278 - val_mae: 25.5851\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 530us/step - loss: 3316.8923 - mse: 3316.8931 - mae: 32.1103 - val_loss: 1451.9191 - val_mse: 1451.9191 - val_mae: 26.5798\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3428.1107 - mse: 3428.1106 - mae: 33.1897 - val_loss: 1447.4699 - val_mse: 1447.4700 - val_mae: 24.9014\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3390.0826 - mse: 3390.0823 - mae: 32.5405 - val_loss: 1445.7707 - val_mse: 1445.7705 - val_mae: 25.1916\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3311.9692 - mse: 3311.9692 - mae: 31.4564 - val_loss: 1447.0965 - val_mse: 1447.0963 - val_mae: 25.0846\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3178.9587 - mse: 3178.9590 - mae: 32.4447 - val_loss: 1446.9831 - val_mse: 1446.9833 - val_mae: 25.6895\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3475.6567 - mse: 3475.6558 - mae: 32.9508 - val_loss: 1447.6721 - val_mse: 1447.6721 - val_mae: 25.1772\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3401.1561 - mse: 3401.1562 - mae: 32.3242 - val_loss: 1450.1385 - val_mse: 1450.1384 - val_mae: 24.9374\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3261.5129 - mse: 3261.5127 - mae: 32.0887 - val_loss: 1447.6927 - val_mse: 1447.6926 - val_mae: 25.4222\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 513us/step - loss: 3365.3000 - mse: 3365.3003 - mae: 31.9966 - val_loss: 1448.6447 - val_mse: 1448.6448 - val_mae: 25.8995\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 644us/step - loss: 3380.4216 - mse: 3380.4216 - mae: 32.4683 - val_loss: 1448.0983 - val_mse: 1448.0981 - val_mae: 25.4067\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3431.8548 - mse: 3431.8547 - mae: 33.1393 - val_loss: 1448.8489 - val_mse: 1448.8489 - val_mae: 25.2637\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3253.0741 - mse: 3253.0742 - mae: 32.0875 - val_loss: 1450.9915 - val_mse: 1450.9915 - val_mae: 24.9177\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3297.4015 - mse: 3297.4021 - mae: 32.4548 - val_loss: 1450.4351 - val_mse: 1450.4352 - val_mae: 25.0274\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 654us/step - loss: 3321.8403 - mse: 3321.8394 - mae: 31.6555 - val_loss: 1450.7260 - val_mse: 1450.7260 - val_mae: 26.0295\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3388.3948 - mse: 3388.3940 - mae: 32.7242 - val_loss: 1450.1132 - val_mse: 1450.1133 - val_mae: 25.9206\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 680us/step - loss: 3194.8950 - mse: 3194.8943 - mae: 31.9748 - val_loss: 1449.6587 - val_mse: 1449.6586 - val_mae: 25.3550\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 672us/step - loss: 3299.3818 - mse: 3299.3816 - mae: 31.9477 - val_loss: 1448.9876 - val_mse: 1448.9877 - val_mae: 25.6488\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 676us/step - loss: 3324.3022 - mse: 3324.3030 - mae: 32.1903 - val_loss: 1449.8296 - val_mse: 1449.8297 - val_mae: 25.2214\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3359.1995 - mse: 3359.2000 - mae: 31.9467 - val_loss: 1450.3801 - val_mse: 1450.3800 - val_mae: 25.8978\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3330.0717 - mse: 3330.0720 - mae: 32.1886 - val_loss: 1450.4162 - val_mse: 1450.4163 - val_mae: 25.5245\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3507.0143 - mse: 3507.0146 - mae: 33.1182 - val_loss: 1451.5597 - val_mse: 1451.5597 - val_mae: 25.2983\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3387.8489 - mse: 3387.8489 - mae: 32.1262 - val_loss: 1452.0635 - val_mse: 1452.0635 - val_mae: 25.4312\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 672us/step - loss: 3285.6367 - mse: 3285.6367 - mae: 31.8057 - val_loss: 1452.1636 - val_mse: 1452.1636 - val_mae: 25.4846\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3324.5997 - mse: 3324.6001 - mae: 31.9462 - val_loss: 1452.2712 - val_mse: 1452.2712 - val_mae: 25.9858\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 633us/step - loss: 3362.9650 - mse: 3362.9653 - mae: 31.9667 - val_loss: 1451.7913 - val_mse: 1451.7913 - val_mae: 25.5982\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 653us/step - loss: 3305.7408 - mse: 3305.7405 - mae: 32.2581 - val_loss: 1452.3694 - val_mse: 1452.3693 - val_mae: 25.7237\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3284.0475 - mse: 3284.0471 - mae: 31.9050 - val_loss: 1452.6322 - val_mse: 1452.6323 - val_mae: 25.3931\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3376.4115 - mse: 3376.4116 - mae: 32.0916 - val_loss: 1453.6315 - val_mse: 1453.6315 - val_mae: 26.0271\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3254.6000 - mse: 3254.6003 - mae: 32.4864 - val_loss: 1452.7603 - val_mse: 1452.7605 - val_mae: 25.3670\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3232.5710 - mse: 3232.5715 - mae: 31.9791 - val_loss: 1453.1029 - val_mse: 1453.1028 - val_mae: 25.3859\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3225.7691 - mse: 3225.7695 - mae: 31.5440 - val_loss: 1452.9382 - val_mse: 1452.9384 - val_mae: 25.4871\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 633us/step - loss: 3268.3892 - mse: 3268.3904 - mae: 31.8523 - val_loss: 1454.1987 - val_mse: 1454.1987 - val_mae: 25.0964\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3199.0616 - mse: 3199.0618 - mae: 31.8458 - val_loss: 1452.1503 - val_mse: 1452.1504 - val_mae: 25.6693\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3344.9196 - mse: 3344.9207 - mae: 32.1790 - val_loss: 1453.8605 - val_mse: 1453.8604 - val_mae: 25.0905\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3124.0765 - mse: 3124.0771 - mae: 30.9084 - val_loss: 1453.7324 - val_mse: 1453.7324 - val_mae: 25.8419\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 526us/step - loss: 3369.4445 - mse: 3369.4441 - mae: 32.3038 - val_loss: 1453.3858 - val_mse: 1453.3859 - val_mae: 25.6969\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3327.7684 - mse: 3327.7686 - mae: 32.5977 - val_loss: 1453.2558 - val_mse: 1453.2557 - val_mae: 25.6155\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3339.6180 - mse: 3339.6184 - mae: 31.8514 - val_loss: 1455.0515 - val_mse: 1455.0516 - val_mae: 25.9769\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3338.8259 - mse: 3338.8264 - mae: 32.2889 - val_loss: 1454.6152 - val_mse: 1454.6154 - val_mae: 25.3754\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3245.1639 - mse: 3245.1648 - mae: 31.8631 - val_loss: 1455.8237 - val_mse: 1455.8235 - val_mae: 26.1105\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 662us/step - loss: 3362.4049 - mse: 3362.4053 - mae: 31.7066 - val_loss: 1453.9740 - val_mse: 1453.9741 - val_mae: 25.5524\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 694us/step - loss: 3269.5894 - mse: 3269.5906 - mae: 31.6497 - val_loss: 1454.2610 - val_mse: 1454.2609 - val_mae: 25.2351\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3244.7442 - mse: 3244.7439 - mae: 31.7019 - val_loss: 1454.7121 - val_mse: 1454.7120 - val_mae: 25.3687\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3262.5611 - mse: 3262.5608 - mae: 31.4093 - val_loss: 1454.7287 - val_mse: 1454.7288 - val_mae: 25.3488\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3247.3778 - mse: 3247.3789 - mae: 31.9309 - val_loss: 1454.9820 - val_mse: 1454.9822 - val_mae: 25.7583\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3146.9145 - mse: 3146.9141 - mae: 31.3350 - val_loss: 1455.4045 - val_mse: 1455.4045 - val_mae: 25.9917\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3242.7976 - mse: 3242.7976 - mae: 31.7270 - val_loss: 1454.2188 - val_mse: 1454.2188 - val_mae: 25.4979\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3286.6262 - mse: 3286.6257 - mae: 31.9246 - val_loss: 1456.2149 - val_mse: 1456.2151 - val_mae: 26.1470\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3272.1243 - mse: 3272.1250 - mae: 32.0104 - val_loss: 1454.0018 - val_mse: 1454.0018 - val_mae: 25.5391\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3319.2869 - mse: 3319.2859 - mae: 31.7019 - val_loss: 1454.3923 - val_mse: 1454.3922 - val_mae: 25.6002\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3245.4247 - mse: 3245.4255 - mae: 32.1779 - val_loss: 1454.2754 - val_mse: 1454.2754 - val_mae: 25.8069\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 663us/step - loss: 3327.5320 - mse: 3327.5325 - mae: 32.1865 - val_loss: 1454.2829 - val_mse: 1454.2831 - val_mae: 25.9143\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 654us/step - loss: 3391.5986 - mse: 3391.5984 - mae: 32.1891 - val_loss: 1454.4026 - val_mse: 1454.4026 - val_mae: 25.2345\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3276.3823 - mse: 3276.3823 - mae: 31.1573 - val_loss: 1455.3667 - val_mse: 1455.3668 - val_mae: 26.0313\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3255.5994 - mse: 3255.5994 - mae: 31.5286 - val_loss: 1454.3945 - val_mse: 1454.3944 - val_mae: 25.5399\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3254.5621 - mse: 3254.5625 - mae: 31.2137 - val_loss: 1454.5194 - val_mse: 1454.5192 - val_mae: 25.7693\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3180.6917 - mse: 3180.6909 - mae: 31.5582 - val_loss: 1454.4446 - val_mse: 1454.4445 - val_mae: 25.7189\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3347.1739 - mse: 3347.1748 - mae: 32.1640 - val_loss: 1456.1151 - val_mse: 1456.1151 - val_mae: 25.2008\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3203.7845 - mse: 3203.7847 - mae: 31.4371 - val_loss: 1455.3614 - val_mse: 1455.3615 - val_mae: 25.3444\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3362.0932 - mse: 3362.0947 - mae: 32.5281 - val_loss: 1455.7905 - val_mse: 1455.7906 - val_mae: 25.3057\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 489us/step - loss: 3328.8449 - mse: 3328.8455 - mae: 31.4924 - val_loss: 1455.2888 - val_mse: 1455.2888 - val_mae: 25.6592\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3124.1370 - mse: 3124.1367 - mae: 31.1492 - val_loss: 1458.0605 - val_mse: 1458.0605 - val_mae: 26.3109\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3263.8540 - mse: 3263.8538 - mae: 31.8240 - val_loss: 1454.9880 - val_mse: 1454.9879 - val_mae: 25.6021\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3326.9786 - mse: 3326.9783 - mae: 31.6076 - val_loss: 1455.2354 - val_mse: 1455.2355 - val_mae: 25.6072\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3317.2855 - mse: 3317.2859 - mae: 32.0152 - val_loss: 1456.8444 - val_mse: 1456.8444 - val_mae: 25.0998\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3323.4503 - mse: 3323.4495 - mae: 31.7704 - val_loss: 1456.2791 - val_mse: 1456.2793 - val_mae: 25.1542\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2899.4685 - mse: 2899.4683 - mae: 30.7962 - val_loss: 1068.1960 - val_mse: 1068.1959 - val_mae: 23.4557\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 511us/step - loss: 2918.0649 - mse: 2918.0649 - mae: 30.6709 - val_loss: 1073.5790 - val_mse: 1073.5790 - val_mae: 23.2080\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 519us/step - loss: 2894.7623 - mse: 2894.7617 - mae: 30.8533 - val_loss: 1070.1486 - val_mse: 1070.1486 - val_mae: 23.3388\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2863.8244 - mse: 2863.8245 - mae: 31.0803 - val_loss: 1070.3642 - val_mse: 1070.3641 - val_mae: 23.3325\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2850.6758 - mse: 2850.6748 - mae: 30.3961 - val_loss: 1062.0868 - val_mse: 1062.0869 - val_mae: 23.9863\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2951.4447 - mse: 2951.4438 - mae: 30.7083 - val_loss: 1070.3416 - val_mse: 1070.3417 - val_mae: 23.2722\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2915.1192 - mse: 2915.1191 - mae: 31.0777 - val_loss: 1062.5647 - val_mse: 1062.5646 - val_mae: 23.8445\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2908.2297 - mse: 2908.2295 - mae: 30.8118 - val_loss: 1067.2721 - val_mse: 1067.2721 - val_mae: 23.4054\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 3020.0937 - mse: 3020.0930 - mae: 31.5447 - val_loss: 1063.4501 - val_mse: 1063.4503 - val_mae: 23.7080\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2881.2775 - mse: 2881.2771 - mae: 30.7576 - val_loss: 1062.4382 - val_mse: 1062.4382 - val_mae: 23.9251\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2846.1535 - mse: 2846.1526 - mae: 30.6278 - val_loss: 1061.9204 - val_mse: 1061.9204 - val_mae: 23.8444\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2914.0784 - mse: 2914.0789 - mae: 30.5550 - val_loss: 1060.2630 - val_mse: 1060.2629 - val_mae: 23.9157\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2935.8854 - mse: 2935.8850 - mae: 30.8814 - val_loss: 1059.7721 - val_mse: 1059.7722 - val_mae: 24.1037\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2928.2604 - mse: 2928.2607 - mae: 31.0317 - val_loss: 1058.7398 - val_mse: 1058.7400 - val_mae: 23.8987\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2960.0174 - mse: 2960.0176 - mae: 31.5490 - val_loss: 1066.9694 - val_mse: 1066.9695 - val_mae: 23.2334\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2904.2152 - mse: 2904.2151 - mae: 30.8668 - val_loss: 1065.7268 - val_mse: 1065.7269 - val_mae: 23.3233\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2882.5386 - mse: 2882.5378 - mae: 31.1692 - val_loss: 1062.0860 - val_mse: 1062.0861 - val_mae: 23.5598\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2915.1458 - mse: 2915.1453 - mae: 31.4940 - val_loss: 1062.0542 - val_mse: 1062.0542 - val_mae: 23.5484\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 684us/step - loss: 2886.7691 - mse: 2886.7698 - mae: 31.2801 - val_loss: 1062.2958 - val_mse: 1062.2958 - val_mae: 23.5098\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2817.0532 - mse: 2817.0532 - mae: 30.9871 - val_loss: 1059.1993 - val_mse: 1059.1993 - val_mae: 23.8879\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2876.9799 - mse: 2876.9805 - mae: 30.7280 - val_loss: 1060.5089 - val_mse: 1060.5090 - val_mae: 23.7082\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2879.1801 - mse: 2879.1804 - mae: 30.2436 - val_loss: 1059.0134 - val_mse: 1059.0134 - val_mae: 23.9693\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2870.0818 - mse: 2870.0820 - mae: 30.8822 - val_loss: 1059.3627 - val_mse: 1059.3628 - val_mae: 23.8221\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2854.8429 - mse: 2854.8428 - mae: 30.8840 - val_loss: 1058.3388 - val_mse: 1058.3389 - val_mae: 24.0364\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2865.3142 - mse: 2865.3140 - mae: 30.6499 - val_loss: 1059.7145 - val_mse: 1059.7146 - val_mae: 23.6983\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2822.6651 - mse: 2822.6648 - mae: 30.9116 - val_loss: 1060.0885 - val_mse: 1060.0883 - val_mae: 23.6848\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2871.0134 - mse: 2871.0125 - mae: 31.1869 - val_loss: 1059.4781 - val_mse: 1059.4780 - val_mae: 23.7573\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2865.3653 - mse: 2865.3650 - mae: 30.7704 - val_loss: 1059.9456 - val_mse: 1059.9457 - val_mae: 23.6193\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2972.5477 - mse: 2972.5479 - mae: 31.2175 - val_loss: 1059.4088 - val_mse: 1059.4089 - val_mae: 23.6570\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2910.2882 - mse: 2910.2883 - mae: 30.5952 - val_loss: 1058.2843 - val_mse: 1058.2843 - val_mae: 23.7270\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2806.5678 - mse: 2806.5676 - mae: 29.8172 - val_loss: 1058.4310 - val_mse: 1058.4312 - val_mae: 23.7408\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2974.2135 - mse: 2974.2141 - mae: 31.1412 - val_loss: 1057.9247 - val_mse: 1057.9247 - val_mae: 23.9149\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2828.8385 - mse: 2828.8394 - mae: 30.5701 - val_loss: 1057.4554 - val_mse: 1057.4554 - val_mae: 23.9157\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2931.0365 - mse: 2931.0364 - mae: 30.8871 - val_loss: 1057.7918 - val_mse: 1057.7917 - val_mae: 23.7287\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2845.0540 - mse: 2845.0540 - mae: 30.7105 - val_loss: 1056.9207 - val_mse: 1056.9208 - val_mae: 23.8059\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2873.1169 - mse: 2873.1172 - mae: 30.5434 - val_loss: 1056.8330 - val_mse: 1056.8329 - val_mae: 23.7019\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 692us/step - loss: 2891.6882 - mse: 2891.6885 - mae: 30.5855 - val_loss: 1057.4807 - val_mse: 1057.4808 - val_mae: 23.6224\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2913.2901 - mse: 2913.2905 - mae: 31.1626 - val_loss: 1055.7048 - val_mse: 1055.7050 - val_mae: 23.7336\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 540us/step - loss: 2832.5837 - mse: 2832.5833 - mae: 30.4616 - val_loss: 1056.1888 - val_mse: 1056.1888 - val_mae: 23.6331\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2901.3542 - mse: 2901.3542 - mae: 30.7337 - val_loss: 1055.5928 - val_mse: 1055.5927 - val_mae: 23.7811\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2872.1329 - mse: 2872.1338 - mae: 30.6324 - val_loss: 1054.8919 - val_mse: 1054.8920 - val_mae: 23.8908\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2832.9036 - mse: 2832.9038 - mae: 30.5154 - val_loss: 1054.3963 - val_mse: 1054.3964 - val_mae: 24.0882\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 530us/step - loss: 2807.0758 - mse: 2807.0762 - mae: 30.7226 - val_loss: 1053.8904 - val_mse: 1053.8903 - val_mae: 23.8256\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2893.0434 - mse: 2893.0435 - mae: 30.7451 - val_loss: 1053.8948 - val_mse: 1053.8948 - val_mae: 24.0759\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2886.0951 - mse: 2886.0952 - mae: 30.8362 - val_loss: 1055.1613 - val_mse: 1055.1614 - val_mae: 23.6147\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2843.2683 - mse: 2843.2678 - mae: 30.1794 - val_loss: 1055.0463 - val_mse: 1055.0463 - val_mae: 23.6357\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2876.8400 - mse: 2876.8398 - mae: 30.4663 - val_loss: 1056.6718 - val_mse: 1056.6719 - val_mae: 23.3940\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2891.7329 - mse: 2891.7334 - mae: 30.7676 - val_loss: 1054.6069 - val_mse: 1054.6069 - val_mae: 23.5991\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2858.6310 - mse: 2858.6313 - mae: 30.5447 - val_loss: 1053.1099 - val_mse: 1053.1101 - val_mae: 24.0012\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2865.9679 - mse: 2865.9678 - mae: 30.6500 - val_loss: 1053.9329 - val_mse: 1053.9330 - val_mae: 23.6241\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 488us/step - loss: 2814.4841 - mse: 2814.4844 - mae: 30.3222 - val_loss: 1054.3578 - val_mse: 1054.3577 - val_mae: 23.6324\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 484us/step - loss: 2804.7471 - mse: 2804.7480 - mae: 29.4611 - val_loss: 1053.2337 - val_mse: 1053.2336 - val_mae: 24.1980\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 651us/step - loss: 2792.1216 - mse: 2792.1216 - mae: 29.9961 - val_loss: 1052.7204 - val_mse: 1052.7205 - val_mae: 23.7001\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 459us/step - loss: 2815.8253 - mse: 2815.8245 - mae: 30.2811 - val_loss: 1052.6665 - val_mse: 1052.6666 - val_mae: 23.6394\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2845.7051 - mse: 2845.7051 - mae: 30.1383 - val_loss: 1051.8273 - val_mse: 1051.8273 - val_mae: 23.8017\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2872.7025 - mse: 2872.7029 - mae: 30.5834 - val_loss: 1051.8987 - val_mse: 1051.8987 - val_mae: 23.7874\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2817.0434 - mse: 2817.0430 - mae: 30.4211 - val_loss: 1051.6525 - val_mse: 1051.6525 - val_mae: 24.1077\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2781.0226 - mse: 2781.0229 - mae: 30.2580 - val_loss: 1054.6791 - val_mse: 1054.6791 - val_mae: 23.4650\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2881.6272 - mse: 2881.6277 - mae: 30.5691 - val_loss: 1053.4101 - val_mse: 1053.4100 - val_mae: 23.6150\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2731.2825 - mse: 2731.2822 - mae: 29.9256 - val_loss: 1052.1958 - val_mse: 1052.1958 - val_mae: 23.7991\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2901.2849 - mse: 2901.2859 - mae: 30.6500 - val_loss: 1052.3012 - val_mse: 1052.3013 - val_mae: 23.7681\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 476us/step - loss: 2819.5977 - mse: 2819.5977 - mae: 29.4439 - val_loss: 1052.1046 - val_mse: 1052.1047 - val_mae: 23.9066\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2877.0621 - mse: 2877.0620 - mae: 30.3132 - val_loss: 1052.7952 - val_mse: 1052.7953 - val_mae: 24.1395\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2811.3126 - mse: 2811.3123 - mae: 30.2640 - val_loss: 1051.7453 - val_mse: 1051.7452 - val_mae: 23.9641\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2835.0889 - mse: 2835.0891 - mae: 29.9269 - val_loss: 1051.8219 - val_mse: 1051.8219 - val_mae: 23.8374\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2905.4687 - mse: 2905.4680 - mae: 30.3363 - val_loss: 1052.7904 - val_mse: 1052.7904 - val_mae: 23.5913\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2848.4160 - mse: 2848.4163 - mae: 30.6260 - val_loss: 1051.3109 - val_mse: 1051.3109 - val_mae: 23.7429\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 537us/step - loss: 2804.8241 - mse: 2804.8245 - mae: 29.4757 - val_loss: 1050.7745 - val_mse: 1050.7747 - val_mae: 23.7144\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2792.0639 - mse: 2792.0635 - mae: 29.6113 - val_loss: 1050.1154 - val_mse: 1050.1154 - val_mae: 24.0084\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2879.8360 - mse: 2879.8369 - mae: 30.4221 - val_loss: 1050.7271 - val_mse: 1050.7272 - val_mae: 23.9132\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2846.6576 - mse: 2846.6570 - mae: 30.3869 - val_loss: 1051.2543 - val_mse: 1051.2543 - val_mae: 24.0293\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 643us/step - loss: 2879.1197 - mse: 2879.1196 - mae: 30.4386 - val_loss: 1050.8082 - val_mse: 1050.8082 - val_mae: 24.1947\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2790.8367 - mse: 2790.8376 - mae: 30.0014 - val_loss: 1050.4129 - val_mse: 1050.4128 - val_mae: 24.1672\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2855.1180 - mse: 2855.1177 - mae: 30.2508 - val_loss: 1050.4167 - val_mse: 1050.4167 - val_mae: 24.1112\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 680us/step - loss: 2868.6736 - mse: 2868.6738 - mae: 30.3285 - val_loss: 1050.6164 - val_mse: 1050.6165 - val_mae: 23.7940\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2881.0123 - mse: 2881.0129 - mae: 30.6142 - val_loss: 1051.4034 - val_mse: 1051.4034 - val_mae: 24.1008\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2789.9420 - mse: 2789.9414 - mae: 29.8834 - val_loss: 1050.4687 - val_mse: 1050.4686 - val_mae: 23.9101\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2847.0442 - mse: 2847.0444 - mae: 30.1603 - val_loss: 1048.9374 - val_mse: 1048.9374 - val_mae: 24.0590\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2740.9466 - mse: 2740.9465 - mae: 29.9799 - val_loss: 1048.4023 - val_mse: 1048.4023 - val_mae: 24.1395\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2796.4027 - mse: 2796.4026 - mae: 30.1170 - val_loss: 1047.6830 - val_mse: 1047.6830 - val_mae: 23.8492\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2571.0327 - mse: 2571.0322 - mae: 29.7095 - val_loss: 1532.6471 - val_mse: 1532.6473 - val_mae: 28.0170\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 657us/step - loss: 2597.0626 - mse: 2597.0635 - mae: 30.0429 - val_loss: 1533.1231 - val_mse: 1533.1232 - val_mae: 27.9581\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 647us/step - loss: 2574.2968 - mse: 2574.2969 - mae: 29.7603 - val_loss: 1531.9198 - val_mse: 1531.9197 - val_mae: 27.9895\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 518us/step - loss: 2582.8617 - mse: 2582.8623 - mae: 29.5219 - val_loss: 1528.8043 - val_mse: 1528.8044 - val_mae: 28.1797\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2532.1194 - mse: 2532.1194 - mae: 29.5915 - val_loss: 1528.8111 - val_mse: 1528.8110 - val_mae: 28.1121\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2598.5907 - mse: 2598.5916 - mae: 30.0067 - val_loss: 1526.5305 - val_mse: 1526.5305 - val_mae: 28.2145\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2511.0545 - mse: 2511.0549 - mae: 29.4905 - val_loss: 1540.0593 - val_mse: 1540.0594 - val_mae: 27.4570\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2564.9522 - mse: 2564.9524 - mae: 29.7681 - val_loss: 1527.0058 - val_mse: 1527.0059 - val_mae: 28.1099\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2576.1487 - mse: 2576.1487 - mae: 30.1444 - val_loss: 1526.8105 - val_mse: 1526.8105 - val_mae: 28.1076\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 520us/step - loss: 2572.3260 - mse: 2572.3250 - mae: 29.9130 - val_loss: 1538.4947 - val_mse: 1538.4946 - val_mae: 27.4937\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2507.8283 - mse: 2507.8291 - mae: 29.4594 - val_loss: 1529.1010 - val_mse: 1529.1011 - val_mae: 27.9248\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 543us/step - loss: 2523.9726 - mse: 2523.9722 - mae: 29.7526 - val_loss: 1528.3840 - val_mse: 1528.3840 - val_mae: 27.9472\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2539.5683 - mse: 2539.5684 - mae: 29.8047 - val_loss: 1527.7683 - val_mse: 1527.7686 - val_mae: 27.9473\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2575.9798 - mse: 2575.9792 - mae: 29.3373 - val_loss: 1525.2201 - val_mse: 1525.2201 - val_mae: 28.0813\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2515.2652 - mse: 2515.2654 - mae: 29.4929 - val_loss: 1525.0598 - val_mse: 1525.0598 - val_mae: 28.0899\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 669us/step - loss: 2534.8275 - mse: 2534.8281 - mae: 29.4384 - val_loss: 1530.6067 - val_mse: 1530.6068 - val_mae: 27.7200\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2472.5777 - mse: 2472.5774 - mae: 29.1839 - val_loss: 1518.9683 - val_mse: 1518.9683 - val_mae: 28.6320\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2519.6104 - mse: 2519.6104 - mae: 29.5055 - val_loss: 1531.3170 - val_mse: 1531.3171 - val_mae: 27.6317\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2495.0493 - mse: 2495.0488 - mae: 29.3504 - val_loss: 1521.1964 - val_mse: 1521.1962 - val_mae: 28.2287\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2514.8444 - mse: 2514.8442 - mae: 29.2309 - val_loss: 1529.2964 - val_mse: 1529.2965 - val_mae: 27.7090\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2518.4381 - mse: 2518.4380 - mae: 29.3425 - val_loss: 1520.3748 - val_mse: 1520.3748 - val_mae: 28.2640\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 541us/step - loss: 2517.5287 - mse: 2517.5288 - mae: 29.3752 - val_loss: 1522.0247 - val_mse: 1522.0248 - val_mae: 28.1023\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2522.2321 - mse: 2522.2332 - mae: 29.1490 - val_loss: 1522.6620 - val_mse: 1522.6620 - val_mae: 28.0293\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2536.1426 - mse: 2536.1421 - mae: 29.6997 - val_loss: 1528.7706 - val_mse: 1528.7705 - val_mae: 27.6713\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2552.4197 - mse: 2552.4202 - mae: 29.7165 - val_loss: 1524.0562 - val_mse: 1524.0563 - val_mae: 27.9101\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 670us/step - loss: 2496.8550 - mse: 2496.8545 - mae: 29.4344 - val_loss: 1520.6726 - val_mse: 1520.6726 - val_mae: 28.1244\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2553.0861 - mse: 2553.0864 - mae: 29.9553 - val_loss: 1520.8016 - val_mse: 1520.8015 - val_mae: 28.1048\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2532.0028 - mse: 2532.0029 - mae: 29.3856 - val_loss: 1525.8301 - val_mse: 1525.8302 - val_mae: 27.7913\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2494.0098 - mse: 2494.0093 - mae: 29.3846 - val_loss: 1521.7746 - val_mse: 1521.7745 - val_mae: 27.9972\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2465.8607 - mse: 2465.8616 - mae: 29.3042 - val_loss: 1521.2309 - val_mse: 1521.2308 - val_mae: 28.0022\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2481.9342 - mse: 2481.9351 - mae: 29.2149 - val_loss: 1521.7703 - val_mse: 1521.7705 - val_mae: 27.9333\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2531.4922 - mse: 2531.4917 - mae: 29.2188 - val_loss: 1524.5941 - val_mse: 1524.5942 - val_mae: 27.7509\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2548.3517 - mse: 2548.3523 - mae: 29.3994 - val_loss: 1515.4509 - val_mse: 1515.4508 - val_mae: 28.4271\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2561.5137 - mse: 2561.5139 - mae: 29.8330 - val_loss: 1522.4526 - val_mse: 1522.4526 - val_mae: 27.8481\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 506us/step - loss: 2501.9487 - mse: 2501.9495 - mae: 29.1743 - val_loss: 1516.5095 - val_mse: 1516.5093 - val_mae: 28.2435\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2507.2264 - mse: 2507.2258 - mae: 29.0325 - val_loss: 1518.5432 - val_mse: 1518.5431 - val_mae: 28.0360\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2541.9006 - mse: 2541.9004 - mae: 29.6369 - val_loss: 1522.5419 - val_mse: 1522.5417 - val_mae: 27.7847\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 536us/step - loss: 2524.6869 - mse: 2524.6865 - mae: 29.6853 - val_loss: 1518.6591 - val_mse: 1518.6591 - val_mae: 28.0087\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2518.4491 - mse: 2518.4492 - mae: 29.4472 - val_loss: 1520.5745 - val_mse: 1520.5745 - val_mae: 27.8642\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2495.4334 - mse: 2495.4331 - mae: 29.1459 - val_loss: 1514.9412 - val_mse: 1514.9413 - val_mae: 28.2582\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2481.4734 - mse: 2481.4734 - mae: 29.3901 - val_loss: 1517.5507 - val_mse: 1517.5507 - val_mae: 28.0250\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 526us/step - loss: 2534.6698 - mse: 2534.6697 - mae: 29.5350 - val_loss: 1517.9009 - val_mse: 1517.9009 - val_mae: 27.9939\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 477us/step - loss: 2494.9653 - mse: 2494.9661 - mae: 29.3828 - val_loss: 1517.2513 - val_mse: 1517.2511 - val_mae: 28.0507\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2469.0643 - mse: 2469.0640 - mae: 28.7921 - val_loss: 1516.2015 - val_mse: 1516.2015 - val_mae: 28.1469\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2466.8649 - mse: 2466.8657 - mae: 29.0891 - val_loss: 1520.3097 - val_mse: 1520.3098 - val_mae: 27.8207\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2553.6348 - mse: 2553.6353 - mae: 29.8011 - val_loss: 1525.4598 - val_mse: 1525.4597 - val_mae: 27.5543\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2506.2728 - mse: 2506.2729 - mae: 29.2601 - val_loss: 1516.7316 - val_mse: 1516.7312 - val_mae: 28.0199\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2470.5331 - mse: 2470.5332 - mae: 29.3769 - val_loss: 1516.9737 - val_mse: 1516.9739 - val_mae: 27.9377\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2525.3732 - mse: 2525.3738 - mae: 29.2483 - val_loss: 1521.1950 - val_mse: 1521.1948 - val_mae: 27.6727\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 496us/step - loss: 2494.1277 - mse: 2494.1272 - mae: 29.3998 - val_loss: 1520.8875 - val_mse: 1520.8875 - val_mae: 27.6470\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2532.1434 - mse: 2532.1431 - mae: 29.3008 - val_loss: 1513.4906 - val_mse: 1513.4906 - val_mae: 28.0983\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2454.0069 - mse: 2454.0073 - mae: 29.4131 - val_loss: 1515.6567 - val_mse: 1515.6565 - val_mae: 27.9078\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2515.3579 - mse: 2515.3584 - mae: 29.3057 - val_loss: 1515.1532 - val_mse: 1515.1532 - val_mae: 27.9539\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2486.4901 - mse: 2486.4912 - mae: 29.1164 - val_loss: 1518.5819 - val_mse: 1518.5819 - val_mae: 27.7378\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2454.9918 - mse: 2454.9924 - mae: 28.8709 - val_loss: 1512.5606 - val_mse: 1512.5605 - val_mae: 28.1125\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 703us/step - loss: 2496.6721 - mse: 2496.6724 - mae: 29.1369 - val_loss: 1513.7602 - val_mse: 1513.7600 - val_mae: 28.0052\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 700us/step - loss: 2456.8380 - mse: 2456.8376 - mae: 28.6008 - val_loss: 1510.9428 - val_mse: 1510.9426 - val_mae: 28.2147\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2493.7247 - mse: 2493.7239 - mae: 29.1568 - val_loss: 1513.9696 - val_mse: 1513.9696 - val_mae: 27.9648\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2489.2810 - mse: 2489.2812 - mae: 29.0246 - val_loss: 1518.5990 - val_mse: 1518.5991 - val_mae: 27.6850\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2509.6636 - mse: 2509.6643 - mae: 29.1142 - val_loss: 1512.9293 - val_mse: 1512.9294 - val_mae: 27.9795\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 497us/step - loss: 2476.5788 - mse: 2476.5791 - mae: 29.0327 - val_loss: 1509.6405 - val_mse: 1509.6406 - val_mae: 28.2277\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2494.0010 - mse: 2494.0024 - mae: 29.0880 - val_loss: 1511.3289 - val_mse: 1511.3290 - val_mae: 28.0399\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2450.1671 - mse: 2450.1667 - mae: 28.8494 - val_loss: 1509.0337 - val_mse: 1509.0336 - val_mae: 28.2268\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2435.0373 - mse: 2435.0374 - mae: 29.1121 - val_loss: 1511.6750 - val_mse: 1511.6749 - val_mae: 27.9345\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 681us/step - loss: 2431.6175 - mse: 2431.6184 - mae: 28.7982 - val_loss: 1508.6745 - val_mse: 1508.6744 - val_mae: 28.1102\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2465.3459 - mse: 2465.3469 - mae: 28.7895 - val_loss: 1510.7047 - val_mse: 1510.7047 - val_mae: 27.9206\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2474.9750 - mse: 2474.9753 - mae: 29.5982 - val_loss: 1510.9063 - val_mse: 1510.9062 - val_mae: 27.8716\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2490.6437 - mse: 2490.6431 - mae: 28.6817 - val_loss: 1510.1393 - val_mse: 1510.1393 - val_mae: 27.8977\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 539us/step - loss: 2453.9650 - mse: 2453.9644 - mae: 28.7751 - val_loss: 1510.1817 - val_mse: 1510.1816 - val_mae: 27.8536\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2517.5707 - mse: 2517.5708 - mae: 28.6006 - val_loss: 1508.5803 - val_mse: 1508.5802 - val_mae: 27.9811\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 538us/step - loss: 2489.8727 - mse: 2489.8723 - mae: 29.1573 - val_loss: 1511.0752 - val_mse: 1511.0750 - val_mae: 27.7615\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2423.3555 - mse: 2423.3547 - mae: 28.8240 - val_loss: 1510.3681 - val_mse: 1510.3682 - val_mae: 27.7909\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2518.8197 - mse: 2518.8198 - mae: 29.0619 - val_loss: 1520.0940 - val_mse: 1520.0939 - val_mae: 27.2824\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2455.6367 - mse: 2455.6362 - mae: 28.9493 - val_loss: 1502.6793 - val_mse: 1502.6793 - val_mae: 28.4039\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2517.3245 - mse: 2517.3252 - mae: 29.3893 - val_loss: 1513.1134 - val_mse: 1513.1134 - val_mae: 27.5428\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2427.3841 - mse: 2427.3850 - mae: 28.6220 - val_loss: 1505.8919 - val_mse: 1505.8918 - val_mae: 27.9878\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2464.9574 - mse: 2464.9575 - mae: 29.3344 - val_loss: 1504.0916 - val_mse: 1504.0916 - val_mae: 28.1093\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 680us/step - loss: 2433.3442 - mse: 2433.3457 - mae: 28.6514 - val_loss: 1505.3002 - val_mse: 1505.3000 - val_mae: 27.9726\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 668us/step - loss: 2422.8583 - mse: 2422.8584 - mae: 28.8165 - val_loss: 1508.3119 - val_mse: 1508.3119 - val_mae: 27.7279\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 668us/step - loss: 2434.4128 - mse: 2434.4131 - mae: 28.5808 - val_loss: 1501.5740 - val_mse: 1501.5739 - val_mae: 28.2223\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2325.1790 - mse: 2325.1787 - mae: 29.0918 - val_loss: 3704.0913 - val_mse: 3704.0923 - val_mae: 23.6272\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2374.6896 - mse: 2374.6892 - mae: 28.9579 - val_loss: 3704.5696 - val_mse: 3704.5696 - val_mae: 24.4207\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 679us/step - loss: 2320.0440 - mse: 2320.0439 - mae: 29.3271 - val_loss: 3703.9216 - val_mse: 3703.9209 - val_mae: 23.7427\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2386.8631 - mse: 2386.8633 - mae: 29.8029 - val_loss: 3704.1589 - val_mse: 3704.1587 - val_mae: 23.6541\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2353.6224 - mse: 2353.6228 - mae: 29.6714 - val_loss: 3704.4729 - val_mse: 3704.4731 - val_mae: 23.4700\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2422.0269 - mse: 2422.0269 - mae: 29.6298 - val_loss: 3705.2925 - val_mse: 3705.2932 - val_mae: 23.4117\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2348.8820 - mse: 2348.8821 - mae: 29.2762 - val_loss: 3701.9248 - val_mse: 3701.9248 - val_mae: 24.0118\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2398.9089 - mse: 2398.9087 - mae: 29.4410 - val_loss: 3701.9055 - val_mse: 3701.9060 - val_mae: 23.9222\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2357.1495 - mse: 2357.1489 - mae: 29.7725 - val_loss: 3703.1476 - val_mse: 3703.1470 - val_mae: 23.5765\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 691us/step - loss: 2356.2026 - mse: 2356.2029 - mae: 29.2521 - val_loss: 3703.9827 - val_mse: 3703.9827 - val_mae: 23.8191\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2377.4348 - mse: 2377.4343 - mae: 29.0162 - val_loss: 3703.8983 - val_mse: 3703.8979 - val_mae: 23.9408\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2315.0864 - mse: 2315.0859 - mae: 29.0426 - val_loss: 3704.0149 - val_mse: 3704.0146 - val_mae: 24.0792\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2349.0428 - mse: 2349.0427 - mae: 29.3767 - val_loss: 3704.5798 - val_mse: 3704.5796 - val_mae: 24.0860\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2387.7675 - mse: 2387.7678 - mae: 29.5471 - val_loss: 3706.7702 - val_mse: 3706.7700 - val_mae: 23.5291\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2341.0861 - mse: 2341.0864 - mae: 29.0100 - val_loss: 3705.9658 - val_mse: 3705.9651 - val_mae: 23.8209\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2372.5445 - mse: 2372.5444 - mae: 29.4033 - val_loss: 3706.7861 - val_mse: 3706.7861 - val_mae: 23.8500\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2376.0980 - mse: 2376.0981 - mae: 29.4976 - val_loss: 3707.2263 - val_mse: 3707.2275 - val_mae: 23.5183\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2316.5656 - mse: 2316.5654 - mae: 29.2964 - val_loss: 3707.3999 - val_mse: 3707.3999 - val_mae: 23.5687\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2333.3056 - mse: 2333.3059 - mae: 29.3100 - val_loss: 3706.4933 - val_mse: 3706.4929 - val_mae: 23.8108\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 675us/step - loss: 2416.0444 - mse: 2416.0449 - mae: 29.4224 - val_loss: 3706.6760 - val_mse: 3706.6768 - val_mae: 23.5681\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 669us/step - loss: 2379.5060 - mse: 2379.5061 - mae: 29.4335 - val_loss: 3703.7737 - val_mse: 3703.7727 - val_mae: 23.7677\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 655us/step - loss: 2324.1307 - mse: 2324.1304 - mae: 28.9975 - val_loss: 3703.8670 - val_mse: 3703.8672 - val_mae: 24.0385\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2365.2814 - mse: 2365.2812 - mae: 29.1714 - val_loss: 3704.4241 - val_mse: 3704.4241 - val_mae: 24.3789\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2350.3862 - mse: 2350.3857 - mae: 29.2619 - val_loss: 3705.9041 - val_mse: 3705.9041 - val_mae: 24.1892\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2367.4501 - mse: 2367.4507 - mae: 29.4692 - val_loss: 3704.8808 - val_mse: 3704.8809 - val_mae: 23.7347\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2385.4262 - mse: 2385.4258 - mae: 29.4173 - val_loss: 3704.0188 - val_mse: 3704.0198 - val_mae: 23.8839\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2375.5352 - mse: 2375.5352 - mae: 29.5298 - val_loss: 3702.8152 - val_mse: 3702.8142 - val_mae: 23.8666\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2390.1460 - mse: 2390.1465 - mae: 29.6036 - val_loss: 3704.0392 - val_mse: 3704.0386 - val_mae: 23.6112\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 1s 480us/step - loss: 2337.2058 - mse: 2337.2065 - mae: 29.0693 - val_loss: 3705.1249 - val_mse: 3705.1248 - val_mae: 23.7810\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2381.6835 - mse: 2381.6841 - mae: 29.6166 - val_loss: 3702.8323 - val_mse: 3702.8320 - val_mae: 23.9120\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2363.1613 - mse: 2363.1624 - mae: 29.4989 - val_loss: 3702.6445 - val_mse: 3702.6443 - val_mae: 23.8229\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2348.7599 - mse: 2348.7598 - mae: 29.0585 - val_loss: 3703.4519 - val_mse: 3703.4524 - val_mae: 23.8410\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2404.5960 - mse: 2404.5959 - mae: 29.6566 - val_loss: 3702.6835 - val_mse: 3702.6833 - val_mae: 23.7345\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2339.2726 - mse: 2339.2722 - mae: 29.1111 - val_loss: 3703.8059 - val_mse: 3703.8062 - val_mae: 23.6055\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 505us/step - loss: 2365.1232 - mse: 2365.1243 - mae: 29.1971 - val_loss: 3702.3914 - val_mse: 3702.3911 - val_mae: 23.8021\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2306.2602 - mse: 2306.2598 - mae: 29.2397 - val_loss: 3702.4657 - val_mse: 3702.4663 - val_mae: 24.1307\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2285.0232 - mse: 2285.0239 - mae: 28.6727 - val_loss: 3703.2218 - val_mse: 3703.2219 - val_mae: 24.1913\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2372.9168 - mse: 2372.9163 - mae: 29.5177 - val_loss: 3703.2613 - val_mse: 3703.2610 - val_mae: 23.7450\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2325.9504 - mse: 2325.9504 - mae: 28.9172 - val_loss: 3704.4533 - val_mse: 3704.4534 - val_mae: 23.7807\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2375.2120 - mse: 2375.2124 - mae: 29.1241 - val_loss: 3706.2052 - val_mse: 3706.2056 - val_mae: 23.9537\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2335.8432 - mse: 2335.8435 - mae: 28.9070 - val_loss: 3707.0928 - val_mse: 3707.0923 - val_mae: 23.9937\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2372.9902 - mse: 2372.9902 - mae: 29.2370 - val_loss: 3706.5000 - val_mse: 3706.4990 - val_mae: 23.7277\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2317.3217 - mse: 2317.3213 - mae: 29.1287 - val_loss: 3706.7484 - val_mse: 3706.7490 - val_mae: 23.9186\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2331.6035 - mse: 2331.6045 - mae: 28.9466 - val_loss: 3706.6090 - val_mse: 3706.6089 - val_mae: 23.9032\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2324.6649 - mse: 2324.6655 - mae: 29.1570 - val_loss: 3708.0400 - val_mse: 3708.0400 - val_mae: 24.2601\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2375.0603 - mse: 2375.0605 - mae: 29.5914 - val_loss: 3706.7580 - val_mse: 3706.7583 - val_mae: 23.9679\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2341.2228 - mse: 2341.2229 - mae: 29.2699 - val_loss: 3705.6135 - val_mse: 3705.6130 - val_mae: 23.7257\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 529us/step - loss: 2321.2598 - mse: 2321.2603 - mae: 29.1398 - val_loss: 3705.7251 - val_mse: 3705.7253 - val_mae: 23.8900\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 532us/step - loss: 2380.8117 - mse: 2380.8125 - mae: 29.2684 - val_loss: 3704.2882 - val_mse: 3704.2881 - val_mae: 24.2695\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 506us/step - loss: 2320.0824 - mse: 2320.0823 - mae: 29.1421 - val_loss: 3705.0284 - val_mse: 3705.0273 - val_mae: 23.8000\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 560us/step - loss: 2365.2720 - mse: 2365.2727 - mae: 29.2515 - val_loss: 3704.7971 - val_mse: 3704.7979 - val_mae: 24.4678\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2388.6730 - mse: 2388.6731 - mae: 29.4552 - val_loss: 3702.7500 - val_mse: 3702.7507 - val_mae: 23.8493\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 519us/step - loss: 2334.4489 - mse: 2334.4485 - mae: 29.0741 - val_loss: 3702.5128 - val_mse: 3702.5127 - val_mae: 24.0472\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2367.8022 - mse: 2367.8025 - mae: 29.0823 - val_loss: 3703.0299 - val_mse: 3703.0300 - val_mae: 23.5483\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2348.3288 - mse: 2348.3298 - mae: 28.9839 - val_loss: 3703.7513 - val_mse: 3703.7512 - val_mae: 23.7793\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2334.2183 - mse: 2334.2178 - mae: 28.7822 - val_loss: 3706.0559 - val_mse: 3706.0562 - val_mae: 24.0270\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2316.7480 - mse: 2316.7480 - mae: 28.8181 - val_loss: 3705.4486 - val_mse: 3705.4485 - val_mae: 23.7953\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2301.8179 - mse: 2301.8171 - mae: 28.6316 - val_loss: 3706.6193 - val_mse: 3706.6191 - val_mae: 23.8629\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2327.4250 - mse: 2327.4246 - mae: 29.0715 - val_loss: 3707.0750 - val_mse: 3707.0752 - val_mae: 24.0617\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2322.0749 - mse: 2322.0754 - mae: 28.7435 - val_loss: 3705.7807 - val_mse: 3705.7805 - val_mae: 24.0790\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2278.9772 - mse: 2278.9768 - mae: 28.6995 - val_loss: 3704.5885 - val_mse: 3704.5876 - val_mae: 24.1726\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2325.5462 - mse: 2325.5459 - mae: 29.0556 - val_loss: 3704.8386 - val_mse: 3704.8386 - val_mae: 23.9405\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2321.9487 - mse: 2321.9485 - mae: 28.9866 - val_loss: 3706.9294 - val_mse: 3706.9299 - val_mae: 24.2124\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2361.8144 - mse: 2361.8145 - mae: 29.5368 - val_loss: 3707.4418 - val_mse: 3707.4424 - val_mae: 24.0278\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 674us/step - loss: 2363.5752 - mse: 2363.5754 - mae: 29.1932 - val_loss: 3706.5645 - val_mse: 3706.5647 - val_mae: 24.0456\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2336.1352 - mse: 2336.1348 - mae: 28.9056 - val_loss: 3706.2221 - val_mse: 3706.2212 - val_mae: 23.8712\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2327.1563 - mse: 2327.1555 - mae: 28.7203 - val_loss: 3708.5820 - val_mse: 3708.5813 - val_mae: 23.8428\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2311.5106 - mse: 2311.5105 - mae: 29.2952 - val_loss: 3708.0956 - val_mse: 3708.0945 - val_mae: 23.5256\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2267.9724 - mse: 2267.9727 - mae: 28.5414 - val_loss: 3707.6525 - val_mse: 3707.6533 - val_mae: 23.6503\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2282.4232 - mse: 2282.4231 - mae: 28.8076 - val_loss: 3707.0048 - val_mse: 3707.0056 - val_mae: 24.1474\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2326.7854 - mse: 2326.7854 - mae: 28.7281 - val_loss: 3706.8763 - val_mse: 3706.8765 - val_mae: 23.8461\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2287.1004 - mse: 2287.1008 - mae: 28.5129 - val_loss: 3707.3783 - val_mse: 3707.3789 - val_mae: 23.8830\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2306.6909 - mse: 2306.6912 - mae: 29.2058 - val_loss: 3707.3266 - val_mse: 3707.3259 - val_mae: 23.7246\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2360.1985 - mse: 2360.1987 - mae: 29.0646 - val_loss: 3705.8516 - val_mse: 3705.8508 - val_mae: 24.0873\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2316.8202 - mse: 2316.8201 - mae: 29.2587 - val_loss: 3705.2933 - val_mse: 3705.2939 - val_mae: 23.8174\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2281.2584 - mse: 2281.2590 - mae: 28.7200 - val_loss: 3705.6297 - val_mse: 3705.6301 - val_mae: 23.9335\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 541us/step - loss: 2296.4183 - mse: 2296.4189 - mae: 28.9246 - val_loss: 3708.3642 - val_mse: 3708.3630 - val_mae: 24.0173\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2327.4734 - mse: 2327.4736 - mae: 28.9548 - val_loss: 3708.1006 - val_mse: 3708.1008 - val_mae: 23.9892\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2350.2156 - mse: 2350.2158 - mae: 29.0728 - val_loss: 3710.1061 - val_mse: 3710.1057 - val_mae: 23.9453\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2277.0647 - mse: 2277.0640 - mae: 28.9582 - val_loss: 3712.1934 - val_mse: 3712.1931 - val_mae: 23.6844\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2688.2184 - mse: 2688.2178 - mae: 28.4481 - val_loss: 2191.0083 - val_mse: 2191.0083 - val_mae: 26.8673\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2725.6735 - mse: 2725.6733 - mae: 28.4983 - val_loss: 2204.4183 - val_mse: 2204.4182 - val_mae: 26.7320\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2665.4823 - mse: 2665.4827 - mae: 28.2432 - val_loss: 2201.2775 - val_mse: 2201.2778 - val_mae: 27.0152\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2734.4569 - mse: 2734.4578 - mae: 28.6049 - val_loss: 2217.2245 - val_mse: 2217.2244 - val_mae: 26.4619\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 532us/step - loss: 2738.1764 - mse: 2738.1758 - mae: 28.2884 - val_loss: 2214.3100 - val_mse: 2214.3098 - val_mae: 26.8133\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2687.8664 - mse: 2687.8662 - mae: 28.1806 - val_loss: 2214.5552 - val_mse: 2214.5554 - val_mae: 26.9560\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2733.9959 - mse: 2733.9966 - mae: 28.8254 - val_loss: 2229.4343 - val_mse: 2229.4346 - val_mae: 26.6348\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 655us/step - loss: 2714.3349 - mse: 2714.3345 - mae: 29.0334 - val_loss: 2224.8271 - val_mse: 2224.8269 - val_mae: 26.8031\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 548us/step - loss: 2696.1398 - mse: 2696.1406 - mae: 28.3936 - val_loss: 2223.7946 - val_mse: 2223.7947 - val_mae: 26.6232\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2725.0606 - mse: 2725.0610 - mae: 28.3293 - val_loss: 2219.1490 - val_mse: 2219.1487 - val_mae: 26.6172\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2674.2017 - mse: 2674.2024 - mae: 28.2405 - val_loss: 2214.6879 - val_mse: 2214.6880 - val_mae: 26.8530\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2680.0331 - mse: 2680.0342 - mae: 28.2443 - val_loss: 2216.7267 - val_mse: 2216.7266 - val_mae: 26.6777\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2656.4936 - mse: 2656.4927 - mae: 28.3337 - val_loss: 2204.7390 - val_mse: 2204.7385 - val_mae: 26.9193\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2681.9025 - mse: 2681.9021 - mae: 28.5839 - val_loss: 2208.1778 - val_mse: 2208.1780 - val_mae: 26.7373\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2718.6516 - mse: 2718.6516 - mae: 28.5958 - val_loss: 2214.0256 - val_mse: 2214.0256 - val_mae: 26.7949\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 476us/step - loss: 2681.5564 - mse: 2681.5562 - mae: 28.1768 - val_loss: 2217.6733 - val_mse: 2217.6733 - val_mae: 26.9090\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2684.0429 - mse: 2684.0427 - mae: 28.3346 - val_loss: 2220.9102 - val_mse: 2220.9104 - val_mae: 26.6535\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2731.2585 - mse: 2731.2588 - mae: 28.6534 - val_loss: 2242.7608 - val_mse: 2242.7610 - val_mae: 26.3756\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 677us/step - loss: 2690.1568 - mse: 2690.1560 - mae: 28.4382 - val_loss: 2227.7229 - val_mse: 2227.7231 - val_mae: 26.8014\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2731.8192 - mse: 2731.8181 - mae: 28.3248 - val_loss: 2234.0284 - val_mse: 2234.0281 - val_mae: 26.5529\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2689.8891 - mse: 2689.8901 - mae: 28.3334 - val_loss: 2218.0520 - val_mse: 2218.0520 - val_mae: 27.0641\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2624.0606 - mse: 2624.0603 - mae: 27.9111 - val_loss: 2220.6225 - val_mse: 2220.6223 - val_mae: 26.7666\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2665.3754 - mse: 2665.3752 - mae: 28.4304 - val_loss: 2211.7185 - val_mse: 2211.7183 - val_mae: 27.1001\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2696.6925 - mse: 2696.6926 - mae: 28.4228 - val_loss: 2227.2002 - val_mse: 2227.2002 - val_mae: 26.6206\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2701.7486 - mse: 2701.7478 - mae: 28.4948 - val_loss: 2219.6298 - val_mse: 2219.6296 - val_mae: 26.9696\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2757.5675 - mse: 2757.5679 - mae: 28.7897 - val_loss: 2221.7223 - val_mse: 2221.7227 - val_mae: 26.6654\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 645us/step - loss: 2736.9816 - mse: 2736.9814 - mae: 28.3467 - val_loss: 2234.4968 - val_mse: 2234.4966 - val_mae: 26.5987\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2692.2061 - mse: 2692.2058 - mae: 28.1681 - val_loss: 2226.2408 - val_mse: 2226.2407 - val_mae: 26.7823\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2729.1865 - mse: 2729.1855 - mae: 28.2300 - val_loss: 2220.3921 - val_mse: 2220.3918 - val_mae: 26.8129\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2665.1097 - mse: 2665.1099 - mae: 28.4179 - val_loss: 2219.9337 - val_mse: 2219.9336 - val_mae: 26.9456\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2675.7962 - mse: 2675.7954 - mae: 28.1090 - val_loss: 2224.9291 - val_mse: 2224.9290 - val_mae: 26.8006\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2679.6635 - mse: 2679.6626 - mae: 28.5104 - val_loss: 2232.7311 - val_mse: 2232.7312 - val_mae: 26.6393\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2649.8422 - mse: 2649.8418 - mae: 28.0080 - val_loss: 2225.5579 - val_mse: 2225.5579 - val_mae: 26.8429\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2676.3855 - mse: 2676.3865 - mae: 28.4160 - val_loss: 2237.0303 - val_mse: 2237.0305 - val_mae: 26.5779\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2669.7341 - mse: 2669.7339 - mae: 28.0423 - val_loss: 2220.7739 - val_mse: 2220.7734 - val_mae: 26.9558\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2677.9745 - mse: 2677.9744 - mae: 28.0829 - val_loss: 2213.4280 - val_mse: 2213.4275 - val_mae: 27.1112\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2683.6983 - mse: 2683.6985 - mae: 28.1441 - val_loss: 2229.5112 - val_mse: 2229.5110 - val_mae: 27.2591\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2690.1904 - mse: 2690.1909 - mae: 28.5027 - val_loss: 2242.4623 - val_mse: 2242.4622 - val_mae: 26.9001\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2718.1227 - mse: 2718.1228 - mae: 28.4204 - val_loss: 2222.7169 - val_mse: 2222.7173 - val_mae: 27.2708\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2687.4674 - mse: 2687.4678 - mae: 28.6633 - val_loss: 2227.5252 - val_mse: 2227.5254 - val_mae: 26.7800\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2718.2772 - mse: 2718.2766 - mae: 28.4962 - val_loss: 2238.1186 - val_mse: 2238.1187 - val_mae: 26.6158\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2695.0486 - mse: 2695.0491 - mae: 28.3271 - val_loss: 2224.7918 - val_mse: 2224.7917 - val_mae: 26.9065\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2680.8344 - mse: 2680.8350 - mae: 28.3692 - val_loss: 2231.7587 - val_mse: 2231.7590 - val_mae: 27.1205\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2699.6371 - mse: 2699.6372 - mae: 28.6061 - val_loss: 2227.7919 - val_mse: 2227.7922 - val_mae: 27.1058\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 552us/step - loss: 2714.6135 - mse: 2714.6138 - mae: 28.6122 - val_loss: 2234.7304 - val_mse: 2234.7305 - val_mae: 26.8184\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 548us/step - loss: 2677.4740 - mse: 2677.4749 - mae: 28.3575 - val_loss: 2236.4458 - val_mse: 2236.4458 - val_mae: 26.7195\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2685.2526 - mse: 2685.2532 - mae: 28.2677 - val_loss: 2225.3585 - val_mse: 2225.3584 - val_mae: 26.8556\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2637.2150 - mse: 2637.2148 - mae: 28.3343 - val_loss: 2214.1849 - val_mse: 2214.1853 - val_mae: 27.1834\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 681us/step - loss: 2675.9890 - mse: 2675.9885 - mae: 28.2999 - val_loss: 2231.3107 - val_mse: 2231.3105 - val_mae: 26.8540\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 674us/step - loss: 2690.9609 - mse: 2690.9617 - mae: 28.4010 - val_loss: 2233.5722 - val_mse: 2233.5725 - val_mae: 26.8464\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2714.1202 - mse: 2714.1208 - mae: 28.5197 - val_loss: 2227.5782 - val_mse: 2227.5779 - val_mae: 26.7690\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2701.0710 - mse: 2701.0710 - mae: 28.4318 - val_loss: 2224.1396 - val_mse: 2224.1396 - val_mae: 27.2275\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2668.5351 - mse: 2668.5349 - mae: 28.2673 - val_loss: 2240.6788 - val_mse: 2240.6787 - val_mae: 26.9241\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2657.9588 - mse: 2657.9585 - mae: 27.9205 - val_loss: 2225.0455 - val_mse: 2225.0454 - val_mae: 27.3405\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2652.5340 - mse: 2652.5337 - mae: 28.3884 - val_loss: 2234.0673 - val_mse: 2234.0671 - val_mae: 26.9759\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2693.6307 - mse: 2693.6304 - mae: 28.4395 - val_loss: 2241.4839 - val_mse: 2241.4839 - val_mae: 26.8285\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2672.9659 - mse: 2672.9653 - mae: 28.5028 - val_loss: 2231.5890 - val_mse: 2231.5896 - val_mae: 26.8342\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2653.7870 - mse: 2653.7869 - mae: 28.3917 - val_loss: 2233.2370 - val_mse: 2233.2371 - val_mae: 26.9499\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2703.8999 - mse: 2703.8992 - mae: 28.4054 - val_loss: 2240.2403 - val_mse: 2240.2402 - val_mae: 26.9425\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 653us/step - loss: 2676.7204 - mse: 2676.7205 - mae: 28.0702 - val_loss: 2241.3413 - val_mse: 2241.3413 - val_mae: 26.8517\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2718.9719 - mse: 2718.9719 - mae: 28.3875 - val_loss: 2243.4990 - val_mse: 2243.4993 - val_mae: 27.0085\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2676.6536 - mse: 2676.6536 - mae: 28.0450 - val_loss: 2242.4530 - val_mse: 2242.4531 - val_mae: 27.1456\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2731.1659 - mse: 2731.1658 - mae: 28.6972 - val_loss: 2245.7622 - val_mse: 2245.7622 - val_mae: 26.8560\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2722.5049 - mse: 2722.5044 - mae: 28.4607 - val_loss: 2245.3743 - val_mse: 2245.3745 - val_mae: 26.9686\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2660.2749 - mse: 2660.2749 - mae: 27.9827 - val_loss: 2245.8436 - val_mse: 2245.8435 - val_mae: 26.8828\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2683.8284 - mse: 2683.8281 - mae: 28.0066 - val_loss: 2236.3060 - val_mse: 2236.3059 - val_mae: 27.1231\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2661.1755 - mse: 2661.1753 - mae: 27.8599 - val_loss: 2242.6839 - val_mse: 2242.6836 - val_mae: 26.8177\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2712.1942 - mse: 2712.1931 - mae: 28.6133 - val_loss: 2246.2529 - val_mse: 2246.2529 - val_mae: 26.8522\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2706.1940 - mse: 2706.1936 - mae: 28.4350 - val_loss: 2250.8062 - val_mse: 2250.8059 - val_mae: 26.8875\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2670.2763 - mse: 2670.2769 - mae: 28.2880 - val_loss: 2252.9477 - val_mse: 2252.9478 - val_mae: 27.0964\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2631.3224 - mse: 2631.3225 - mae: 28.1424 - val_loss: 2253.4165 - val_mse: 2253.4165 - val_mae: 27.1299\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2647.9950 - mse: 2647.9946 - mae: 27.9895 - val_loss: 2248.2160 - val_mse: 2248.2163 - val_mae: 26.9501\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2654.8225 - mse: 2654.8223 - mae: 28.2893 - val_loss: 2249.4939 - val_mse: 2249.4944 - val_mae: 26.8642\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2695.1658 - mse: 2695.1650 - mae: 28.2859 - val_loss: 2228.3181 - val_mse: 2228.3179 - val_mae: 27.0858\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 673us/step - loss: 2687.5659 - mse: 2687.5657 - mae: 28.2315 - val_loss: 2244.6919 - val_mse: 2244.6919 - val_mae: 26.6619\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2655.7655 - mse: 2655.7656 - mae: 27.9676 - val_loss: 2229.0658 - val_mse: 2229.0659 - val_mae: 27.1029\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2660.3265 - mse: 2660.3262 - mae: 28.4306 - val_loss: 2238.4134 - val_mse: 2238.4136 - val_mae: 27.0370\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2679.6992 - mse: 2679.6995 - mae: 28.4636 - val_loss: 2245.9464 - val_mse: 2245.9463 - val_mae: 27.0191\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2660.0466 - mse: 2660.0476 - mae: 28.2644 - val_loss: 2243.7216 - val_mse: 2243.7217 - val_mae: 27.3229\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2682.7934 - mse: 2682.7939 - mae: 28.2570 - val_loss: 2245.3681 - val_mse: 2245.3679 - val_mae: 26.9705\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 13326.1896 - mse: 13326.1895 - mae: 109.8912 - val_loss: 34604.3198 - val_mse: 34604.3203 - val_mae: 132.6814\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 13152.3368 - mse: 13152.3369 - mae: 109.1003 - val_loss: 34275.0484 - val_mse: 34275.0469 - val_mae: 131.4549\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 640us/step - loss: 12686.5313 - mse: 12686.5322 - mae: 106.9297 - val_loss: 33313.7200 - val_mse: 33313.7227 - val_mae: 127.8122\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 11367.8841 - mse: 11367.8838 - mae: 100.5159 - val_loss: 30733.2783 - val_mse: 30733.2793 - val_mae: 117.4837\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 8222.3314 - mse: 8222.3320 - mae: 82.9042 - val_loss: 24919.6538 - val_mse: 24919.6562 - val_mae: 90.0315\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 726us/step - loss: 3859.9624 - mse: 3859.9624 - mae: 49.5457 - val_loss: 17910.3719 - val_mse: 17910.3691 - val_mae: 40.0975\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 2688.8780 - mse: 2688.8779 - mae: 37.2931 - val_loss: 17312.7057 - val_mse: 17312.7031 - val_mae: 36.8391\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 645us/step - loss: 2776.1326 - mse: 2776.1328 - mae: 37.2383 - val_loss: 17728.3289 - val_mse: 17728.3301 - val_mae: 38.7531\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 659us/step - loss: 2530.9742 - mse: 2530.9744 - mae: 36.7008 - val_loss: 17401.2238 - val_mse: 17401.2246 - val_mae: 36.9991\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 645us/step - loss: 2575.1832 - mse: 2575.1836 - mae: 36.1316 - val_loss: 17474.1769 - val_mse: 17474.1758 - val_mae: 37.2391\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2632.2328 - mse: 2632.2327 - mae: 36.6347 - val_loss: 17609.6366 - val_mse: 17609.6367 - val_mae: 37.9022\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 680us/step - loss: 2484.4800 - mse: 2484.4800 - mae: 36.4665 - val_loss: 17395.5285 - val_mse: 17395.5273 - val_mae: 36.8922\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 2631.3825 - mse: 2631.3826 - mae: 35.8923 - val_loss: 17676.9043 - val_mse: 17676.9043 - val_mae: 38.2190\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 2604.7022 - mse: 2604.7021 - mae: 36.9250 - val_loss: 17624.0540 - val_mse: 17624.0547 - val_mae: 37.8324\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 607us/step - loss: 2610.8456 - mse: 2610.8455 - mae: 36.2380 - val_loss: 17475.4323 - val_mse: 17475.4336 - val_mae: 37.0542\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2507.4895 - mse: 2507.4897 - mae: 36.7298 - val_loss: 17625.1299 - val_mse: 17625.1309 - val_mae: 37.7766\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 705us/step - loss: 2458.4029 - mse: 2458.4031 - mae: 36.3246 - val_loss: 17499.5205 - val_mse: 17499.5195 - val_mae: 37.0810\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2370.9359 - mse: 2370.9355 - mae: 34.6113 - val_loss: 17498.2466 - val_mse: 17498.2461 - val_mae: 37.0383\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2562.7856 - mse: 2562.7854 - mae: 35.7804 - val_loss: 17455.8471 - val_mse: 17455.8477 - val_mae: 36.8608\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2543.9714 - mse: 2543.9714 - mae: 36.1398 - val_loss: 17656.6369 - val_mse: 17656.6367 - val_mae: 37.7547\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2363.2324 - mse: 2363.2324 - mae: 34.9201 - val_loss: 17587.7077 - val_mse: 17587.7070 - val_mae: 37.3228\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2319.1439 - mse: 2319.1438 - mae: 33.3842 - val_loss: 17415.5440 - val_mse: 17415.5430 - val_mae: 36.6950\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 2351.3155 - mse: 2351.3154 - mae: 34.9699 - val_loss: 17423.0344 - val_mse: 17423.0332 - val_mae: 36.6915\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 650us/step - loss: 2327.5111 - mse: 2327.5110 - mae: 34.0130 - val_loss: 17337.5042 - val_mse: 17337.5039 - val_mae: 36.5541\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2507.2500 - mse: 2507.2500 - mae: 35.1986 - val_loss: 17354.4797 - val_mse: 17354.4805 - val_mae: 36.5698\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2303.3000 - mse: 2303.3000 - mae: 33.0294 - val_loss: 17594.4606 - val_mse: 17594.4609 - val_mae: 37.1754\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2289.6873 - mse: 2289.6873 - mae: 33.5727 - val_loss: 17441.0828 - val_mse: 17441.0820 - val_mae: 36.6650\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 676us/step - loss: 2202.9860 - mse: 2202.9861 - mae: 32.8182 - val_loss: 17508.0370 - val_mse: 17508.0352 - val_mae: 36.8023\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 2537.1382 - mse: 2537.1382 - mae: 34.5037 - val_loss: 17561.3089 - val_mse: 17561.3086 - val_mae: 36.9470\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2281.0093 - mse: 2281.0093 - mae: 33.4981 - val_loss: 17397.4633 - val_mse: 17397.4648 - val_mae: 36.5399\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 2156.6749 - mse: 2156.6750 - mae: 33.5971 - val_loss: 17364.1103 - val_mse: 17364.1094 - val_mae: 36.4786\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 2224.1005 - mse: 2224.1006 - mae: 33.2153 - val_loss: 17523.9669 - val_mse: 17523.9668 - val_mae: 36.7327\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2328.2574 - mse: 2328.2576 - mae: 33.6031 - val_loss: 17542.3597 - val_mse: 17542.3594 - val_mae: 36.7753\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2114.9822 - mse: 2114.9822 - mae: 32.5991 - val_loss: 17349.1763 - val_mse: 17349.1758 - val_mae: 36.4309\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 465us/step - loss: 2222.2809 - mse: 2222.2810 - mae: 32.5691 - val_loss: 17459.4755 - val_mse: 17459.4766 - val_mae: 36.5570\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 495us/step - loss: 2253.5993 - mse: 2253.5996 - mae: 33.6727 - val_loss: 17445.4333 - val_mse: 17445.4336 - val_mae: 36.5249\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2261.6376 - mse: 2261.6375 - mae: 32.2830 - val_loss: 17400.2831 - val_mse: 17400.2832 - val_mae: 36.4534\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 2167.3526 - mse: 2167.3525 - mae: 32.5923 - val_loss: 17517.4537 - val_mse: 17517.4531 - val_mae: 36.6448\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2135.8525 - mse: 2135.8525 - mae: 32.6371 - val_loss: 17561.9878 - val_mse: 17561.9883 - val_mae: 36.7329\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2277.4926 - mse: 2277.4922 - mae: 33.8934 - val_loss: 17263.9339 - val_mse: 17263.9336 - val_mae: 36.2843\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 693us/step - loss: 1966.0961 - mse: 1966.0961 - mae: 31.5182 - val_loss: 17407.3362 - val_mse: 17407.3359 - val_mae: 36.4304\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2193.1043 - mse: 2193.1042 - mae: 32.3706 - val_loss: 17462.1330 - val_mse: 17462.1328 - val_mae: 36.4809\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 685us/step - loss: 2108.5185 - mse: 2108.5186 - mae: 31.8959 - val_loss: 17530.4605 - val_mse: 17530.4609 - val_mae: 36.6036\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 2077.5829 - mse: 2077.5830 - mae: 31.9478 - val_loss: 17521.2888 - val_mse: 17521.2871 - val_mae: 36.5772\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2124.2808 - mse: 2124.2810 - mae: 32.1832 - val_loss: 17448.4362 - val_mse: 17448.4355 - val_mae: 36.4460\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2220.8536 - mse: 2220.8535 - mae: 33.0338 - val_loss: 17500.4749 - val_mse: 17500.4766 - val_mae: 36.5191\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 2161.2678 - mse: 2161.2678 - mae: 32.0954 - val_loss: 17691.6455 - val_mse: 17691.6465 - val_mae: 37.0867\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2031.8060 - mse: 2031.8062 - mae: 31.6521 - val_loss: 17317.8344 - val_mse: 17317.8359 - val_mae: 36.2940\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 2096.4807 - mse: 2096.4810 - mae: 31.9118 - val_loss: 17484.9556 - val_mse: 17484.9551 - val_mae: 36.4780\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 2075.1126 - mse: 2075.1125 - mae: 30.7336 - val_loss: 17471.1820 - val_mse: 17471.1816 - val_mae: 36.4566\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 706us/step - loss: 1964.6356 - mse: 1964.6355 - mae: 31.4390 - val_loss: 17271.6413 - val_mse: 17271.6426 - val_mae: 36.2443\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 2083.3323 - mse: 2083.3325 - mae: 32.0128 - val_loss: 17519.3894 - val_mse: 17519.3887 - val_mae: 36.5110\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 691us/step - loss: 1856.8537 - mse: 1856.8539 - mae: 30.7588 - val_loss: 17425.3558 - val_mse: 17425.3555 - val_mae: 36.4025\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 774us/step - loss: 2047.4508 - mse: 2047.4509 - mae: 30.9837 - val_loss: 17371.8856 - val_mse: 17371.8867 - val_mae: 36.3427\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 698us/step - loss: 2000.9606 - mse: 2000.9606 - mae: 31.8497 - val_loss: 17428.2297 - val_mse: 17428.2305 - val_mae: 36.3933\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2013.4963 - mse: 2013.4965 - mae: 31.5826 - val_loss: 17401.8197 - val_mse: 17401.8203 - val_mae: 36.3512\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 1963.0206 - mse: 1963.0208 - mae: 30.4048 - val_loss: 17429.9444 - val_mse: 17429.9434 - val_mae: 36.3654\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 704us/step - loss: 2059.0660 - mse: 2059.0659 - mae: 30.8087 - val_loss: 17525.2163 - val_mse: 17525.2168 - val_mae: 36.4605\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 2032.5786 - mse: 2032.5785 - mae: 30.9876 - val_loss: 17339.7509 - val_mse: 17339.7500 - val_mae: 36.2445\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 525us/step - loss: 1997.2933 - mse: 1997.2935 - mae: 29.9964 - val_loss: 17441.8020 - val_mse: 17441.8027 - val_mae: 36.3405\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 1911.3164 - mse: 1911.3163 - mae: 30.2027 - val_loss: 17518.9058 - val_mse: 17518.9062 - val_mae: 36.4190\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 2067.0581 - mse: 2067.0583 - mae: 30.8084 - val_loss: 17502.2178 - val_mse: 17502.2168 - val_mae: 36.3900\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 1968.6067 - mse: 1968.6066 - mae: 30.5363 - val_loss: 17412.3359 - val_mse: 17412.3359 - val_mae: 36.2951\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 2062.1319 - mse: 2062.1318 - mae: 30.6894 - val_loss: 17505.4123 - val_mse: 17505.4121 - val_mae: 36.4089\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2014.1877 - mse: 2014.1877 - mae: 30.9456 - val_loss: 17572.1487 - val_mse: 17572.1484 - val_mae: 36.5142\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 663us/step - loss: 1903.4153 - mse: 1903.4155 - mae: 30.4827 - val_loss: 17480.5438 - val_mse: 17480.5449 - val_mae: 36.3874\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2157.7373 - mse: 2157.7375 - mae: 31.7215 - val_loss: 17559.6745 - val_mse: 17559.6738 - val_mae: 36.4826\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 2100.0486 - mse: 2100.0488 - mae: 32.4980 - val_loss: 17411.3907 - val_mse: 17411.3926 - val_mae: 36.3132\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2012.4254 - mse: 2012.4255 - mae: 30.9065 - val_loss: 17462.9637 - val_mse: 17462.9648 - val_mae: 36.3883\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 1963.4817 - mse: 1963.4816 - mae: 30.4135 - val_loss: 17596.8331 - val_mse: 17596.8320 - val_mae: 36.5842\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 2021.9322 - mse: 2021.9324 - mae: 31.0186 - val_loss: 17470.8648 - val_mse: 17470.8633 - val_mae: 36.4088\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 537us/step - loss: 2023.0843 - mse: 2023.0845 - mae: 30.5340 - val_loss: 17491.4976 - val_mse: 17491.4980 - val_mae: 36.4233\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 511us/step - loss: 1847.8506 - mse: 1847.8506 - mae: 30.0067 - val_loss: 17392.1087 - val_mse: 17392.1094 - val_mae: 36.3287\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 525us/step - loss: 2046.6966 - mse: 2046.6962 - mae: 31.5492 - val_loss: 17481.1952 - val_mse: 17481.1934 - val_mae: 36.4165\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 480us/step - loss: 1947.9528 - mse: 1947.9528 - mae: 29.7587 - val_loss: 17528.5245 - val_mse: 17528.5254 - val_mae: 36.4712\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 491us/step - loss: 2091.8275 - mse: 2091.8276 - mae: 30.7572 - val_loss: 17476.9934 - val_mse: 17476.9941 - val_mae: 36.4218\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 450us/step - loss: 1771.3992 - mse: 1771.3992 - mae: 28.6019 - val_loss: 17216.3175 - val_mse: 17216.3184 - val_mae: 36.5057\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 565us/step - loss: 1993.3318 - mse: 1993.3318 - mae: 31.5885 - val_loss: 17515.4194 - val_mse: 17515.4180 - val_mae: 36.4529\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 491us/step - loss: 2039.1879 - mse: 2039.1877 - mae: 30.8146 - val_loss: 17501.5119 - val_mse: 17501.5117 - val_mae: 36.4174\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 512us/step - loss: 1876.5956 - mse: 1876.5956 - mae: 29.5742 - val_loss: 17398.3825 - val_mse: 17398.3848 - val_mae: 36.3179\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 517us/step - loss: 4278.0366 - mse: 4278.0366 - mae: 34.6333 - val_loss: 2163.6187 - val_mse: 2163.6187 - val_mae: 30.4741\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 472us/step - loss: 4219.1755 - mse: 4219.1753 - mae: 34.1611 - val_loss: 2153.1600 - val_mse: 2153.1602 - val_mae: 30.4365\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 468us/step - loss: 4307.4251 - mse: 4307.4243 - mae: 36.4355 - val_loss: 2290.2095 - val_mse: 2290.2092 - val_mae: 30.9691\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 0s 481us/step - loss: 4166.4272 - mse: 4166.4268 - mae: 35.3344 - val_loss: 2227.1236 - val_mse: 2227.1235 - val_mae: 30.6845\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 0s 485us/step - loss: 4301.2166 - mse: 4301.2168 - mae: 35.9661 - val_loss: 2301.6751 - val_mse: 2301.6753 - val_mae: 31.0158\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 0s 490us/step - loss: 4228.0253 - mse: 4228.0254 - mae: 34.5528 - val_loss: 2258.6004 - val_mse: 2258.6006 - val_mae: 30.8180\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 503us/step - loss: 4074.3060 - mse: 4074.3062 - mae: 33.9844 - val_loss: 2324.1711 - val_mse: 2324.1711 - val_mae: 31.1125\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 369us/step - loss: 3987.9097 - mse: 3987.9097 - mae: 32.9426 - val_loss: 2254.1920 - val_mse: 2254.1921 - val_mae: 30.7884\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 459us/step - loss: 4283.8727 - mse: 4283.8726 - mae: 35.6898 - val_loss: 2352.6850 - val_mse: 2352.6851 - val_mae: 31.2285\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 0s 479us/step - loss: 4137.3187 - mse: 4137.3188 - mae: 35.4039 - val_loss: 2344.9829 - val_mse: 2344.9827 - val_mae: 31.1923\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 472us/step - loss: 4061.1157 - mse: 4061.1160 - mae: 33.5174 - val_loss: 2276.3976 - val_mse: 2276.3975 - val_mae: 30.8748\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4382.6517 - mse: 4382.6514 - mae: 34.8036 - val_loss: 2395.6456 - val_mse: 2395.6458 - val_mae: 31.4249\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4090.7299 - mse: 4090.7297 - mae: 32.9965 - val_loss: 2302.9424 - val_mse: 2302.9421 - val_mae: 30.9842\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4271.5890 - mse: 4271.5884 - mae: 35.2069 - val_loss: 2335.7219 - val_mse: 2335.7219 - val_mae: 31.1334\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 649us/step - loss: 4156.4462 - mse: 4156.4458 - mae: 34.8546 - val_loss: 2314.7380 - val_mse: 2314.7385 - val_mae: 31.0335\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4162.5015 - mse: 4162.5015 - mae: 34.1055 - val_loss: 2275.7663 - val_mse: 2275.7664 - val_mae: 30.8553\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 691us/step - loss: 4172.6389 - mse: 4172.6396 - mae: 34.4435 - val_loss: 2282.1529 - val_mse: 2282.1531 - val_mae: 30.8831\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4046.8658 - mse: 4046.8655 - mae: 33.6645 - val_loss: 2217.0118 - val_mse: 2217.0120 - val_mae: 30.5953\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4040.5890 - mse: 4040.5891 - mae: 33.8262 - val_loss: 2241.3239 - val_mse: 2241.3240 - val_mae: 30.6891\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 661us/step - loss: 4098.0274 - mse: 4098.0269 - mae: 33.3709 - val_loss: 2226.3335 - val_mse: 2226.3337 - val_mae: 30.6219\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 686us/step - loss: 4060.5710 - mse: 4060.5708 - mae: 34.2095 - val_loss: 2283.6324 - val_mse: 2283.6326 - val_mae: 30.8766\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4200.5675 - mse: 4200.5674 - mae: 34.9908 - val_loss: 2296.8094 - val_mse: 2296.8093 - val_mae: 30.9377\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 524us/step - loss: 3988.4595 - mse: 3988.4590 - mae: 33.4263 - val_loss: 2310.2222 - val_mse: 2310.2222 - val_mae: 31.0006\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4182.6733 - mse: 4182.6738 - mae: 34.1932 - val_loss: 2374.1899 - val_mse: 2374.1897 - val_mae: 31.2931\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4261.8798 - mse: 4261.8809 - mae: 34.4534 - val_loss: 2371.6632 - val_mse: 2371.6633 - val_mae: 31.2782\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 511us/step - loss: 4206.6359 - mse: 4206.6362 - mae: 34.3384 - val_loss: 2307.7949 - val_mse: 2307.7949 - val_mae: 30.9827\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4099.9412 - mse: 4099.9419 - mae: 34.5532 - val_loss: 2268.3696 - val_mse: 2268.3701 - val_mae: 30.7964\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4172.8592 - mse: 4172.8584 - mae: 34.1030 - val_loss: 2344.0236 - val_mse: 2344.0234 - val_mae: 31.1368\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 443us/step - loss: 4244.3796 - mse: 4244.3799 - mae: 33.7274 - val_loss: 2328.7941 - val_mse: 2328.7939 - val_mae: 31.0709\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4005.3074 - mse: 4005.3076 - mae: 33.2798 - val_loss: 2292.6781 - val_mse: 2292.6780 - val_mae: 30.9097\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4137.4116 - mse: 4137.4116 - mae: 34.1143 - val_loss: 2338.4100 - val_mse: 2338.4099 - val_mae: 31.1139\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 759us/step - loss: 4061.2961 - mse: 4061.2969 - mae: 33.2327 - val_loss: 2344.8669 - val_mse: 2344.8669 - val_mae: 31.1458\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 707us/step - loss: 4009.0734 - mse: 4009.0735 - mae: 33.5175 - val_loss: 2366.5651 - val_mse: 2366.5649 - val_mae: 31.2489\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 735us/step - loss: 3998.0513 - mse: 3998.0513 - mae: 33.0672 - val_loss: 2259.9129 - val_mse: 2259.9128 - val_mae: 30.7725\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4038.7514 - mse: 4038.7510 - mae: 33.9311 - val_loss: 2235.6245 - val_mse: 2235.6245 - val_mae: 30.6721\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 4114.5026 - mse: 4114.5029 - mae: 34.5356 - val_loss: 2342.9200 - val_mse: 2342.9202 - val_mae: 31.1425\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4056.0616 - mse: 4056.0623 - mae: 34.0629 - val_loss: 2315.4714 - val_mse: 2315.4712 - val_mae: 31.0212\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4213.6019 - mse: 4213.6025 - mae: 34.8460 - val_loss: 2282.7595 - val_mse: 2282.7598 - val_mae: 30.8777\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 639us/step - loss: 4131.6496 - mse: 4131.6504 - mae: 33.8377 - val_loss: 2312.1698 - val_mse: 2312.1697 - val_mae: 31.0112\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4138.5196 - mse: 4138.5200 - mae: 34.1226 - val_loss: 2287.1973 - val_mse: 2287.1973 - val_mae: 30.8983\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 3921.4181 - mse: 3921.4185 - mae: 33.3924 - val_loss: 2288.6497 - val_mse: 2288.6497 - val_mae: 30.9022\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 548us/step - loss: 4115.2071 - mse: 4115.2075 - mae: 34.1923 - val_loss: 2250.4574 - val_mse: 2250.4573 - val_mae: 30.7385\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 519us/step - loss: 4041.3047 - mse: 4041.3047 - mae: 34.4517 - val_loss: 2311.1294 - val_mse: 2311.1292 - val_mae: 31.0009\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4094.4949 - mse: 4094.4956 - mae: 33.9761 - val_loss: 2295.2155 - val_mse: 2295.2156 - val_mae: 30.9247\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 4200.5099 - mse: 4200.5098 - mae: 34.4571 - val_loss: 2406.4147 - val_mse: 2406.4148 - val_mae: 31.4386\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4062.7763 - mse: 4062.7766 - mae: 33.6745 - val_loss: 2279.7388 - val_mse: 2279.7385 - val_mae: 30.8609\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 681us/step - loss: 4105.5280 - mse: 4105.5273 - mae: 34.3289 - val_loss: 2358.7009 - val_mse: 2358.7007 - val_mae: 31.2168\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 3875.8803 - mse: 3875.8801 - mae: 33.2411 - val_loss: 2320.2657 - val_mse: 2320.2654 - val_mae: 31.0423\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4082.6388 - mse: 4082.6382 - mae: 33.6176 - val_loss: 2309.5767 - val_mse: 2309.5767 - val_mae: 30.9918\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4175.8375 - mse: 4175.8374 - mae: 33.1285 - val_loss: 2300.7184 - val_mse: 2300.7183 - val_mae: 30.9481\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4006.9748 - mse: 4006.9746 - mae: 33.2220 - val_loss: 2258.2583 - val_mse: 2258.2583 - val_mae: 30.7593\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 564us/step - loss: 4131.1156 - mse: 4131.1157 - mae: 33.9395 - val_loss: 2338.4800 - val_mse: 2338.4800 - val_mae: 31.1173\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 3939.1926 - mse: 3939.1924 - mae: 33.4564 - val_loss: 2335.3446 - val_mse: 2335.3447 - val_mae: 31.1080\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4082.0903 - mse: 4082.0906 - mae: 33.3992 - val_loss: 2348.4833 - val_mse: 2348.4829 - val_mae: 31.1674\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 531us/step - loss: 4017.2800 - mse: 4017.2800 - mae: 33.1833 - val_loss: 2322.2727 - val_mse: 2322.2727 - val_mae: 31.0499\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 3985.3790 - mse: 3985.3789 - mae: 33.2527 - val_loss: 2261.6954 - val_mse: 2261.6958 - val_mae: 30.7785\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 4150.2690 - mse: 4150.2695 - mae: 33.7359 - val_loss: 2354.4853 - val_mse: 2354.4856 - val_mae: 31.1902\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4130.6911 - mse: 4130.6909 - mae: 33.9801 - val_loss: 2305.6425 - val_mse: 2305.6426 - val_mae: 30.9759\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 682us/step - loss: 4227.9971 - mse: 4227.9976 - mae: 33.9821 - val_loss: 2339.8506 - val_mse: 2339.8506 - val_mae: 31.1359\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 708us/step - loss: 3864.2322 - mse: 3864.2319 - mae: 32.5002 - val_loss: 2267.8496 - val_mse: 2267.8496 - val_mae: 30.8219\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 3933.7538 - mse: 3933.7537 - mae: 33.1116 - val_loss: 2303.3441 - val_mse: 2303.3442 - val_mae: 30.9786\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 661us/step - loss: 4026.6572 - mse: 4026.6577 - mae: 32.8997 - val_loss: 2307.3062 - val_mse: 2307.3064 - val_mae: 30.9958\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4148.4484 - mse: 4148.4487 - mae: 33.9060 - val_loss: 2331.6693 - val_mse: 2331.6694 - val_mae: 31.0969\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4008.2179 - mse: 4008.2178 - mae: 34.5399 - val_loss: 2275.4097 - val_mse: 2275.4097 - val_mae: 30.8539\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4104.2284 - mse: 4104.2280 - mae: 32.7260 - val_loss: 2298.3156 - val_mse: 2298.3157 - val_mae: 30.9523\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4101.9699 - mse: 4101.9697 - mae: 33.2745 - val_loss: 2315.2183 - val_mse: 2315.2183 - val_mae: 31.0280\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4058.4913 - mse: 4058.4912 - mae: 33.2623 - val_loss: 2268.5241 - val_mse: 2268.5242 - val_mae: 30.8275\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 524us/step - loss: 4105.7530 - mse: 4105.7529 - mae: 33.9337 - val_loss: 2282.0614 - val_mse: 2282.0615 - val_mae: 30.8844\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4051.3800 - mse: 4051.3796 - mae: 32.5656 - val_loss: 2296.3349 - val_mse: 2296.3350 - val_mae: 30.9454\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4103.1569 - mse: 4103.1567 - mae: 32.2585 - val_loss: 2348.6441 - val_mse: 2348.6440 - val_mae: 31.1819\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4072.7378 - mse: 4072.7378 - mae: 32.9135 - val_loss: 2319.8038 - val_mse: 2319.8037 - val_mae: 31.0535\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 3925.9615 - mse: 3925.9614 - mae: 32.5699 - val_loss: 2348.1275 - val_mse: 2348.1274 - val_mae: 31.1795\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 3917.2662 - mse: 3917.2661 - mae: 33.2874 - val_loss: 2277.9888 - val_mse: 2277.9885 - val_mae: 30.8778\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 3863.3318 - mse: 3863.3311 - mae: 32.2773 - val_loss: 2292.1957 - val_mse: 2292.1958 - val_mae: 30.9359\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 4094.2041 - mse: 4094.2039 - mae: 33.4180 - val_loss: 2298.0953 - val_mse: 2298.0957 - val_mae: 30.9626\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 3876.0204 - mse: 3876.0208 - mae: 33.2365 - val_loss: 2318.1900 - val_mse: 2318.1899 - val_mae: 31.0516\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 4052.5349 - mse: 4052.5344 - mae: 33.5151 - val_loss: 2338.3867 - val_mse: 2338.3867 - val_mae: 31.1413\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 676us/step - loss: 4048.1872 - mse: 4048.1870 - mae: 33.2712 - val_loss: 2280.0453 - val_mse: 2280.0454 - val_mae: 30.8892\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4028.3882 - mse: 4028.3875 - mae: 33.2655 - val_loss: 2346.0515 - val_mse: 2346.0515 - val_mae: 31.1713\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 3844.5956 - mse: 3844.5955 - mae: 30.9282 - val_loss: 2296.2875 - val_mse: 2296.2878 - val_mae: 30.9480\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3343.3410 - mse: 3343.3411 - mae: 32.6604 - val_loss: 1449.2707 - val_mse: 1449.2708 - val_mae: 25.2184\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3441.5569 - mse: 3441.5571 - mae: 33.4747 - val_loss: 1466.0884 - val_mse: 1466.0884 - val_mae: 24.4078\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3292.5941 - mse: 3292.5950 - mae: 31.9789 - val_loss: 1452.0647 - val_mse: 1452.0647 - val_mae: 25.0252\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 659us/step - loss: 3320.1535 - mse: 3320.1536 - mae: 32.1188 - val_loss: 1446.8902 - val_mse: 1446.8903 - val_mae: 25.5055\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3168.2893 - mse: 3168.2893 - mae: 32.1931 - val_loss: 1460.1042 - val_mse: 1460.1042 - val_mae: 24.5625\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 783us/step - loss: 3260.9239 - mse: 3260.9243 - mae: 32.1340 - val_loss: 1448.1500 - val_mse: 1448.1500 - val_mae: 25.2417\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 528us/step - loss: 3362.6910 - mse: 3362.6909 - mae: 32.8356 - val_loss: 1453.0187 - val_mse: 1453.0186 - val_mae: 24.8751\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3338.7658 - mse: 3338.7666 - mae: 32.5391 - val_loss: 1450.4872 - val_mse: 1450.4872 - val_mae: 25.0445\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3264.4929 - mse: 3264.4937 - mae: 32.6382 - val_loss: 1448.0014 - val_mse: 1448.0015 - val_mae: 25.2367\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3302.0037 - mse: 3302.0037 - mae: 32.2033 - val_loss: 1449.7971 - val_mse: 1449.7972 - val_mae: 25.1013\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 656us/step - loss: 3281.2929 - mse: 3281.2932 - mae: 32.3304 - val_loss: 1453.2809 - val_mse: 1453.2809 - val_mae: 24.8532\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3369.9321 - mse: 3369.9329 - mae: 32.8816 - val_loss: 1450.8792 - val_mse: 1450.8792 - val_mae: 24.9897\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3359.2962 - mse: 3359.2959 - mae: 32.4282 - val_loss: 1449.6273 - val_mse: 1449.6272 - val_mae: 25.1274\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 682us/step - loss: 3307.8321 - mse: 3307.8318 - mae: 32.3478 - val_loss: 1453.7138 - val_mse: 1453.7137 - val_mae: 24.8731\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3323.8511 - mse: 3323.8511 - mae: 32.5724 - val_loss: 1449.9590 - val_mse: 1449.9591 - val_mae: 25.1183\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3320.7089 - mse: 3320.7087 - mae: 32.4985 - val_loss: 1451.1058 - val_mse: 1451.1058 - val_mae: 25.0550\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3270.1164 - mse: 3270.1160 - mae: 31.9087 - val_loss: 1450.6962 - val_mse: 1450.6962 - val_mae: 25.1200\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3345.9471 - mse: 3345.9475 - mae: 33.0736 - val_loss: 1454.8014 - val_mse: 1454.8015 - val_mae: 24.8786\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3289.8595 - mse: 3289.8596 - mae: 32.3285 - val_loss: 1449.1321 - val_mse: 1449.1320 - val_mae: 25.3013\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3348.6760 - mse: 3348.6765 - mae: 32.1435 - val_loss: 1448.3969 - val_mse: 1448.3969 - val_mae: 25.4228\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3328.0715 - mse: 3328.0720 - mae: 31.7251 - val_loss: 1453.0047 - val_mse: 1453.0046 - val_mae: 25.0057\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3407.2960 - mse: 3407.2959 - mae: 33.0874 - val_loss: 1453.6004 - val_mse: 1453.6005 - val_mae: 24.9670\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3262.1052 - mse: 3262.1050 - mae: 32.1286 - val_loss: 1450.1580 - val_mse: 1450.1581 - val_mae: 25.1421\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3282.3566 - mse: 3282.3569 - mae: 32.1823 - val_loss: 1450.6694 - val_mse: 1450.6694 - val_mae: 25.0788\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3435.0039 - mse: 3435.0044 - mae: 32.0295 - val_loss: 1447.7363 - val_mse: 1447.7363 - val_mae: 25.4039\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3370.6103 - mse: 3370.6106 - mae: 32.0588 - val_loss: 1448.8394 - val_mse: 1448.8392 - val_mae: 25.2610\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3409.9110 - mse: 3409.9104 - mae: 32.3153 - val_loss: 1448.0826 - val_mse: 1448.0828 - val_mae: 25.2610\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3377.9764 - mse: 3377.9756 - mae: 32.0658 - val_loss: 1450.9866 - val_mse: 1450.9865 - val_mae: 25.0450\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 537us/step - loss: 3310.3576 - mse: 3310.3572 - mae: 32.0842 - val_loss: 1450.6503 - val_mse: 1450.6503 - val_mae: 25.1329\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 683us/step - loss: 3376.7029 - mse: 3376.7026 - mae: 32.3860 - val_loss: 1448.6429 - val_mse: 1448.6431 - val_mae: 25.3088\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3327.0266 - mse: 3327.0264 - mae: 31.8421 - val_loss: 1448.5967 - val_mse: 1448.5966 - val_mae: 25.3963\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 496us/step - loss: 3325.7236 - mse: 3325.7236 - mae: 32.5273 - val_loss: 1451.8646 - val_mse: 1451.8647 - val_mae: 25.0233\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3410.5253 - mse: 3410.5247 - mae: 32.5951 - val_loss: 1448.5529 - val_mse: 1448.5530 - val_mae: 25.4476\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3377.4101 - mse: 3377.4104 - mae: 32.0723 - val_loss: 1449.5343 - val_mse: 1449.5343 - val_mae: 25.3680\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3307.0266 - mse: 3307.0264 - mae: 31.8785 - val_loss: 1448.7967 - val_mse: 1448.7968 - val_mae: 25.5104\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3366.4803 - mse: 3366.4810 - mae: 32.5014 - val_loss: 1452.4277 - val_mse: 1452.4279 - val_mae: 25.0479\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3345.6721 - mse: 3345.6711 - mae: 32.2038 - val_loss: 1451.4284 - val_mse: 1451.4285 - val_mae: 25.0837\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3328.6158 - mse: 3328.6157 - mae: 32.4046 - val_loss: 1450.7174 - val_mse: 1450.7174 - val_mae: 25.1097\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3450.4572 - mse: 3450.4568 - mae: 32.3851 - val_loss: 1451.0289 - val_mse: 1451.0291 - val_mae: 25.0848\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3334.2305 - mse: 3334.2307 - mae: 32.1627 - val_loss: 1448.8305 - val_mse: 1448.8303 - val_mae: 25.3099\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3216.9701 - mse: 3216.9695 - mae: 31.8385 - val_loss: 1450.0084 - val_mse: 1450.0083 - val_mae: 25.1981\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3240.6332 - mse: 3240.6331 - mae: 32.2367 - val_loss: 1451.9000 - val_mse: 1451.8999 - val_mae: 25.0333\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3237.1024 - mse: 3237.1013 - mae: 31.9074 - val_loss: 1450.3888 - val_mse: 1450.3887 - val_mae: 25.2045\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3300.5388 - mse: 3300.5388 - mae: 31.3278 - val_loss: 1449.8199 - val_mse: 1449.8198 - val_mae: 25.3252\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 497us/step - loss: 3313.9392 - mse: 3313.9397 - mae: 31.7114 - val_loss: 1449.6832 - val_mse: 1449.6833 - val_mae: 25.4016\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3209.4633 - mse: 3209.4629 - mae: 31.4297 - val_loss: 1452.8085 - val_mse: 1452.8085 - val_mae: 25.0749\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3304.6685 - mse: 3304.6689 - mae: 31.9142 - val_loss: 1455.1855 - val_mse: 1455.1855 - val_mae: 24.9263\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3248.6703 - mse: 3248.6711 - mae: 32.0368 - val_loss: 1452.7828 - val_mse: 1452.7827 - val_mae: 25.0765\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3327.5211 - mse: 3327.5208 - mae: 32.1391 - val_loss: 1454.4816 - val_mse: 1454.4816 - val_mae: 24.9455\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3356.2686 - mse: 3356.2688 - mae: 32.0226 - val_loss: 1453.3236 - val_mse: 1453.3237 - val_mae: 25.0211\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3164.3220 - mse: 3164.3228 - mae: 32.0210 - val_loss: 1451.2789 - val_mse: 1451.2791 - val_mae: 25.1538\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 689us/step - loss: 3111.4267 - mse: 3111.4275 - mae: 31.5840 - val_loss: 1451.5965 - val_mse: 1451.5964 - val_mae: 25.1732\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3270.5852 - mse: 3270.5852 - mae: 31.8662 - val_loss: 1452.2808 - val_mse: 1452.2809 - val_mae: 25.0851\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3297.3418 - mse: 3297.3416 - mae: 31.9751 - val_loss: 1448.6838 - val_mse: 1448.6837 - val_mae: 25.6371\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3083.7611 - mse: 3083.7622 - mae: 30.9028 - val_loss: 1448.8989 - val_mse: 1448.8989 - val_mae: 25.4158\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3292.6956 - mse: 3292.6963 - mae: 31.8230 - val_loss: 1450.5491 - val_mse: 1450.5491 - val_mae: 25.1487\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3242.4985 - mse: 3242.4990 - mae: 31.0982 - val_loss: 1448.9171 - val_mse: 1448.9172 - val_mae: 25.3267\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3238.7986 - mse: 3238.7993 - mae: 31.2680 - val_loss: 1448.7652 - val_mse: 1448.7653 - val_mae: 25.4197\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 676us/step - loss: 3220.4027 - mse: 3220.4033 - mae: 30.5236 - val_loss: 1449.6231 - val_mse: 1449.6232 - val_mae: 25.3531\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3281.3528 - mse: 3281.3528 - mae: 31.8968 - val_loss: 1449.9662 - val_mse: 1449.9662 - val_mae: 25.2915\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 692us/step - loss: 3285.7431 - mse: 3285.7417 - mae: 31.4688 - val_loss: 1450.0529 - val_mse: 1450.0530 - val_mae: 25.2424\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3366.8860 - mse: 3366.8862 - mae: 32.2691 - val_loss: 1450.5434 - val_mse: 1450.5433 - val_mae: 25.2580\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3196.4686 - mse: 3196.4692 - mae: 31.1945 - val_loss: 1449.5484 - val_mse: 1449.5483 - val_mae: 25.3785\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3274.2425 - mse: 3274.2424 - mae: 31.5676 - val_loss: 1449.6582 - val_mse: 1449.6582 - val_mae: 25.3028\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 683us/step - loss: 3236.0137 - mse: 3236.0134 - mae: 31.9031 - val_loss: 1448.4597 - val_mse: 1448.4597 - val_mae: 25.5857\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3395.8877 - mse: 3395.8882 - mae: 31.9276 - val_loss: 1449.5916 - val_mse: 1449.5918 - val_mae: 25.4020\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3295.3486 - mse: 3295.3484 - mae: 31.8823 - val_loss: 1448.9645 - val_mse: 1448.9646 - val_mae: 25.4037\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3286.6006 - mse: 3286.6006 - mae: 31.7142 - val_loss: 1448.6925 - val_mse: 1448.6925 - val_mae: 25.5981\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3282.0456 - mse: 3282.0459 - mae: 31.3606 - val_loss: 1450.3992 - val_mse: 1450.3994 - val_mae: 25.2024\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3226.0882 - mse: 3226.0876 - mae: 31.70 - 1s 550us/step - loss: 3194.9117 - mse: 3194.9114 - mae: 31.7067 - val_loss: 1450.2749 - val_mse: 1450.2749 - val_mae: 25.3638\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 674us/step - loss: 3208.9294 - mse: 3208.9297 - mae: 31.4302 - val_loss: 1451.9221 - val_mse: 1451.9221 - val_mae: 25.1569\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3222.0699 - mse: 3222.0703 - mae: 31.2371 - val_loss: 1449.8289 - val_mse: 1449.8289 - val_mae: 25.9022\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3240.1731 - mse: 3240.1733 - mae: 31.4068 - val_loss: 1451.2696 - val_mse: 1451.2695 - val_mae: 25.1958\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3257.7683 - mse: 3257.7683 - mae: 31.9794 - val_loss: 1452.7742 - val_mse: 1452.7743 - val_mae: 25.0478\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3308.2356 - mse: 3308.2358 - mae: 31.6392 - val_loss: 1448.7137 - val_mse: 1448.7136 - val_mae: 25.6909\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3250.2677 - mse: 3250.2668 - mae: 31.7668 - val_loss: 1450.2835 - val_mse: 1450.2834 - val_mae: 25.3431\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 694us/step - loss: 3246.4510 - mse: 3246.4512 - mae: 31.1221 - val_loss: 1449.5765 - val_mse: 1449.5765 - val_mae: 25.5242\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 701us/step - loss: 3231.2685 - mse: 3231.2688 - mae: 31.2196 - val_loss: 1450.4310 - val_mse: 1450.4308 - val_mae: 25.3739\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3140.3959 - mse: 3140.3960 - mae: 30.8739 - val_loss: 1449.8092 - val_mse: 1449.8092 - val_mae: 25.8112\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3170.0011 - mse: 3170.0020 - mae: 31.9009 - val_loss: 1450.0552 - val_mse: 1450.0552 - val_mae: 25.3832\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2953.8849 - mse: 2953.8843 - mae: 31.0387 - val_loss: 1076.1696 - val_mse: 1076.1694 - val_mae: 23.5341\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2863.8340 - mse: 2863.8354 - mae: 30.5736 - val_loss: 1077.3695 - val_mse: 1077.3696 - val_mae: 23.3760\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2946.8137 - mse: 2946.8132 - mae: 30.7535 - val_loss: 1071.1105 - val_mse: 1071.1105 - val_mae: 23.5919\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 671us/step - loss: 2870.9028 - mse: 2870.9028 - mae: 30.4422 - val_loss: 1067.5600 - val_mse: 1067.5602 - val_mae: 23.8067\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2877.5624 - mse: 2877.5632 - mae: 31.0854 - val_loss: 1073.8968 - val_mse: 1073.8970 - val_mae: 23.3305\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2865.4173 - mse: 2865.4163 - mae: 30.7181 - val_loss: 1069.6349 - val_mse: 1069.6350 - val_mae: 23.4872\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2904.6925 - mse: 2904.6921 - mae: 31.3052 - val_loss: 1068.8086 - val_mse: 1068.8086 - val_mae: 23.4738\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2797.8256 - mse: 2797.8262 - mae: 30.7012 - val_loss: 1066.8284 - val_mse: 1066.8285 - val_mae: 23.5674\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2970.4192 - mse: 2970.4189 - mae: 31.2411 - val_loss: 1067.4071 - val_mse: 1067.4071 - val_mae: 23.4628\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2901.5851 - mse: 2901.5852 - mae: 31.2714 - val_loss: 1067.9759 - val_mse: 1067.9760 - val_mae: 23.4145\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2850.4906 - mse: 2850.4910 - mae: 31.0257 - val_loss: 1065.8636 - val_mse: 1065.8636 - val_mae: 23.4960\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 522us/step - loss: 2877.0122 - mse: 2877.0117 - mae: 30.8009 - val_loss: 1061.8931 - val_mse: 1061.8932 - val_mae: 23.7513\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 507us/step - loss: 2872.2969 - mse: 2872.2971 - mae: 30.9406 - val_loss: 1065.4827 - val_mse: 1065.4827 - val_mae: 23.3973\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2889.7514 - mse: 2889.7520 - mae: 30.9640 - val_loss: 1064.9612 - val_mse: 1064.9614 - val_mae: 23.4064\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 526us/step - loss: 2790.0700 - mse: 2790.0706 - mae: 30.0439 - val_loss: 1061.3389 - val_mse: 1061.3389 - val_mae: 23.5787\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2782.7627 - mse: 2782.7629 - mae: 30.7650 - val_loss: 1061.4094 - val_mse: 1061.4093 - val_mae: 23.5282\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 634us/step - loss: 2944.4598 - mse: 2944.4595 - mae: 31.2438 - val_loss: 1058.3485 - val_mse: 1058.3484 - val_mae: 23.7610\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2842.5220 - mse: 2842.5217 - mae: 30.5669 - val_loss: 1057.9404 - val_mse: 1057.9406 - val_mae: 23.7098\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 673us/step - loss: 2926.9128 - mse: 2926.9128 - mae: 30.7641 - val_loss: 1058.0434 - val_mse: 1058.0435 - val_mae: 23.6689\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2956.6491 - mse: 2956.6489 - mae: 31.1489 - val_loss: 1062.6494 - val_mse: 1062.6493 - val_mae: 23.3085\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2772.3019 - mse: 2772.3025 - mae: 30.0629 - val_loss: 1055.9484 - val_mse: 1055.9484 - val_mae: 23.7519\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2912.9014 - mse: 2912.9023 - mae: 31.3126 - val_loss: 1060.4026 - val_mse: 1060.4027 - val_mae: 23.3695\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2800.4449 - mse: 2800.4446 - mae: 30.0374 - val_loss: 1058.5606 - val_mse: 1058.5607 - val_mae: 23.4320\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2860.3961 - mse: 2860.3965 - mae: 30.7535 - val_loss: 1054.1245 - val_mse: 1054.1245 - val_mae: 23.7615\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2870.6695 - mse: 2870.6692 - mae: 30.5598 - val_loss: 1053.8666 - val_mse: 1053.8665 - val_mae: 23.6650\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2918.4291 - mse: 2918.4297 - mae: 31.0112 - val_loss: 1053.9263 - val_mse: 1053.9263 - val_mae: 23.5879\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 671us/step - loss: 2886.1229 - mse: 2886.1228 - mae: 30.5959 - val_loss: 1056.2049 - val_mse: 1056.2048 - val_mae: 23.4202\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2904.3830 - mse: 2904.3833 - mae: 30.9266 - val_loss: 1057.8603 - val_mse: 1057.8602 - val_mae: 23.3354\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 640us/step - loss: 2871.7723 - mse: 2871.7720 - mae: 30.8320 - val_loss: 1057.9756 - val_mse: 1057.9755 - val_mae: 23.3284\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2923.0024 - mse: 2923.0037 - mae: 30.8025 - val_loss: 1053.2213 - val_mse: 1053.2214 - val_mae: 23.5759\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2906.3312 - mse: 2906.3303 - mae: 30.9034 - val_loss: 1051.4303 - val_mse: 1051.4304 - val_mae: 23.8200\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 691us/step - loss: 2856.6624 - mse: 2856.6614 - mae: 30.5291 - val_loss: 1053.6219 - val_mse: 1053.6219 - val_mae: 23.5218\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 672us/step - loss: 2867.6660 - mse: 2867.6663 - mae: 31.1429 - val_loss: 1053.2877 - val_mse: 1053.2876 - val_mae: 23.5240\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2830.0432 - mse: 2830.0435 - mae: 30.6542 - val_loss: 1050.8432 - val_mse: 1050.8431 - val_mae: 23.5983\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2842.8960 - mse: 2842.8965 - mae: 30.4840 - val_loss: 1050.1148 - val_mse: 1050.1147 - val_mae: 23.6071\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2893.1154 - mse: 2893.1157 - mae: 30.4201 - val_loss: 1055.6291 - val_mse: 1055.6292 - val_mae: 23.1503\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2897.0161 - mse: 2897.0159 - mae: 30.9215 - val_loss: 1050.1811 - val_mse: 1050.1812 - val_mae: 23.4321\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2905.6511 - mse: 2905.6514 - mae: 30.6264 - val_loss: 1047.0298 - val_mse: 1047.0299 - val_mae: 23.6972\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2855.2577 - mse: 2855.2578 - mae: 30.3783 - val_loss: 1047.4087 - val_mse: 1047.4084 - val_mae: 23.6047\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2872.0810 - mse: 2872.0813 - mae: 30.4653 - val_loss: 1050.9955 - val_mse: 1050.9955 - val_mae: 23.2578\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2796.7990 - mse: 2796.7998 - mae: 30.0322 - val_loss: 1051.8859 - val_mse: 1051.8857 - val_mae: 23.1634\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2812.5077 - mse: 2812.5083 - mae: 30.5126 - val_loss: 1044.7386 - val_mse: 1044.7386 - val_mae: 23.6118\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2887.2165 - mse: 2887.2161 - mae: 30.7183 - val_loss: 1051.1076 - val_mse: 1051.1078 - val_mae: 23.1390\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 509us/step - loss: 2819.3960 - mse: 2819.3960 - mae: 30.4282 - val_loss: 1046.7379 - val_mse: 1046.7378 - val_mae: 23.4189\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 477us/step - loss: 2805.5522 - mse: 2805.5522 - mae: 30.4787 - val_loss: 1044.1139 - val_mse: 1044.1139 - val_mae: 23.4747\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 545us/step - loss: 2744.0079 - mse: 2744.0061 - mae: 29.9619 - val_loss: 1041.6136 - val_mse: 1041.6136 - val_mae: 23.8085\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2908.2989 - mse: 2908.2981 - mae: 31.2581 - val_loss: 1045.3910 - val_mse: 1045.3910 - val_mae: 23.3194\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 651us/step - loss: 2821.7436 - mse: 2821.7439 - mae: 30.5194 - val_loss: 1044.7563 - val_mse: 1044.7563 - val_mae: 23.4371\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2862.9254 - mse: 2862.9255 - mae: 30.3432 - val_loss: 1044.2472 - val_mse: 1044.2472 - val_mae: 23.3688\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 461us/step - loss: 2788.7139 - mse: 2788.7139 - mae: 30.0900 - val_loss: 1041.5360 - val_mse: 1041.5361 - val_mae: 23.5874\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2893.5436 - mse: 2893.5427 - mae: 30.6804 - val_loss: 1041.6329 - val_mse: 1041.6328 - val_mae: 23.5137\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2810.7775 - mse: 2810.7776 - mae: 29.6277 - val_loss: 1040.1741 - val_mse: 1040.1742 - val_mae: 23.5517\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2762.4873 - mse: 2762.4863 - mae: 30.1348 - val_loss: 1043.6709 - val_mse: 1043.6708 - val_mae: 23.1746\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2752.0749 - mse: 2752.0740 - mae: 29.9632 - val_loss: 1039.4444 - val_mse: 1039.4443 - val_mae: 23.4435\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2900.9789 - mse: 2900.9795 - mae: 30.3182 - val_loss: 1042.8758 - val_mse: 1042.8757 - val_mae: 23.1914\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2825.9641 - mse: 2825.9639 - mae: 30.0765 - val_loss: 1041.9529 - val_mse: 1041.9530 - val_mae: 23.2790\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 678us/step - loss: 2844.8210 - mse: 2844.8206 - mae: 30.3007 - val_loss: 1042.1612 - val_mse: 1042.1614 - val_mae: 23.2136\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 634us/step - loss: 2875.4150 - mse: 2875.4146 - mae: 29.9206 - val_loss: 1042.0399 - val_mse: 1042.0399 - val_mae: 23.2368\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2775.9588 - mse: 2775.9592 - mae: 30.1394 - val_loss: 1039.3051 - val_mse: 1039.3051 - val_mae: 23.3044\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2902.2654 - mse: 2902.2654 - mae: 30.2618 - val_loss: 1049.0156 - val_mse: 1049.0156 - val_mae: 22.8912\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2881.1439 - mse: 2881.1438 - mae: 30.3501 - val_loss: 1039.4650 - val_mse: 1039.4648 - val_mae: 23.3237\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2798.0136 - mse: 2798.0132 - mae: 30.3969 - val_loss: 1046.9413 - val_mse: 1046.9413 - val_mae: 22.8699\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 668us/step - loss: 2820.0887 - mse: 2820.0886 - mae: 30.5936 - val_loss: 1036.4384 - val_mse: 1036.4385 - val_mae: 23.3867\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2823.2509 - mse: 2823.2505 - mae: 30.5329 - val_loss: 1036.2189 - val_mse: 1036.2190 - val_mae: 23.2117\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 551us/step - loss: 2821.3522 - mse: 2821.3521 - mae: 30.3401 - val_loss: 1037.4852 - val_mse: 1037.4851 - val_mae: 23.0466\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2770.0249 - mse: 2770.0259 - mae: 29.9975 - val_loss: 1032.0018 - val_mse: 1032.0018 - val_mae: 23.3675\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2759.1215 - mse: 2759.1211 - mae: 30.4139 - val_loss: 1035.2763 - val_mse: 1035.2762 - val_mae: 23.1553\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2783.4554 - mse: 2783.4556 - mae: 29.7453 - val_loss: 1031.7406 - val_mse: 1031.7407 - val_mae: 23.3892\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 675us/step - loss: 2672.8467 - mse: 2672.8462 - mae: 29.5552 - val_loss: 1034.3438 - val_mse: 1034.3436 - val_mae: 23.2397\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2876.0764 - mse: 2876.0764 - mae: 31.0715 - val_loss: 1040.2486 - val_mse: 1040.2485 - val_mae: 22.9349\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 677us/step - loss: 2740.5360 - mse: 2740.5361 - mae: 29.5546 - val_loss: 1035.6357 - val_mse: 1035.6356 - val_mae: 23.1971\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2754.0088 - mse: 2754.0088 - mae: 30.1632 - val_loss: 1034.2407 - val_mse: 1034.2407 - val_mae: 23.3539\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2822.3137 - mse: 2822.3145 - mae: 30.2592 - val_loss: 1042.2104 - val_mse: 1042.2104 - val_mae: 22.8593\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 649us/step - loss: 2872.4794 - mse: 2872.4795 - mae: 30.1865 - val_loss: 1033.2752 - val_mse: 1033.2751 - val_mae: 23.3543\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2781.4577 - mse: 2781.4578 - mae: 30.1481 - val_loss: 1033.1518 - val_mse: 1033.1519 - val_mae: 23.2258\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2793.6338 - mse: 2793.6343 - mae: 29.8326 - val_loss: 1034.4202 - val_mse: 1034.4202 - val_mae: 23.0334\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2817.0340 - mse: 2817.0339 - mae: 30.5980 - val_loss: 1037.9943 - val_mse: 1037.9941 - val_mae: 22.8887\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 516us/step - loss: 2894.3885 - mse: 2894.3882 - mae: 30.6812 - val_loss: 1030.6798 - val_mse: 1030.6797 - val_mae: 23.2680\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2798.7633 - mse: 2798.7627 - mae: 30.2389 - val_loss: 1034.7576 - val_mse: 1034.7577 - val_mae: 23.0064\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2879.2819 - mse: 2879.2822 - mae: 30.5929 - val_loss: 1037.2755 - val_mse: 1037.2754 - val_mae: 22.8838\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2484.1878 - mse: 2484.1873 - mae: 29.3227 - val_loss: 1465.6485 - val_mse: 1465.6483 - val_mae: 26.9734\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2471.5501 - mse: 2471.5496 - mae: 29.2830 - val_loss: 1471.2932 - val_mse: 1471.2931 - val_mae: 26.7312\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2480.7385 - mse: 2480.7380 - mae: 29.2461 - val_loss: 1468.6441 - val_mse: 1468.6440 - val_mae: 26.7266\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2492.4891 - mse: 2492.4885 - mae: 29.7795 - val_loss: 1477.0141 - val_mse: 1477.0144 - val_mae: 26.4627\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2571.5214 - mse: 2571.5212 - mae: 29.5316 - val_loss: 1463.1546 - val_mse: 1463.1545 - val_mae: 26.7655\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2533.9678 - mse: 2533.9680 - mae: 29.4269 - val_loss: 1482.1930 - val_mse: 1482.1934 - val_mae: 26.2961\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 602us/step - loss: 2486.1177 - mse: 2486.1177 - mae: 28.5612 - val_loss: 1464.9531 - val_mse: 1464.9530 - val_mae: 26.7102\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2429.9412 - mse: 2429.9419 - mae: 28.7809 - val_loss: 1470.6126 - val_mse: 1470.6125 - val_mae: 26.5598\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2529.6599 - mse: 2529.6599 - mae: 29.4905 - val_loss: 1478.3934 - val_mse: 1478.3934 - val_mae: 26.3600\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 697us/step - loss: 2529.2894 - mse: 2529.2896 - mae: 29.7280 - val_loss: 1475.3570 - val_mse: 1475.3571 - val_mae: 26.4067\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 717us/step - loss: 2496.7963 - mse: 2496.7959 - mae: 29.6061 - val_loss: 1474.8300 - val_mse: 1474.8301 - val_mae: 26.4286\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2587.6880 - mse: 2587.6875 - mae: 29.4250 - val_loss: 1472.6131 - val_mse: 1472.6133 - val_mae: 26.4962\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2546.2657 - mse: 2546.2656 - mae: 29.6383 - val_loss: 1461.7024 - val_mse: 1461.7024 - val_mae: 26.7469\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2512.7459 - mse: 2512.7454 - mae: 29.5950 - val_loss: 1468.0791 - val_mse: 1468.0790 - val_mae: 26.6122\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2553.5497 - mse: 2553.5503 - mae: 29.9735 - val_loss: 1470.7238 - val_mse: 1470.7236 - val_mae: 26.5844\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2432.5305 - mse: 2432.5303 - mae: 29.0115 - val_loss: 1475.4508 - val_mse: 1475.4508 - val_mae: 26.4222\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2506.5331 - mse: 2506.5334 - mae: 29.1702 - val_loss: 1460.7020 - val_mse: 1460.7020 - val_mae: 26.7541\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2457.2120 - mse: 2457.2109 - mae: 29.3179 - val_loss: 1471.2120 - val_mse: 1471.2118 - val_mae: 26.4211\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2526.1639 - mse: 2526.1643 - mae: 29.4004 - val_loss: 1479.3249 - val_mse: 1479.3251 - val_mae: 26.2871\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 700us/step - loss: 2531.5423 - mse: 2531.5425 - mae: 29.5920 - val_loss: 1481.5604 - val_mse: 1481.5605 - val_mae: 26.2490\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2509.2075 - mse: 2509.2070 - mae: 29.3090 - val_loss: 1473.0924 - val_mse: 1473.0924 - val_mae: 26.4198\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 692us/step - loss: 2445.9148 - mse: 2445.9150 - mae: 28.6855 - val_loss: 1463.1911 - val_mse: 1463.1912 - val_mae: 26.6957\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 670us/step - loss: 2553.2019 - mse: 2553.2019 - mae: 29.5413 - val_loss: 1475.9264 - val_mse: 1475.9265 - val_mae: 26.3736\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2577.8274 - mse: 2577.8274 - mae: 29.5716 - val_loss: 1467.8517 - val_mse: 1467.8517 - val_mae: 26.5040\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2518.0022 - mse: 2518.0017 - mae: 29.3339 - val_loss: 1460.6747 - val_mse: 1460.6748 - val_mae: 26.6150\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2452.4637 - mse: 2452.4636 - mae: 28.9073 - val_loss: 1472.5884 - val_mse: 1472.5885 - val_mae: 26.3038\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2455.9693 - mse: 2455.9692 - mae: 29.3576 - val_loss: 1469.9441 - val_mse: 1469.9440 - val_mae: 26.2943\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2495.1448 - mse: 2495.1448 - mae: 29.4733 - val_loss: 1476.0495 - val_mse: 1476.0494 - val_mae: 26.1322\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2472.8676 - mse: 2472.8672 - mae: 28.9796 - val_loss: 1457.7973 - val_mse: 1457.7975 - val_mae: 26.5730\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2447.2519 - mse: 2447.2517 - mae: 29.3220 - val_loss: 1473.6436 - val_mse: 1473.6434 - val_mae: 26.2366\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2531.8847 - mse: 2531.8840 - mae: 29.5395 - val_loss: 1476.6210 - val_mse: 1476.6210 - val_mae: 26.1482\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2442.5055 - mse: 2442.5054 - mae: 29.0213 - val_loss: 1469.8463 - val_mse: 1469.8461 - val_mae: 26.2521\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2577.8119 - mse: 2577.8118 - mae: 29.7411 - val_loss: 1465.7701 - val_mse: 1465.7701 - val_mae: 26.3112\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 474us/step - loss: 2485.5486 - mse: 2485.5491 - mae: 29.1927 - val_loss: 1465.9593 - val_mse: 1465.9596 - val_mae: 26.2773\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2501.5694 - mse: 2501.5686 - mae: 29.3007 - val_loss: 1467.4285 - val_mse: 1467.4286 - val_mae: 26.2519\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2500.6896 - mse: 2500.6892 - mae: 29.5814 - val_loss: 1459.6564 - val_mse: 1459.6565 - val_mae: 26.4684\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2520.1381 - mse: 2520.1377 - mae: 29.1342 - val_loss: 1474.4428 - val_mse: 1474.4427 - val_mae: 26.2297\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2488.6927 - mse: 2488.6929 - mae: 29.2459 - val_loss: 1461.4575 - val_mse: 1461.4575 - val_mae: 26.4539\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2503.8672 - mse: 2503.8669 - mae: 29.6274 - val_loss: 1478.9797 - val_mse: 1478.9797 - val_mae: 26.0879\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2472.2554 - mse: 2472.2551 - mae: 29.1576 - val_loss: 1458.8188 - val_mse: 1458.8188 - val_mae: 26.3365\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 664us/step - loss: 2463.3001 - mse: 2463.2998 - mae: 28.9512 - val_loss: 1464.5450 - val_mse: 1464.5452 - val_mae: 26.1626\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 671us/step - loss: 2438.8133 - mse: 2438.8130 - mae: 28.7200 - val_loss: 1459.9517 - val_mse: 1459.9519 - val_mae: 26.2979\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2409.7353 - mse: 2409.7349 - mae: 28.9916 - val_loss: 1459.1010 - val_mse: 1459.1011 - val_mae: 26.2613\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2460.5865 - mse: 2460.5864 - mae: 28.7510 - val_loss: 1450.0842 - val_mse: 1450.0842 - val_mae: 26.4254\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2411.2710 - mse: 2411.2705 - mae: 28.9771 - val_loss: 1449.8332 - val_mse: 1449.8333 - val_mae: 26.3305\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2437.8457 - mse: 2437.8452 - mae: 29.3053 - val_loss: 1459.3558 - val_mse: 1459.3557 - val_mae: 26.1926\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2440.1756 - mse: 2440.1758 - mae: 28.8013 - val_loss: 1460.9670 - val_mse: 1460.9669 - val_mae: 26.1611\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2503.8903 - mse: 2503.8899 - mae: 29.5115 - val_loss: 1450.3220 - val_mse: 1450.3219 - val_mae: 26.3894\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2462.1571 - mse: 2462.1567 - mae: 29.0635 - val_loss: 1466.0860 - val_mse: 1466.0857 - val_mae: 26.1424\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2487.9069 - mse: 2487.9072 - mae: 29.3543 - val_loss: 1472.3935 - val_mse: 1472.3936 - val_mae: 25.9336\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2434.6383 - mse: 2434.6382 - mae: 28.8670 - val_loss: 1459.5659 - val_mse: 1459.5659 - val_mae: 26.1308\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2494.2660 - mse: 2494.2656 - mae: 29.0177 - val_loss: 1458.3319 - val_mse: 1458.3320 - val_mae: 26.0804\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2443.8358 - mse: 2443.8354 - mae: 28.8682 - val_loss: 1452.1873 - val_mse: 1452.1873 - val_mae: 26.1496\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2472.1270 - mse: 2472.1272 - mae: 28.8278 - val_loss: 1441.2771 - val_mse: 1441.2772 - val_mae: 26.4001\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2519.6608 - mse: 2519.6606 - mae: 29.8018 - val_loss: 1468.5892 - val_mse: 1468.5892 - val_mae: 25.8258\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2461.9760 - mse: 2461.9766 - mae: 28.7340 - val_loss: 1442.9138 - val_mse: 1442.9138 - val_mae: 26.2781\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2452.8101 - mse: 2452.8105 - mae: 28.8285 - val_loss: 1443.7858 - val_mse: 1443.7861 - val_mae: 26.2197\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 697us/step - loss: 2413.0418 - mse: 2413.0415 - mae: 28.7108 - val_loss: 1453.1678 - val_mse: 1453.1680 - val_mae: 26.0341\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2406.1641 - mse: 2406.1643 - mae: 28.5022 - val_loss: 1444.5159 - val_mse: 1444.5160 - val_mae: 26.2230\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2473.7568 - mse: 2473.7568 - mae: 29.1925 - val_loss: 1452.1458 - val_mse: 1452.1459 - val_mae: 26.0753\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2442.8944 - mse: 2442.8948 - mae: 29.0931 - val_loss: 1444.1808 - val_mse: 1444.1808 - val_mae: 26.1783\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2431.1396 - mse: 2431.1396 - mae: 29.1509 - val_loss: 1454.3291 - val_mse: 1454.3291 - val_mae: 25.9823\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2411.6152 - mse: 2411.6147 - mae: 28.9327 - val_loss: 1448.0831 - val_mse: 1448.0830 - val_mae: 26.0095\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2499.4228 - mse: 2499.4231 - mae: 28.8373 - val_loss: 1442.4751 - val_mse: 1442.4750 - val_mae: 26.1938\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2448.0199 - mse: 2448.0205 - mae: 28.8512 - val_loss: 1449.5290 - val_mse: 1449.5289 - val_mae: 26.1016\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2458.5129 - mse: 2458.5127 - mae: 28.7896 - val_loss: 1445.3147 - val_mse: 1445.3148 - val_mae: 26.1743\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 649us/step - loss: 2401.5825 - mse: 2401.5825 - mae: 28.5900 - val_loss: 1446.8166 - val_mse: 1446.8163 - val_mae: 26.1030\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2404.3947 - mse: 2404.3943 - mae: 28.9104 - val_loss: 1453.0950 - val_mse: 1453.0950 - val_mae: 25.9523\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2415.4498 - mse: 2415.4485 - mae: 28.5378 - val_loss: 1459.2484 - val_mse: 1459.2482 - val_mae: 25.7576\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2399.1790 - mse: 2399.1785 - mae: 28.4727 - val_loss: 1440.2669 - val_mse: 1440.2668 - val_mae: 26.1100\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 675us/step - loss: 2470.8804 - mse: 2470.8806 - mae: 29.0983 - val_loss: 1449.5580 - val_mse: 1449.5582 - val_mae: 25.9300\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2445.5747 - mse: 2445.5740 - mae: 28.9824 - val_loss: 1445.4843 - val_mse: 1445.4845 - val_mae: 26.0040\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 649us/step - loss: 2451.7317 - mse: 2451.7317 - mae: 28.8241 - val_loss: 1440.4882 - val_mse: 1440.4883 - val_mae: 26.1756\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 647us/step - loss: 2426.2103 - mse: 2426.2104 - mae: 28.6841 - val_loss: 1440.4883 - val_mse: 1440.4882 - val_mae: 26.2465\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2485.6793 - mse: 2485.6792 - mae: 28.7170 - val_loss: 1436.9289 - val_mse: 1436.9290 - val_mae: 26.3139\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2450.3093 - mse: 2450.3088 - mae: 29.0349 - val_loss: 1442.9136 - val_mse: 1442.9136 - val_mae: 26.0975\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2449.8463 - mse: 2449.8467 - mae: 29.0002 - val_loss: 1434.8365 - val_mse: 1434.8365 - val_mae: 26.1700\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2369.4245 - mse: 2369.4250 - mae: 28.6243 - val_loss: 1440.0996 - val_mse: 1440.0997 - val_mae: 26.0061\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2432.8118 - mse: 2432.8123 - mae: 29.0528 - val_loss: 1442.1497 - val_mse: 1442.1497 - val_mae: 25.9668\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2455.6931 - mse: 2455.6924 - mae: 28.4316 - val_loss: 1461.7548 - val_mse: 1461.7548 - val_mae: 25.6618\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2334.2316 - mse: 2334.2314 - mae: 28.8328 - val_loss: 3669.3311 - val_mse: 3669.3320 - val_mae: 24.4234\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2330.0084 - mse: 2330.0083 - mae: 28.9805 - val_loss: 3667.5253 - val_mse: 3667.5247 - val_mae: 23.7645\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2346.8274 - mse: 2346.8271 - mae: 29.0561 - val_loss: 3669.4901 - val_mse: 3669.4902 - val_mae: 23.5889\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2288.2949 - mse: 2288.2947 - mae: 28.9994 - val_loss: 3669.1966 - val_mse: 3669.1968 - val_mae: 23.8261\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2349.1020 - mse: 2349.1023 - mae: 29.6660 - val_loss: 3669.2896 - val_mse: 3669.2891 - val_mae: 23.6549\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2326.5631 - mse: 2326.5630 - mae: 28.9816 - val_loss: 3668.5052 - val_mse: 3668.5054 - val_mae: 23.5745\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2379.1294 - mse: 2379.1296 - mae: 29.5155 - val_loss: 3672.5493 - val_mse: 3672.5503 - val_mae: 22.8713\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2336.5762 - mse: 2336.5762 - mae: 29.0069 - val_loss: 3672.2889 - val_mse: 3672.2881 - val_mae: 24.0838\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2317.9910 - mse: 2317.9907 - mae: 29.0969 - val_loss: 3668.3345 - val_mse: 3668.3340 - val_mae: 23.6077\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2345.6478 - mse: 2345.6482 - mae: 29.1434 - val_loss: 3668.7682 - val_mse: 3668.7683 - val_mae: 24.1224\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 514us/step - loss: 2357.6758 - mse: 2357.6758 - mae: 29.1950 - val_loss: 3667.6467 - val_mse: 3667.6465 - val_mae: 23.4144\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2343.9779 - mse: 2343.9775 - mae: 28.9710 - val_loss: 3668.3299 - val_mse: 3668.3291 - val_mae: 23.4675\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2342.5929 - mse: 2342.5925 - mae: 29.4044 - val_loss: 3669.3597 - val_mse: 3669.3589 - val_mae: 23.0223\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2301.5513 - mse: 2301.5520 - mae: 28.9311 - val_loss: 3668.0663 - val_mse: 3668.0662 - val_mae: 23.6785\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2297.1735 - mse: 2297.1731 - mae: 29.0815 - val_loss: 3667.8257 - val_mse: 3667.8274 - val_mae: 23.5186\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 682us/step - loss: 2326.0475 - mse: 2326.0481 - mae: 29.0777 - val_loss: 3669.8585 - val_mse: 3669.8582 - val_mae: 23.4293\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2320.9226 - mse: 2320.9233 - mae: 28.9669 - val_loss: 3669.9993 - val_mse: 3669.9990 - val_mae: 24.0107\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2301.5337 - mse: 2301.5337 - mae: 28.6826 - val_loss: 3668.4594 - val_mse: 3668.4592 - val_mae: 23.8348\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2297.7248 - mse: 2297.7249 - mae: 28.9691 - val_loss: 3667.6033 - val_mse: 3667.6038 - val_mae: 23.4371\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2279.9030 - mse: 2279.9028 - mae: 29.0660 - val_loss: 3669.2492 - val_mse: 3669.2498 - val_mae: 23.2392\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 656us/step - loss: 2405.7730 - mse: 2405.7727 - mae: 29.3344 - val_loss: 3667.0305 - val_mse: 3667.0308 - val_mae: 23.5567\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 568us/step - loss: 2323.3170 - mse: 2323.3176 - mae: 28.8010 - val_loss: 3668.6141 - val_mse: 3668.6135 - val_mae: 23.9314\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 677us/step - loss: 2308.8553 - mse: 2308.8545 - mae: 29.0119 - val_loss: 3667.6731 - val_mse: 3667.6741 - val_mae: 23.8344\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2345.3892 - mse: 2345.3887 - mae: 29.4401 - val_loss: 3668.1615 - val_mse: 3668.1614 - val_mae: 23.9587\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2310.5324 - mse: 2310.5322 - mae: 28.8791 - val_loss: 3666.8476 - val_mse: 3666.8481 - val_mae: 24.2256\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 536us/step - loss: 2305.4413 - mse: 2305.4412 - mae: 29.3729 - val_loss: 3666.9988 - val_mse: 3666.9990 - val_mae: 23.7952\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 676us/step - loss: 2305.6446 - mse: 2305.6445 - mae: 28.7729 - val_loss: 3666.4186 - val_mse: 3666.4197 - val_mae: 23.5430\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2361.4480 - mse: 2361.4480 - mae: 29.6103 - val_loss: 3666.4910 - val_mse: 3666.4907 - val_mae: 23.6166\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2393.6543 - mse: 2393.6543 - mae: 29.5040 - val_loss: 3666.1640 - val_mse: 3666.1646 - val_mae: 23.6734\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2369.8894 - mse: 2369.8887 - mae: 29.0946 - val_loss: 3666.6494 - val_mse: 3666.6497 - val_mae: 23.6387\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2327.1888 - mse: 2327.1885 - mae: 28.9277 - val_loss: 3663.7984 - val_mse: 3663.7986 - val_mae: 23.6474\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2353.1756 - mse: 2353.1758 - mae: 28.8839 - val_loss: 3662.2705 - val_mse: 3662.2708 - val_mae: 23.6844\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 656us/step - loss: 2294.4066 - mse: 2294.4060 - mae: 28.8467 - val_loss: 3663.9160 - val_mse: 3663.9167 - val_mae: 23.9473\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2332.1181 - mse: 2332.1182 - mae: 29.2385 - val_loss: 3664.2782 - val_mse: 3664.2788 - val_mae: 23.8398\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2325.9907 - mse: 2325.9905 - mae: 29.2529 - val_loss: 3664.2445 - val_mse: 3664.2437 - val_mae: 23.7054\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2314.6129 - mse: 2314.6130 - mae: 28.9285 - val_loss: 3667.2658 - val_mse: 3667.2661 - val_mae: 23.7872\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2297.2688 - mse: 2297.2686 - mae: 28.8626 - val_loss: 3666.7848 - val_mse: 3666.7849 - val_mae: 23.8111\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2348.9238 - mse: 2348.9243 - mae: 28.9968 - val_loss: 3666.8397 - val_mse: 3666.8394 - val_mae: 24.2051\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 712us/step - loss: 2309.7002 - mse: 2309.7002 - mae: 28.5945 - val_loss: 3665.7563 - val_mse: 3665.7561 - val_mae: 23.8529\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2371.0532 - mse: 2371.0530 - mae: 29.5283 - val_loss: 3663.7332 - val_mse: 3663.7329 - val_mae: 23.4524\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2277.0039 - mse: 2277.0042 - mae: 29.0507 - val_loss: 3666.8192 - val_mse: 3666.8188 - val_mae: 23.6385\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2291.8383 - mse: 2291.8391 - mae: 28.9404 - val_loss: 3669.6050 - val_mse: 3669.6050 - val_mae: 24.0132\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2357.8574 - mse: 2357.8572 - mae: 29.0413 - val_loss: 3672.8550 - val_mse: 3672.8545 - val_mae: 24.2743\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2311.9945 - mse: 2311.9944 - mae: 28.6332 - val_loss: 3677.6944 - val_mse: 3677.6951 - val_mae: 24.9911\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2332.4525 - mse: 2332.4521 - mae: 29.2634 - val_loss: 3669.0646 - val_mse: 3669.0642 - val_mae: 23.6487\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 675us/step - loss: 2344.1855 - mse: 2344.1853 - mae: 28.7902 - val_loss: 3668.0327 - val_mse: 3668.0334 - val_mae: 24.0267\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2326.8745 - mse: 2326.8745 - mae: 29.0764 - val_loss: 3668.3028 - val_mse: 3668.3035 - val_mae: 23.7096\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 670us/step - loss: 2260.0023 - mse: 2260.0027 - mae: 28.3346 - val_loss: 3670.9422 - val_mse: 3670.9417 - val_mae: 24.0802\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2297.4470 - mse: 2297.4468 - mae: 29.0482 - val_loss: 3665.5836 - val_mse: 3665.5833 - val_mae: 23.8846\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2286.5825 - mse: 2286.5823 - mae: 28.5708 - val_loss: 3663.0617 - val_mse: 3663.0615 - val_mae: 23.7659\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 642us/step - loss: 2340.1392 - mse: 2340.1387 - mae: 28.9776 - val_loss: 3663.0177 - val_mse: 3663.0181 - val_mae: 23.8067\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2253.6673 - mse: 2253.6667 - mae: 28.6847 - val_loss: 3663.4803 - val_mse: 3663.4802 - val_mae: 23.6290\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2295.6899 - mse: 2295.6909 - mae: 28.6161 - val_loss: 3665.4749 - val_mse: 3665.4749 - val_mae: 23.9795\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2287.5078 - mse: 2287.5073 - mae: 28.7226 - val_loss: 3666.0867 - val_mse: 3666.0874 - val_mae: 23.7750\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2278.3593 - mse: 2278.3582 - mae: 28.5948 - val_loss: 3665.7173 - val_mse: 3665.7173 - val_mae: 23.7521\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2290.0471 - mse: 2290.0476 - mae: 28.8370 - val_loss: 3662.9727 - val_mse: 3662.9727 - val_mae: 23.5229\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2333.3190 - mse: 2333.3181 - mae: 28.4689 - val_loss: 3663.3752 - val_mse: 3663.3762 - val_mae: 23.5629\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 697us/step - loss: 2306.5816 - mse: 2306.5813 - mae: 28.9091 - val_loss: 3662.1770 - val_mse: 3662.1768 - val_mae: 23.4634\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2275.6715 - mse: 2275.6711 - mae: 28.5994 - val_loss: 3663.2650 - val_mse: 3663.2644 - val_mae: 23.2708\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2296.1257 - mse: 2296.1252 - mae: 28.6008 - val_loss: 3663.3656 - val_mse: 3663.3657 - val_mae: 23.7887\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 671us/step - loss: 2297.8347 - mse: 2297.8350 - mae: 28.7256 - val_loss: 3666.2238 - val_mse: 3666.2236 - val_mae: 24.3278\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 680us/step - loss: 2290.5109 - mse: 2290.5105 - mae: 28.7061 - val_loss: 3665.0652 - val_mse: 3665.0657 - val_mae: 24.0807\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2308.7510 - mse: 2308.7517 - mae: 28.8252 - val_loss: 3664.1879 - val_mse: 3664.1885 - val_mae: 23.5897\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2314.0374 - mse: 2314.0374 - mae: 28.9545 - val_loss: 3664.3706 - val_mse: 3664.3704 - val_mae: 23.7845\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2296.8048 - mse: 2296.8054 - mae: 28.6997 - val_loss: 3668.8304 - val_mse: 3668.8303 - val_mae: 24.0790\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2264.4875 - mse: 2264.4873 - mae: 28.7453 - val_loss: 3666.7783 - val_mse: 3666.7778 - val_mae: 23.7548\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 681us/step - loss: 2308.9529 - mse: 2308.9534 - mae: 28.8023 - val_loss: 3667.7902 - val_mse: 3667.7898 - val_mae: 23.9113\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2274.3167 - mse: 2274.3169 - mae: 28.6844 - val_loss: 3668.3480 - val_mse: 3668.3484 - val_mae: 24.3453\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2284.3719 - mse: 2284.3726 - mae: 28.7909 - val_loss: 3665.2744 - val_mse: 3665.2747 - val_mae: 23.7749\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2300.7629 - mse: 2300.7625 - mae: 28.7981 - val_loss: 3666.1950 - val_mse: 3666.1948 - val_mae: 23.7609\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2289.8895 - mse: 2289.8894 - mae: 28.4172 - val_loss: 3666.5768 - val_mse: 3666.5764 - val_mae: 23.6357\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2285.6783 - mse: 2285.6787 - mae: 28.5129 - val_loss: 3665.2233 - val_mse: 3665.2229 - val_mae: 23.7348\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2286.7834 - mse: 2286.7827 - mae: 28.5130 - val_loss: 3665.4731 - val_mse: 3665.4729 - val_mae: 24.0626\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2305.4053 - mse: 2305.4043 - mae: 28.7675 - val_loss: 3664.4628 - val_mse: 3664.4629 - val_mae: 23.6594\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2287.4214 - mse: 2287.4214 - mae: 28.6637 - val_loss: 3667.2296 - val_mse: 3667.2300 - val_mae: 23.2725\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2294.1661 - mse: 2294.1663 - mae: 28.8852 - val_loss: 3669.7938 - val_mse: 3669.7942 - val_mae: 23.8899\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2276.9218 - mse: 2276.9216 - mae: 28.3121 - val_loss: 3665.5764 - val_mse: 3665.5764 - val_mae: 23.6761\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2316.3101 - mse: 2316.3105 - mae: 28.9346 - val_loss: 3664.5135 - val_mse: 3664.5142 - val_mae: 23.5472\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2280.8311 - mse: 2280.8313 - mae: 28.5181 - val_loss: 3665.3022 - val_mse: 3665.3032 - val_mae: 23.4563\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2297.1336 - mse: 2297.1321 - mae: 28.8051 - val_loss: 3665.0567 - val_mse: 3665.0566 - val_mae: 23.8410\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2771.5872 - mse: 2771.5876 - mae: 28.7207 - val_loss: 2125.5618 - val_mse: 2125.5620 - val_mae: 25.8595\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2681.4943 - mse: 2681.4946 - mae: 28.1267 - val_loss: 2113.6743 - val_mse: 2113.6746 - val_mae: 26.0963\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2668.9240 - mse: 2668.9236 - mae: 28.2811 - val_loss: 2120.6710 - val_mse: 2120.6709 - val_mae: 25.9402\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2708.4065 - mse: 2708.4070 - mae: 28.4777 - val_loss: 2102.9619 - val_mse: 2102.9624 - val_mae: 26.2575\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2695.2195 - mse: 2695.2205 - mae: 28.5806 - val_loss: 2109.1537 - val_mse: 2109.1536 - val_mae: 26.3219\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2629.2661 - mse: 2629.2668 - mae: 27.9292 - val_loss: 2093.4184 - val_mse: 2093.4185 - val_mae: 26.5357\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2738.2611 - mse: 2738.2607 - mae: 28.5977 - val_loss: 2111.9923 - val_mse: 2111.9924 - val_mae: 26.1513\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2705.4365 - mse: 2705.4358 - mae: 28.5441 - val_loss: 2120.7805 - val_mse: 2120.7808 - val_mae: 26.0154\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2665.4600 - mse: 2665.4604 - mae: 28.2022 - val_loss: 2131.6141 - val_mse: 2131.6138 - val_mae: 25.8829\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2690.3951 - mse: 2690.3945 - mae: 28.1908 - val_loss: 2136.4121 - val_mse: 2136.4124 - val_mae: 26.1426\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2614.6594 - mse: 2614.6599 - mae: 27.7570 - val_loss: 2135.1647 - val_mse: 2135.1648 - val_mae: 25.9099\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2697.8878 - mse: 2697.8882 - mae: 28.3907 - val_loss: 2141.5809 - val_mse: 2141.5813 - val_mae: 25.7308\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2627.9014 - mse: 2627.9006 - mae: 27.8413 - val_loss: 2127.7062 - val_mse: 2127.7061 - val_mae: 26.3257\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 665us/step - loss: 2706.2185 - mse: 2706.2188 - mae: 28.5220 - val_loss: 2132.7818 - val_mse: 2132.7817 - val_mae: 26.3108\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2672.2661 - mse: 2672.2656 - mae: 28.2236 - val_loss: 2132.1422 - val_mse: 2132.1421 - val_mae: 26.2226\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 666us/step - loss: 2713.2504 - mse: 2713.2500 - mae: 28.5331 - val_loss: 2133.3341 - val_mse: 2133.3342 - val_mae: 26.0517\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 665us/step - loss: 2678.2188 - mse: 2678.2180 - mae: 28.4195 - val_loss: 2126.6874 - val_mse: 2126.6870 - val_mae: 26.1527\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2692.3661 - mse: 2692.3662 - mae: 28.0594 - val_loss: 2118.8625 - val_mse: 2118.8628 - val_mae: 26.2883\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2653.3641 - mse: 2653.3647 - mae: 28.1201 - val_loss: 2114.5755 - val_mse: 2114.5752 - val_mae: 26.4529\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2659.3267 - mse: 2659.3274 - mae: 27.9355 - val_loss: 2117.6070 - val_mse: 2117.6069 - val_mae: 26.1544\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2698.9171 - mse: 2698.9177 - mae: 28.2199 - val_loss: 2127.4467 - val_mse: 2127.4470 - val_mae: 25.9597\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2659.9740 - mse: 2659.9734 - mae: 28.1310 - val_loss: 2139.1665 - val_mse: 2139.1665 - val_mae: 25.8599\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2676.7973 - mse: 2676.7971 - mae: 28.5766 - val_loss: 2131.0366 - val_mse: 2131.0364 - val_mae: 26.2213\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2659.2629 - mse: 2659.2632 - mae: 28.1166 - val_loss: 2124.7723 - val_mse: 2124.7722 - val_mae: 26.5529\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2664.0591 - mse: 2664.0596 - mae: 28.2013 - val_loss: 2134.1579 - val_mse: 2134.1582 - val_mae: 26.3035\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2651.8363 - mse: 2651.8367 - mae: 28.0303 - val_loss: 2144.6736 - val_mse: 2144.6738 - val_mae: 26.1413\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2664.9092 - mse: 2664.9099 - mae: 28.5158 - val_loss: 2127.8688 - val_mse: 2127.8689 - val_mae: 26.3095\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2692.9173 - mse: 2692.9180 - mae: 28.2344 - val_loss: 2138.2139 - val_mse: 2138.2144 - val_mae: 26.0976\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2659.9653 - mse: 2659.9656 - mae: 28.1780 - val_loss: 2138.7046 - val_mse: 2138.7043 - val_mae: 26.1000\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 678us/step - loss: 2674.6246 - mse: 2674.6255 - mae: 28.2792 - val_loss: 2129.2131 - val_mse: 2129.2131 - val_mae: 26.2291\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2706.6151 - mse: 2706.6150 - mae: 28.5164 - val_loss: 2122.8336 - val_mse: 2122.8335 - val_mae: 26.1655\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2659.6889 - mse: 2659.6897 - mae: 27.9465 - val_loss: 2100.7914 - val_mse: 2100.7913 - val_mae: 26.7588\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2629.4174 - mse: 2629.4172 - mae: 28.1343 - val_loss: 2122.2862 - val_mse: 2122.2864 - val_mae: 26.2379\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 711us/step - loss: 2649.5213 - mse: 2649.5220 - mae: 28.2573 - val_loss: 2128.7181 - val_mse: 2128.7175 - val_mae: 26.0963\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2648.7330 - mse: 2648.7334 - mae: 28.1472 - val_loss: 2122.5722 - val_mse: 2122.5725 - val_mae: 26.2238\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2712.5308 - mse: 2712.5303 - mae: 28.4408 - val_loss: 2138.5636 - val_mse: 2138.5637 - val_mae: 26.1094\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2667.4284 - mse: 2667.4285 - mae: 28.1980 - val_loss: 2126.9058 - val_mse: 2126.9058 - val_mae: 26.0899\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2695.6690 - mse: 2695.6687 - mae: 27.9176 - val_loss: 2113.3536 - val_mse: 2113.3538 - val_mae: 26.0961\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 655us/step - loss: 2716.7255 - mse: 2716.7251 - mae: 28.4328 - val_loss: 2127.3684 - val_mse: 2127.3684 - val_mae: 25.9529\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2692.5711 - mse: 2692.5723 - mae: 28.0838 - val_loss: 2125.9576 - val_mse: 2125.9575 - val_mae: 26.1745\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2682.1157 - mse: 2682.1157 - mae: 28.2805 - val_loss: 2117.8781 - val_mse: 2117.8779 - val_mae: 26.2242\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 689us/step - loss: 2633.3915 - mse: 2633.3909 - mae: 27.9514 - val_loss: 2117.1762 - val_mse: 2117.1763 - val_mae: 26.6293\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2651.3275 - mse: 2651.3271 - mae: 28.1926 - val_loss: 2132.8069 - val_mse: 2132.8064 - val_mae: 26.2383\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2686.6450 - mse: 2686.6448 - mae: 28.5758 - val_loss: 2133.0074 - val_mse: 2133.0076 - val_mae: 26.1308\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2656.1079 - mse: 2656.1082 - mae: 28.3079 - val_loss: 2127.5171 - val_mse: 2127.5171 - val_mae: 26.3183\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 680us/step - loss: 2656.9281 - mse: 2656.9277 - mae: 27.9643 - val_loss: 2129.2865 - val_mse: 2129.2866 - val_mae: 26.1039\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 674us/step - loss: 2627.3665 - mse: 2627.3667 - mae: 27.9065 - val_loss: 2125.2267 - val_mse: 2125.2268 - val_mae: 26.5428\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2681.0894 - mse: 2681.0886 - mae: 28.1080 - val_loss: 2140.3384 - val_mse: 2140.3381 - val_mae: 25.9567\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2667.2433 - mse: 2667.2439 - mae: 28.0116 - val_loss: 2132.5190 - val_mse: 2132.5190 - val_mae: 26.2240\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2692.4090 - mse: 2692.4097 - mae: 28.3841 - val_loss: 2148.9508 - val_mse: 2148.9509 - val_mae: 25.6869\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 668us/step - loss: 2695.4511 - mse: 2695.4512 - mae: 28.4217 - val_loss: 2141.0275 - val_mse: 2141.0271 - val_mae: 26.0863\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2635.1043 - mse: 2635.1030 - mae: 27.8717 - val_loss: 2135.1292 - val_mse: 2135.1294 - val_mae: 25.9906\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2662.0655 - mse: 2662.0654 - mae: 27.6799 - val_loss: 2133.4712 - val_mse: 2133.4714 - val_mae: 26.0427\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2653.0615 - mse: 2653.0605 - mae: 27.9107 - val_loss: 2130.3929 - val_mse: 2130.3928 - val_mae: 26.2332\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2684.4344 - mse: 2684.4331 - mae: 28.2495 - val_loss: 2143.2869 - val_mse: 2143.2871 - val_mae: 26.2232\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 520us/step - loss: 2650.9576 - mse: 2650.9573 - mae: 28.0804 - val_loss: 2130.7437 - val_mse: 2130.7437 - val_mae: 26.5312\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 668us/step - loss: 2687.8684 - mse: 2687.8689 - mae: 28.3398 - val_loss: 2131.6951 - val_mse: 2131.6948 - val_mae: 26.1575\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2695.7383 - mse: 2695.7385 - mae: 28.1971 - val_loss: 2126.6914 - val_mse: 2126.6914 - val_mae: 26.2949\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2621.5048 - mse: 2621.5046 - mae: 28.1005 - val_loss: 2115.4706 - val_mse: 2115.4707 - val_mae: 26.5013\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2715.5851 - mse: 2715.5847 - mae: 28.0653 - val_loss: 2145.8503 - val_mse: 2145.8501 - val_mae: 26.0711\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2674.2108 - mse: 2674.2112 - mae: 28.3078 - val_loss: 2126.4837 - val_mse: 2126.4834 - val_mae: 26.4215\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2687.0599 - mse: 2687.0596 - mae: 28.0813 - val_loss: 2121.2042 - val_mse: 2121.2039 - val_mae: 26.4626\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 659us/step - loss: 2637.6811 - mse: 2637.6814 - mae: 28.0524 - val_loss: 2146.0069 - val_mse: 2146.0066 - val_mae: 25.7558\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2652.1175 - mse: 2652.1174 - mae: 27.7780 - val_loss: 2129.6639 - val_mse: 2129.6638 - val_mae: 26.3687\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2652.0937 - mse: 2652.0938 - mae: 28.1969 - val_loss: 2124.9387 - val_mse: 2124.9387 - val_mae: 26.3524\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2672.9621 - mse: 2672.9622 - mae: 28.2541 - val_loss: 2120.2795 - val_mse: 2120.2793 - val_mae: 26.4230\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2640.9882 - mse: 2640.9883 - mae: 28.1198 - val_loss: 2118.4684 - val_mse: 2118.4685 - val_mae: 26.1174\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2668.2705 - mse: 2668.2703 - mae: 28.0944 - val_loss: 2112.7506 - val_mse: 2112.7502 - val_mae: 26.6527\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2659.9283 - mse: 2659.9280 - mae: 28.3648 - val_loss: 2125.2293 - val_mse: 2125.2292 - val_mae: 26.3652\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2645.3988 - mse: 2645.3989 - mae: 27.9330 - val_loss: 2127.1974 - val_mse: 2127.1978 - val_mae: 26.4098\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2700.5969 - mse: 2700.5959 - mae: 28.3086 - val_loss: 2141.1549 - val_mse: 2141.1548 - val_mae: 26.0908\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2709.1435 - mse: 2709.1431 - mae: 28.0934 - val_loss: 2132.3149 - val_mse: 2132.3152 - val_mae: 26.1510\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 652us/step - loss: 2619.3005 - mse: 2619.2998 - mae: 27.7011 - val_loss: 2120.7464 - val_mse: 2120.7468 - val_mae: 26.6142\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2588.5614 - mse: 2588.5615 - mae: 27.6570 - val_loss: 2120.9561 - val_mse: 2120.9561 - val_mae: 26.3529\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2695.0725 - mse: 2695.0732 - mae: 28.3404 - val_loss: 2123.6178 - val_mse: 2123.6179 - val_mae: 25.8881\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2671.8147 - mse: 2671.8145 - mae: 28.3360 - val_loss: 2129.6399 - val_mse: 2129.6394 - val_mae: 26.2578\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2612.2348 - mse: 2612.2341 - mae: 27.7707 - val_loss: 2118.1815 - val_mse: 2118.1819 - val_mae: 26.5565\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2640.9841 - mse: 2640.9841 - mae: 28.0928 - val_loss: 2130.7602 - val_mse: 2130.7603 - val_mae: 26.1576\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2700.7604 - mse: 2700.7610 - mae: 28.1317 - val_loss: 2140.1435 - val_mse: 2140.1438 - val_mae: 26.0164\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 655us/step - loss: 2643.5013 - mse: 2643.5010 - mae: 27.8361 - val_loss: 2128.0560 - val_mse: 2128.0562 - val_mae: 26.0868\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 13315.8187 - mse: 13315.8193 - mae: 109.8477 - val_loss: 34582.7761 - val_mse: 34582.7773 - val_mae: 132.6023\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 13100.6127 - mse: 13100.6113 - mae: 108.8747 - val_loss: 34152.1883 - val_mse: 34152.1875 - val_mae: 131.0022\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 511us/step - loss: 12450.6193 - mse: 12450.6191 - mae: 105.8609 - val_loss: 32851.6375 - val_mse: 32851.6367 - val_mae: 126.0558\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 498us/step - loss: 10649.8615 - mse: 10649.8613 - mae: 96.7698 - val_loss: 29397.6747 - val_mse: 29397.6738 - val_mae: 111.8810\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 6755.4714 - mse: 6755.4712 - mae: 72.6863 - val_loss: 22224.4054 - val_mse: 22224.4043 - val_mae: 74.4249\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 477us/step - loss: 2920.5513 - mse: 2920.5515 - mae: 40.2297 - val_loss: 17320.6680 - val_mse: 17320.6680 - val_mae: 36.4012\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 474us/step - loss: 2645.2500 - mse: 2645.2500 - mae: 37.5337 - val_loss: 17830.3751 - val_mse: 17830.3730 - val_mae: 39.6357\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 451us/step - loss: 2617.0462 - mse: 2617.0461 - mae: 36.2370 - val_loss: 17902.2795 - val_mse: 17902.2793 - val_mae: 40.1952\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 467us/step - loss: 2462.7009 - mse: 2462.7009 - mae: 35.7309 - val_loss: 17600.8846 - val_mse: 17600.8848 - val_mae: 37.9009\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2793.8163 - mse: 2793.8162 - mae: 37.7951 - val_loss: 17676.7398 - val_mse: 17676.7402 - val_mae: 38.3896\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2602.2610 - mse: 2602.2607 - mae: 36.6538 - val_loss: 17724.8148 - val_mse: 17724.8145 - val_mae: 38.7175\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2330.0247 - mse: 2330.0247 - mae: 34.8217 - val_loss: 17535.8400 - val_mse: 17535.8418 - val_mae: 37.4341\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 686us/step - loss: 2683.5469 - mse: 2683.5474 - mae: 37.4000 - val_loss: 17551.8275 - val_mse: 17551.8281 - val_mae: 37.5070\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 614us/step - loss: 2455.3235 - mse: 2455.3235 - mae: 35.4009 - val_loss: 17308.3425 - val_mse: 17308.3418 - val_mae: 36.2681\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2354.8984 - mse: 2354.8984 - mae: 35.7493 - val_loss: 17362.0575 - val_mse: 17362.0586 - val_mae: 36.4684\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2582.4462 - mse: 2582.4460 - mae: 36.4730 - val_loss: 17527.2082 - val_mse: 17527.2070 - val_mae: 37.2902\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 2840.3803 - mse: 2840.3806 - mae: 37.2640 - val_loss: 17910.2354 - val_mse: 17910.2363 - val_mae: 39.9744\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 2326.8640 - mse: 2326.8643 - mae: 34.6071 - val_loss: 17584.6875 - val_mse: 17584.6875 - val_mae: 37.5568\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 2559.7545 - mse: 2559.7542 - mae: 35.4302 - val_loss: 17795.9286 - val_mse: 17795.9277 - val_mae: 39.0332\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2239.7711 - mse: 2239.7710 - mae: 33.9429 - val_loss: 17391.4962 - val_mse: 17391.4961 - val_mae: 36.5609\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 557us/step - loss: 2410.2862 - mse: 2410.2859 - mae: 34.7217 - val_loss: 17504.6499 - val_mse: 17504.6484 - val_mae: 37.0976\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 2493.0573 - mse: 2493.0569 - mae: 35.5911 - val_loss: 17674.0681 - val_mse: 17674.0684 - val_mae: 38.1457\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 645us/step - loss: 2326.9199 - mse: 2326.9197 - mae: 34.9675 - val_loss: 17564.9362 - val_mse: 17564.9375 - val_mae: 37.4079\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 2420.9611 - mse: 2420.9612 - mae: 34.2715 - val_loss: 17494.5086 - val_mse: 17494.5098 - val_mae: 36.9929\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 679us/step - loss: 2464.8885 - mse: 2464.8884 - mae: 35.5525 - val_loss: 17582.6834 - val_mse: 17582.6816 - val_mae: 37.4316\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 2449.1100 - mse: 2449.1099 - mae: 35.0116 - val_loss: 17585.0590 - val_mse: 17585.0586 - val_mae: 37.4123\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2158.3766 - mse: 2158.3767 - mae: 33.6518 - val_loss: 17414.1691 - val_mse: 17414.1699 - val_mae: 36.5580\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 728us/step - loss: 2501.2035 - mse: 2501.2036 - mae: 35.5929 - val_loss: 17435.4085 - val_mse: 17435.4102 - val_mae: 36.6399\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2467.9590 - mse: 2467.9587 - mae: 35.4399 - val_loss: 17680.9552 - val_mse: 17680.9551 - val_mae: 37.9318\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 533us/step - loss: 2269.5405 - mse: 2269.5405 - mae: 33.0787 - val_loss: 17576.9139 - val_mse: 17576.9141 - val_mae: 37.2527\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 2503.2686 - mse: 2503.2688 - mae: 35.0307 - val_loss: 17528.1147 - val_mse: 17528.1152 - val_mae: 36.9963\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2130.5268 - mse: 2130.5269 - mae: 32.0110 - val_loss: 17456.6243 - val_mse: 17456.6270 - val_mae: 36.6772\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 2172.1955 - mse: 2172.1953 - mae: 32.2414 - val_loss: 17369.4401 - val_mse: 17369.4414 - val_mae: 36.3423\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2280.8280 - mse: 2280.8279 - mae: 33.0949 - val_loss: 17400.8041 - val_mse: 17400.8047 - val_mae: 36.4514\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2468.1661 - mse: 2468.1663 - mae: 35.1074 - val_loss: 17559.8848 - val_mse: 17559.8867 - val_mae: 37.1001\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2382.1544 - mse: 2382.1541 - mae: 32.5087 - val_loss: 17526.4515 - val_mse: 17526.4512 - val_mae: 36.8957\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2157.4307 - mse: 2157.4307 - mae: 32.1771 - val_loss: 17334.8409 - val_mse: 17334.8398 - val_mae: 36.1939\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2441.1005 - mse: 2441.1003 - mae: 34.5559 - val_loss: 17758.8794 - val_mse: 17758.8809 - val_mae: 38.2492\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2196.4663 - mse: 2196.4661 - mae: 32.9268 - val_loss: 17403.8710 - val_mse: 17403.8711 - val_mae: 36.4407\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2190.5218 - mse: 2190.5217 - mae: 32.3244 - val_loss: 17560.4057 - val_mse: 17560.4062 - val_mae: 37.0508\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 565us/step - loss: 2217.4912 - mse: 2217.4915 - mae: 32.9450 - val_loss: 17542.9489 - val_mse: 17542.9492 - val_mae: 36.9637\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2200.2760 - mse: 2200.2764 - mae: 33.5142 - val_loss: 17545.5752 - val_mse: 17545.5742 - val_mae: 36.9620\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 466us/step - loss: 2162.2040 - mse: 2162.2043 - mae: 32.9590 - val_loss: 17515.1070 - val_mse: 17515.1055 - val_mae: 36.7900\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 448us/step - loss: 2178.7388 - mse: 2178.7390 - mae: 32.0793 - val_loss: 17375.7640 - val_mse: 17375.7637 - val_mae: 36.2974\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2077.0452 - mse: 2077.0452 - mae: 32.1488 - val_loss: 17401.3528 - val_mse: 17401.3516 - val_mae: 36.3665\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 675us/step - loss: 2134.8808 - mse: 2134.8809 - mae: 32.7432 - val_loss: 17579.6993 - val_mse: 17579.6992 - val_mae: 37.0038\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 2144.6025 - mse: 2144.6023 - mae: 31.1926 - val_loss: 17553.2813 - val_mse: 17553.2812 - val_mae: 36.8772\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 482us/step - loss: 2085.7191 - mse: 2085.7192 - mae: 32.3110 - val_loss: 17477.4772 - val_mse: 17477.4785 - val_mae: 36.5871\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 480us/step - loss: 2176.2372 - mse: 2176.2373 - mae: 32.8965 - val_loss: 17557.9639 - val_mse: 17557.9648 - val_mae: 36.8609\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 408us/step - loss: 2108.2062 - mse: 2108.2061 - mae: 32.2285 - val_loss: 17543.9798 - val_mse: 17543.9824 - val_mae: 36.7893\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 459us/step - loss: 2200.7075 - mse: 2200.7075 - mae: 32.3271 - val_loss: 17426.1127 - val_mse: 17426.1133 - val_mae: 36.3721\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 468us/step - loss: 2342.5767 - mse: 2342.5767 - mae: 33.4140 - val_loss: 17559.3924 - val_mse: 17559.3906 - val_mae: 36.8258\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 500us/step - loss: 2186.0833 - mse: 2186.0833 - mae: 33.1436 - val_loss: 17363.4289 - val_mse: 17363.4297 - val_mae: 36.1632\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 2095.0400 - mse: 2095.0398 - mae: 32.0685 - val_loss: 17481.6467 - val_mse: 17481.6484 - val_mae: 36.5218\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 446us/step - loss: 2065.4224 - mse: 2065.4224 - mae: 30.6873 - val_loss: 17340.0752 - val_mse: 17340.0762 - val_mae: 36.0818\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 516us/step - loss: 2016.4716 - mse: 2016.4714 - mae: 31.6868 - val_loss: 17485.9733 - val_mse: 17485.9746 - val_mae: 36.5276\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 535us/step - loss: 2142.5598 - mse: 2142.5598 - mae: 32.1258 - val_loss: 17264.6423 - val_mse: 17264.6445 - val_mae: 35.8673\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 549us/step - loss: 2389.0213 - mse: 2389.0212 - mae: 33.4809 - val_loss: 17613.6635 - val_mse: 17613.6641 - val_mae: 36.9680\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 476us/step - loss: 2027.6221 - mse: 2027.6219 - mae: 31.0528 - val_loss: 17414.7753 - val_mse: 17414.7754 - val_mae: 36.2866\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 470us/step - loss: 2068.2965 - mse: 2068.2966 - mae: 30.4177 - val_loss: 17586.4738 - val_mse: 17586.4746 - val_mae: 36.8691\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 497us/step - loss: 2141.2195 - mse: 2141.2195 - mae: 32.5932 - val_loss: 17549.1809 - val_mse: 17549.1816 - val_mae: 36.7212\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 470us/step - loss: 2086.2648 - mse: 2086.2646 - mae: 31.4072 - val_loss: 17484.2312 - val_mse: 17484.2324 - val_mae: 36.4791\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 452us/step - loss: 2199.7549 - mse: 2199.7546 - mae: 31.9228 - val_loss: 17421.7392 - val_mse: 17421.7402 - val_mae: 36.2780\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 428us/step - loss: 1984.1751 - mse: 1984.1750 - mae: 31.2995 - val_loss: 17613.8783 - val_mse: 17613.8789 - val_mae: 36.9463\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 555us/step - loss: 2205.0668 - mse: 2205.0667 - mae: 32.2506 - val_loss: 17640.9348 - val_mse: 17640.9336 - val_mae: 37.0538\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 447us/step - loss: 2138.0143 - mse: 2138.0142 - mae: 31.7003 - val_loss: 17438.5277 - val_mse: 17438.5273 - val_mae: 36.3637\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 456us/step - loss: 1968.4493 - mse: 1968.4492 - mae: 30.8016 - val_loss: 17328.8105 - val_mse: 17328.8105 - val_mae: 36.0309\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 2006.4015 - mse: 2006.4014 - mae: 30.6203 - val_loss: 17457.2726 - val_mse: 17457.2715 - val_mae: 36.4449\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 458us/step - loss: 1949.0589 - mse: 1949.0591 - mae: 30.5545 - val_loss: 17411.2939 - val_mse: 17411.2949 - val_mae: 36.3002\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 474us/step - loss: 2153.1734 - mse: 2153.1736 - mae: 31.8996 - val_loss: 17599.7173 - val_mse: 17599.7188 - val_mae: 36.9060\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 533us/step - loss: 1997.0318 - mse: 1997.0316 - mae: 30.6736 - val_loss: 17432.6848 - val_mse: 17432.6836 - val_mae: 36.3605\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2037.2056 - mse: 2037.2054 - mae: 31.0641 - val_loss: 17747.3513 - val_mse: 17747.3516 - val_mae: 37.5198\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 508us/step - loss: 2008.7488 - mse: 2008.7487 - mae: 29.8932 - val_loss: 17522.7749 - val_mse: 17522.7754 - val_mae: 36.5825\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 1833.1982 - mse: 1833.1980 - mae: 29.4845 - val_loss: 17574.6978 - val_mse: 17574.6992 - val_mae: 36.7331\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 704us/step - loss: 2025.8115 - mse: 2025.8116 - mae: 29.5455 - val_loss: 17346.4831 - val_mse: 17346.4844 - val_mae: 36.0325\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 643us/step - loss: 2022.7732 - mse: 2022.7732 - mae: 30.9334 - val_loss: 17581.8481 - val_mse: 17581.8477 - val_mae: 36.7412\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 1970.8485 - mse: 1970.8485 - mae: 29.8747 - val_loss: 17382.8916 - val_mse: 17382.8906 - val_mae: 36.1213\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 1993.6987 - mse: 1993.6989 - mae: 30.1604 - val_loss: 17374.4584 - val_mse: 17374.4570 - val_mae: 36.1182\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 1906.4836 - mse: 1906.4835 - mae: 29.9538 - val_loss: 17386.0666 - val_mse: 17386.0684 - val_mae: 36.1530\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 528us/step - loss: 1987.7820 - mse: 1987.7820 - mae: 30.7151 - val_loss: 17464.3577 - val_mse: 17464.3574 - val_mae: 36.3914\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4175.5750 - mse: 4175.5752 - mae: 34.0447 - val_loss: 2210.6643 - val_mse: 2210.6643 - val_mae: 30.4779\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 4152.1931 - mse: 4152.1934 - mae: 35.1889 - val_loss: 2319.5655 - val_mse: 2319.5652 - val_mae: 30.9889\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4233.4549 - mse: 4233.4556 - mae: 35.1194 - val_loss: 2299.9175 - val_mse: 2299.9175 - val_mae: 30.8847\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4141.1013 - mse: 4141.1016 - mae: 34.8542 - val_loss: 2429.9277 - val_mse: 2429.9275 - val_mae: 31.5427\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4204.2899 - mse: 4204.2896 - mae: 34.8483 - val_loss: 2388.1238 - val_mse: 2388.1238 - val_mae: 31.3174\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 4227.7839 - mse: 4227.7837 - mae: 35.4386 - val_loss: 2437.9705 - val_mse: 2437.9707 - val_mae: 31.5738\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4378.0781 - mse: 4378.0786 - mae: 35.1454 - val_loss: 2463.1968 - val_mse: 2463.1970 - val_mae: 31.7124\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4286.6556 - mse: 4286.6558 - mae: 36.2073 - val_loss: 2355.4195 - val_mse: 2355.4194 - val_mae: 31.1213\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4318.4435 - mse: 4318.4434 - mae: 35.5381 - val_loss: 2483.1107 - val_mse: 2483.1106 - val_mae: 31.8272\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 3999.4453 - mse: 3999.4448 - mae: 33.3534 - val_loss: 2322.0887 - val_mse: 2322.0889 - val_mae: 30.9554\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4250.5873 - mse: 4250.5879 - mae: 35.4119 - val_loss: 2358.1102 - val_mse: 2358.1104 - val_mae: 31.1279\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4204.8239 - mse: 4204.8237 - mae: 33.8427 - val_loss: 2389.7445 - val_mse: 2389.7446 - val_mae: 31.2886\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4225.4259 - mse: 4225.4263 - mae: 33.8634 - val_loss: 2340.5320 - val_mse: 2340.5320 - val_mae: 31.0404\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4027.2147 - mse: 4027.2146 - mae: 34.5576 - val_loss: 2369.3814 - val_mse: 2369.3816 - val_mae: 31.1754\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4295.7447 - mse: 4295.7451 - mae: 35.0901 - val_loss: 2387.2292 - val_mse: 2387.2290 - val_mae: 31.2634\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4121.0332 - mse: 4121.0332 - mae: 34.2851 - val_loss: 2472.4066 - val_mse: 2472.4067 - val_mae: 31.7220\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4163.3076 - mse: 4163.3076 - mae: 35.7869 - val_loss: 2430.5145 - val_mse: 2430.5144 - val_mae: 31.4708\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 661us/step - loss: 4081.8863 - mse: 4081.8865 - mae: 33.7733 - val_loss: 2405.9958 - val_mse: 2405.9961 - val_mae: 31.3391\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4146.7436 - mse: 4146.7441 - mae: 33.7413 - val_loss: 2364.2907 - val_mse: 2364.2905 - val_mae: 31.1340\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4160.4030 - mse: 4160.4023 - mae: 33.3767 - val_loss: 2414.3137 - val_mse: 2414.3135 - val_mae: 31.3771\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 4229.3029 - mse: 4229.3027 - mae: 33.7679 - val_loss: 2364.3954 - val_mse: 2364.3953 - val_mae: 31.1294\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4086.3368 - mse: 4086.3362 - mae: 33.8826 - val_loss: 2353.0566 - val_mse: 2353.0564 - val_mae: 31.0711\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4141.2930 - mse: 4141.2930 - mae: 35.1458 - val_loss: 2447.3237 - val_mse: 2447.3235 - val_mae: 31.5431\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4187.9105 - mse: 4187.9106 - mae: 34.3199 - val_loss: 2408.4918 - val_mse: 2408.4917 - val_mae: 31.3245\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 3936.0574 - mse: 3936.0576 - mae: 33.1817 - val_loss: 2320.4663 - val_mse: 2320.4663 - val_mae: 30.9104\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4169.9759 - mse: 4169.9766 - mae: 34.4128 - val_loss: 2392.9856 - val_mse: 2392.9858 - val_mae: 31.2470\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4218.1501 - mse: 4218.1504 - mae: 34.2461 - val_loss: 2401.1532 - val_mse: 2401.1533 - val_mae: 31.2837\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4117.6863 - mse: 4117.6860 - mae: 33.4931 - val_loss: 2364.8846 - val_mse: 2364.8845 - val_mae: 31.1126\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 3964.7961 - mse: 3964.7961 - mae: 33.6412 - val_loss: 2438.6121 - val_mse: 2438.6121 - val_mae: 31.4740\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4030.1130 - mse: 4030.1133 - mae: 33.6326 - val_loss: 2387.8244 - val_mse: 2387.8242 - val_mae: 31.2153\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 0s 468us/step - loss: 4264.4691 - mse: 4264.4692 - mae: 34.8502 - val_loss: 2407.6926 - val_mse: 2407.6929 - val_mae: 31.3122\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 4073.4418 - mse: 4073.4417 - mae: 34.2845 - val_loss: 2394.2099 - val_mse: 2394.2100 - val_mae: 31.2415\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 3968.6042 - mse: 3968.6038 - mae: 33.7173 - val_loss: 2367.7566 - val_mse: 2367.7566 - val_mae: 31.1259\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 663us/step - loss: 4031.4994 - mse: 4031.4995 - mae: 33.9362 - val_loss: 2363.8536 - val_mse: 2363.8535 - val_mae: 31.1159\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 3971.3834 - mse: 3971.3840 - mae: 34.6179 - val_loss: 2353.1974 - val_mse: 2353.1975 - val_mae: 31.0582\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4043.0496 - mse: 4043.0493 - mae: 33.4728 - val_loss: 2417.0647 - val_mse: 2417.0649 - val_mae: 31.3521\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 520us/step - loss: 4144.1952 - mse: 4144.1953 - mae: 33.9742 - val_loss: 2432.5952 - val_mse: 2432.5955 - val_mae: 31.4321\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 537us/step - loss: 4157.8232 - mse: 4157.8232 - mae: 34.3604 - val_loss: 2403.0975 - val_mse: 2403.0972 - val_mae: 31.2877\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 537us/step - loss: 3979.9376 - mse: 3979.9377 - mae: 34.1646 - val_loss: 2407.1435 - val_mse: 2407.1436 - val_mae: 31.3078\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 556us/step - loss: 4063.8568 - mse: 4063.8569 - mae: 33.3552 - val_loss: 2387.1565 - val_mse: 2387.1565 - val_mae: 31.2233\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4128.8473 - mse: 4128.8467 - mae: 33.7833 - val_loss: 2404.4792 - val_mse: 2404.4792 - val_mae: 31.3030\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 0s 458us/step - loss: 4113.9329 - mse: 4113.9326 - mae: 34.0608 - val_loss: 2468.9187 - val_mse: 2468.9187 - val_mae: 31.6493\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 3840.9676 - mse: 3840.9673 - mae: 32.8999 - val_loss: 2317.9460 - val_mse: 2317.9463 - val_mae: 30.9233\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 658us/step - loss: 4063.3017 - mse: 4063.3018 - mae: 33.3415 - val_loss: 2420.0641 - val_mse: 2420.0642 - val_mae: 31.3836\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4062.0722 - mse: 4062.0720 - mae: 33.8083 - val_loss: 2393.5600 - val_mse: 2393.5603 - val_mae: 31.2615\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 4282.5703 - mse: 4282.5708 - mae: 34.7029 - val_loss: 2427.1016 - val_mse: 2427.1018 - val_mae: 31.4203\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 4004.7253 - mse: 4004.7258 - mae: 32.8426 - val_loss: 2343.4660 - val_mse: 2343.4661 - val_mae: 31.0397\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4253.9645 - mse: 4253.9639 - mae: 34.2939 - val_loss: 2394.3959 - val_mse: 2394.3955 - val_mae: 31.2655\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4092.7364 - mse: 4092.7368 - mae: 33.5280 - val_loss: 2379.8719 - val_mse: 2379.8718 - val_mae: 31.2091\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 3965.2889 - mse: 3965.2896 - mae: 33.4167 - val_loss: 2382.0964 - val_mse: 2382.0962 - val_mae: 31.2308\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 3981.2780 - mse: 3981.2783 - mae: 33.8246 - val_loss: 2389.3115 - val_mse: 2389.3118 - val_mae: 31.2654\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4143.8212 - mse: 4143.8218 - mae: 33.6391 - val_loss: 2423.8933 - val_mse: 2423.8931 - val_mae: 31.4323\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4065.4324 - mse: 4065.4329 - mae: 33.7342 - val_loss: 2380.7248 - val_mse: 2380.7251 - val_mae: 31.2493\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 534us/step - loss: 4061.4075 - mse: 4061.4070 - mae: 33.4959 - val_loss: 2454.8592 - val_mse: 2454.8591 - val_mae: 31.5899\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 535us/step - loss: 3942.1202 - mse: 3942.1204 - mae: 33.3246 - val_loss: 2351.5977 - val_mse: 2351.5974 - val_mae: 31.1197\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 3989.4034 - mse: 3989.4033 - mae: 34.1984 - val_loss: 2354.0879 - val_mse: 2354.0876 - val_mae: 31.1337\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 506us/step - loss: 4053.2371 - mse: 4053.2375 - mae: 33.4587 - val_loss: 2355.4632 - val_mse: 2355.4634 - val_mae: 31.1375\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 473us/step - loss: 3998.0828 - mse: 3998.0830 - mae: 32.9433 - val_loss: 2352.8411 - val_mse: 2352.8413 - val_mae: 31.1264\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 636us/step - loss: 3998.8124 - mse: 3998.8130 - mae: 33.5082 - val_loss: 2500.0369 - val_mse: 2500.0369 - val_mae: 31.8474\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 3742.3406 - mse: 3742.3403 - mae: 32.7898 - val_loss: 2315.2965 - val_mse: 2315.2964 - val_mae: 30.9527\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 3820.3949 - mse: 3820.3953 - mae: 33.3925 - val_loss: 2338.8749 - val_mse: 2338.8748 - val_mae: 31.0617\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 4078.8209 - mse: 4078.8213 - mae: 33.3531 - val_loss: 2462.4385 - val_mse: 2462.4385 - val_mae: 31.6322\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 4151.0273 - mse: 4151.0269 - mae: 33.8311 - val_loss: 2444.1901 - val_mse: 2444.1897 - val_mae: 31.5444\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 4101.2323 - mse: 4101.2324 - mae: 32.6514 - val_loss: 2405.7816 - val_mse: 2405.7812 - val_mae: 31.3771\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 4005.8158 - mse: 4005.8162 - mae: 33.0073 - val_loss: 2415.8207 - val_mse: 2415.8206 - val_mae: 31.4188\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4001.0361 - mse: 4001.0361 - mae: 32.9288 - val_loss: 2394.1518 - val_mse: 2394.1519 - val_mae: 31.3209\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4064.8695 - mse: 4064.8694 - mae: 33.6429 - val_loss: 2339.5686 - val_mse: 2339.5688 - val_mae: 31.0779\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4008.7074 - mse: 4008.7073 - mae: 34.0020 - val_loss: 2392.3126 - val_mse: 2392.3125 - val_mae: 31.3060\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4031.6549 - mse: 4031.6548 - mae: 33.4669 - val_loss: 2384.5963 - val_mse: 2384.5959 - val_mae: 31.2718\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 3959.4114 - mse: 3959.4116 - mae: 32.9205 - val_loss: 2380.7605 - val_mse: 2380.7605 - val_mae: 31.2517\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4295.3023 - mse: 4295.3027 - mae: 34.6642 - val_loss: 2450.2980 - val_mse: 2450.2981 - val_mae: 31.5755\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 4120.2091 - mse: 4120.2090 - mae: 33.4029 - val_loss: 2433.2293 - val_mse: 2433.2295 - val_mae: 31.5035\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4052.1592 - mse: 4052.1592 - mae: 33.9845 - val_loss: 2400.3396 - val_mse: 2400.3396 - val_mae: 31.3497\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4049.6057 - mse: 4049.6052 - mae: 33.3233 - val_loss: 2402.5456 - val_mse: 2402.5457 - val_mae: 31.3665\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4048.4310 - mse: 4048.4304 - mae: 33.3649 - val_loss: 2429.3516 - val_mse: 2429.3513 - val_mae: 31.4960\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 513us/step - loss: 4014.9949 - mse: 4014.9946 - mae: 33.7623 - val_loss: 2400.2402 - val_mse: 2400.2400 - val_mae: 31.3595\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 554us/step - loss: 4067.4271 - mse: 4067.4277 - mae: 32.9352 - val_loss: 2376.9809 - val_mse: 2376.9810 - val_mae: 31.2573\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 0s 478us/step - loss: 4067.3290 - mse: 4067.3291 - mae: 32.6146 - val_loss: 2413.4529 - val_mse: 2413.4529 - val_mae: 31.4209\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 3942.1927 - mse: 3942.1934 - mae: 32.4485 - val_loss: 2369.8137 - val_mse: 2369.8135 - val_mae: 31.2289\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 3974.7212 - mse: 3974.7214 - mae: 32.5390 - val_loss: 2362.5984 - val_mse: 2362.5984 - val_mae: 31.1982\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3417.7646 - mse: 3417.7646 - mae: 33.1087 - val_loss: 1469.2237 - val_mse: 1469.2238 - val_mae: 25.0695\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3356.9520 - mse: 3356.9529 - mae: 32.9515 - val_loss: 1469.9014 - val_mse: 1469.9012 - val_mae: 25.0565\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3392.7643 - mse: 3392.7642 - mae: 32.9857 - val_loss: 1473.3933 - val_mse: 1473.3934 - val_mae: 24.8234\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3459.6197 - mse: 3459.6196 - mae: 33.9337 - val_loss: 1481.9705 - val_mse: 1481.9706 - val_mae: 24.5281\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 482us/step - loss: 3264.4225 - mse: 3264.4229 - mae: 33.0512 - val_loss: 1466.5703 - val_mse: 1466.5703 - val_mae: 25.0349\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3449.7133 - mse: 3449.7126 - mae: 32.8255 - val_loss: 1462.5869 - val_mse: 1462.5869 - val_mae: 25.3231\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3397.9922 - mse: 3397.9919 - mae: 32.9524 - val_loss: 1464.9410 - val_mse: 1464.9410 - val_mae: 25.0858\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3314.8108 - mse: 3314.8108 - mae: 33.1134 - val_loss: 1464.4023 - val_mse: 1464.4025 - val_mae: 25.1173\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 656us/step - loss: 3301.1238 - mse: 3301.1238 - mae: 32.3097 - val_loss: 1464.7496 - val_mse: 1464.7495 - val_mae: 25.1486\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 659us/step - loss: 3439.2926 - mse: 3439.2917 - mae: 33.0229 - val_loss: 1469.7026 - val_mse: 1469.7025 - val_mae: 24.9309\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3380.8952 - mse: 3380.8945 - mae: 32.5310 - val_loss: 1471.0477 - val_mse: 1471.0476 - val_mae: 24.8972\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 687us/step - loss: 3404.7069 - mse: 3404.7068 - mae: 32.8934 - val_loss: 1465.5416 - val_mse: 1465.5416 - val_mae: 25.2205\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 673us/step - loss: 3329.6215 - mse: 3329.6218 - mae: 32.9607 - val_loss: 1469.7637 - val_mse: 1469.7635 - val_mae: 24.9286\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3342.2360 - mse: 3342.2358 - mae: 32.3119 - val_loss: 1461.3772 - val_mse: 1461.3772 - val_mae: 25.5002\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3399.9995 - mse: 3399.9990 - mae: 33.2932 - val_loss: 1463.1194 - val_mse: 1463.1193 - val_mae: 25.1491\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3374.2405 - mse: 3374.2402 - mae: 32.7071 - val_loss: 1459.9348 - val_mse: 1459.9349 - val_mae: 25.4594\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3494.2468 - mse: 3494.2471 - mae: 33.9362 - val_loss: 1459.9170 - val_mse: 1459.9170 - val_mae: 25.5731\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3453.4211 - mse: 3453.4214 - mae: 33.2503 - val_loss: 1470.9360 - val_mse: 1470.9360 - val_mae: 24.8132\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3239.9100 - mse: 3239.9104 - mae: 31.6939 - val_loss: 1460.5364 - val_mse: 1460.5364 - val_mae: 25.6149\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3301.0144 - mse: 3301.0142 - mae: 31.7290 - val_loss: 1461.5183 - val_mse: 1461.5184 - val_mae: 25.4184\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3331.9965 - mse: 3331.9961 - mae: 32.8569 - val_loss: 1463.6691 - val_mse: 1463.6693 - val_mae: 25.2256\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 677us/step - loss: 3372.9994 - mse: 3372.9998 - mae: 32.5108 - val_loss: 1467.0665 - val_mse: 1467.0663 - val_mae: 25.0006\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 701us/step - loss: 3402.4180 - mse: 3402.4180 - mae: 32.9296 - val_loss: 1471.8285 - val_mse: 1471.8285 - val_mae: 24.8168\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3355.1803 - mse: 3355.1807 - mae: 32.5561 - val_loss: 1466.2741 - val_mse: 1466.2739 - val_mae: 25.1054\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 706us/step - loss: 3380.2071 - mse: 3380.2087 - mae: 32.7200 - val_loss: 1462.7133 - val_mse: 1462.7134 - val_mae: 25.2420\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3383.8957 - mse: 3383.8953 - mae: 32.4592 - val_loss: 1464.3793 - val_mse: 1464.3794 - val_mae: 25.1069\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3233.1889 - mse: 3233.1882 - mae: 32.3160 - val_loss: 1462.9394 - val_mse: 1462.9392 - val_mae: 25.2105\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 612us/step - loss: 3273.6895 - mse: 3273.6902 - mae: 31.6815 - val_loss: 1466.8165 - val_mse: 1466.8165 - val_mae: 24.9576\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3403.9955 - mse: 3403.9949 - mae: 32.5638 - val_loss: 1471.9147 - val_mse: 1471.9148 - val_mae: 24.7048\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 675us/step - loss: 3339.4971 - mse: 3339.4973 - mae: 32.2619 - val_loss: 1459.8795 - val_mse: 1459.8795 - val_mae: 25.4288\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3355.9889 - mse: 3355.9893 - mae: 32.6426 - val_loss: 1459.7241 - val_mse: 1459.7241 - val_mae: 25.5691\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3286.4870 - mse: 3286.4875 - mae: 32.3352 - val_loss: 1458.9823 - val_mse: 1458.9823 - val_mae: 25.6153\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3304.0516 - mse: 3304.0513 - mae: 31.3255 - val_loss: 1459.0349 - val_mse: 1459.0349 - val_mae: 25.8410\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3418.8727 - mse: 3418.8723 - mae: 33.5381 - val_loss: 1467.0404 - val_mse: 1467.0403 - val_mae: 25.0195\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3223.5143 - mse: 3223.5142 - mae: 31.4375 - val_loss: 1462.2271 - val_mse: 1462.2273 - val_mae: 25.3221\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 671us/step - loss: 3235.3222 - mse: 3235.3225 - mae: 32.3847 - val_loss: 1460.1184 - val_mse: 1460.1187 - val_mae: 25.6031\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 662us/step - loss: 3302.0299 - mse: 3302.0298 - mae: 32.8563 - val_loss: 1465.0725 - val_mse: 1465.0724 - val_mae: 25.0446\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3333.6708 - mse: 3333.6702 - mae: 32.4153 - val_loss: 1463.4365 - val_mse: 1463.4366 - val_mae: 25.2387\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 481us/step - loss: 3281.9385 - mse: 3281.9382 - mae: 31.9924 - val_loss: 1465.9972 - val_mse: 1465.9972 - val_mae: 25.0571\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3301.5739 - mse: 3301.5750 - mae: 32.0241 - val_loss: 1465.9108 - val_mse: 1465.9108 - val_mae: 25.1454\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3344.5989 - mse: 3344.5986 - mae: 32.3407 - val_loss: 1467.2024 - val_mse: 1467.2024 - val_mae: 25.0621\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3249.0812 - mse: 3249.0813 - mae: 31.6738 - val_loss: 1463.1426 - val_mse: 1463.1428 - val_mae: 25.3954\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 657us/step - loss: 3321.6822 - mse: 3321.6819 - mae: 32.3349 - val_loss: 1467.3016 - val_mse: 1467.3016 - val_mae: 25.0843\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 536us/step - loss: 3440.6643 - mse: 3440.6636 - mae: 33.1595 - val_loss: 1462.8806 - val_mse: 1462.8807 - val_mae: 25.4065\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3284.1584 - mse: 3284.1582 - mae: 32.5289 - val_loss: 1468.0729 - val_mse: 1468.0729 - val_mae: 25.0270\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3275.5062 - mse: 3275.5063 - mae: 31.8762 - val_loss: 1463.5466 - val_mse: 1463.5465 - val_mae: 25.1718\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3313.4152 - mse: 3313.4150 - mae: 32.7151 - val_loss: 1468.1137 - val_mse: 1468.1138 - val_mae: 24.8930\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3273.9195 - mse: 3273.9194 - mae: 31.7398 - val_loss: 1463.9329 - val_mse: 1463.9331 - val_mae: 25.1179\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3200.8393 - mse: 3200.8394 - mae: 30.8636 - val_loss: 1465.5868 - val_mse: 1465.5868 - val_mae: 24.9748\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 532us/step - loss: 3383.5071 - mse: 3383.5066 - mae: 32.2332 - val_loss: 1465.6999 - val_mse: 1465.7001 - val_mae: 25.0636\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3310.1044 - mse: 3310.1047 - mae: 32.0052 - val_loss: 1469.4972 - val_mse: 1469.4972 - val_mae: 24.8752\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3334.2400 - mse: 3334.2402 - mae: 31.9168 - val_loss: 1463.1879 - val_mse: 1463.1879 - val_mae: 25.2599\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3212.1049 - mse: 3212.1042 - mae: 31.3383 - val_loss: 1464.7205 - val_mse: 1464.7205 - val_mae: 25.1778\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3249.5355 - mse: 3249.5352 - mae: 31.6523 - val_loss: 1463.8165 - val_mse: 1463.8163 - val_mae: 25.3478\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3365.1601 - mse: 3365.1602 - mae: 32.3822 - val_loss: 1462.6710 - val_mse: 1462.6709 - val_mae: 25.7167\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3118.9876 - mse: 3118.9871 - mae: 31.2616 - val_loss: 1464.0571 - val_mse: 1464.0570 - val_mae: 25.4566\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3299.1073 - mse: 3299.1069 - mae: 32.0395 - val_loss: 1467.3035 - val_mse: 1467.3036 - val_mae: 25.1796\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3356.3339 - mse: 3356.3342 - mae: 32.1518 - val_loss: 1466.3808 - val_mse: 1466.3807 - val_mae: 25.2698\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3359.2324 - mse: 3359.2332 - mae: 32.2314 - val_loss: 1467.9271 - val_mse: 1467.9270 - val_mae: 25.2351\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3309.6789 - mse: 3309.6792 - mae: 31.8853 - val_loss: 1467.9500 - val_mse: 1467.9501 - val_mae: 25.1475\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3256.0548 - mse: 3256.0544 - mae: 31.5673 - val_loss: 1465.9102 - val_mse: 1465.9103 - val_mae: 25.2808\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 651us/step - loss: 3296.6247 - mse: 3296.6240 - mae: 32.2126 - val_loss: 1464.3904 - val_mse: 1464.3904 - val_mae: 25.5609\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3364.1984 - mse: 3364.1987 - mae: 32.1969 - val_loss: 1466.8803 - val_mse: 1466.8801 - val_mae: 25.2987\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 697us/step - loss: 3241.2060 - mse: 3241.2063 - mae: 31.7910 - val_loss: 1466.6559 - val_mse: 1466.6558 - val_mae: 25.3580\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3123.9664 - mse: 3123.9666 - mae: 30.6954 - val_loss: 1464.2441 - val_mse: 1464.2441 - val_mae: 25.5011\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3274.8811 - mse: 3274.8806 - mae: 31.7116 - val_loss: 1468.2410 - val_mse: 1468.2410 - val_mae: 25.2159\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3201.3889 - mse: 3201.3889 - mae: 31.3586 - val_loss: 1469.2900 - val_mse: 1469.2902 - val_mae: 25.1351\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 533us/step - loss: 3248.7795 - mse: 3248.7795 - mae: 31.6038 - val_loss: 1468.9730 - val_mse: 1468.9731 - val_mae: 25.1854\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3255.3719 - mse: 3255.3723 - mae: 31.2195 - val_loss: 1468.9779 - val_mse: 1468.9778 - val_mae: 25.1839\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3253.4118 - mse: 3253.4126 - mae: 32.4964 - val_loss: 1465.4751 - val_mse: 1465.4751 - val_mae: 25.4110\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3260.2916 - mse: 3260.2910 - mae: 31.5153 - val_loss: 1465.5795 - val_mse: 1465.5792 - val_mae: 25.3459\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3263.1140 - mse: 3263.1140 - mae: 31.9926 - val_loss: 1466.3833 - val_mse: 1466.3832 - val_mae: 25.3081\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3233.6541 - mse: 3233.6543 - mae: 31.6118 - val_loss: 1466.3263 - val_mse: 1466.3262 - val_mae: 25.3904\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3336.8143 - mse: 3336.8145 - mae: 32.1951 - val_loss: 1483.4730 - val_mse: 1483.4729 - val_mae: 24.5843\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3219.0825 - mse: 3219.0815 - mae: 31.2884 - val_loss: 1464.1660 - val_mse: 1464.1659 - val_mae: 25.5012\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3128.1169 - mse: 3128.1174 - mae: 31.6390 - val_loss: 1465.0555 - val_mse: 1465.0555 - val_mae: 25.3966\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3213.2961 - mse: 3213.2959 - mae: 31.4392 - val_loss: 1465.9008 - val_mse: 1465.9008 - val_mae: 25.3575\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 684us/step - loss: 3238.4199 - mse: 3238.4204 - mae: 32.1383 - val_loss: 1463.9153 - val_mse: 1463.9153 - val_mae: 25.5061\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3270.5504 - mse: 3270.5500 - mae: 32.0260 - val_loss: 1470.5031 - val_mse: 1470.5032 - val_mae: 25.0694\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 675us/step - loss: 3283.0618 - mse: 3283.0623 - mae: 31.6891 - val_loss: 1465.6532 - val_mse: 1465.6532 - val_mae: 25.3382\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2900.7453 - mse: 2900.7458 - mae: 31.3365 - val_loss: 1099.8246 - val_mse: 1099.8247 - val_mae: 24.0568\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2953.2438 - mse: 2953.2441 - mae: 30.8578 - val_loss: 1096.3992 - val_mse: 1096.3993 - val_mae: 24.0092\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2887.0440 - mse: 2887.0437 - mae: 30.8922 - val_loss: 1091.8232 - val_mse: 1091.8232 - val_mae: 24.0924\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2919.9734 - mse: 2919.9734 - mae: 31.4668 - val_loss: 1085.9765 - val_mse: 1085.9764 - val_mae: 24.2016\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2969.6015 - mse: 2969.6001 - mae: 30.8791 - val_loss: 1091.6207 - val_mse: 1091.6208 - val_mae: 23.7101\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2823.9426 - mse: 2823.9424 - mae: 30.8492 - val_loss: 1086.4495 - val_mse: 1086.4493 - val_mae: 23.7692\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 550us/step - loss: 2922.8985 - mse: 2922.8992 - mae: 31.0547 - val_loss: 1086.1013 - val_mse: 1086.1012 - val_mae: 23.6815\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2963.5602 - mse: 2963.5610 - mae: 31.3337 - val_loss: 1086.9516 - val_mse: 1086.9517 - val_mae: 23.5868\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2866.8182 - mse: 2866.8176 - mae: 30.3743 - val_loss: 1084.0532 - val_mse: 1084.0532 - val_mae: 23.7671\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2978.2689 - mse: 2978.2693 - mae: 31.4805 - val_loss: 1090.0737 - val_mse: 1090.0736 - val_mae: 23.5065\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2823.1491 - mse: 2823.1494 - mae: 30.7788 - val_loss: 1083.1839 - val_mse: 1083.1838 - val_mae: 23.8708\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2881.8705 - mse: 2881.8696 - mae: 30.8705 - val_loss: 1081.8301 - val_mse: 1081.8302 - val_mae: 23.8804\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2970.7519 - mse: 2970.7524 - mae: 31.6625 - val_loss: 1088.8663 - val_mse: 1088.8662 - val_mae: 23.4481\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2955.6135 - mse: 2955.6133 - mae: 31.1124 - val_loss: 1079.7145 - val_mse: 1079.7145 - val_mae: 23.7930\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 532us/step - loss: 2918.4124 - mse: 2918.4128 - mae: 30.9186 - val_loss: 1078.5616 - val_mse: 1078.5615 - val_mae: 23.7568\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2946.5820 - mse: 2946.5820 - mae: 31.0993 - val_loss: 1076.2543 - val_mse: 1076.2543 - val_mae: 23.7003\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 686us/step - loss: 2915.5591 - mse: 2915.5598 - mae: 31.3706 - val_loss: 1076.3504 - val_mse: 1076.3505 - val_mae: 23.6210\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 720us/step - loss: 2855.5147 - mse: 2855.5146 - mae: 30.7768 - val_loss: 1075.9566 - val_mse: 1075.9567 - val_mae: 23.6459\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2873.9717 - mse: 2873.9722 - mae: 30.6832 - val_loss: 1073.0642 - val_mse: 1073.0641 - val_mae: 23.7776\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2801.3330 - mse: 2801.3335 - mae: 30.2615 - val_loss: 1072.3973 - val_mse: 1072.3975 - val_mae: 23.9596\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 532us/step - loss: 2905.0554 - mse: 2905.0554 - mae: 31.4953 - val_loss: 1072.9743 - val_mse: 1072.9742 - val_mae: 23.8189\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 694us/step - loss: 2911.3209 - mse: 2911.3210 - mae: 31.1712 - val_loss: 1070.6239 - val_mse: 1070.6239 - val_mae: 24.0005\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2901.5395 - mse: 2901.5398 - mae: 31.1108 - val_loss: 1072.6100 - val_mse: 1072.6100 - val_mae: 23.7421\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2946.3449 - mse: 2946.3450 - mae: 31.2301 - val_loss: 1072.9123 - val_mse: 1072.9124 - val_mae: 23.7060\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2902.8775 - mse: 2902.8774 - mae: 30.7669 - val_loss: 1067.2447 - val_mse: 1067.2446 - val_mae: 24.2209\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2893.9773 - mse: 2893.9771 - mae: 30.9980 - val_loss: 1072.2558 - val_mse: 1072.2560 - val_mae: 23.6133\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2920.0232 - mse: 2920.0232 - mae: 31.6880 - val_loss: 1068.0035 - val_mse: 1068.0034 - val_mae: 23.8488\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2839.0865 - mse: 2839.0867 - mae: 31.1105 - val_loss: 1066.2871 - val_mse: 1066.2871 - val_mae: 24.0699\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 686us/step - loss: 2899.8769 - mse: 2899.8770 - mae: 31.1739 - val_loss: 1068.3415 - val_mse: 1068.3414 - val_mae: 23.6672\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2983.0716 - mse: 2983.0720 - mae: 31.1228 - val_loss: 1068.1059 - val_mse: 1068.1058 - val_mae: 23.6696\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2896.7335 - mse: 2896.7339 - mae: 30.6396 - val_loss: 1066.5894 - val_mse: 1066.5894 - val_mae: 23.7667\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2858.7842 - mse: 2858.7847 - mae: 31.1177 - val_loss: 1065.0289 - val_mse: 1065.0289 - val_mae: 23.8537\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2908.9287 - mse: 2908.9285 - mae: 31.1708 - val_loss: 1068.5381 - val_mse: 1068.5380 - val_mae: 23.6474\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2883.2178 - mse: 2883.2170 - mae: 30.7988 - val_loss: 1069.8383 - val_mse: 1069.8381 - val_mae: 23.6155\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2850.3295 - mse: 2850.3293 - mae: 30.5245 - val_loss: 1064.6251 - val_mse: 1064.6250 - val_mae: 23.9503\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2823.5016 - mse: 2823.5015 - mae: 30.5229 - val_loss: 1063.9073 - val_mse: 1063.9073 - val_mae: 24.0719\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2811.5981 - mse: 2811.5979 - mae: 30.4550 - val_loss: 1066.6704 - val_mse: 1066.6703 - val_mae: 23.6771\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2882.4960 - mse: 2882.4971 - mae: 30.9603 - val_loss: 1065.9036 - val_mse: 1065.9036 - val_mae: 23.7479\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2816.7302 - mse: 2816.7290 - mae: 30.0296 - val_loss: 1061.7503 - val_mse: 1061.7502 - val_mae: 24.0227\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2831.5799 - mse: 2831.5803 - mae: 30.8052 - val_loss: 1066.4987 - val_mse: 1066.4987 - val_mae: 23.6596\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 706us/step - loss: 2933.9688 - mse: 2933.9683 - mae: 31.1695 - val_loss: 1062.6708 - val_mse: 1062.6708 - val_mae: 24.0098\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 699us/step - loss: 2860.1295 - mse: 2860.1299 - mae: 30.5951 - val_loss: 1065.2636 - val_mse: 1065.2635 - val_mae: 23.6695\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2820.7407 - mse: 2820.7417 - mae: 30.4517 - val_loss: 1065.2740 - val_mse: 1065.2739 - val_mae: 23.7232\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2911.7502 - mse: 2911.7502 - mae: 31.0174 - val_loss: 1062.7178 - val_mse: 1062.7178 - val_mae: 23.8294\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2948.8293 - mse: 2948.8301 - mae: 31.2700 - val_loss: 1073.3495 - val_mse: 1073.3495 - val_mae: 23.1943\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2844.7071 - mse: 2844.7063 - mae: 30.0394 - val_loss: 1063.7601 - val_mse: 1063.7601 - val_mae: 23.5030\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2941.0285 - mse: 2941.0288 - mae: 31.2515 - val_loss: 1059.8284 - val_mse: 1059.8285 - val_mae: 23.6954\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2854.6328 - mse: 2854.6326 - mae: 30.8680 - val_loss: 1062.2012 - val_mse: 1062.2013 - val_mae: 23.4998\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2791.9258 - mse: 2791.9265 - mae: 29.8883 - val_loss: 1062.8066 - val_mse: 1062.8066 - val_mae: 23.5520\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2904.0461 - mse: 2904.0464 - mae: 30.9353 - val_loss: 1062.7028 - val_mse: 1062.7028 - val_mae: 23.4956\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2808.7946 - mse: 2808.7939 - mae: 30.0879 - val_loss: 1058.0032 - val_mse: 1058.0033 - val_mae: 23.9558\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2817.0334 - mse: 2817.0347 - mae: 30.7435 - val_loss: 1056.7427 - val_mse: 1056.7426 - val_mae: 23.9598\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2858.5671 - mse: 2858.5676 - mae: 30.5050 - val_loss: 1056.2184 - val_mse: 1056.2184 - val_mae: 23.8819\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2820.3247 - mse: 2820.3245 - mae: 30.5517 - val_loss: 1057.8846 - val_mse: 1057.8846 - val_mae: 23.6887\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2872.3474 - mse: 2872.3474 - mae: 30.1525 - val_loss: 1058.7260 - val_mse: 1058.7262 - val_mae: 23.6261\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2822.0790 - mse: 2822.0784 - mae: 30.6092 - val_loss: 1058.1383 - val_mse: 1058.1383 - val_mae: 23.6243\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2784.9280 - mse: 2784.9285 - mae: 30.0163 - val_loss: 1057.8492 - val_mse: 1057.8494 - val_mae: 23.7617\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 668us/step - loss: 2786.7519 - mse: 2786.7520 - mae: 30.5754 - val_loss: 1059.5153 - val_mse: 1059.5154 - val_mae: 23.5869\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2838.1405 - mse: 2838.1416 - mae: 30.0169 - val_loss: 1059.3758 - val_mse: 1059.3760 - val_mae: 23.5892\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 527us/step - loss: 2820.1251 - mse: 2820.1255 - mae: 30.7308 - val_loss: 1057.5397 - val_mse: 1057.5397 - val_mae: 23.6897\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 676us/step - loss: 2768.8862 - mse: 2768.8872 - mae: 30.7786 - val_loss: 1056.9134 - val_mse: 1056.9133 - val_mae: 23.6534\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2879.3871 - mse: 2879.3877 - mae: 30.5245 - val_loss: 1057.5241 - val_mse: 1057.5242 - val_mae: 23.5908\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2778.7558 - mse: 2778.7566 - mae: 29.8109 - val_loss: 1055.9343 - val_mse: 1055.9342 - val_mae: 23.6890\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2825.1929 - mse: 2825.1929 - mae: 30.4981 - val_loss: 1058.1645 - val_mse: 1058.1643 - val_mae: 23.3481\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2785.6094 - mse: 2785.6094 - mae: 30.2452 - val_loss: 1051.6102 - val_mse: 1051.6102 - val_mae: 23.7496\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 2s 890us/step - loss: 2815.2972 - mse: 2815.2971 - mae: 30.3236 - val_loss: 1050.1991 - val_mse: 1050.1990 - val_mae: 23.7245\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2818.9433 - mse: 2818.9436 - mae: 30.2393 - val_loss: 1053.5234 - val_mse: 1053.5234 - val_mae: 23.3969\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2786.4238 - mse: 2786.4243 - mae: 30.1830 - val_loss: 1052.2689 - val_mse: 1052.2689 - val_mae: 23.3650\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2924.2146 - mse: 2924.2151 - mae: 31.1387 - val_loss: 1051.9720 - val_mse: 1051.9720 - val_mae: 23.4479\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 664us/step - loss: 2828.6054 - mse: 2828.6050 - mae: 30.2130 - val_loss: 1050.2341 - val_mse: 1050.2340 - val_mae: 23.5071\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2873.9011 - mse: 2873.9014 - mae: 30.1987 - val_loss: 1048.5812 - val_mse: 1048.5811 - val_mae: 23.5410\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2798.6833 - mse: 2798.6836 - mae: 30.5192 - val_loss: 1048.9029 - val_mse: 1048.9030 - val_mae: 23.5528\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2848.8689 - mse: 2848.8682 - mae: 30.5603 - val_loss: 1046.2168 - val_mse: 1046.2168 - val_mae: 23.6562\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2834.9833 - mse: 2834.9832 - mae: 30.5427 - val_loss: 1044.4526 - val_mse: 1044.4526 - val_mae: 23.8207\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 685us/step - loss: 2839.7833 - mse: 2839.7830 - mae: 30.1791 - val_loss: 1046.2325 - val_mse: 1046.2325 - val_mae: 23.4589\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2860.5193 - mse: 2860.5195 - mae: 30.4902 - val_loss: 1050.9911 - val_mse: 1050.9911 - val_mae: 23.2204\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 672us/step - loss: 2883.1452 - mse: 2883.1443 - mae: 30.3125 - val_loss: 1047.9523 - val_mse: 1047.9524 - val_mae: 23.4511\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2868.2193 - mse: 2868.2195 - mae: 30.2737 - val_loss: 1053.7380 - val_mse: 1053.7379 - val_mae: 23.1430\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2805.7231 - mse: 2805.7246 - mae: 30.2517 - val_loss: 1051.8242 - val_mse: 1051.8242 - val_mae: 23.1840\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2829.1007 - mse: 2829.1003 - mae: 30.8651 - val_loss: 1048.1977 - val_mse: 1048.1976 - val_mae: 23.3929\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 685us/step - loss: 2497.3802 - mse: 2497.3799 - mae: 29.5625 - val_loss: 1496.1160 - val_mse: 1496.1158 - val_mae: 27.0279\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2608.9245 - mse: 2608.9255 - mae: 30.1279 - val_loss: 1514.3591 - val_mse: 1514.3591 - val_mae: 26.5575\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2542.4014 - mse: 2542.4009 - mae: 29.7804 - val_loss: 1498.9191 - val_mse: 1498.9191 - val_mae: 26.8546\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 649us/step - loss: 2593.1925 - mse: 2593.1912 - mae: 29.6966 - val_loss: 1502.5825 - val_mse: 1502.5824 - val_mae: 26.7319\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2528.0191 - mse: 2528.0190 - mae: 29.5402 - val_loss: 1490.3345 - val_mse: 1490.3346 - val_mae: 27.0096\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2619.0613 - mse: 2619.0610 - mae: 30.2511 - val_loss: 1492.7885 - val_mse: 1492.7886 - val_mae: 26.8764\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2531.0477 - mse: 2531.0481 - mae: 29.8585 - val_loss: 1501.1125 - val_mse: 1501.1128 - val_mae: 26.6448\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2562.7365 - mse: 2562.7354 - mae: 30.4066 - val_loss: 1509.8798 - val_mse: 1509.8798 - val_mae: 26.4685\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2548.8024 - mse: 2548.8020 - mae: 29.4701 - val_loss: 1494.1254 - val_mse: 1494.1254 - val_mae: 26.7554\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2485.6808 - mse: 2485.6816 - mae: 29.1973 - val_loss: 1486.7922 - val_mse: 1486.7922 - val_mae: 26.8869\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2539.8508 - mse: 2539.8501 - mae: 29.7193 - val_loss: 1486.4407 - val_mse: 1486.4407 - val_mae: 26.8305\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 669us/step - loss: 2518.7082 - mse: 2518.7083 - mae: 29.6431 - val_loss: 1484.4696 - val_mse: 1484.4696 - val_mae: 26.8409\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 2s 659us/step - loss: 2540.1431 - mse: 2540.1431 - mae: 29.4129 - val_loss: 1493.4560 - val_mse: 1493.4562 - val_mae: 26.5399\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2577.0045 - mse: 2577.0044 - mae: 29.9048 - val_loss: 1487.3357 - val_mse: 1487.3358 - val_mae: 26.5820\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2552.4104 - mse: 2552.4102 - mae: 29.8357 - val_loss: 1475.8412 - val_mse: 1475.8412 - val_mae: 26.8649\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2542.4818 - mse: 2542.4817 - mae: 29.3816 - val_loss: 1486.9815 - val_mse: 1486.9814 - val_mae: 26.5669\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2569.8595 - mse: 2569.8586 - mae: 29.8165 - val_loss: 1489.2649 - val_mse: 1489.2649 - val_mae: 26.4869\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2461.9942 - mse: 2461.9951 - mae: 29.7658 - val_loss: 1469.3720 - val_mse: 1469.3719 - val_mae: 26.9459\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 721us/step - loss: 2529.4872 - mse: 2529.4880 - mae: 29.4827 - val_loss: 1483.5314 - val_mse: 1483.5315 - val_mae: 26.5184\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2474.3734 - mse: 2474.3745 - mae: 28.9167 - val_loss: 1473.3712 - val_mse: 1473.3711 - val_mae: 26.7097\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2523.3920 - mse: 2523.3923 - mae: 29.6327 - val_loss: 1465.5428 - val_mse: 1465.5430 - val_mae: 26.8247\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2489.3611 - mse: 2489.3611 - mae: 29.6526 - val_loss: 1467.4838 - val_mse: 1467.4838 - val_mae: 26.6686\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2531.5479 - mse: 2531.5474 - mae: 29.8480 - val_loss: 1470.9181 - val_mse: 1470.9181 - val_mae: 26.5383\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2545.4952 - mse: 2545.4958 - mae: 29.7991 - val_loss: 1459.8660 - val_mse: 1459.8662 - val_mae: 26.7408\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2605.2909 - mse: 2605.2913 - mae: 29.7963 - val_loss: 1470.1283 - val_mse: 1470.1285 - val_mae: 26.4428\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2580.8757 - mse: 2580.8757 - mae: 29.9589 - val_loss: 1456.5409 - val_mse: 1456.5408 - val_mae: 26.8717\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2496.5412 - mse: 2496.5405 - mae: 29.6769 - val_loss: 1469.6070 - val_mse: 1469.6071 - val_mae: 26.4610\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2536.5640 - mse: 2536.5632 - mae: 29.7116 - val_loss: 1461.8242 - val_mse: 1461.8241 - val_mae: 26.5912\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2445.9435 - mse: 2445.9438 - mae: 28.9827 - val_loss: 1460.0278 - val_mse: 1460.0278 - val_mae: 26.5796\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 683us/step - loss: 2491.7712 - mse: 2491.7710 - mae: 29.5377 - val_loss: 1444.6769 - val_mse: 1444.6768 - val_mae: 26.9339\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 652us/step - loss: 2444.6019 - mse: 2444.6023 - mae: 29.0827 - val_loss: 1447.7466 - val_mse: 1447.7465 - val_mae: 26.7403\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2452.8837 - mse: 2452.8840 - mae: 29.2474 - val_loss: 1440.4456 - val_mse: 1440.4456 - val_mae: 27.0957\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2476.8608 - mse: 2476.8608 - mae: 29.1616 - val_loss: 1458.9356 - val_mse: 1458.9354 - val_mae: 26.4242\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2448.9164 - mse: 2448.9167 - mae: 29.1624 - val_loss: 1442.0119 - val_mse: 1442.0117 - val_mae: 26.9777\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2469.0870 - mse: 2469.0869 - mae: 29.8961 - val_loss: 1449.1709 - val_mse: 1449.1709 - val_mae: 26.7695\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 673us/step - loss: 2498.6926 - mse: 2498.6926 - mae: 29.1390 - val_loss: 1446.2114 - val_mse: 1446.2113 - val_mae: 26.8260\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2498.1731 - mse: 2498.1726 - mae: 29.2837 - val_loss: 1451.2617 - val_mse: 1451.2620 - val_mae: 26.5987\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2475.4407 - mse: 2475.4409 - mae: 29.2730 - val_loss: 1450.0916 - val_mse: 1450.0916 - val_mae: 26.6347\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 666us/step - loss: 2480.7055 - mse: 2480.7056 - mae: 29.4576 - val_loss: 1446.8898 - val_mse: 1446.8898 - val_mae: 26.7132\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2490.2496 - mse: 2490.2500 - mae: 29.3232 - val_loss: 1443.0238 - val_mse: 1443.0237 - val_mae: 26.8485\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 663us/step - loss: 2455.8458 - mse: 2455.8455 - mae: 29.4895 - val_loss: 1443.1553 - val_mse: 1443.1553 - val_mae: 26.8426\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2511.2777 - mse: 2511.2783 - mae: 29.2755 - val_loss: 1447.5889 - val_mse: 1447.5891 - val_mae: 26.6211\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2588.5207 - mse: 2588.5205 - mae: 29.7440 - val_loss: 1441.9890 - val_mse: 1441.9891 - val_mae: 26.8668\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2529.3215 - mse: 2529.3213 - mae: 28.9586 - val_loss: 1437.2945 - val_mse: 1437.2944 - val_mae: 26.9492\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2506.6519 - mse: 2506.6521 - mae: 29.1874 - val_loss: 1435.4653 - val_mse: 1435.4653 - val_mae: 26.9658\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2531.4031 - mse: 2531.4031 - mae: 29.1350 - val_loss: 1436.6346 - val_mse: 1436.6348 - val_mae: 26.8767\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2520.4764 - mse: 2520.4768 - mae: 29.5242 - val_loss: 1449.6441 - val_mse: 1449.6442 - val_mae: 26.3736\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2494.2677 - mse: 2494.2681 - mae: 28.8535 - val_loss: 1436.9047 - val_mse: 1436.9045 - val_mae: 26.7028\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2490.0033 - mse: 2490.0037 - mae: 29.5203 - val_loss: 1437.3120 - val_mse: 1437.3120 - val_mae: 26.6132\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2473.0711 - mse: 2473.0708 - mae: 29.4130 - val_loss: 1438.1865 - val_mse: 1438.1866 - val_mae: 26.5137\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2451.6424 - mse: 2451.6431 - mae: 29.0985 - val_loss: 1431.6877 - val_mse: 1431.6879 - val_mae: 26.7435\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2557.9625 - mse: 2557.9631 - mae: 29.6638 - val_loss: 1430.4485 - val_mse: 1430.4485 - val_mae: 26.7532\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2563.6916 - mse: 2563.6917 - mae: 29.2813 - val_loss: 1428.6085 - val_mse: 1428.6085 - val_mae: 26.7405\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 671us/step - loss: 2444.9697 - mse: 2444.9700 - mae: 29.2530 - val_loss: 1429.8078 - val_mse: 1429.8079 - val_mae: 26.7032\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 687us/step - loss: 2454.7529 - mse: 2454.7534 - mae: 29.3635 - val_loss: 1434.5068 - val_mse: 1434.5067 - val_mae: 26.4805\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2498.1239 - mse: 2498.1250 - mae: 28.8298 - val_loss: 1421.2610 - val_mse: 1421.2610 - val_mae: 27.0552\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 685us/step - loss: 2547.6724 - mse: 2547.6716 - mae: 29.3010 - val_loss: 1431.1854 - val_mse: 1431.1853 - val_mae: 26.5790\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2518.7380 - mse: 2518.7383 - mae: 29.1917 - val_loss: 1429.7031 - val_mse: 1429.7031 - val_mae: 26.6416\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 664us/step - loss: 2451.8404 - mse: 2451.8408 - mae: 28.8078 - val_loss: 1434.3702 - val_mse: 1434.3704 - val_mae: 26.4982\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2501.8180 - mse: 2501.8184 - mae: 29.5426 - val_loss: 1431.8180 - val_mse: 1431.8181 - val_mae: 26.6026\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 686us/step - loss: 2441.8640 - mse: 2441.8647 - mae: 28.9447 - val_loss: 1431.2292 - val_mse: 1431.2292 - val_mae: 26.5572\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 657us/step - loss: 2458.5973 - mse: 2458.5972 - mae: 28.9576 - val_loss: 1432.2332 - val_mse: 1432.2332 - val_mae: 26.5694\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2431.2861 - mse: 2431.2866 - mae: 29.0141 - val_loss: 1422.3396 - val_mse: 1422.3395 - val_mae: 27.0005\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2450.3262 - mse: 2450.3271 - mae: 28.7352 - val_loss: 1433.8747 - val_mse: 1433.8748 - val_mae: 26.5252\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2503.8907 - mse: 2503.8906 - mae: 29.0348 - val_loss: 1426.6421 - val_mse: 1426.6421 - val_mae: 26.7521\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2477.2732 - mse: 2477.2725 - mae: 28.9431 - val_loss: 1426.7669 - val_mse: 1426.7668 - val_mae: 26.7189\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2496.5226 - mse: 2496.5220 - mae: 29.1295 - val_loss: 1432.0664 - val_mse: 1432.0663 - val_mae: 26.5857\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2470.9006 - mse: 2470.9011 - mae: 29.0687 - val_loss: 1428.6228 - val_mse: 1428.6228 - val_mae: 26.6162\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2492.3239 - mse: 2492.3247 - mae: 29.0704 - val_loss: 1419.7609 - val_mse: 1419.7607 - val_mae: 27.3349\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2442.4885 - mse: 2442.4893 - mae: 29.0488 - val_loss: 1419.9652 - val_mse: 1419.9652 - val_mae: 27.3456\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2508.2936 - mse: 2508.2942 - mae: 29.5505 - val_loss: 1430.3756 - val_mse: 1430.3754 - val_mae: 26.6751\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2430.3537 - mse: 2430.3528 - mae: 28.5567 - val_loss: 1422.1704 - val_mse: 1422.1707 - val_mae: 27.1367\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 566us/step - loss: 2482.4699 - mse: 2482.4690 - mae: 29.1328 - val_loss: 1426.5771 - val_mse: 1426.5770 - val_mae: 26.8816\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 543us/step - loss: 2519.5136 - mse: 2519.5134 - mae: 29.4388 - val_loss: 1424.9244 - val_mse: 1424.9243 - val_mae: 26.9101\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2461.8025 - mse: 2461.8025 - mae: 29.0889 - val_loss: 1422.4244 - val_mse: 1422.4243 - val_mae: 27.0223\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 518us/step - loss: 2376.9219 - mse: 2376.9216 - mae: 29.0360 - val_loss: 1428.9833 - val_mse: 1428.9832 - val_mae: 26.7030\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2509.4420 - mse: 2509.4417 - mae: 29.8495 - val_loss: 1434.7117 - val_mse: 1434.7114 - val_mae: 26.4542\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2439.2100 - mse: 2439.2100 - mae: 28.8116 - val_loss: 1425.9390 - val_mse: 1425.9390 - val_mae: 27.0546\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2450.0507 - mse: 2450.0508 - mae: 28.8098 - val_loss: 1426.8742 - val_mse: 1426.8743 - val_mae: 26.9191\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2456.0426 - mse: 2456.0422 - mae: 28.9469 - val_loss: 1423.6868 - val_mse: 1423.6869 - val_mae: 27.1369\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 564us/step - loss: 2348.6435 - mse: 2348.6436 - mae: 29.2648 - val_loss: 3648.1556 - val_mse: 3648.1567 - val_mae: 24.1594\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2337.3693 - mse: 2337.3708 - mae: 29.2891 - val_loss: 3647.9134 - val_mse: 3647.9131 - val_mae: 24.1278\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2316.8036 - mse: 2316.8049 - mae: 29.1628 - val_loss: 3647.2270 - val_mse: 3647.2268 - val_mae: 23.5463\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2426.8382 - mse: 2426.8386 - mae: 29.7581 - val_loss: 3648.0052 - val_mse: 3648.0046 - val_mae: 23.6855\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2341.6618 - mse: 2341.6614 - mae: 29.1939 - val_loss: 3648.9438 - val_mse: 3648.9431 - val_mae: 23.9609\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2363.7863 - mse: 2363.7871 - mae: 29.2320 - val_loss: 3649.0829 - val_mse: 3649.0830 - val_mae: 24.0282\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2317.4551 - mse: 2317.4551 - mae: 29.2120 - val_loss: 3649.6886 - val_mse: 3649.6880 - val_mae: 23.8606\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 689us/step - loss: 2308.9505 - mse: 2308.9502 - mae: 29.1720 - val_loss: 3649.4395 - val_mse: 3649.4392 - val_mae: 23.8401\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2369.4583 - mse: 2369.4583 - mae: 29.2648 - val_loss: 3647.2080 - val_mse: 3647.2078 - val_mae: 23.6218\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2328.5286 - mse: 2328.5293 - mae: 29.2135 - val_loss: 3647.2888 - val_mse: 3647.2891 - val_mae: 23.0674\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 637us/step - loss: 2365.1583 - mse: 2365.1577 - mae: 29.6696 - val_loss: 3647.2589 - val_mse: 3647.2588 - val_mae: 22.9247\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2394.4738 - mse: 2394.4736 - mae: 29.3828 - val_loss: 3646.3819 - val_mse: 3646.3821 - val_mae: 23.7410\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2328.0186 - mse: 2328.0186 - mae: 29.0861 - val_loss: 3646.2963 - val_mse: 3646.2949 - val_mae: 23.3211\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2292.1384 - mse: 2292.1387 - mae: 29.0863 - val_loss: 3648.3410 - val_mse: 3648.3406 - val_mae: 24.0644\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2377.1956 - mse: 2377.1956 - mae: 29.2766 - val_loss: 3646.5574 - val_mse: 3646.5569 - val_mae: 23.4724\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2352.1298 - mse: 2352.1289 - mae: 29.0490 - val_loss: 3647.2406 - val_mse: 3647.2410 - val_mae: 23.8193\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 720us/step - loss: 2331.2718 - mse: 2331.2712 - mae: 29.1348 - val_loss: 3646.7088 - val_mse: 3646.7095 - val_mae: 23.1409\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2374.7378 - mse: 2374.7375 - mae: 29.3513 - val_loss: 3647.3427 - val_mse: 3647.3430 - val_mae: 23.9579\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2288.7594 - mse: 2288.7598 - mae: 29.0524 - val_loss: 3647.5667 - val_mse: 3647.5667 - val_mae: 23.9825\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2301.2944 - mse: 2301.2944 - mae: 29.0372 - val_loss: 3646.9307 - val_mse: 3646.9307 - val_mae: 23.6493\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2300.7501 - mse: 2300.7498 - mae: 29.1796 - val_loss: 3650.3399 - val_mse: 3650.3386 - val_mae: 24.3431\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2368.4502 - mse: 2368.4502 - mae: 29.5089 - val_loss: 3644.7452 - val_mse: 3644.7466 - val_mae: 23.1699\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2314.8608 - mse: 2314.8608 - mae: 28.9316 - val_loss: 3647.1852 - val_mse: 3647.1848 - val_mae: 23.8186\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2299.9375 - mse: 2299.9382 - mae: 29.1109 - val_loss: 3647.4519 - val_mse: 3647.4519 - val_mae: 23.8984\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2326.9105 - mse: 2326.9104 - mae: 28.8183 - val_loss: 3645.0603 - val_mse: 3645.0596 - val_mae: 23.4242\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2315.8025 - mse: 2315.8020 - mae: 29.0758 - val_loss: 3646.4769 - val_mse: 3646.4773 - val_mae: 23.9219\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2329.7634 - mse: 2329.7637 - mae: 29.1479 - val_loss: 3645.2473 - val_mse: 3645.2473 - val_mae: 23.3899\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2337.5832 - mse: 2337.5823 - mae: 29.0346 - val_loss: 3643.9274 - val_mse: 3643.9268 - val_mae: 23.3629\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2301.2673 - mse: 2301.2676 - mae: 29.4046 - val_loss: 3645.8337 - val_mse: 3645.8342 - val_mae: 23.6187\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2330.1804 - mse: 2330.1799 - mae: 29.2789 - val_loss: 3646.5679 - val_mse: 3646.5676 - val_mae: 23.9091\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2354.2885 - mse: 2354.2876 - mae: 29.2096 - val_loss: 3645.9562 - val_mse: 3645.9561 - val_mae: 23.6217\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2336.7845 - mse: 2336.7854 - mae: 29.3596 - val_loss: 3647.5398 - val_mse: 3647.5398 - val_mae: 22.8230\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2318.3399 - mse: 2318.3401 - mae: 28.8664 - val_loss: 3648.6609 - val_mse: 3648.6604 - val_mae: 23.8926\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 680us/step - loss: 2327.6036 - mse: 2327.6038 - mae: 29.2006 - val_loss: 3647.7611 - val_mse: 3647.7610 - val_mae: 23.0398\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2287.8819 - mse: 2287.8818 - mae: 28.8851 - val_loss: 3650.0270 - val_mse: 3650.0271 - val_mae: 24.2457\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2399.3498 - mse: 2399.3496 - mae: 29.3004 - val_loss: 3647.3792 - val_mse: 3647.3799 - val_mae: 23.7864\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2351.8220 - mse: 2351.8220 - mae: 28.7077 - val_loss: 3648.9262 - val_mse: 3648.9270 - val_mae: 24.2571\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2357.3345 - mse: 2357.3342 - mae: 29.0374 - val_loss: 3647.8695 - val_mse: 3647.8699 - val_mae: 23.9955\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2254.7098 - mse: 2254.7109 - mae: 28.9086 - val_loss: 3649.4383 - val_mse: 3649.4387 - val_mae: 24.2238\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2294.9570 - mse: 2294.9568 - mae: 28.6729 - val_loss: 3646.3897 - val_mse: 3646.3899 - val_mae: 23.4650\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2357.5197 - mse: 2357.5203 - mae: 29.1320 - val_loss: 3647.1728 - val_mse: 3647.1731 - val_mae: 23.8415\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2296.6454 - mse: 2296.6460 - mae: 28.7104 - val_loss: 3646.4037 - val_mse: 3646.4033 - val_mae: 23.5886\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 687us/step - loss: 2330.3492 - mse: 2330.3496 - mae: 28.7884 - val_loss: 3646.1551 - val_mse: 3646.1555 - val_mae: 23.7217\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2323.0168 - mse: 2323.0168 - mae: 28.7314 - val_loss: 3647.3039 - val_mse: 3647.3044 - val_mae: 23.1879\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2309.6730 - mse: 2309.6726 - mae: 28.9029 - val_loss: 3649.2369 - val_mse: 3649.2371 - val_mae: 24.0469\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2278.4951 - mse: 2278.4954 - mae: 28.5863 - val_loss: 3648.2954 - val_mse: 3648.2961 - val_mae: 23.8889\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2342.5146 - mse: 2342.5144 - mae: 29.3956 - val_loss: 3646.7203 - val_mse: 3646.7205 - val_mae: 23.6192\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2267.5152 - mse: 2267.5146 - mae: 28.6463 - val_loss: 3650.8664 - val_mse: 3650.8665 - val_mae: 24.3113\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2327.5772 - mse: 2327.5767 - mae: 29.1565 - val_loss: 3651.0452 - val_mse: 3651.0449 - val_mae: 24.0212\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2385.9336 - mse: 2385.9341 - mae: 29.3121 - val_loss: 3651.6592 - val_mse: 3651.6589 - val_mae: 23.9922\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 634us/step - loss: 2316.8225 - mse: 2316.8220 - mae: 28.6721 - val_loss: 3649.1343 - val_mse: 3649.1353 - val_mae: 23.6487\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2335.2285 - mse: 2335.2292 - mae: 29.2957 - val_loss: 3648.6098 - val_mse: 3648.6099 - val_mae: 23.7438\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2339.1579 - mse: 2339.1582 - mae: 29.0734 - val_loss: 3649.3995 - val_mse: 3649.3997 - val_mae: 23.8461\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2312.5689 - mse: 2312.5688 - mae: 29.0081 - val_loss: 3649.6252 - val_mse: 3649.6252 - val_mae: 23.9975\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2306.2241 - mse: 2306.2249 - mae: 28.8596 - val_loss: 3649.3033 - val_mse: 3649.3040 - val_mae: 23.8678\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2335.8255 - mse: 2335.8252 - mae: 28.7996 - val_loss: 3648.0366 - val_mse: 3648.0361 - val_mae: 23.6716\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2286.7117 - mse: 2286.7119 - mae: 28.8863 - val_loss: 3646.6063 - val_mse: 3646.6062 - val_mae: 23.6801\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2275.2609 - mse: 2275.2615 - mae: 28.8334 - val_loss: 3648.9691 - val_mse: 3648.9685 - val_mae: 24.0525\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2293.6185 - mse: 2293.6189 - mae: 28.7190 - val_loss: 3651.4125 - val_mse: 3651.4124 - val_mae: 24.4142\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2334.5818 - mse: 2334.5820 - mae: 29.0954 - val_loss: 3649.2547 - val_mse: 3649.2546 - val_mae: 23.9814\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2323.7420 - mse: 2323.7422 - mae: 28.7996 - val_loss: 3646.4424 - val_mse: 3646.4431 - val_mae: 23.2728\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2311.9943 - mse: 2311.9946 - mae: 28.6834 - val_loss: 3646.4539 - val_mse: 3646.4539 - val_mae: 23.4655\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2340.3078 - mse: 2340.3071 - mae: 28.9223 - val_loss: 3645.9316 - val_mse: 3645.9321 - val_mae: 23.4725\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2369.9521 - mse: 2369.9521 - mae: 29.0290 - val_loss: 3647.8617 - val_mse: 3647.8608 - val_mae: 23.4110\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2362.9331 - mse: 2362.9338 - mae: 28.9047 - val_loss: 3648.0687 - val_mse: 3648.0693 - val_mae: 23.7218\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 656us/step - loss: 2264.2299 - mse: 2264.2305 - mae: 28.4879 - val_loss: 3649.8081 - val_mse: 3649.8079 - val_mae: 24.0593\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 678us/step - loss: 2276.4641 - mse: 2276.4644 - mae: 28.4041 - val_loss: 3647.2631 - val_mse: 3647.2637 - val_mae: 23.9989\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 687us/step - loss: 2286.2444 - mse: 2286.2439 - mae: 28.8654 - val_loss: 3647.5367 - val_mse: 3647.5371 - val_mae: 24.0385\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2335.5055 - mse: 2335.5051 - mae: 29.3456 - val_loss: 3647.6626 - val_mse: 3647.6631 - val_mae: 24.0048\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 685us/step - loss: 2310.7787 - mse: 2310.7781 - mae: 28.8188 - val_loss: 3648.4886 - val_mse: 3648.4883 - val_mae: 23.7551\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2341.0093 - mse: 2341.0088 - mae: 28.9859 - val_loss: 3649.4767 - val_mse: 3649.4773 - val_mae: 24.0774\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2339.6132 - mse: 2339.6133 - mae: 29.1955 - val_loss: 3647.7275 - val_mse: 3647.7273 - val_mae: 23.3490\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2298.9744 - mse: 2298.9749 - mae: 28.7589 - val_loss: 3651.8447 - val_mse: 3651.8445 - val_mae: 24.5985\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2313.9455 - mse: 2313.9453 - mae: 29.0673 - val_loss: 3651.0396 - val_mse: 3651.0396 - val_mae: 24.4874\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 527us/step - loss: 2278.5201 - mse: 2278.5212 - mae: 28.5458 - val_loss: 3652.1250 - val_mse: 3652.1243 - val_mae: 24.5701\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2305.8865 - mse: 2305.8870 - mae: 28.8883 - val_loss: 3648.2438 - val_mse: 3648.2437 - val_mae: 24.1154\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2304.9256 - mse: 2304.9260 - mae: 28.8850 - val_loss: 3646.8735 - val_mse: 3646.8735 - val_mae: 23.5594\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 671us/step - loss: 2308.1895 - mse: 2308.1897 - mae: 28.5759 - val_loss: 3648.8943 - val_mse: 3648.8943 - val_mae: 23.9975\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2292.2761 - mse: 2292.2761 - mae: 28.7563 - val_loss: 3648.7497 - val_mse: 3648.7493 - val_mae: 23.9227\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2324.0026 - mse: 2324.0024 - mae: 29.0778 - val_loss: 3654.1526 - val_mse: 3654.1526 - val_mae: 24.3540\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2697.0349 - mse: 2697.0349 - mae: 28.7616 - val_loss: 2049.6512 - val_mse: 2049.6511 - val_mae: 27.2027\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2715.7529 - mse: 2715.7524 - mae: 28.5057 - val_loss: 2077.9404 - val_mse: 2077.9409 - val_mae: 26.3600\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2730.5258 - mse: 2730.5259 - mae: 28.7717 - val_loss: 2077.0379 - val_mse: 2077.0378 - val_mae: 26.9737\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2752.9818 - mse: 2752.9819 - mae: 28.5566 - val_loss: 2088.7546 - val_mse: 2088.7544 - val_mae: 26.5697\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 544us/step - loss: 2633.3053 - mse: 2633.3054 - mae: 28.3332 - val_loss: 2081.5625 - val_mse: 2081.5625 - val_mae: 27.4808\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2711.2480 - mse: 2711.2490 - mae: 28.5500 - val_loss: 2087.5141 - val_mse: 2087.5139 - val_mae: 26.6993\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2737.3499 - mse: 2737.3499 - mae: 28.7033 - val_loss: 2095.2584 - val_mse: 2095.2581 - val_mae: 26.3233\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2662.7304 - mse: 2662.7302 - mae: 28.2934 - val_loss: 2081.8724 - val_mse: 2081.8723 - val_mae: 27.2704\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2678.3323 - mse: 2678.3325 - mae: 28.2309 - val_loss: 2096.4207 - val_mse: 2096.4207 - val_mae: 26.4844\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2751.8515 - mse: 2751.8528 - mae: 28.5857 - val_loss: 2092.1592 - val_mse: 2092.1592 - val_mae: 26.8897\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2671.2654 - mse: 2671.2651 - mae: 28.6626 - val_loss: 2089.4104 - val_mse: 2089.4106 - val_mae: 27.1777\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2680.2506 - mse: 2680.2502 - mae: 28.4522 - val_loss: 2091.4517 - val_mse: 2091.4517 - val_mae: 26.5100\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2737.1763 - mse: 2737.1765 - mae: 28.3076 - val_loss: 2103.2993 - val_mse: 2103.2993 - val_mae: 26.5073\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2686.5963 - mse: 2686.5957 - mae: 28.5745 - val_loss: 2101.8449 - val_mse: 2101.8450 - val_mae: 26.5961\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2675.0407 - mse: 2675.0405 - mae: 28.1834 - val_loss: 2088.5635 - val_mse: 2088.5635 - val_mae: 26.7853\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2656.1639 - mse: 2656.1638 - mae: 28.1570 - val_loss: 2079.1971 - val_mse: 2079.1973 - val_mae: 27.1727\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2691.1432 - mse: 2691.1436 - mae: 28.5125 - val_loss: 2086.1826 - val_mse: 2086.1829 - val_mae: 26.8856\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2737.7508 - mse: 2737.7517 - mae: 28.5152 - val_loss: 2089.0715 - val_mse: 2089.0715 - val_mae: 26.6335\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 688us/step - loss: 2675.5702 - mse: 2675.5713 - mae: 28.3030 - val_loss: 2095.8780 - val_mse: 2095.8784 - val_mae: 26.4643\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2702.5752 - mse: 2702.5757 - mae: 28.3107 - val_loss: 2082.7219 - val_mse: 2082.7217 - val_mae: 26.4341\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2675.4379 - mse: 2675.4380 - mae: 28.3781 - val_loss: 2093.7504 - val_mse: 2093.7505 - val_mae: 26.7814\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2685.3012 - mse: 2685.3025 - mae: 28.1261 - val_loss: 2088.8868 - val_mse: 2088.8867 - val_mae: 27.2239\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2665.2324 - mse: 2665.2319 - mae: 28.5037 - val_loss: 2111.7183 - val_mse: 2111.7180 - val_mae: 26.9255\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2690.2027 - mse: 2690.2024 - mae: 28.2281 - val_loss: 2110.6237 - val_mse: 2110.6235 - val_mae: 26.8214\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 706us/step - loss: 2708.0295 - mse: 2708.0300 - mae: 28.4607 - val_loss: 2100.9987 - val_mse: 2100.9990 - val_mae: 27.2624\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2702.3133 - mse: 2702.3130 - mae: 28.2889 - val_loss: 2105.2487 - val_mse: 2105.2485 - val_mae: 26.6601\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2672.2324 - mse: 2672.2324 - mae: 28.3065 - val_loss: 2101.9349 - val_mse: 2101.9346 - val_mae: 27.3043\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2679.9441 - mse: 2679.9434 - mae: 28.3801 - val_loss: 2105.9882 - val_mse: 2105.9880 - val_mae: 27.3459\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2696.4067 - mse: 2696.4060 - mae: 28.3344 - val_loss: 2120.7683 - val_mse: 2120.7683 - val_mae: 26.7598\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2727.0916 - mse: 2727.0923 - mae: 28.5191 - val_loss: 2120.0973 - val_mse: 2120.0967 - val_mae: 27.4582\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2694.7968 - mse: 2694.7966 - mae: 28.4906 - val_loss: 2109.5632 - val_mse: 2109.5630 - val_mae: 27.2546\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2642.2351 - mse: 2642.2361 - mae: 28.0469 - val_loss: 2108.0355 - val_mse: 2108.0356 - val_mae: 27.2248\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 649us/step - loss: 2680.1818 - mse: 2680.1816 - mae: 28.4051 - val_loss: 2110.1350 - val_mse: 2110.1350 - val_mae: 27.1669\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2704.9228 - mse: 2704.9236 - mae: 28.3759 - val_loss: 2105.8948 - val_mse: 2105.8950 - val_mae: 27.4551\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2677.7950 - mse: 2677.7952 - mae: 28.3038 - val_loss: 2101.7963 - val_mse: 2101.7961 - val_mae: 27.3799\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2698.9176 - mse: 2698.9170 - mae: 28.4766 - val_loss: 2115.5364 - val_mse: 2115.5361 - val_mae: 27.1094\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2709.0218 - mse: 2709.0212 - mae: 28.5076 - val_loss: 2124.7825 - val_mse: 2124.7822 - val_mae: 26.8246\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2697.7997 - mse: 2697.7998 - mae: 28.0558 - val_loss: 2118.9643 - val_mse: 2118.9648 - val_mae: 26.7832\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2701.6862 - mse: 2701.6860 - mae: 28.0540 - val_loss: 2124.8149 - val_mse: 2124.8147 - val_mae: 26.5448\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 652us/step - loss: 2671.3677 - mse: 2671.3662 - mae: 28.2654 - val_loss: 2118.3066 - val_mse: 2118.3069 - val_mae: 26.9390\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2694.6692 - mse: 2694.6694 - mae: 28.4638 - val_loss: 2122.2800 - val_mse: 2122.2800 - val_mae: 26.6535\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2686.3434 - mse: 2686.3430 - mae: 28.5041 - val_loss: 2107.1078 - val_mse: 2107.1079 - val_mae: 27.2114\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2698.0967 - mse: 2698.0964 - mae: 28.5403 - val_loss: 2106.1250 - val_mse: 2106.1248 - val_mae: 26.8353\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 529us/step - loss: 2655.8465 - mse: 2655.8457 - mae: 28.2420 - val_loss: 2092.2824 - val_mse: 2092.2825 - val_mae: 27.5772\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2727.5134 - mse: 2727.5137 - mae: 28.8230 - val_loss: 2100.0141 - val_mse: 2100.0144 - val_mae: 27.2890\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2678.8206 - mse: 2678.8198 - mae: 28.1737 - val_loss: 2095.0119 - val_mse: 2095.0120 - val_mae: 27.3989\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2640.1566 - mse: 2640.1558 - mae: 28.2763 - val_loss: 2095.9598 - val_mse: 2095.9597 - val_mae: 27.1666\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2714.0927 - mse: 2714.0930 - mae: 28.7271 - val_loss: 2088.9276 - val_mse: 2088.9275 - val_mae: 27.2970\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2648.2402 - mse: 2648.2407 - mae: 28.1224 - val_loss: 2096.3298 - val_mse: 2096.3296 - val_mae: 27.4893\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2695.8209 - mse: 2695.8215 - mae: 28.7548 - val_loss: 2099.6784 - val_mse: 2099.6787 - val_mae: 26.9903\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2681.7354 - mse: 2681.7351 - mae: 28.0720 - val_loss: 2103.1347 - val_mse: 2103.1348 - val_mae: 26.7839\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2662.2496 - mse: 2662.2488 - mae: 28.0932 - val_loss: 2107.0835 - val_mse: 2107.0835 - val_mae: 26.8456\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2666.1813 - mse: 2666.1819 - mae: 28.0573 - val_loss: 2098.3639 - val_mse: 2098.3638 - val_mae: 27.1105\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2692.8957 - mse: 2692.8965 - mae: 28.0527 - val_loss: 2101.8264 - val_mse: 2101.8267 - val_mae: 26.7722\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2706.9144 - mse: 2706.9133 - mae: 28.5064 - val_loss: 2101.1962 - val_mse: 2101.1960 - val_mae: 26.7052\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 694us/step - loss: 2672.0196 - mse: 2672.0198 - mae: 28.1506 - val_loss: 2110.8241 - val_mse: 2110.8240 - val_mae: 26.7451\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2677.5492 - mse: 2677.5493 - mae: 28.3307 - val_loss: 2122.9628 - val_mse: 2122.9626 - val_mae: 26.6047\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 637us/step - loss: 2653.5377 - mse: 2653.5381 - mae: 27.9009 - val_loss: 2113.9195 - val_mse: 2113.9197 - val_mae: 27.1669\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 679us/step - loss: 2689.0386 - mse: 2689.0388 - mae: 28.3387 - val_loss: 2117.4776 - val_mse: 2117.4775 - val_mae: 26.7035\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2653.4138 - mse: 2653.4138 - mae: 28.0739 - val_loss: 2115.2975 - val_mse: 2115.2976 - val_mae: 27.3298\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2632.7671 - mse: 2632.7664 - mae: 27.9710 - val_loss: 2095.9983 - val_mse: 2095.9985 - val_mae: 27.1561\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2668.9253 - mse: 2668.9250 - mae: 28.3891 - val_loss: 2092.5533 - val_mse: 2092.5535 - val_mae: 27.0679\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2684.4941 - mse: 2684.4941 - mae: 28.2351 - val_loss: 2103.8665 - val_mse: 2103.8662 - val_mae: 27.3480\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2669.1970 - mse: 2669.1968 - mae: 28.1972 - val_loss: 2117.3411 - val_mse: 2117.3408 - val_mae: 26.8818\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2654.0602 - mse: 2654.0610 - mae: 28.2555 - val_loss: 2110.8021 - val_mse: 2110.8018 - val_mae: 26.9750\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2680.8434 - mse: 2680.8433 - mae: 28.1807 - val_loss: 2115.9585 - val_mse: 2115.9583 - val_mae: 26.8320\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2666.1704 - mse: 2666.1697 - mae: 27.8662 - val_loss: 2108.6881 - val_mse: 2108.6877 - val_mae: 26.7572\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2633.0153 - mse: 2633.0146 - mae: 28.2176 - val_loss: 2096.9748 - val_mse: 2096.9749 - val_mae: 27.2260\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2679.8088 - mse: 2679.8088 - mae: 28.1893 - val_loss: 2104.8239 - val_mse: 2104.8237 - val_mae: 27.1873\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2687.0191 - mse: 2687.0193 - mae: 28.6131 - val_loss: 2127.9193 - val_mse: 2127.9192 - val_mae: 26.7826\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2663.0764 - mse: 2663.0752 - mae: 28.1029 - val_loss: 2125.1827 - val_mse: 2125.1829 - val_mae: 27.0279\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 672us/step - loss: 2658.5632 - mse: 2658.5640 - mae: 28.0219 - val_loss: 2131.6657 - val_mse: 2131.6658 - val_mae: 27.1694\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2663.9453 - mse: 2663.9448 - mae: 28.1497 - val_loss: 2129.0618 - val_mse: 2129.0618 - val_mae: 26.9159\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2648.9483 - mse: 2648.9475 - mae: 28.2962 - val_loss: 2120.8615 - val_mse: 2120.8611 - val_mae: 27.2882\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2668.9868 - mse: 2668.9873 - mae: 28.3962 - val_loss: 2126.4630 - val_mse: 2126.4631 - val_mae: 26.7645\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2678.8211 - mse: 2678.8206 - mae: 28.2012 - val_loss: 2117.0080 - val_mse: 2117.0078 - val_mae: 26.9509\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2666.0793 - mse: 2666.0798 - mae: 28.1367 - val_loss: 2125.4255 - val_mse: 2125.4255 - val_mae: 26.9344\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 679us/step - loss: 2661.4775 - mse: 2661.4775 - mae: 27.9628 - val_loss: 2120.0791 - val_mse: 2120.0791 - val_mae: 26.6753\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2680.1845 - mse: 2680.1848 - mae: 28.1032 - val_loss: 2107.3997 - val_mse: 2107.3999 - val_mae: 27.2124\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2658.0005 - mse: 2658.0010 - mae: 28.0505 - val_loss: 2110.3503 - val_mse: 2110.3506 - val_mae: 26.8448\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 13290.8002 - mse: 13290.7998 - mae: 109.7274 - val_loss: 34538.7093 - val_mse: 34538.7070 - val_mae: 132.4442\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 711us/step - loss: 13005.3386 - mse: 13005.3379 - mae: 108.4516 - val_loss: 34007.5706 - val_mse: 34007.5703 - val_mae: 130.4672\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 12287.2171 - mse: 12287.2148 - mae: 105.0471 - val_loss: 32506.8904 - val_mse: 32506.8906 - val_mae: 124.7120\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 10278.7432 - mse: 10278.7441 - mae: 94.8351 - val_loss: 28629.0139 - val_mse: 28629.0137 - val_mae: 108.4344\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 6521.2937 - mse: 6521.2939 - mae: 70.5258 - val_loss: 21179.4337 - val_mse: 21179.4316 - val_mae: 66.9227\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 3023.4449 - mse: 3023.4448 - mae: 41.6229 - val_loss: 17175.7806 - val_mse: 17175.7793 - val_mae: 38.2298\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2929.4510 - mse: 2929.4509 - mae: 39.7519 - val_loss: 17566.4932 - val_mse: 17566.4941 - val_mae: 39.3685\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 556us/step - loss: 2869.6148 - mse: 2869.6145 - mae: 38.7265 - val_loss: 17659.4734 - val_mse: 17659.4746 - val_mae: 39.7002\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2371.4828 - mse: 2371.4832 - mae: 35.6961 - val_loss: 17500.0820 - val_mse: 17500.0840 - val_mae: 38.9746\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2733.1087 - mse: 2733.1089 - mae: 37.4683 - val_loss: 17603.1804 - val_mse: 17603.1797 - val_mae: 39.3282\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2824.5917 - mse: 2824.5918 - mae: 38.7392 - val_loss: 17655.7934 - val_mse: 17655.7930 - val_mae: 39.5460\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 676us/step - loss: 2760.5805 - mse: 2760.5803 - mae: 38.6236 - val_loss: 17566.9668 - val_mse: 17566.9668 - val_mae: 39.0185\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 2854.2193 - mse: 2854.2192 - mae: 38.1013 - val_loss: 17534.3621 - val_mse: 17534.3613 - val_mae: 38.8159\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2464.7848 - mse: 2464.7847 - mae: 35.8863 - val_loss: 17405.3119 - val_mse: 17405.3105 - val_mae: 38.2937\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 2588.0400 - mse: 2588.0403 - mae: 36.9874 - val_loss: 17401.1724 - val_mse: 17401.1699 - val_mae: 38.2326\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2575.4423 - mse: 2575.4424 - mae: 35.8542 - val_loss: 17240.4778 - val_mse: 17240.4785 - val_mae: 37.6733\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2433.4727 - mse: 2433.4727 - mae: 36.2776 - val_loss: 17425.0294 - val_mse: 17425.0293 - val_mae: 38.1753\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 512us/step - loss: 2525.7228 - mse: 2525.7227 - mae: 37.0510 - val_loss: 17384.9758 - val_mse: 17384.9766 - val_mae: 37.9733\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2562.8363 - mse: 2562.8359 - mae: 37.0349 - val_loss: 17572.0330 - val_mse: 17572.0332 - val_mae: 38.7421\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2833.4899 - mse: 2833.4900 - mae: 38.1003 - val_loss: 17730.2790 - val_mse: 17730.2793 - val_mae: 39.5995\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2489.2702 - mse: 2489.2703 - mae: 36.1829 - val_loss: 17557.5910 - val_mse: 17557.5898 - val_mae: 38.5787\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 2421.1466 - mse: 2421.1467 - mae: 34.0385 - val_loss: 17306.9792 - val_mse: 17306.9785 - val_mae: 37.4512\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2274.2162 - mse: 2274.2161 - mae: 34.9084 - val_loss: 17376.0045 - val_mse: 17376.0039 - val_mae: 37.6372\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 736us/step - loss: 2559.8264 - mse: 2559.8267 - mae: 35.5278 - val_loss: 17393.0478 - val_mse: 17393.0488 - val_mae: 37.6519\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 832us/step - loss: 2450.4908 - mse: 2450.4907 - mae: 35.2002 - val_loss: 17555.3243 - val_mse: 17555.3242 - val_mae: 38.4012\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 710us/step - loss: 2322.0049 - mse: 2322.0046 - mae: 34.7830 - val_loss: 17469.9511 - val_mse: 17469.9512 - val_mae: 37.9173\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 676us/step - loss: 2438.8298 - mse: 2438.8301 - mae: 34.3431 - val_loss: 17409.3749 - val_mse: 17409.3770 - val_mae: 37.5985\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 2555.0901 - mse: 2555.0906 - mae: 36.2417 - val_loss: 17600.0431 - val_mse: 17600.0430 - val_mae: 38.5397\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2470.9414 - mse: 2470.9417 - mae: 34.1923 - val_loss: 17722.9053 - val_mse: 17722.9043 - val_mae: 39.2781\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2394.1176 - mse: 2394.1179 - mae: 35.0346 - val_loss: 17579.9036 - val_mse: 17579.9043 - val_mae: 38.3276\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 671us/step - loss: 2412.9775 - mse: 2412.9778 - mae: 34.4237 - val_loss: 17536.6489 - val_mse: 17536.6484 - val_mae: 38.0447\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2578.6612 - mse: 2578.6616 - mae: 35.3543 - val_loss: 17652.1118 - val_mse: 17652.1113 - val_mae: 38.6764\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 483us/step - loss: 2431.5118 - mse: 2431.5117 - mae: 34.5731 - val_loss: 17655.9731 - val_mse: 17655.9727 - val_mae: 38.6663\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 470us/step - loss: 2316.5611 - mse: 2316.5610 - mae: 33.6110 - val_loss: 17411.4364 - val_mse: 17411.4355 - val_mae: 37.2785\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 2289.1219 - mse: 2289.1218 - mae: 33.7983 - val_loss: 17494.3701 - val_mse: 17494.3691 - val_mae: 37.6507\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 512us/step - loss: 2012.4185 - mse: 2012.4185 - mae: 32.3549 - val_loss: 17467.0436 - val_mse: 17467.0449 - val_mae: 37.4852\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 677us/step - loss: 2232.5079 - mse: 2232.5081 - mae: 34.1343 - val_loss: 17459.7623 - val_mse: 17459.7637 - val_mae: 37.4391\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2330.7840 - mse: 2330.7839 - mae: 34.1280 - val_loss: 17463.0707 - val_mse: 17463.0703 - val_mae: 37.4225\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 455us/step - loss: 2050.7908 - mse: 2050.7910 - mae: 31.9001 - val_loss: 17439.6436 - val_mse: 17439.6445 - val_mae: 37.2904\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2168.1923 - mse: 2168.1924 - mae: 32.6215 - val_loss: 17375.4655 - val_mse: 17375.4648 - val_mae: 37.0212\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 563us/step - loss: 2386.7549 - mse: 2386.7546 - mae: 33.0886 - val_loss: 17716.9069 - val_mse: 17716.9062 - val_mae: 38.8657\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2049.5065 - mse: 2049.5066 - mae: 31.3335 - val_loss: 17158.5569 - val_mse: 17158.5566 - val_mae: 36.5690\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2048.6450 - mse: 2048.6450 - mae: 31.4858 - val_loss: 17452.0785 - val_mse: 17452.0781 - val_mae: 37.2396\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2168.7869 - mse: 2168.7871 - mae: 32.9041 - val_loss: 17436.0811 - val_mse: 17436.0820 - val_mae: 37.1711\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 2247.6582 - mse: 2247.6580 - mae: 33.3186 - val_loss: 17643.9161 - val_mse: 17643.9180 - val_mae: 38.3144\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 1978.6137 - mse: 1978.6135 - mae: 31.3349 - val_loss: 17523.4949 - val_mse: 17523.4941 - val_mae: 37.5416\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 700us/step - loss: 2294.9762 - mse: 2294.9761 - mae: 32.8604 - val_loss: 17682.7855 - val_mse: 17682.7852 - val_mae: 38.5248\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 2075.7492 - mse: 2075.7493 - mae: 32.8614 - val_loss: 17441.0137 - val_mse: 17441.0137 - val_mae: 37.1122\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2135.0680 - mse: 2135.0681 - mae: 31.9168 - val_loss: 17431.5616 - val_mse: 17431.5625 - val_mae: 37.0618\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 531us/step - loss: 2226.3562 - mse: 2226.3560 - mae: 32.2523 - val_loss: 17500.1611 - val_mse: 17500.1621 - val_mae: 37.3165\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 693us/step - loss: 1951.8228 - mse: 1951.8228 - mae: 30.9378 - val_loss: 17441.6817 - val_mse: 17441.6816 - val_mae: 37.0565\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 651us/step - loss: 2106.8768 - mse: 2106.8767 - mae: 31.6944 - val_loss: 17514.2619 - val_mse: 17514.2617 - val_mae: 37.3530\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2267.2143 - mse: 2267.2146 - mae: 32.3983 - val_loss: 17685.9135 - val_mse: 17685.9121 - val_mae: 38.3293\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 533us/step - loss: 1920.7686 - mse: 1920.7686 - mae: 31.4001 - val_loss: 17398.4918 - val_mse: 17398.4902 - val_mae: 36.8532\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 405us/step - loss: 2093.5223 - mse: 2093.5225 - mae: 31.0523 - val_loss: 17476.8143 - val_mse: 17476.8164 - val_mae: 37.1566\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 434us/step - loss: 2073.0171 - mse: 2073.0171 - mae: 32.3890 - val_loss: 17664.7453 - val_mse: 17664.7461 - val_mae: 38.1534\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 1928.0163 - mse: 1928.0162 - mae: 29.7711 - val_loss: 17366.8977 - val_mse: 17366.8984 - val_mae: 36.7239\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 2011.1095 - mse: 2011.1097 - mae: 30.9579 - val_loss: 17508.9506 - val_mse: 17508.9492 - val_mae: 37.2337\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 400us/step - loss: 1906.5976 - mse: 1906.5979 - mae: 30.9961 - val_loss: 17585.0912 - val_mse: 17585.0898 - val_mae: 37.5775\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 2082.5328 - mse: 2082.5327 - mae: 32.1209 - val_loss: 17371.8826 - val_mse: 17371.8828 - val_mae: 36.6810\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 479us/step - loss: 2077.7796 - mse: 2077.7795 - mae: 31.9520 - val_loss: 17715.7550 - val_mse: 17715.7559 - val_mae: 38.3476\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 422us/step - loss: 1795.3352 - mse: 1795.3351 - mae: 29.5342 - val_loss: 17539.0061 - val_mse: 17539.0059 - val_mae: 37.2889\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 519us/step - loss: 2085.2425 - mse: 2085.2422 - mae: 31.2909 - val_loss: 17514.2349 - val_mse: 17514.2363 - val_mae: 37.1626\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 481us/step - loss: 1801.8466 - mse: 1801.8467 - mae: 29.1833 - val_loss: 17568.3330 - val_mse: 17568.3340 - val_mae: 37.4034\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 454us/step - loss: 1918.9168 - mse: 1918.9166 - mae: 30.4551 - val_loss: 17554.8472 - val_mse: 17554.8457 - val_mae: 37.3477\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 373us/step - loss: 2081.4802 - mse: 2081.4802 - mae: 31.2546 - val_loss: 17636.8888 - val_mse: 17636.8887 - val_mae: 37.7705\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 467us/step - loss: 1901.4203 - mse: 1901.4204 - mae: 30.5614 - val_loss: 17492.4783 - val_mse: 17492.4785 - val_mae: 37.0305\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 472us/step - loss: 1803.7983 - mse: 1803.7983 - mae: 30.0838 - val_loss: 17470.6151 - val_mse: 17470.6152 - val_mae: 36.9222\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 462us/step - loss: 2064.9537 - mse: 2064.9539 - mae: 30.1471 - val_loss: 17478.0368 - val_mse: 17478.0352 - val_mae: 36.9319\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 486us/step - loss: 1975.7589 - mse: 1975.7590 - mae: 30.2266 - val_loss: 17410.6463 - val_mse: 17410.6484 - val_mae: 36.6823\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 485us/step - loss: 1888.0975 - mse: 1888.0975 - mae: 30.8961 - val_loss: 17450.8171 - val_mse: 17450.8184 - val_mae: 36.7805\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 446us/step - loss: 1943.3160 - mse: 1943.3162 - mae: 30.5159 - val_loss: 17499.3265 - val_mse: 17499.3281 - val_mae: 36.9424\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 488us/step - loss: 2050.2636 - mse: 2050.2637 - mae: 31.8186 - val_loss: 17546.6750 - val_mse: 17546.6738 - val_mae: 37.1076\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 473us/step - loss: 2000.4292 - mse: 2000.4292 - mae: 30.5210 - val_loss: 17432.5761 - val_mse: 17432.5762 - val_mae: 36.6899\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 493us/step - loss: 1935.9859 - mse: 1935.9860 - mae: 29.7827 - val_loss: 17431.9908 - val_mse: 17431.9902 - val_mae: 36.6884\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 511us/step - loss: 1959.4832 - mse: 1959.4830 - mae: 30.8828 - val_loss: 17522.4807 - val_mse: 17522.4805 - val_mae: 36.9638\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 409us/step - loss: 1976.0895 - mse: 1976.0895 - mae: 31.4152 - val_loss: 17492.3239 - val_mse: 17492.3242 - val_mae: 36.8388\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 484us/step - loss: 1938.2181 - mse: 1938.2180 - mae: 30.4118 - val_loss: 17456.6113 - val_mse: 17456.6113 - val_mae: 36.7225\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 421us/step - loss: 1803.5558 - mse: 1803.5557 - mae: 28.4307 - val_loss: 17447.6598 - val_mse: 17447.6602 - val_mae: 36.7019\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 434us/step - loss: 1964.5778 - mse: 1964.5776 - mae: 30.3108 - val_loss: 17676.8228 - val_mse: 17676.8242 - val_mae: 37.6468\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4253.2510 - mse: 4253.2515 - mae: 34.9199 - val_loss: 2203.4704 - val_mse: 2203.4702 - val_mae: 30.3022\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 471us/step - loss: 4208.8638 - mse: 4208.8643 - mae: 35.0176 - val_loss: 2097.1314 - val_mse: 2097.1313 - val_mae: 29.8929\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4176.9631 - mse: 4176.9634 - mae: 35.2386 - val_loss: 2136.6302 - val_mse: 2136.6304 - val_mae: 30.0598\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4098.2217 - mse: 4098.2212 - mae: 34.9979 - val_loss: 2193.5857 - val_mse: 2193.5857 - val_mae: 30.2903\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4214.4882 - mse: 4214.4883 - mae: 35.0228 - val_loss: 2215.5422 - val_mse: 2215.5422 - val_mae: 30.3902\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4297.1862 - mse: 4297.1875 - mae: 35.5943 - val_loss: 2240.1601 - val_mse: 2240.1599 - val_mae: 30.5004\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4052.4771 - mse: 4052.4766 - mae: 35.1173 - val_loss: 2270.6097 - val_mse: 2270.6096 - val_mae: 30.6445\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 0s 485us/step - loss: 4182.3618 - mse: 4182.3618 - mae: 35.2141 - val_loss: 2283.6428 - val_mse: 2283.6431 - val_mae: 30.7054\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4205.5023 - mse: 4205.5029 - mae: 34.5272 - val_loss: 2271.8150 - val_mse: 2271.8149 - val_mae: 30.6515\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 531us/step - loss: 4278.8334 - mse: 4278.8335 - mae: 34.6737 - val_loss: 2306.1615 - val_mse: 2306.1616 - val_mae: 30.8200\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 0s 485us/step - loss: 4310.5027 - mse: 4310.5029 - mae: 34.3225 - val_loss: 2294.2964 - val_mse: 2294.2964 - val_mae: 30.7635\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4009.8538 - mse: 4009.8540 - mae: 33.4573 - val_loss: 2163.2425 - val_mse: 2163.2424 - val_mae: 30.1722\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 3992.7312 - mse: 3992.7314 - mae: 34.8477 - val_loss: 2274.5174 - val_mse: 2274.5173 - val_mae: 30.6686\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 715us/step - loss: 4052.8677 - mse: 4052.8679 - mae: 34.7562 - val_loss: 2280.6218 - val_mse: 2280.6218 - val_mae: 30.6918\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4044.5898 - mse: 4044.5901 - mae: 33.8056 - val_loss: 2324.4203 - val_mse: 2324.4202 - val_mae: 30.9020\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 3893.3189 - mse: 3893.3186 - mae: 33.0870 - val_loss: 2165.6612 - val_mse: 2165.6611 - val_mae: 30.1854\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 658us/step - loss: 4038.7446 - mse: 4038.7446 - mae: 34.1516 - val_loss: 2235.2849 - val_mse: 2235.2847 - val_mae: 30.4855\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4062.3395 - mse: 4062.3401 - mae: 33.8155 - val_loss: 2254.0488 - val_mse: 2254.0488 - val_mae: 30.5577\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 662us/step - loss: 4246.5107 - mse: 4246.5107 - mae: 34.1713 - val_loss: 2310.3693 - val_mse: 2310.3694 - val_mae: 30.8234\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4108.3221 - mse: 4108.3223 - mae: 33.2269 - val_loss: 2222.8454 - val_mse: 2222.8452 - val_mae: 30.4082\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 4146.8089 - mse: 4146.8086 - mae: 35.2967 - val_loss: 2250.9736 - val_mse: 2250.9736 - val_mae: 30.5399\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 3886.3779 - mse: 3886.3774 - mae: 33.1796 - val_loss: 2204.6196 - val_mse: 2204.6196 - val_mae: 30.3296\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 666us/step - loss: 3903.0585 - mse: 3903.0581 - mae: 33.2743 - val_loss: 2239.9720 - val_mse: 2239.9722 - val_mae: 30.4877\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 688us/step - loss: 4160.9041 - mse: 4160.9043 - mae: 34.1431 - val_loss: 2300.4449 - val_mse: 2300.4451 - val_mae: 30.7761\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 701us/step - loss: 4023.7072 - mse: 4023.7065 - mae: 34.5721 - val_loss: 2347.5760 - val_mse: 2347.5757 - val_mae: 31.0081\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 3832.6923 - mse: 3832.6924 - mae: 33.4147 - val_loss: 2226.4177 - val_mse: 2226.4175 - val_mae: 30.4283\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 685us/step - loss: 4094.6530 - mse: 4094.6531 - mae: 34.0089 - val_loss: 2232.3336 - val_mse: 2232.3337 - val_mae: 30.4544\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 0s 440us/step - loss: 4238.8022 - mse: 4238.8022 - mae: 34.2696 - val_loss: 2310.6562 - val_mse: 2310.6562 - val_mae: 30.8297\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4123.6072 - mse: 4123.6074 - mae: 34.0679 - val_loss: 2239.7570 - val_mse: 2239.7573 - val_mae: 30.4891\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4019.1501 - mse: 4019.1499 - mae: 33.8969 - val_loss: 2263.6412 - val_mse: 2263.6416 - val_mae: 30.5986\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4191.0334 - mse: 4191.0327 - mae: 34.8807 - val_loss: 2285.9340 - val_mse: 2285.9341 - val_mae: 30.7050\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4222.9318 - mse: 4222.9321 - mae: 34.5939 - val_loss: 2299.1441 - val_mse: 2299.1440 - val_mae: 30.7788\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4126.2408 - mse: 4126.2407 - mae: 33.5207 - val_loss: 2325.0910 - val_mse: 2325.0913 - val_mae: 30.8968\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 728us/step - loss: 4078.9804 - mse: 4078.9805 - mae: 32.9569 - val_loss: 2280.2645 - val_mse: 2280.2642 - val_mae: 30.6777\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 4005.3972 - mse: 4005.3972 - mae: 33.2117 - val_loss: 2285.6374 - val_mse: 2285.6375 - val_mae: 30.6960\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4011.1838 - mse: 4011.1841 - mae: 33.7133 - val_loss: 2258.5490 - val_mse: 2258.5493 - val_mae: 30.5649\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4069.8178 - mse: 4069.8174 - mae: 33.4274 - val_loss: 2252.5965 - val_mse: 2252.5962 - val_mae: 30.5372\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 4077.7596 - mse: 4077.7588 - mae: 32.9805 - val_loss: 2253.4502 - val_mse: 2253.4502 - val_mae: 30.5399\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 3904.4933 - mse: 3904.4932 - mae: 33.6923 - val_loss: 2165.8624 - val_mse: 2165.8623 - val_mae: 30.1871\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4049.8992 - mse: 4049.8997 - mae: 34.0305 - val_loss: 2221.5568 - val_mse: 2221.5569 - val_mae: 30.4069\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 3964.5156 - mse: 3964.5154 - mae: 33.8735 - val_loss: 2250.1301 - val_mse: 2250.1299 - val_mae: 30.5383\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 4130.9189 - mse: 4130.9185 - mae: 33.8015 - val_loss: 2243.1185 - val_mse: 2243.1184 - val_mae: 30.5053\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 739us/step - loss: 3939.3247 - mse: 3939.3247 - mae: 33.6616 - val_loss: 2284.0422 - val_mse: 2284.0422 - val_mae: 30.6892\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4059.5276 - mse: 4059.5273 - mae: 32.5866 - val_loss: 2258.9758 - val_mse: 2258.9758 - val_mae: 30.5793\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 3957.0229 - mse: 3957.0229 - mae: 33.5265 - val_loss: 2297.2944 - val_mse: 2297.2944 - val_mae: 30.7603\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 695us/step - loss: 4045.1376 - mse: 4045.1377 - mae: 32.9511 - val_loss: 2308.4947 - val_mse: 2308.4946 - val_mae: 30.8129\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4112.5610 - mse: 4112.5605 - mae: 33.9058 - val_loss: 2302.4588 - val_mse: 2302.4587 - val_mae: 30.7843\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 706us/step - loss: 3941.5649 - mse: 3941.5654 - mae: 32.6484 - val_loss: 2256.0526 - val_mse: 2256.0527 - val_mae: 30.5739\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 614us/step - loss: 4068.5899 - mse: 4068.5896 - mae: 34.5143 - val_loss: 2247.3140 - val_mse: 2247.3140 - val_mae: 30.5270\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 675us/step - loss: 4179.8907 - mse: 4179.8906 - mae: 33.6344 - val_loss: 2307.1195 - val_mse: 2307.1194 - val_mae: 30.8005\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 676us/step - loss: 4041.4979 - mse: 4041.4983 - mae: 33.1642 - val_loss: 2291.3848 - val_mse: 2291.3848 - val_mae: 30.7252\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 664us/step - loss: 4178.1655 - mse: 4178.1660 - mae: 34.2728 - val_loss: 2290.1555 - val_mse: 2290.1558 - val_mae: 30.7170\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 4219.7398 - mse: 4219.7402 - mae: 34.6157 - val_loss: 2277.7327 - val_mse: 2277.7324 - val_mae: 30.6549\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4100.0431 - mse: 4100.0430 - mae: 33.2166 - val_loss: 2236.0755 - val_mse: 2236.0752 - val_mae: 30.4717\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4120.9228 - mse: 4120.9224 - mae: 33.6550 - val_loss: 2303.7150 - val_mse: 2303.7153 - val_mae: 30.7677\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 650us/step - loss: 4034.3377 - mse: 4034.3372 - mae: 33.5144 - val_loss: 2275.9776 - val_mse: 2275.9773 - val_mae: 30.6373\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4239.2428 - mse: 4239.2432 - mae: 34.4925 - val_loss: 2359.6186 - val_mse: 2359.6184 - val_mae: 31.0515\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 722us/step - loss: 4125.4865 - mse: 4125.4858 - mae: 33.8627 - val_loss: 2311.3039 - val_mse: 2311.3037 - val_mae: 30.8108\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 533us/step - loss: 4179.1639 - mse: 4179.1641 - mae: 33.5074 - val_loss: 2264.9885 - val_mse: 2264.9885 - val_mae: 30.5965\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 3946.0783 - mse: 3946.0784 - mae: 33.0755 - val_loss: 2259.0984 - val_mse: 2259.0984 - val_mae: 30.5713\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 3992.7147 - mse: 3992.7151 - mae: 32.9508 - val_loss: 2264.8477 - val_mse: 2264.8474 - val_mae: 30.5999\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 4089.6651 - mse: 4089.6655 - mae: 33.4504 - val_loss: 2297.8703 - val_mse: 2297.8704 - val_mae: 30.7619\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 711us/step - loss: 3997.3044 - mse: 3997.3042 - mae: 33.2538 - val_loss: 2318.7673 - val_mse: 2318.7676 - val_mae: 30.8575\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 704us/step - loss: 4043.7259 - mse: 4043.7261 - mae: 33.1544 - val_loss: 2248.6262 - val_mse: 2248.6260 - val_mae: 30.5562\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 675us/step - loss: 4045.0148 - mse: 4045.0149 - mae: 33.7664 - val_loss: 2262.7833 - val_mse: 2262.7830 - val_mae: 30.6200\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 552us/step - loss: 4017.5981 - mse: 4017.5979 - mae: 32.5657 - val_loss: 2292.9296 - val_mse: 2292.9294 - val_mae: 30.7496\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 658us/step - loss: 3938.1310 - mse: 3938.1309 - mae: 32.2836 - val_loss: 2286.2289 - val_mse: 2286.2290 - val_mae: 30.7165\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 3971.1321 - mse: 3971.1316 - mae: 32.7226 - val_loss: 2314.8789 - val_mse: 2314.8789 - val_mae: 30.8361\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 635us/step - loss: 4148.4290 - mse: 4148.4292 - mae: 33.3311 - val_loss: 2365.6409 - val_mse: 2365.6409 - val_mae: 31.0819\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 3914.4880 - mse: 3914.4880 - mae: 32.2946 - val_loss: 2271.6206 - val_mse: 2271.6206 - val_mae: 30.6319\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4123.7086 - mse: 4123.7085 - mae: 33.0419 - val_loss: 2301.9520 - val_mse: 2301.9521 - val_mae: 30.7673\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 3913.2499 - mse: 3913.2495 - mae: 32.2579 - val_loss: 2315.6903 - val_mse: 2315.6902 - val_mae: 30.8324\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 744us/step - loss: 3912.9167 - mse: 3912.9167 - mae: 33.7038 - val_loss: 2306.3955 - val_mse: 2306.3955 - val_mae: 30.7955\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 4056.1997 - mse: 4056.2000 - mae: 33.0232 - val_loss: 2232.8321 - val_mse: 2232.8320 - val_mae: 30.4907\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 506us/step - loss: 3951.1762 - mse: 3951.1763 - mae: 33.2617 - val_loss: 2261.6730 - val_mse: 2261.6731 - val_mae: 30.5987\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 3964.0033 - mse: 3964.0027 - mae: 32.6879 - val_loss: 2222.1870 - val_mse: 2222.1873 - val_mae: 30.4425\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 3994.5699 - mse: 3994.5706 - mae: 33.3910 - val_loss: 2295.9098 - val_mse: 2295.9097 - val_mae: 30.7349\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 3876.1437 - mse: 3876.1431 - mae: 32.9927 - val_loss: 2261.0899 - val_mse: 2261.0898 - val_mae: 30.5819\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4083.1146 - mse: 4083.1145 - mae: 34.1357 - val_loss: 2302.3245 - val_mse: 2302.3245 - val_mae: 30.7659\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 4052.1360 - mse: 4052.1365 - mae: 33.5011 - val_loss: 2317.7000 - val_mse: 2317.7000 - val_mae: 30.8465\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3421.3389 - mse: 3421.3389 - mae: 32.7140 - val_loss: 1467.0169 - val_mse: 1467.0168 - val_mae: 25.0566\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3421.6006 - mse: 3421.6003 - mae: 33.0257 - val_loss: 1468.4934 - val_mse: 1468.4935 - val_mae: 25.0017\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3337.5234 - mse: 3337.5232 - mae: 33.1891 - val_loss: 1472.2535 - val_mse: 1472.2535 - val_mae: 24.9043\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3410.3457 - mse: 3410.3455 - mae: 32.6526 - val_loss: 1472.9014 - val_mse: 1472.9016 - val_mae: 24.9037\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3387.7732 - mse: 3387.7729 - mae: 32.5806 - val_loss: 1468.3291 - val_mse: 1468.3291 - val_mae: 25.0638\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3362.7653 - mse: 3362.7651 - mae: 32.4461 - val_loss: 1485.9222 - val_mse: 1485.9221 - val_mae: 24.6130\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3415.4601 - mse: 3415.4602 - mae: 32.6429 - val_loss: 1464.7200 - val_mse: 1464.7200 - val_mae: 25.4195\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3263.6561 - mse: 3263.6555 - mae: 32.8194 - val_loss: 1473.4371 - val_mse: 1473.4371 - val_mae: 24.9887\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 692us/step - loss: 3209.2925 - mse: 3209.2930 - mae: 31.8486 - val_loss: 1469.3411 - val_mse: 1469.3411 - val_mae: 25.2064\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3421.3520 - mse: 3421.3525 - mae: 32.7650 - val_loss: 1472.2769 - val_mse: 1472.2771 - val_mae: 25.0694\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3285.5554 - mse: 3285.5544 - mae: 31.8962 - val_loss: 1478.4408 - val_mse: 1478.4408 - val_mae: 24.8984\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 539us/step - loss: 3340.4147 - mse: 3340.4153 - mae: 32.0987 - val_loss: 1471.5816 - val_mse: 1471.5814 - val_mae: 25.1533\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3444.7418 - mse: 3444.7412 - mae: 32.6295 - val_loss: 1476.2424 - val_mse: 1476.2423 - val_mae: 24.9967\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3430.5341 - mse: 3430.5334 - mae: 33.1306 - val_loss: 1471.5999 - val_mse: 1471.6000 - val_mae: 25.2030\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3192.8132 - mse: 3192.8135 - mae: 32.2375 - val_loss: 1471.2284 - val_mse: 1471.2284 - val_mae: 25.2625\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3302.4987 - mse: 3302.4990 - mae: 31.5800 - val_loss: 1472.2111 - val_mse: 1472.2111 - val_mae: 25.2220\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 663us/step - loss: 3340.1189 - mse: 3340.1201 - mae: 32.3580 - val_loss: 1472.5292 - val_mse: 1472.5291 - val_mae: 25.2946\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3403.0092 - mse: 3403.0098 - mae: 32.4754 - val_loss: 1476.0627 - val_mse: 1476.0626 - val_mae: 25.1327\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3329.2353 - mse: 3329.2354 - mae: 31.9684 - val_loss: 1470.5241 - val_mse: 1470.5240 - val_mae: 25.5201\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3245.5957 - mse: 3245.5952 - mae: 32.3848 - val_loss: 1472.1900 - val_mse: 1472.1898 - val_mae: 25.4214\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3382.8207 - mse: 3382.8210 - mae: 33.1011 - val_loss: 1476.9738 - val_mse: 1476.9739 - val_mae: 25.1295\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3344.5367 - mse: 3344.5371 - mae: 31.9387 - val_loss: 1470.5139 - val_mse: 1470.5139 - val_mae: 25.6048\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3382.3771 - mse: 3382.3772 - mae: 33.2526 - val_loss: 1480.0626 - val_mse: 1480.0625 - val_mae: 25.0254\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 485us/step - loss: 3305.8608 - mse: 3305.8608 - mae: 32.3667 - val_loss: 1471.5553 - val_mse: 1471.5553 - val_mae: 25.4762\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3284.9217 - mse: 3284.9214 - mae: 32.5415 - val_loss: 1474.8267 - val_mse: 1474.8269 - val_mae: 25.2248\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3269.8717 - mse: 3269.8716 - mae: 32.7689 - val_loss: 1474.6174 - val_mse: 1474.6174 - val_mae: 25.2500\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3352.3429 - mse: 3352.3428 - mae: 32.1571 - val_loss: 1476.6003 - val_mse: 1476.6002 - val_mae: 25.2067\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3243.3289 - mse: 3243.3293 - mae: 32.0404 - val_loss: 1471.8336 - val_mse: 1471.8335 - val_mae: 25.6275\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 670us/step - loss: 3237.6961 - mse: 3237.6960 - mae: 32.9370 - val_loss: 1473.9407 - val_mse: 1473.9406 - val_mae: 25.5106\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 691us/step - loss: 3421.5121 - mse: 3421.5120 - mae: 33.1197 - val_loss: 1475.9131 - val_mse: 1475.9130 - val_mae: 25.3517\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3337.0258 - mse: 3337.0269 - mae: 32.9123 - val_loss: 1479.3285 - val_mse: 1479.3285 - val_mae: 25.2037\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3251.6134 - mse: 3251.6133 - mae: 31.8981 - val_loss: 1478.3629 - val_mse: 1478.3628 - val_mae: 25.2706\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3138.8855 - mse: 3138.8853 - mae: 31.3163 - val_loss: 1475.4413 - val_mse: 1475.4413 - val_mae: 25.4947\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3171.9232 - mse: 3171.9233 - mae: 30.9076 - val_loss: 1474.0864 - val_mse: 1474.0864 - val_mae: 26.0699\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3285.2597 - mse: 3285.2603 - mae: 32.3725 - val_loss: 1473.6857 - val_mse: 1473.6858 - val_mae: 25.8751\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3277.3182 - mse: 3277.3188 - mae: 32.1751 - val_loss: 1474.6979 - val_mse: 1474.6980 - val_mae: 25.5799\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3368.8717 - mse: 3368.8718 - mae: 32.9547 - val_loss: 1477.9081 - val_mse: 1477.9081 - val_mae: 25.3731\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 681us/step - loss: 3252.2096 - mse: 3252.2100 - mae: 31.9712 - val_loss: 1479.9517 - val_mse: 1479.9515 - val_mae: 25.2445\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3311.5816 - mse: 3311.5820 - mae: 32.2872 - val_loss: 1478.5869 - val_mse: 1478.5870 - val_mae: 25.3144\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3242.2886 - mse: 3242.2878 - mae: 31.6470 - val_loss: 1477.9813 - val_mse: 1477.9813 - val_mae: 25.3498\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3284.8558 - mse: 3284.8555 - mae: 32.4062 - val_loss: 1477.3162 - val_mse: 1477.3162 - val_mae: 25.4440\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3288.3944 - mse: 3288.3943 - mae: 32.1485 - val_loss: 1476.8386 - val_mse: 1476.8387 - val_mae: 25.5175\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 690us/step - loss: 3322.2343 - mse: 3322.2339 - mae: 32.2828 - val_loss: 1476.9704 - val_mse: 1476.9702 - val_mae: 25.5044\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3243.7752 - mse: 3243.7749 - mae: 32.0969 - val_loss: 1476.0807 - val_mse: 1476.0806 - val_mae: 25.9916\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3271.0140 - mse: 3271.0142 - mae: 31.4643 - val_loss: 1476.6196 - val_mse: 1476.6196 - val_mae: 25.8758\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3363.8524 - mse: 3363.8528 - mae: 33.0087 - val_loss: 1481.6289 - val_mse: 1481.6290 - val_mae: 25.3548\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3232.0495 - mse: 3232.0498 - mae: 32.3521 - val_loss: 1480.5755 - val_mse: 1480.5756 - val_mae: 25.4264\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3234.8735 - mse: 3234.8740 - mae: 31.8858 - val_loss: 1486.4477 - val_mse: 1486.4478 - val_mae: 25.1920\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 683us/step - loss: 3330.1208 - mse: 3330.1204 - mae: 32.0219 - val_loss: 1480.6677 - val_mse: 1480.6677 - val_mae: 25.4923\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3276.5106 - mse: 3276.5115 - mae: 31.8632 - val_loss: 1477.7974 - val_mse: 1477.7975 - val_mae: 25.8140\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3240.7651 - mse: 3240.7646 - mae: 32.1076 - val_loss: 1478.7774 - val_mse: 1478.7775 - val_mae: 25.7559\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3339.0407 - mse: 3339.0410 - mae: 32.2617 - val_loss: 1480.9545 - val_mse: 1480.9543 - val_mae: 25.5222\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3326.7193 - mse: 3326.7195 - mae: 31.9253 - val_loss: 1480.6082 - val_mse: 1480.6082 - val_mae: 25.5803\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3339.3692 - mse: 3339.3689 - mae: 32.6284 - val_loss: 1482.4006 - val_mse: 1482.4004 - val_mae: 25.3976\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3294.6081 - mse: 3294.6091 - mae: 32.0658 - val_loss: 1482.0631 - val_mse: 1482.0631 - val_mae: 25.4480\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3259.8786 - mse: 3259.8782 - mae: 32.3175 - val_loss: 1481.4121 - val_mse: 1481.4121 - val_mae: 25.5577\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3263.9673 - mse: 3263.9675 - mae: 31.4514 - val_loss: 1482.8244 - val_mse: 1482.8242 - val_mae: 25.4913\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3261.0535 - mse: 3261.0535 - mae: 31.2007 - val_loss: 1479.8744 - val_mse: 1479.8744 - val_mae: 25.8130\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3302.5511 - mse: 3302.5505 - mae: 31.7620 - val_loss: 1482.2480 - val_mse: 1482.2482 - val_mae: 25.6142\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 678us/step - loss: 3238.0774 - mse: 3238.0776 - mae: 31.7768 - val_loss: 1482.3003 - val_mse: 1482.3004 - val_mae: 25.5847\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3191.3934 - mse: 3191.3931 - mae: 31.6413 - val_loss: 1481.5911 - val_mse: 1481.5909 - val_mae: 25.7296\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3301.8257 - mse: 3301.8254 - mae: 32.1301 - val_loss: 1482.1867 - val_mse: 1482.1865 - val_mae: 25.6764\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3228.0775 - mse: 3228.0771 - mae: 32.6447 - val_loss: 1485.1080 - val_mse: 1485.1079 - val_mae: 25.5423\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3270.7036 - mse: 3270.7041 - mae: 31.8470 - val_loss: 1489.6816 - val_mse: 1489.6815 - val_mae: 25.3080\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3295.1398 - mse: 3295.1392 - mae: 32.0491 - val_loss: 1492.1207 - val_mse: 1492.1206 - val_mae: 25.2201\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 475us/step - loss: 3295.2219 - mse: 3295.2224 - mae: 31.2256 - val_loss: 1489.2412 - val_mse: 1489.2411 - val_mae: 25.3267\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3270.4232 - mse: 3270.4233 - mae: 31.7476 - val_loss: 1486.8342 - val_mse: 1486.8342 - val_mae: 25.4472\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 653us/step - loss: 3179.4757 - mse: 3179.4758 - mae: 31.9194 - val_loss: 1487.3875 - val_mse: 1487.3876 - val_mae: 25.3709\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3314.0383 - mse: 3314.0374 - mae: 32.2874 - val_loss: 1488.3657 - val_mse: 1488.3658 - val_mae: 25.3968\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3085.0600 - mse: 3085.0605 - mae: 30.4378 - val_loss: 1486.8786 - val_mse: 1486.8787 - val_mae: 25.4920\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3268.1933 - mse: 3268.1934 - mae: 31.2730 - val_loss: 1485.6883 - val_mse: 1485.6884 - val_mae: 25.4881\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3419.9800 - mse: 3419.9795 - mae: 32.5704 - val_loss: 1488.2276 - val_mse: 1488.2274 - val_mae: 25.3499\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3214.2774 - mse: 3214.2778 - mae: 32.0509 - val_loss: 1484.5952 - val_mse: 1484.5952 - val_mae: 25.6446\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3343.4700 - mse: 3343.4709 - mae: 32.6042 - val_loss: 1486.4263 - val_mse: 1486.4261 - val_mae: 25.4834\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3196.0681 - mse: 3196.0679 - mae: 31.0800 - val_loss: 1485.0752 - val_mse: 1485.0751 - val_mae: 25.6326\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3245.1399 - mse: 3245.1404 - mae: 31.5915 - val_loss: 1485.7164 - val_mse: 1485.7164 - val_mae: 25.4730\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3337.3567 - mse: 3337.3569 - mae: 31.9950 - val_loss: 1483.9874 - val_mse: 1483.9877 - val_mae: 25.6432\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3243.3605 - mse: 3243.3601 - mae: 31.5982 - val_loss: 1482.3983 - val_mse: 1482.3982 - val_mae: 25.8671\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3245.0916 - mse: 3245.0920 - mae: 31.3342 - val_loss: 1483.0853 - val_mse: 1483.0852 - val_mae: 25.7157\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3135.7437 - mse: 3135.7432 - mae: 31.2638 - val_loss: 1484.0436 - val_mse: 1484.0436 - val_mae: 25.6756\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2906.6727 - mse: 2906.6733 - mae: 31.5457 - val_loss: 1083.5254 - val_mse: 1083.5253 - val_mae: 24.1955\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2961.5696 - mse: 2961.5696 - mae: 31.7832 - val_loss: 1082.4216 - val_mse: 1082.4215 - val_mae: 24.3162\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2932.9078 - mse: 2932.9082 - mae: 31.1992 - val_loss: 1081.8116 - val_mse: 1081.8116 - val_mae: 24.2438\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2944.3404 - mse: 2944.3408 - mae: 30.9450 - val_loss: 1082.6021 - val_mse: 1082.6021 - val_mae: 24.0655\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2888.0898 - mse: 2888.0901 - mae: 30.7995 - val_loss: 1080.4569 - val_mse: 1080.4569 - val_mae: 24.1871\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2918.3901 - mse: 2918.3899 - mae: 30.8880 - val_loss: 1079.7301 - val_mse: 1079.7301 - val_mae: 24.1702\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2879.7049 - mse: 2879.7053 - mae: 30.8811 - val_loss: 1083.9255 - val_mse: 1083.9253 - val_mae: 23.8742\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2940.8269 - mse: 2940.8274 - mae: 30.8473 - val_loss: 1078.5463 - val_mse: 1078.5464 - val_mae: 24.1644\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2879.6501 - mse: 2879.6494 - mae: 31.2629 - val_loss: 1079.0341 - val_mse: 1079.0342 - val_mae: 24.0519\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2859.8999 - mse: 2859.8999 - mae: 30.5421 - val_loss: 1078.2733 - val_mse: 1078.2734 - val_mae: 24.1032\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2876.1801 - mse: 2876.1807 - mae: 31.1882 - val_loss: 1078.1080 - val_mse: 1078.1080 - val_mae: 24.0529\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2859.3488 - mse: 2859.3491 - mae: 30.8991 - val_loss: 1076.3063 - val_mse: 1076.3064 - val_mae: 24.1642\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2909.3290 - mse: 2909.3286 - mae: 30.7013 - val_loss: 1076.8187 - val_mse: 1076.8187 - val_mae: 24.0620\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2931.5368 - mse: 2931.5364 - mae: 31.2096 - val_loss: 1073.5266 - val_mse: 1073.5266 - val_mae: 24.6085\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2835.7449 - mse: 2835.7446 - mae: 30.7464 - val_loss: 1073.2243 - val_mse: 1073.2242 - val_mae: 24.3841\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2951.0759 - mse: 2951.0757 - mae: 31.2863 - val_loss: 1073.1207 - val_mse: 1073.1207 - val_mae: 24.2853\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2853.8119 - mse: 2853.8125 - mae: 30.9093 - val_loss: 1072.6876 - val_mse: 1072.6874 - val_mae: 24.1616\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2896.1693 - mse: 2896.1697 - mae: 30.5735 - val_loss: 1072.0608 - val_mse: 1072.0608 - val_mae: 24.1551\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2922.0321 - mse: 2922.0322 - mae: 30.8380 - val_loss: 1072.6204 - val_mse: 1072.6202 - val_mae: 24.0674\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 634us/step - loss: 2892.8376 - mse: 2892.8381 - mae: 30.3188 - val_loss: 1069.9900 - val_mse: 1069.9901 - val_mae: 24.2363\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2825.7038 - mse: 2825.7036 - mae: 30.8399 - val_loss: 1069.2598 - val_mse: 1069.2599 - val_mae: 24.2712\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2842.1779 - mse: 2842.1768 - mae: 30.6781 - val_loss: 1068.7863 - val_mse: 1068.7864 - val_mae: 24.3154\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2862.6422 - mse: 2862.6423 - mae: 30.8663 - val_loss: 1069.8824 - val_mse: 1069.8826 - val_mae: 24.0596\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2891.6032 - mse: 2891.6038 - mae: 30.7308 - val_loss: 1068.5124 - val_mse: 1068.5125 - val_mae: 24.1455\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2787.4391 - mse: 2787.4390 - mae: 31.3377 - val_loss: 1072.1344 - val_mse: 1072.1345 - val_mae: 23.8272\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2914.3798 - mse: 2914.3789 - mae: 30.5953 - val_loss: 1067.5038 - val_mse: 1067.5038 - val_mae: 24.4947\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2905.1291 - mse: 2905.1296 - mae: 31.1495 - val_loss: 1066.9515 - val_mse: 1066.9515 - val_mae: 24.6008\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2866.7694 - mse: 2866.7695 - mae: 30.9968 - val_loss: 1066.7679 - val_mse: 1066.7678 - val_mae: 24.4213\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2907.3239 - mse: 2907.3240 - mae: 30.9434 - val_loss: 1070.2974 - val_mse: 1070.2974 - val_mae: 23.9158\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 666us/step - loss: 2858.8105 - mse: 2858.8098 - mae: 30.6746 - val_loss: 1067.5489 - val_mse: 1067.5488 - val_mae: 24.1572\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 688us/step - loss: 2901.4702 - mse: 2901.4695 - mae: 30.8254 - val_loss: 1065.8810 - val_mse: 1065.8810 - val_mae: 24.3990\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2926.0672 - mse: 2926.0671 - mae: 31.4796 - val_loss: 1071.7373 - val_mse: 1071.7372 - val_mae: 23.7859\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2833.8505 - mse: 2833.8506 - mae: 30.0059 - val_loss: 1066.5041 - val_mse: 1066.5042 - val_mae: 24.1911\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2908.6285 - mse: 2908.6292 - mae: 31.3251 - val_loss: 1065.6629 - val_mse: 1065.6630 - val_mae: 24.4270\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2877.4617 - mse: 2877.4622 - mae: 30.4086 - val_loss: 1065.5623 - val_mse: 1065.5624 - val_mae: 24.4477\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 673us/step - loss: 2838.4427 - mse: 2838.4431 - mae: 30.4135 - val_loss: 1065.8944 - val_mse: 1065.8943 - val_mae: 24.5399\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2844.5846 - mse: 2844.5845 - mae: 31.5816 - val_loss: 1070.1636 - val_mse: 1070.1636 - val_mae: 23.9017\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2847.2100 - mse: 2847.2104 - mae: 30.6639 - val_loss: 1067.4839 - val_mse: 1067.4839 - val_mae: 24.1335\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2918.8770 - mse: 2918.8777 - mae: 31.0158 - val_loss: 1066.6702 - val_mse: 1066.6700 - val_mae: 24.2609\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2844.1699 - mse: 2844.1699 - mae: 30.5904 - val_loss: 1066.5564 - val_mse: 1066.5564 - val_mae: 24.3000\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2893.8062 - mse: 2893.8057 - mae: 31.1137 - val_loss: 1066.1393 - val_mse: 1066.1393 - val_mae: 24.2402\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2856.2551 - mse: 2856.2556 - mae: 30.6551 - val_loss: 1067.1114 - val_mse: 1067.1113 - val_mae: 24.0491\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2855.1246 - mse: 2855.1252 - mae: 30.9285 - val_loss: 1065.5668 - val_mse: 1065.5669 - val_mae: 24.1460\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2875.2019 - mse: 2875.2019 - mae: 30.7667 - val_loss: 1064.1068 - val_mse: 1064.1068 - val_mae: 24.2977\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 690us/step - loss: 2860.4719 - mse: 2860.4719 - mae: 30.3644 - val_loss: 1063.6384 - val_mse: 1063.6385 - val_mae: 24.3761\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2918.1148 - mse: 2918.1150 - mae: 31.3120 - val_loss: 1066.1583 - val_mse: 1066.1583 - val_mae: 23.9196\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2813.8950 - mse: 2813.8948 - mae: 30.4619 - val_loss: 1062.6148 - val_mse: 1062.6149 - val_mae: 24.2993\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2881.2840 - mse: 2881.2837 - mae: 30.6097 - val_loss: 1063.6867 - val_mse: 1063.6868 - val_mae: 24.0549\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 660us/step - loss: 2888.7745 - mse: 2888.7744 - mae: 30.8170 - val_loss: 1062.4155 - val_mse: 1062.4155 - val_mae: 24.2157\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2787.3368 - mse: 2787.3372 - mae: 30.3795 - val_loss: 1061.6312 - val_mse: 1061.6312 - val_mae: 24.3594\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2794.0783 - mse: 2794.0779 - mae: 30.4785 - val_loss: 1063.5562 - val_mse: 1063.5564 - val_mae: 24.0179\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2871.5253 - mse: 2871.5249 - mae: 30.3653 - val_loss: 1061.7257 - val_mse: 1061.7256 - val_mae: 24.2859\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2791.9784 - mse: 2791.9783 - mae: 30.1596 - val_loss: 1060.5144 - val_mse: 1060.5143 - val_mae: 24.3969\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 649us/step - loss: 2703.4560 - mse: 2703.4565 - mae: 30.2703 - val_loss: 1060.0068 - val_mse: 1060.0067 - val_mae: 24.6069\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2905.7497 - mse: 2905.7490 - mae: 31.0397 - val_loss: 1059.6186 - val_mse: 1059.6185 - val_mae: 24.5086\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2812.7112 - mse: 2812.7109 - mae: 30.1883 - val_loss: 1058.5256 - val_mse: 1058.5256 - val_mae: 24.6234\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2824.2869 - mse: 2824.2876 - mae: 30.3407 - val_loss: 1058.3659 - val_mse: 1058.3658 - val_mae: 24.1632\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2817.3745 - mse: 2817.3745 - mae: 30.3836 - val_loss: 1057.9154 - val_mse: 1057.9153 - val_mae: 24.4523\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2865.4974 - mse: 2865.4971 - mae: 30.7976 - val_loss: 1059.3850 - val_mse: 1059.3849 - val_mae: 24.1401\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2803.6115 - mse: 2803.6111 - mae: 30.3458 - val_loss: 1058.0243 - val_mse: 1058.0243 - val_mae: 23.9600\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2783.6963 - mse: 2783.6963 - mae: 30.1177 - val_loss: 1056.2702 - val_mse: 1056.2701 - val_mae: 24.1141\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2887.1086 - mse: 2887.1086 - mae: 30.7908 - val_loss: 1057.4073 - val_mse: 1057.4073 - val_mae: 23.9765\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2796.6252 - mse: 2796.6255 - mae: 30.1206 - val_loss: 1056.4342 - val_mse: 1056.4342 - val_mae: 24.0647\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2812.7308 - mse: 2812.7317 - mae: 30.3954 - val_loss: 1056.0941 - val_mse: 1056.0941 - val_mae: 24.2241\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2878.8742 - mse: 2878.8755 - mae: 30.7303 - val_loss: 1056.1441 - val_mse: 1056.1442 - val_mae: 24.0975\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2797.7993 - mse: 2797.7993 - mae: 30.6301 - val_loss: 1055.4335 - val_mse: 1055.4335 - val_mae: 24.1055\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2832.9125 - mse: 2832.9128 - mae: 30.4189 - val_loss: 1057.6903 - val_mse: 1057.6903 - val_mae: 23.9047\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2907.6056 - mse: 2907.6050 - mae: 30.6934 - val_loss: 1055.6056 - val_mse: 1055.6057 - val_mae: 24.4079\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2880.9723 - mse: 2880.9724 - mae: 30.3259 - val_loss: 1056.4052 - val_mse: 1056.4052 - val_mae: 24.0965\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 651us/step - loss: 2812.8026 - mse: 2812.8020 - mae: 30.3782 - val_loss: 1054.9492 - val_mse: 1054.9492 - val_mae: 24.2762\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2840.9100 - mse: 2840.9099 - mae: 30.4986 - val_loss: 1058.4446 - val_mse: 1058.4445 - val_mae: 23.7451\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2790.1597 - mse: 2790.1592 - mae: 30.0824 - val_loss: 1055.4466 - val_mse: 1055.4465 - val_mae: 24.1250\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2752.6993 - mse: 2752.6995 - mae: 30.2718 - val_loss: 1055.6897 - val_mse: 1055.6895 - val_mae: 24.2598\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2900.8681 - mse: 2900.8674 - mae: 30.9188 - val_loss: 1055.1524 - val_mse: 1055.1523 - val_mae: 24.2074\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2837.2899 - mse: 2837.2896 - mae: 30.6364 - val_loss: 1054.5147 - val_mse: 1054.5145 - val_mae: 24.0179\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2820.6839 - mse: 2820.6836 - mae: 30.4746 - val_loss: 1054.3515 - val_mse: 1054.3516 - val_mae: 23.9623\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 549us/step - loss: 2849.4101 - mse: 2849.4094 - mae: 30.7067 - val_loss: 1053.9340 - val_mse: 1053.9338 - val_mae: 24.0317\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2841.9591 - mse: 2841.9592 - mae: 30.5091 - val_loss: 1054.2713 - val_mse: 1054.2711 - val_mae: 23.9597\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2877.0986 - mse: 2877.0974 - mae: 30.2321 - val_loss: 1054.5534 - val_mse: 1054.5535 - val_mae: 23.9734\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2788.9184 - mse: 2788.9182 - mae: 29.7798 - val_loss: 1054.2895 - val_mse: 1054.2894 - val_mae: 23.9105\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2579.4858 - mse: 2579.4858 - mae: 30.1381 - val_loss: 1563.4413 - val_mse: 1563.4414 - val_mae: 26.4957\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2490.6829 - mse: 2490.6831 - mae: 29.5411 - val_loss: 1538.8416 - val_mse: 1538.8416 - val_mae: 26.8249\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2509.8602 - mse: 2509.8599 - mae: 29.3620 - val_loss: 1540.8349 - val_mse: 1540.8351 - val_mae: 26.7510\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2470.2155 - mse: 2470.2151 - mae: 29.7726 - val_loss: 1543.5347 - val_mse: 1543.5347 - val_mae: 26.7113\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2489.2667 - mse: 2489.2664 - mae: 29.5374 - val_loss: 1540.8475 - val_mse: 1540.8475 - val_mae: 26.7292\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2536.5234 - mse: 2536.5232 - mae: 29.7451 - val_loss: 1551.4842 - val_mse: 1551.4841 - val_mae: 26.5648\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2497.8276 - mse: 2497.8274 - mae: 29.7497 - val_loss: 1535.3411 - val_mse: 1535.3411 - val_mae: 26.7576\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2605.3277 - mse: 2605.3274 - mae: 30.2884 - val_loss: 1545.1693 - val_mse: 1545.1693 - val_mae: 26.6154\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2477.6510 - mse: 2477.6506 - mae: 29.5683 - val_loss: 1530.1880 - val_mse: 1530.1881 - val_mae: 26.8024\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2466.0237 - mse: 2466.0239 - mae: 29.4617 - val_loss: 1540.4209 - val_mse: 1540.4209 - val_mae: 26.6109\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2541.2594 - mse: 2541.2598 - mae: 29.6362 - val_loss: 1536.9558 - val_mse: 1536.9558 - val_mae: 26.6194\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 558us/step - loss: 2556.0346 - mse: 2556.0347 - mae: 29.5888 - val_loss: 1547.2186 - val_mse: 1547.2188 - val_mae: 26.4860\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2561.8575 - mse: 2561.8572 - mae: 29.6460 - val_loss: 1553.0162 - val_mse: 1553.0164 - val_mae: 26.4187\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2547.4547 - mse: 2547.4531 - mae: 29.8147 - val_loss: 1535.2593 - val_mse: 1535.2593 - val_mae: 26.5893\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2490.3382 - mse: 2490.3389 - mae: 29.7013 - val_loss: 1534.1900 - val_mse: 1534.1901 - val_mae: 26.5954\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2430.6293 - mse: 2430.6299 - mae: 29.2347 - val_loss: 1529.5954 - val_mse: 1529.5955 - val_mae: 26.6502\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2534.6766 - mse: 2534.6770 - mae: 30.0575 - val_loss: 1541.0851 - val_mse: 1541.0850 - val_mae: 26.5014\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2545.7087 - mse: 2545.7097 - mae: 30.2007 - val_loss: 1545.5507 - val_mse: 1545.5509 - val_mae: 26.4502\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2496.6300 - mse: 2496.6284 - mae: 29.4345 - val_loss: 1534.2069 - val_mse: 1534.2068 - val_mae: 26.5607\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2513.6783 - mse: 2513.6770 - mae: 29.6302 - val_loss: 1538.0000 - val_mse: 1538.0002 - val_mae: 26.4988\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2485.2122 - mse: 2485.2117 - mae: 29.5895 - val_loss: 1537.0010 - val_mse: 1537.0010 - val_mae: 26.4615\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2518.8131 - mse: 2518.8135 - mae: 29.5156 - val_loss: 1538.1757 - val_mse: 1538.1758 - val_mae: 26.4652\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 523us/step - loss: 2534.4229 - mse: 2534.4233 - mae: 29.5671 - val_loss: 1545.3902 - val_mse: 1545.3900 - val_mae: 26.3499\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2519.7666 - mse: 2519.7664 - mae: 29.6948 - val_loss: 1537.5689 - val_mse: 1537.5688 - val_mae: 26.4341\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 462us/step - loss: 2527.6052 - mse: 2527.6052 - mae: 29.6922 - val_loss: 1535.5908 - val_mse: 1535.5908 - val_mae: 26.4169\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2490.6359 - mse: 2490.6357 - mae: 29.4835 - val_loss: 1536.4893 - val_mse: 1536.4896 - val_mae: 26.3645\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2466.5874 - mse: 2466.5869 - mae: 29.2890 - val_loss: 1528.7021 - val_mse: 1528.7021 - val_mae: 26.4534\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 615us/step - loss: 2403.4732 - mse: 2403.4722 - mae: 28.9210 - val_loss: 1530.2348 - val_mse: 1530.2349 - val_mae: 26.4176\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2517.8527 - mse: 2517.8525 - mae: 29.3034 - val_loss: 1529.3782 - val_mse: 1529.3782 - val_mae: 26.4102\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2449.5758 - mse: 2449.5754 - mae: 29.1948 - val_loss: 1522.2203 - val_mse: 1522.2201 - val_mae: 26.4923\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2513.7408 - mse: 2513.7405 - mae: 29.2158 - val_loss: 1507.5198 - val_mse: 1507.5199 - val_mae: 26.7050\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2520.9957 - mse: 2520.9951 - mae: 29.4549 - val_loss: 1519.9306 - val_mse: 1519.9307 - val_mae: 26.4766\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2513.3908 - mse: 2513.3896 - mae: 29.7342 - val_loss: 1523.3618 - val_mse: 1523.3618 - val_mae: 26.3878\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2555.0018 - mse: 2555.0027 - mae: 29.5260 - val_loss: 1538.6479 - val_mse: 1538.6478 - val_mae: 26.2724\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2521.0409 - mse: 2521.0410 - mae: 29.7532 - val_loss: 1546.7288 - val_mse: 1546.7285 - val_mae: 26.1837\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2425.4251 - mse: 2425.4258 - mae: 29.2001 - val_loss: 1532.1784 - val_mse: 1532.1785 - val_mae: 26.2744\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2543.2624 - mse: 2543.2615 - mae: 29.5757 - val_loss: 1532.1034 - val_mse: 1532.1033 - val_mae: 26.2636\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2448.6915 - mse: 2448.6914 - mae: 29.4093 - val_loss: 1522.1148 - val_mse: 1522.1147 - val_mae: 26.3427\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2511.7082 - mse: 2511.7083 - mae: 29.5098 - val_loss: 1518.5510 - val_mse: 1518.5510 - val_mae: 26.3677\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 682us/step - loss: 2422.4371 - mse: 2422.4365 - mae: 28.9541 - val_loss: 1500.9654 - val_mse: 1500.9653 - val_mae: 26.5677\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2556.4321 - mse: 2556.4316 - mae: 30.0045 - val_loss: 1518.7824 - val_mse: 1518.7823 - val_mae: 26.2997\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 508us/step - loss: 2513.9069 - mse: 2513.9062 - mae: 29.8112 - val_loss: 1516.3164 - val_mse: 1516.3164 - val_mae: 26.2954\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2464.0383 - mse: 2464.0376 - mae: 29.2957 - val_loss: 1514.9123 - val_mse: 1514.9124 - val_mae: 26.2617\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2497.8029 - mse: 2497.8030 - mae: 29.2351 - val_loss: 1498.3693 - val_mse: 1498.3695 - val_mae: 26.4141\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2479.2542 - mse: 2479.2534 - mae: 29.7932 - val_loss: 1510.5000 - val_mse: 1510.5000 - val_mae: 26.2480\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 2s 747us/step - loss: 2475.2293 - mse: 2475.2295 - mae: 29.1775 - val_loss: 1525.9905 - val_mse: 1525.9907 - val_mae: 26.0854\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2489.6380 - mse: 2489.6375 - mae: 29.2195 - val_loss: 1510.1649 - val_mse: 1510.1650 - val_mae: 26.2833\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2495.4620 - mse: 2495.4622 - mae: 29.7810 - val_loss: 1513.8521 - val_mse: 1513.8522 - val_mae: 26.2042\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2465.4519 - mse: 2465.4519 - mae: 29.1947 - val_loss: 1510.1933 - val_mse: 1510.1934 - val_mae: 26.2443\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 561us/step - loss: 2471.7612 - mse: 2471.7617 - mae: 29.1237 - val_loss: 1505.2516 - val_mse: 1505.2517 - val_mae: 26.3422\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2481.1311 - mse: 2481.1318 - mae: 29.5265 - val_loss: 1526.1555 - val_mse: 1526.1555 - val_mae: 26.0955\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2411.0441 - mse: 2411.0447 - mae: 29.0381 - val_loss: 1509.9859 - val_mse: 1509.9860 - val_mae: 26.2601\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2471.1971 - mse: 2471.1965 - mae: 29.4599 - val_loss: 1525.8532 - val_mse: 1525.8530 - val_mae: 26.0811\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2452.5293 - mse: 2452.5295 - mae: 28.8930 - val_loss: 1511.7809 - val_mse: 1511.7809 - val_mae: 26.2280\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2488.5251 - mse: 2488.5254 - mae: 29.6480 - val_loss: 1507.4461 - val_mse: 1507.4462 - val_mae: 26.2822\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2472.2147 - mse: 2472.2144 - mae: 29.3523 - val_loss: 1499.6646 - val_mse: 1499.6647 - val_mae: 26.3917\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 679us/step - loss: 2504.7876 - mse: 2504.7869 - mae: 29.6553 - val_loss: 1506.4230 - val_mse: 1506.4229 - val_mae: 26.2489\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 693us/step - loss: 2451.4649 - mse: 2451.4644 - mae: 28.8507 - val_loss: 1506.1794 - val_mse: 1506.1793 - val_mae: 26.2719\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2504.5545 - mse: 2504.5540 - mae: 29.1657 - val_loss: 1519.7877 - val_mse: 1519.7880 - val_mae: 26.1248\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2428.7589 - mse: 2428.7590 - mae: 29.0431 - val_loss: 1507.6796 - val_mse: 1507.6794 - val_mae: 26.2296\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2475.4628 - mse: 2475.4624 - mae: 29.1452 - val_loss: 1499.5411 - val_mse: 1499.5410 - val_mae: 26.3387\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2457.9952 - mse: 2457.9951 - mae: 28.9183 - val_loss: 1519.4122 - val_mse: 1519.4121 - val_mae: 26.0784\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2435.0960 - mse: 2435.0959 - mae: 28.9956 - val_loss: 1511.9028 - val_mse: 1511.9028 - val_mae: 26.1313\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2513.4059 - mse: 2513.4055 - mae: 29.4875 - val_loss: 1500.6315 - val_mse: 1500.6312 - val_mae: 26.2445\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2402.1696 - mse: 2402.1702 - mae: 28.4884 - val_loss: 1505.9628 - val_mse: 1505.9628 - val_mae: 26.2041\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 537us/step - loss: 2416.5766 - mse: 2416.5774 - mae: 28.5607 - val_loss: 1509.7513 - val_mse: 1509.7512 - val_mae: 26.1225\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2436.8341 - mse: 2436.8337 - mae: 28.7248 - val_loss: 1512.7805 - val_mse: 1512.7804 - val_mae: 26.0675\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2397.2347 - mse: 2397.2341 - mae: 28.8684 - val_loss: 1502.6001 - val_mse: 1502.6000 - val_mae: 26.1904\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2493.7644 - mse: 2493.7654 - mae: 29.4513 - val_loss: 1503.9611 - val_mse: 1503.9612 - val_mae: 26.1587\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2508.6860 - mse: 2508.6860 - mae: 29.5553 - val_loss: 1507.3289 - val_mse: 1507.3287 - val_mae: 26.1144\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 701us/step - loss: 2440.2926 - mse: 2440.2927 - mae: 29.2056 - val_loss: 1503.7304 - val_mse: 1503.7306 - val_mae: 26.1391\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2501.5717 - mse: 2501.5710 - mae: 29.2501 - val_loss: 1515.3046 - val_mse: 1515.3047 - val_mae: 25.9840\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2393.3995 - mse: 2393.3987 - mae: 28.8820 - val_loss: 1509.3481 - val_mse: 1509.3480 - val_mae: 26.0399\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2451.9258 - mse: 2451.9268 - mae: 28.7784 - val_loss: 1497.7327 - val_mse: 1497.7325 - val_mae: 26.1430\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2479.3441 - mse: 2479.3442 - mae: 28.8148 - val_loss: 1510.6120 - val_mse: 1510.6118 - val_mae: 25.9905\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 645us/step - loss: 2465.8573 - mse: 2465.8579 - mae: 29.1029 - val_loss: 1509.3558 - val_mse: 1509.3560 - val_mae: 25.9949\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2464.1916 - mse: 2464.1917 - mae: 28.8096 - val_loss: 1499.3152 - val_mse: 1499.3151 - val_mae: 25.9899\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2451.1614 - mse: 2451.1619 - mae: 29.0363 - val_loss: 1509.6349 - val_mse: 1509.6349 - val_mae: 25.8994\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2338.1327 - mse: 2338.1316 - mae: 28.8584 - val_loss: 1489.8768 - val_mse: 1489.8766 - val_mae: 26.1012\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2423.6395 - mse: 2423.6394 - mae: 29.1386 - val_loss: 1504.1550 - val_mse: 1504.1552 - val_mae: 25.9673\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2381.8595 - mse: 2381.8594 - mae: 29.4311 - val_loss: 3643.0611 - val_mse: 3643.0613 - val_mae: 22.9567\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 776us/step - loss: 2366.0396 - mse: 2366.0393 - mae: 29.3034 - val_loss: 3643.8561 - val_mse: 3643.8564 - val_mae: 22.9899\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2352.1455 - mse: 2352.1450 - mae: 29.7092 - val_loss: 3643.4558 - val_mse: 3643.4551 - val_mae: 23.3215\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2321.5038 - mse: 2321.5037 - mae: 29.3328 - val_loss: 3643.6083 - val_mse: 3643.6084 - val_mae: 22.9933\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2393.5330 - mse: 2393.5327 - mae: 29.7292 - val_loss: 3644.3607 - val_mse: 3644.3601 - val_mae: 22.5391\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2428.7129 - mse: 2428.7124 - mae: 29.8289 - val_loss: 3644.1924 - val_mse: 3644.1921 - val_mae: 22.6770\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2354.6064 - mse: 2354.6067 - mae: 29.1978 - val_loss: 3643.8859 - val_mse: 3643.8848 - val_mae: 22.6794\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2387.0962 - mse: 2387.0959 - mae: 29.3878 - val_loss: 3642.4316 - val_mse: 3642.4316 - val_mae: 22.8904\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2318.9137 - mse: 2318.9136 - mae: 29.0615 - val_loss: 3644.5080 - val_mse: 3644.5088 - val_mae: 22.6653\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2335.9905 - mse: 2335.9907 - mae: 29.3244 - val_loss: 3642.9312 - val_mse: 3642.9314 - val_mae: 23.0382\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2328.1494 - mse: 2328.1487 - mae: 29.3947 - val_loss: 3646.0712 - val_mse: 3646.0713 - val_mae: 22.5421\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2320.0407 - mse: 2320.0400 - mae: 28.8499 - val_loss: 3643.0402 - val_mse: 3643.0391 - val_mae: 22.8586\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2352.3930 - mse: 2352.3928 - mae: 29.3257 - val_loss: 3642.1024 - val_mse: 3642.1023 - val_mae: 22.9124\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 658us/step - loss: 2330.4971 - mse: 2330.4976 - mae: 29.2932 - val_loss: 3642.2988 - val_mse: 3642.2986 - val_mae: 23.0672\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2378.5362 - mse: 2378.5361 - mae: 29.4248 - val_loss: 3642.2692 - val_mse: 3642.2690 - val_mae: 22.8783\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2342.6163 - mse: 2342.6162 - mae: 29.4828 - val_loss: 3642.4120 - val_mse: 3642.4114 - val_mae: 22.9356\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2317.0755 - mse: 2317.0754 - mae: 29.3214 - val_loss: 3642.2121 - val_mse: 3642.2131 - val_mae: 23.2803\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2315.3523 - mse: 2315.3518 - mae: 28.8350 - val_loss: 3644.0355 - val_mse: 3644.0359 - val_mae: 23.3525\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2299.3724 - mse: 2299.3726 - mae: 29.1764 - val_loss: 3645.0384 - val_mse: 3645.0386 - val_mae: 22.8586\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 536us/step - loss: 2397.1209 - mse: 2397.1208 - mae: 29.1750 - val_loss: 3644.0168 - val_mse: 3644.0178 - val_mae: 23.1019\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 552us/step - loss: 2341.2582 - mse: 2341.2578 - mae: 29.0769 - val_loss: 3644.1354 - val_mse: 3644.1353 - val_mae: 23.0548\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2383.4901 - mse: 2383.4902 - mae: 29.2351 - val_loss: 3643.5929 - val_mse: 3643.5918 - val_mae: 22.7924\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2332.4645 - mse: 2332.4651 - mae: 28.9200 - val_loss: 3642.3984 - val_mse: 3642.3987 - val_mae: 23.2607\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2376.4069 - mse: 2376.4058 - mae: 29.3543 - val_loss: 3643.1721 - val_mse: 3643.1716 - val_mae: 22.8484\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2340.4223 - mse: 2340.4219 - mae: 29.2892 - val_loss: 3645.4184 - val_mse: 3645.4187 - val_mae: 22.6672\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2375.4634 - mse: 2375.4636 - mae: 29.2906 - val_loss: 3642.8601 - val_mse: 3642.8594 - val_mae: 22.8752\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2253.0029 - mse: 2253.0024 - mae: 28.7754 - val_loss: 3644.1715 - val_mse: 3644.1721 - val_mae: 22.7414\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2350.6355 - mse: 2350.6350 - mae: 29.3297 - val_loss: 3644.7590 - val_mse: 3644.7598 - val_mae: 22.7281\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2303.9138 - mse: 2303.9133 - mae: 28.7531 - val_loss: 3643.5121 - val_mse: 3643.5120 - val_mae: 23.3423\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2272.0744 - mse: 2272.0745 - mae: 28.8772 - val_loss: 3643.5419 - val_mse: 3643.5415 - val_mae: 23.3348\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2378.8751 - mse: 2378.8745 - mae: 29.4408 - val_loss: 3644.0653 - val_mse: 3644.0652 - val_mae: 23.1519\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2334.9161 - mse: 2334.9163 - mae: 28.8865 - val_loss: 3644.7855 - val_mse: 3644.7854 - val_mae: 23.1423\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2306.1750 - mse: 2306.1753 - mae: 29.0003 - val_loss: 3645.5826 - val_mse: 3645.5820 - val_mae: 22.9245\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2297.9874 - mse: 2297.9875 - mae: 29.0802 - val_loss: 3646.5827 - val_mse: 3646.5828 - val_mae: 22.8915\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 665us/step - loss: 2331.6366 - mse: 2331.6379 - mae: 29.2629 - val_loss: 3648.1666 - val_mse: 3648.1670 - val_mae: 22.6871\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2304.9800 - mse: 2304.9805 - mae: 28.4687 - val_loss: 3645.6271 - val_mse: 3645.6270 - val_mae: 23.2434\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2292.7825 - mse: 2292.7822 - mae: 28.8445 - val_loss: 3647.7151 - val_mse: 3647.7163 - val_mae: 22.6718\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 693us/step - loss: 2322.2466 - mse: 2322.2466 - mae: 29.0336 - val_loss: 3646.5353 - val_mse: 3646.5359 - val_mae: 23.3405\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2319.7034 - mse: 2319.7034 - mae: 29.3544 - val_loss: 3647.8966 - val_mse: 3647.8965 - val_mae: 23.2477\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 536us/step - loss: 2334.8855 - mse: 2334.8865 - mae: 29.0352 - val_loss: 3648.1864 - val_mse: 3648.1860 - val_mae: 23.1461\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2252.7144 - mse: 2252.7136 - mae: 28.7000 - val_loss: 3649.2194 - val_mse: 3649.2200 - val_mae: 23.2505\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2313.3179 - mse: 2313.3176 - mae: 29.1033 - val_loss: 3647.8780 - val_mse: 3647.8784 - val_mae: 22.9383\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2355.8482 - mse: 2355.8481 - mae: 29.1775 - val_loss: 3646.8189 - val_mse: 3646.8186 - val_mae: 22.7259\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 659us/step - loss: 2393.6691 - mse: 2393.6687 - mae: 29.2983 - val_loss: 3647.1068 - val_mse: 3647.1055 - val_mae: 22.8407\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 656us/step - loss: 2312.1892 - mse: 2312.1897 - mae: 28.8878 - val_loss: 3648.1065 - val_mse: 3648.1067 - val_mae: 22.7533\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2353.2547 - mse: 2353.2551 - mae: 29.3112 - val_loss: 3646.6247 - val_mse: 3646.6245 - val_mae: 23.0442\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2326.1801 - mse: 2326.1799 - mae: 29.2642 - val_loss: 3645.8836 - val_mse: 3645.8835 - val_mae: 23.2048\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2281.4855 - mse: 2281.4858 - mae: 28.8330 - val_loss: 3646.5376 - val_mse: 3646.5376 - val_mae: 23.3039\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2250.3798 - mse: 2250.3792 - mae: 28.6315 - val_loss: 3646.9986 - val_mse: 3646.9990 - val_mae: 23.3895\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2302.8793 - mse: 2302.8804 - mae: 28.9806 - val_loss: 3649.4657 - val_mse: 3649.4656 - val_mae: 22.7160\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2317.2002 - mse: 2317.2004 - mae: 29.0673 - val_loss: 3651.2929 - val_mse: 3651.2932 - val_mae: 22.5460\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 523us/step - loss: 2319.2987 - mse: 2319.2996 - mae: 28.8255 - val_loss: 3648.2877 - val_mse: 3648.2878 - val_mae: 23.7190\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2343.8168 - mse: 2343.8169 - mae: 29.2079 - val_loss: 3646.7177 - val_mse: 3646.7178 - val_mae: 23.1076\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2384.6683 - mse: 2384.6685 - mae: 29.3678 - val_loss: 3646.4378 - val_mse: 3646.4385 - val_mae: 23.1871\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2339.0733 - mse: 2339.0737 - mae: 29.1353 - val_loss: 3646.7937 - val_mse: 3646.7932 - val_mae: 23.0302\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2269.8259 - mse: 2269.8262 - mae: 28.8542 - val_loss: 3646.5427 - val_mse: 3646.5422 - val_mae: 23.0072\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 651us/step - loss: 2290.5025 - mse: 2290.5032 - mae: 28.7622 - val_loss: 3650.4618 - val_mse: 3650.4614 - val_mae: 22.7277\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2291.3774 - mse: 2291.3777 - mae: 28.7923 - val_loss: 3653.3296 - val_mse: 3653.3306 - val_mae: 22.9674\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2336.7313 - mse: 2336.7310 - mae: 29.2057 - val_loss: 3653.2168 - val_mse: 3653.2163 - val_mae: 23.4345\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2343.8362 - mse: 2343.8367 - mae: 29.3646 - val_loss: 3652.5453 - val_mse: 3652.5449 - val_mae: 23.7392\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2412.0268 - mse: 2412.0266 - mae: 29.3630 - val_loss: 3653.0611 - val_mse: 3653.0615 - val_mae: 22.9113\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2273.8761 - mse: 2273.8762 - mae: 28.5197 - val_loss: 3651.1273 - val_mse: 3651.1274 - val_mae: 23.3501\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2270.9566 - mse: 2270.9565 - mae: 28.9537 - val_loss: 3653.3097 - val_mse: 3653.3096 - val_mae: 22.9053\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 536us/step - loss: 2305.3039 - mse: 2305.3040 - mae: 28.7639 - val_loss: 3651.4080 - val_mse: 3651.4082 - val_mae: 23.6003\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2269.7959 - mse: 2269.7969 - mae: 28.8243 - val_loss: 3650.4609 - val_mse: 3650.4609 - val_mae: 23.2821\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2281.7162 - mse: 2281.7163 - mae: 28.4422 - val_loss: 3651.4544 - val_mse: 3651.4551 - val_mae: 23.0589\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 672us/step - loss: 2229.3134 - mse: 2229.3137 - mae: 28.6205 - val_loss: 3650.6910 - val_mse: 3650.6917 - val_mae: 23.5780\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2353.1441 - mse: 2353.1440 - mae: 29.2981 - val_loss: 3651.2364 - val_mse: 3651.2363 - val_mae: 23.2057\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2291.0122 - mse: 2291.0122 - mae: 28.5568 - val_loss: 3652.0887 - val_mse: 3652.0886 - val_mae: 22.7048\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2332.5095 - mse: 2332.5105 - mae: 28.6522 - val_loss: 3649.8885 - val_mse: 3649.8887 - val_mae: 23.3134\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 687us/step - loss: 2316.2177 - mse: 2316.2178 - mae: 28.9002 - val_loss: 3649.3997 - val_mse: 3649.3997 - val_mae: 23.1217\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2307.0202 - mse: 2307.0195 - mae: 28.9776 - val_loss: 3648.0877 - val_mse: 3648.0886 - val_mae: 23.1252\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2305.7006 - mse: 2305.7012 - mae: 28.8851 - val_loss: 3649.3041 - val_mse: 3649.3044 - val_mae: 23.4484\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 552us/step - loss: 2289.7229 - mse: 2289.7227 - mae: 28.6511 - val_loss: 3653.3092 - val_mse: 3653.3096 - val_mae: 22.6681\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 636us/step - loss: 2307.5111 - mse: 2307.5110 - mae: 28.7796 - val_loss: 3654.7272 - val_mse: 3654.7266 - val_mae: 22.6300\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2277.9993 - mse: 2277.9983 - mae: 28.6664 - val_loss: 3651.2429 - val_mse: 3651.2439 - val_mae: 23.2768\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2328.7856 - mse: 2328.7854 - mae: 29.1896 - val_loss: 3652.6787 - val_mse: 3652.6794 - val_mae: 23.1062\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2314.8565 - mse: 2314.8564 - mae: 28.9254 - val_loss: 3651.2209 - val_mse: 3651.2209 - val_mae: 23.4583\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2389.6794 - mse: 2389.6785 - mae: 29.3855 - val_loss: 3649.0023 - val_mse: 3649.0020 - val_mae: 23.4792\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2316.4959 - mse: 2316.4956 - mae: 29.1613 - val_loss: 3649.4092 - val_mse: 3649.4087 - val_mae: 23.1616\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 512us/step - loss: 2648.6562 - mse: 2648.6567 - mae: 28.2298 - val_loss: 2180.6567 - val_mse: 2180.6572 - val_mae: 25.7710\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2723.1737 - mse: 2723.1733 - mae: 28.5225 - val_loss: 2165.3867 - val_mse: 2165.3867 - val_mae: 26.2823\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2661.9661 - mse: 2661.9661 - mae: 28.0920 - val_loss: 2170.4214 - val_mse: 2170.4216 - val_mae: 26.2685\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 673us/step - loss: 2686.8887 - mse: 2686.8896 - mae: 28.4777 - val_loss: 2175.7601 - val_mse: 2175.7600 - val_mae: 26.1088\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2708.2734 - mse: 2708.2727 - mae: 28.4876 - val_loss: 2173.5652 - val_mse: 2173.5649 - val_mae: 26.2335\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2691.0169 - mse: 2691.0164 - mae: 28.0839 - val_loss: 2190.2629 - val_mse: 2190.2629 - val_mae: 25.7552\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 666us/step - loss: 2692.6844 - mse: 2692.6843 - mae: 28.4732 - val_loss: 2174.8131 - val_mse: 2174.8132 - val_mae: 26.1671\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 552us/step - loss: 2705.9621 - mse: 2705.9612 - mae: 28.1462 - val_loss: 2182.2656 - val_mse: 2182.2659 - val_mae: 25.9482\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2663.5454 - mse: 2663.5457 - mae: 28.2219 - val_loss: 2175.3129 - val_mse: 2175.3125 - val_mae: 26.0057\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2700.9545 - mse: 2700.9553 - mae: 28.3366 - val_loss: 2172.1179 - val_mse: 2172.1177 - val_mae: 26.0569\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2654.3460 - mse: 2654.3459 - mae: 28.0145 - val_loss: 2167.9741 - val_mse: 2167.9739 - val_mae: 25.9952\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2655.7709 - mse: 2655.7715 - mae: 28.4810 - val_loss: 2173.4225 - val_mse: 2173.4226 - val_mae: 25.8863\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2665.3723 - mse: 2665.3726 - mae: 28.0889 - val_loss: 2161.8773 - val_mse: 2161.8772 - val_mae: 26.4921\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 544us/step - loss: 2650.7671 - mse: 2650.7676 - mae: 28.0762 - val_loss: 2162.4313 - val_mse: 2162.4314 - val_mae: 26.1763\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 661us/step - loss: 2635.1956 - mse: 2635.1951 - mae: 28.1329 - val_loss: 2151.4839 - val_mse: 2151.4839 - val_mae: 26.3022\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2684.3472 - mse: 2684.3469 - mae: 28.1495 - val_loss: 2164.3010 - val_mse: 2164.3008 - val_mae: 26.1203\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2658.5023 - mse: 2658.5015 - mae: 28.1492 - val_loss: 2162.0313 - val_mse: 2162.0317 - val_mae: 26.2811\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2687.6872 - mse: 2687.6860 - mae: 28.3490 - val_loss: 2173.6912 - val_mse: 2173.6912 - val_mae: 26.1375\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 683us/step - loss: 2674.1220 - mse: 2674.1216 - mae: 28.0033 - val_loss: 2174.9612 - val_mse: 2174.9612 - val_mae: 26.1607\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 652us/step - loss: 2690.5283 - mse: 2690.5283 - mae: 28.0865 - val_loss: 2178.2485 - val_mse: 2178.2485 - val_mae: 26.1997\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2649.3788 - mse: 2649.3787 - mae: 28.3936 - val_loss: 2179.8085 - val_mse: 2179.8086 - val_mae: 25.9841\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2631.6691 - mse: 2631.6699 - mae: 27.6999 - val_loss: 2177.2615 - val_mse: 2177.2615 - val_mae: 26.0234\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2671.3745 - mse: 2671.3743 - mae: 28.0722 - val_loss: 2168.7740 - val_mse: 2168.7739 - val_mae: 26.5621\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2693.8633 - mse: 2693.8638 - mae: 28.5273 - val_loss: 2172.3973 - val_mse: 2172.3970 - val_mae: 26.3695\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 678us/step - loss: 2626.3638 - mse: 2626.3635 - mae: 28.0762 - val_loss: 2181.0910 - val_mse: 2181.0908 - val_mae: 25.9310\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2639.0931 - mse: 2639.0923 - mae: 27.9042 - val_loss: 2167.1033 - val_mse: 2167.1030 - val_mae: 26.2744\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2663.8437 - mse: 2663.8440 - mae: 27.9214 - val_loss: 2171.5668 - val_mse: 2171.5669 - val_mae: 26.0898\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2632.5895 - mse: 2632.5898 - mae: 28.1777 - val_loss: 2160.6414 - val_mse: 2160.6416 - val_mae: 26.1745\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2699.8830 - mse: 2699.8835 - mae: 28.1495 - val_loss: 2158.3617 - val_mse: 2158.3616 - val_mae: 26.1943\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2656.1386 - mse: 2656.1387 - mae: 27.8633 - val_loss: 2173.5914 - val_mse: 2173.5913 - val_mae: 25.9681\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2686.9975 - mse: 2686.9966 - mae: 28.1964 - val_loss: 2173.5568 - val_mse: 2173.5569 - val_mae: 25.7682\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2649.8578 - mse: 2649.8564 - mae: 28.0127 - val_loss: 2159.4253 - val_mse: 2159.4253 - val_mae: 26.2621\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 574us/step - loss: 2665.3201 - mse: 2665.3196 - mae: 28.3114 - val_loss: 2165.5191 - val_mse: 2165.5193 - val_mae: 26.0242\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2643.1761 - mse: 2643.1758 - mae: 28.0114 - val_loss: 2175.0912 - val_mse: 2175.0911 - val_mae: 25.7812\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2665.6523 - mse: 2665.6523 - mae: 28.3166 - val_loss: 2174.0444 - val_mse: 2174.0447 - val_mae: 26.2301\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2622.4041 - mse: 2622.4038 - mae: 28.1964 - val_loss: 2170.7744 - val_mse: 2170.7742 - val_mae: 26.1588\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2646.0653 - mse: 2646.0652 - mae: 27.9622 - val_loss: 2167.0576 - val_mse: 2167.0574 - val_mae: 26.2900\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2713.3607 - mse: 2713.3611 - mae: 28.5758 - val_loss: 2170.5111 - val_mse: 2170.5110 - val_mae: 26.3494\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2631.6273 - mse: 2631.6267 - mae: 28.0968 - val_loss: 2164.5298 - val_mse: 2164.5298 - val_mae: 26.2353\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 667us/step - loss: 2724.8139 - mse: 2724.8145 - mae: 28.4733 - val_loss: 2162.4183 - val_mse: 2162.4185 - val_mae: 26.2351\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2698.9100 - mse: 2698.9104 - mae: 28.1976 - val_loss: 2164.9669 - val_mse: 2164.9670 - val_mae: 26.1279\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2590.6265 - mse: 2590.6267 - mae: 27.8878 - val_loss: 2172.4629 - val_mse: 2172.4629 - val_mae: 26.1178\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2684.7242 - mse: 2684.7246 - mae: 28.1712 - val_loss: 2166.8845 - val_mse: 2166.8843 - val_mae: 26.2941\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2684.4549 - mse: 2684.4553 - mae: 28.6199 - val_loss: 2193.9527 - val_mse: 2193.9526 - val_mae: 25.7838\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2687.0163 - mse: 2687.0168 - mae: 28.0866 - val_loss: 2171.6951 - val_mse: 2171.6948 - val_mae: 26.3043\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2609.7504 - mse: 2609.7495 - mae: 28.0005 - val_loss: 2168.8942 - val_mse: 2168.8943 - val_mae: 26.2819\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2637.2818 - mse: 2637.2825 - mae: 28.1209 - val_loss: 2172.3837 - val_mse: 2172.3835 - val_mae: 26.2826\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2671.3631 - mse: 2671.3630 - mae: 28.2718 - val_loss: 2168.2784 - val_mse: 2168.2783 - val_mae: 26.3162\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2653.2889 - mse: 2653.2883 - mae: 28.3889 - val_loss: 2168.3036 - val_mse: 2168.3037 - val_mae: 26.1041\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 693us/step - loss: 2624.0535 - mse: 2624.0535 - mae: 27.6701 - val_loss: 2165.1620 - val_mse: 2165.1624 - val_mae: 26.2012\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2653.1330 - mse: 2653.1335 - mae: 28.2382 - val_loss: 2166.7488 - val_mse: 2166.7488 - val_mae: 26.1685\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2671.5958 - mse: 2671.5957 - mae: 28.2262 - val_loss: 2172.2486 - val_mse: 2172.2483 - val_mae: 26.2159\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2647.7054 - mse: 2647.7053 - mae: 28.1330 - val_loss: 2168.2490 - val_mse: 2168.2490 - val_mae: 26.2504\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 522us/step - loss: 2641.9376 - mse: 2641.9380 - mae: 27.9682 - val_loss: 2169.8968 - val_mse: 2169.8967 - val_mae: 26.2883\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2631.1386 - mse: 2631.1379 - mae: 28.0224 - val_loss: 2171.0113 - val_mse: 2171.0110 - val_mae: 26.1950\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2636.7583 - mse: 2636.7581 - mae: 27.8824 - val_loss: 2168.6925 - val_mse: 2168.6926 - val_mae: 26.0608\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2692.0392 - mse: 2692.0393 - mae: 28.0454 - val_loss: 2178.3426 - val_mse: 2178.3420 - val_mae: 25.9086\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2683.2646 - mse: 2683.2639 - mae: 28.0580 - val_loss: 2160.3166 - val_mse: 2160.3167 - val_mae: 26.3690\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2688.0421 - mse: 2688.0425 - mae: 28.2939 - val_loss: 2158.9674 - val_mse: 2158.9673 - val_mae: 26.1393\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 524us/step - loss: 2649.4684 - mse: 2649.4680 - mae: 28.0804 - val_loss: 2164.3743 - val_mse: 2164.3745 - val_mae: 26.2592\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2675.9114 - mse: 2675.9114 - mae: 28.1749 - val_loss: 2168.9782 - val_mse: 2168.9780 - val_mae: 26.1453\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2679.9554 - mse: 2679.9553 - mae: 28.0596 - val_loss: 2176.9157 - val_mse: 2176.9158 - val_mae: 25.9456\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 543us/step - loss: 2654.5922 - mse: 2654.5923 - mae: 27.9921 - val_loss: 2162.7358 - val_mse: 2162.7354 - val_mae: 26.3341\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2684.8630 - mse: 2684.8638 - mae: 28.6394 - val_loss: 2165.0343 - val_mse: 2165.0347 - val_mae: 26.1352\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2657.7390 - mse: 2657.7395 - mae: 28.0072 - val_loss: 2164.1412 - val_mse: 2164.1414 - val_mae: 26.1802\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2631.7211 - mse: 2631.7212 - mae: 27.9197 - val_loss: 2167.7627 - val_mse: 2167.7627 - val_mae: 26.1784\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2625.3671 - mse: 2625.3667 - mae: 28.2637 - val_loss: 2162.7614 - val_mse: 2162.7612 - val_mae: 26.2698\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 514us/step - loss: 2655.0164 - mse: 2655.0173 - mae: 27.9773 - val_loss: 2165.2830 - val_mse: 2165.2832 - val_mae: 26.2794\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2632.8586 - mse: 2632.8591 - mae: 27.8193 - val_loss: 2163.9212 - val_mse: 2163.9211 - val_mae: 26.2259\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2674.6790 - mse: 2674.6790 - mae: 27.9046 - val_loss: 2171.1296 - val_mse: 2171.1294 - val_mae: 26.1969\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2642.7358 - mse: 2642.7354 - mae: 28.0927 - val_loss: 2161.2679 - val_mse: 2161.2678 - val_mae: 26.2101\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2662.2946 - mse: 2662.2942 - mae: 28.2814 - val_loss: 2158.7435 - val_mse: 2158.7437 - val_mae: 26.4058\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2662.6632 - mse: 2662.6638 - mae: 28.2382 - val_loss: 2161.0684 - val_mse: 2161.0686 - val_mae: 26.1801\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2608.9128 - mse: 2608.9131 - mae: 28.1050 - val_loss: 2158.5750 - val_mse: 2158.5752 - val_mae: 26.1518\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2637.9606 - mse: 2637.9609 - mae: 27.9911 - val_loss: 2167.5092 - val_mse: 2167.5095 - val_mae: 25.9536\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2648.7541 - mse: 2648.7542 - mae: 27.9693 - val_loss: 2158.5375 - val_mse: 2158.5374 - val_mae: 26.2053\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2693.3338 - mse: 2693.3345 - mae: 28.2809 - val_loss: 2176.0246 - val_mse: 2176.0247 - val_mae: 25.9288\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2646.2712 - mse: 2646.2703 - mae: 28.1548 - val_loss: 2166.6131 - val_mse: 2166.6133 - val_mae: 26.1557\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2611.9405 - mse: 2611.9409 - mae: 28.0010 - val_loss: 2157.4712 - val_mse: 2157.4712 - val_mae: 26.3266\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2653.7192 - mse: 2653.7200 - mae: 27.7401 - val_loss: 2144.6670 - val_mse: 2144.6672 - val_mae: 26.3304\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 13316.7349 - mse: 13316.7354 - mae: 109.8478 - val_loss: 34575.2474 - val_mse: 34575.2461 - val_mae: 132.5738\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 643us/step - loss: 13130.2964 - mse: 13130.2949 - mae: 109.0037 - val_loss: 34181.5247 - val_mse: 34181.5234 - val_mae: 131.1049\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 12563.8686 - mse: 12563.8691 - mae: 106.3756 - val_loss: 33061.7718 - val_mse: 33061.7695 - val_mae: 126.8383\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 11279.9778 - mse: 11279.9805 - mae: 100.0511 - val_loss: 30330.3445 - val_mse: 30330.3438 - val_mae: 115.7807\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 542us/step - loss: 8536.6104 - mse: 8536.6113 - mae: 84.4112 - val_loss: 24473.2671 - val_mse: 24473.2676 - val_mae: 87.5081\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 675us/step - loss: 4302.8127 - mse: 4302.8125 - mae: 52.9165 - val_loss: 17726.2185 - val_mse: 17726.2188 - val_mae: 39.7941\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 620us/step - loss: 2632.4022 - mse: 2632.4021 - mae: 38.1481 - val_loss: 17009.5702 - val_mse: 17009.5703 - val_mae: 38.2990\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 695us/step - loss: 2822.3490 - mse: 2822.3491 - mae: 39.7012 - val_loss: 17395.5522 - val_mse: 17395.5508 - val_mae: 38.6634\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 684us/step - loss: 2819.5611 - mse: 2819.5610 - mae: 38.4866 - val_loss: 17259.9553 - val_mse: 17259.9551 - val_mae: 38.3294\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 677us/step - loss: 2730.6129 - mse: 2730.6130 - mae: 37.9417 - val_loss: 17135.6204 - val_mse: 17135.6211 - val_mae: 38.0938\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 673us/step - loss: 2621.1793 - mse: 2621.1797 - mae: 37.5496 - val_loss: 17205.9741 - val_mse: 17205.9746 - val_mae: 38.1141\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 730us/step - loss: 2769.9265 - mse: 2769.9265 - mae: 38.0677 - val_loss: 17267.8116 - val_mse: 17267.8125 - val_mae: 38.1284\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2796.8685 - mse: 2796.8684 - mae: 38.6843 - val_loss: 17244.3646 - val_mse: 17244.3633 - val_mae: 38.0226\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 693us/step - loss: 2731.6572 - mse: 2731.6570 - mae: 38.5314 - val_loss: 17200.5907 - val_mse: 17200.5898 - val_mae: 37.8892\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 601us/step - loss: 2606.8640 - mse: 2606.8640 - mae: 38.0753 - val_loss: 17330.2270 - val_mse: 17330.2285 - val_mae: 38.0531\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 2403.0259 - mse: 2403.0259 - mae: 36.1459 - val_loss: 17248.9629 - val_mse: 17248.9629 - val_mae: 37.8228\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 2664.0411 - mse: 2664.0415 - mae: 37.7503 - val_loss: 17234.5275 - val_mse: 17234.5273 - val_mae: 37.7545\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 447us/step - loss: 2751.4026 - mse: 2751.4026 - mae: 37.3369 - val_loss: 17184.6817 - val_mse: 17184.6816 - val_mae: 37.6529\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 460us/step - loss: 2605.4921 - mse: 2605.4917 - mae: 37.4616 - val_loss: 17400.8161 - val_mse: 17400.8164 - val_mae: 37.9766\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 529us/step - loss: 2566.3984 - mse: 2566.3984 - mae: 36.2308 - val_loss: 17154.6077 - val_mse: 17154.6074 - val_mae: 37.5356\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2302.7284 - mse: 2302.7285 - mae: 35.9941 - val_loss: 17232.1990 - val_mse: 17232.1992 - val_mae: 37.5392\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2560.7137 - mse: 2560.7139 - mae: 36.6834 - val_loss: 17250.2207 - val_mse: 17250.2207 - val_mae: 37.5164\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 522us/step - loss: 2450.0410 - mse: 2450.0413 - mae: 35.8249 - val_loss: 17297.0797 - val_mse: 17297.0781 - val_mae: 37.5417\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2323.4823 - mse: 2323.4822 - mae: 34.0553 - val_loss: 17058.1514 - val_mse: 17058.1504 - val_mae: 37.3453\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 686us/step - loss: 2528.0717 - mse: 2528.0715 - mae: 35.2924 - val_loss: 17229.0431 - val_mse: 17229.0449 - val_mae: 37.3759\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 710us/step - loss: 2368.8776 - mse: 2368.8777 - mae: 34.6569 - val_loss: 17143.9078 - val_mse: 17143.9082 - val_mae: 37.2676\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2300.2589 - mse: 2300.2588 - mae: 34.1046 - val_loss: 17180.2650 - val_mse: 17180.2656 - val_mae: 37.2512\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 2363.0974 - mse: 2363.0974 - mae: 34.8123 - val_loss: 17236.7064 - val_mse: 17236.7051 - val_mae: 37.2657\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 647us/step - loss: 2596.2896 - mse: 2596.2896 - mae: 36.2745 - val_loss: 17307.7069 - val_mse: 17307.7070 - val_mae: 37.3282\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 438us/step - loss: 2439.7707 - mse: 2439.7708 - mae: 35.3164 - val_loss: 17459.9086 - val_mse: 17459.9082 - val_mae: 37.5215\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 452us/step - loss: 2216.8274 - mse: 2216.8274 - mae: 33.4011 - val_loss: 17287.4989 - val_mse: 17287.5000 - val_mae: 37.2119\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 2228.6032 - mse: 2228.6030 - mae: 33.6461 - val_loss: 17164.7238 - val_mse: 17164.7246 - val_mae: 37.0510\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2273.8206 - mse: 2273.8206 - mae: 34.3533 - val_loss: 17327.7038 - val_mse: 17327.7051 - val_mae: 37.2301\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2200.5213 - mse: 2200.5212 - mae: 32.6196 - val_loss: 17180.0235 - val_mse: 17180.0215 - val_mae: 37.0091\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 694us/step - loss: 2246.8682 - mse: 2246.8682 - mae: 33.7503 - val_loss: 17086.8856 - val_mse: 17086.8867 - val_mae: 36.9449\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 2175.5489 - mse: 2175.5488 - mae: 33.0498 - val_loss: 17309.1094 - val_mse: 17309.1094 - val_mae: 37.1187\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 539us/step - loss: 2149.9914 - mse: 2149.9917 - mae: 31.5452 - val_loss: 17322.8455 - val_mse: 17322.8457 - val_mae: 37.1135\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 643us/step - loss: 2389.9979 - mse: 2389.9980 - mae: 33.7810 - val_loss: 17290.3172 - val_mse: 17290.3184 - val_mae: 37.0341\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 2209.3565 - mse: 2209.3564 - mae: 33.0678 - val_loss: 17274.1433 - val_mse: 17274.1426 - val_mae: 36.9793\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2232.3343 - mse: 2232.3342 - mae: 33.8043 - val_loss: 17335.4840 - val_mse: 17335.4844 - val_mae: 37.0225\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 658us/step - loss: 2306.5487 - mse: 2306.5488 - mae: 33.4300 - val_loss: 17197.9783 - val_mse: 17197.9785 - val_mae: 36.8293\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2115.6735 - mse: 2115.6736 - mae: 32.0995 - val_loss: 17252.9811 - val_mse: 17252.9805 - val_mae: 36.8678\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2029.6250 - mse: 2029.6250 - mae: 32.1825 - val_loss: 17106.7902 - val_mse: 17106.7891 - val_mae: 36.7360\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 698us/step - loss: 2319.0042 - mse: 2319.0039 - mae: 33.0929 - val_loss: 17194.2899 - val_mse: 17194.2891 - val_mae: 36.7464\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 551us/step - loss: 2267.4197 - mse: 2267.4199 - mae: 34.4007 - val_loss: 17369.4302 - val_mse: 17369.4316 - val_mae: 36.9477\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 2071.5234 - mse: 2071.5234 - mae: 31.5540 - val_loss: 17077.2015 - val_mse: 17077.1992 - val_mae: 36.6934\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 550us/step - loss: 2360.5221 - mse: 2360.5222 - mae: 33.6786 - val_loss: 17292.2784 - val_mse: 17292.2773 - val_mae: 36.8361\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 556us/step - loss: 2200.6304 - mse: 2200.6304 - mae: 33.1202 - val_loss: 17363.4143 - val_mse: 17363.4141 - val_mae: 36.9069\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 528us/step - loss: 2062.4544 - mse: 2062.4546 - mae: 31.8503 - val_loss: 17064.8683 - val_mse: 17064.8672 - val_mae: 36.6748\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 2230.7906 - mse: 2230.7905 - mae: 32.8394 - val_loss: 17266.7281 - val_mse: 17266.7285 - val_mae: 36.7172\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 472us/step - loss: 2189.8038 - mse: 2189.8037 - mae: 33.4806 - val_loss: 17416.5621 - val_mse: 17416.5625 - val_mae: 36.9117\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 2106.4830 - mse: 2106.4829 - mae: 31.6955 - val_loss: 17267.4748 - val_mse: 17267.4766 - val_mae: 36.6730\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 2115.6081 - mse: 2115.6079 - mae: 31.8860 - val_loss: 17228.2322 - val_mse: 17228.2324 - val_mae: 36.6042\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 592us/step - loss: 2129.1088 - mse: 2129.1084 - mae: 32.2634 - val_loss: 17262.0453 - val_mse: 17262.0449 - val_mae: 36.6195\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 1970.7547 - mse: 1970.7546 - mae: 31.0325 - val_loss: 17189.5505 - val_mse: 17189.5527 - val_mae: 36.5458\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 1985.2348 - mse: 1985.2347 - mae: 30.1060 - val_loss: 17173.3404 - val_mse: 17173.3398 - val_mae: 36.5305\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 525us/step - loss: 2093.6181 - mse: 2093.6182 - mae: 30.8742 - val_loss: 17297.2496 - val_mse: 17297.2480 - val_mae: 36.6110\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 530us/step - loss: 1874.5225 - mse: 1874.5225 - mae: 30.7059 - val_loss: 17253.8779 - val_mse: 17253.8789 - val_mae: 36.5624\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 1957.7544 - mse: 1957.7543 - mae: 31.4035 - val_loss: 17449.9108 - val_mse: 17449.9121 - val_mae: 36.8606\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 544us/step - loss: 1991.0071 - mse: 1991.0070 - mae: 31.5301 - val_loss: 17224.5655 - val_mse: 17224.5664 - val_mae: 36.5273\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 587us/step - loss: 1966.4348 - mse: 1966.4347 - mae: 31.0685 - val_loss: 17312.7319 - val_mse: 17312.7324 - val_mae: 36.5852\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 1959.5901 - mse: 1959.5901 - mae: 30.8973 - val_loss: 17171.6924 - val_mse: 17171.6914 - val_mae: 36.4895\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 462us/step - loss: 2128.1979 - mse: 2128.1975 - mae: 31.1287 - val_loss: 17328.2397 - val_mse: 17328.2383 - val_mae: 36.5779\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 1984.0764 - mse: 1984.0763 - mae: 31.1069 - val_loss: 17213.6533 - val_mse: 17213.6543 - val_mae: 36.4998\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 1885.1028 - mse: 1885.1028 - mae: 30.7179 - val_loss: 17165.6488 - val_mse: 17165.6504 - val_mae: 36.4881\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 439us/step - loss: 1769.3443 - mse: 1769.3445 - mae: 29.0455 - val_loss: 17195.3273 - val_mse: 17195.3281 - val_mae: 36.4834\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 490us/step - loss: 2060.1573 - mse: 2060.1572 - mae: 31.4951 - val_loss: 17241.4736 - val_mse: 17241.4746 - val_mae: 36.5163\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 1923.4737 - mse: 1923.4740 - mae: 29.8801 - val_loss: 17399.1118 - val_mse: 17399.1113 - val_mae: 36.6890\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 332us/step - loss: 2044.7867 - mse: 2044.7866 - mae: 31.4764 - val_loss: 17442.6420 - val_mse: 17442.6426 - val_mae: 36.7712\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 404us/step - loss: 1782.5020 - mse: 1782.5020 - mae: 29.1159 - val_loss: 17219.9685 - val_mse: 17219.9688 - val_mae: 36.4967\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 1768.5673 - mse: 1768.5675 - mae: 29.3762 - val_loss: 17229.9641 - val_mse: 17229.9648 - val_mae: 36.4968\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 1765.1717 - mse: 1765.1718 - mae: 30.3761 - val_loss: 17214.8238 - val_mse: 17214.8242 - val_mae: 36.4938\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 446us/step - loss: 1874.6678 - mse: 1874.6678 - mae: 29.9758 - val_loss: 17197.1938 - val_mse: 17197.1934 - val_mae: 36.4917\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 523us/step - loss: 1845.2029 - mse: 1845.2030 - mae: 30.0501 - val_loss: 17179.8239 - val_mse: 17179.8242 - val_mae: 36.4968\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 517us/step - loss: 2096.3548 - mse: 2096.3550 - mae: 32.1710 - val_loss: 17329.6003 - val_mse: 17329.5996 - val_mae: 36.5426\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 452us/step - loss: 2022.5563 - mse: 2022.5563 - mae: 30.9923 - val_loss: 17333.0557 - val_mse: 17333.0547 - val_mae: 36.5422\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 463us/step - loss: 1943.1982 - mse: 1943.1982 - mae: 30.9193 - val_loss: 17209.1228 - val_mse: 17209.1230 - val_mae: 36.4966\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 479us/step - loss: 1878.9691 - mse: 1878.9690 - mae: 29.9905 - val_loss: 17258.9499 - val_mse: 17258.9492 - val_mae: 36.5024\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 2029.9083 - mse: 2029.9083 - mae: 31.0281 - val_loss: 17389.9905 - val_mse: 17389.9922 - val_mae: 36.6478\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 1831.7912 - mse: 1831.7913 - mae: 29.5443 - val_loss: 17343.7849 - val_mse: 17343.7852 - val_mae: 36.5802\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 526us/step - loss: 4305.3870 - mse: 4305.3867 - mae: 34.8942 - val_loss: 2139.2785 - val_mse: 2139.2783 - val_mae: 29.4914\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 0s 446us/step - loss: 4212.2684 - mse: 4212.2681 - mae: 34.3558 - val_loss: 2114.5734 - val_mse: 2114.5732 - val_mae: 29.3558\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 521us/step - loss: 4301.4057 - mse: 4301.4058 - mae: 34.2838 - val_loss: 2218.5933 - val_mse: 2218.5933 - val_mae: 29.8604\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 3903.7839 - mse: 3903.7842 - mae: 34.1243 - val_loss: 2145.4655 - val_mse: 2145.4656 - val_mae: 29.4785\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 691us/step - loss: 4103.5903 - mse: 4103.5898 - mae: 34.0506 - val_loss: 2216.6781 - val_mse: 2216.6780 - val_mae: 29.8241\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 504us/step - loss: 4024.9593 - mse: 4024.9587 - mae: 33.3786 - val_loss: 2226.7433 - val_mse: 2226.7432 - val_mae: 29.8550\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4091.1935 - mse: 4091.1938 - mae: 34.1821 - val_loss: 2183.3812 - val_mse: 2183.3813 - val_mae: 29.6077\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4033.9335 - mse: 4033.9333 - mae: 34.3693 - val_loss: 2232.2512 - val_mse: 2232.2512 - val_mae: 29.8659\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 3801.8169 - mse: 3801.8167 - mae: 32.9563 - val_loss: 2084.1556 - val_mse: 2084.1555 - val_mae: 29.1034\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 4343.7321 - mse: 4343.7319 - mae: 35.1290 - val_loss: 2186.8455 - val_mse: 2186.8452 - val_mae: 29.5885\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4147.5214 - mse: 4147.5215 - mae: 33.8078 - val_loss: 2190.7616 - val_mse: 2190.7615 - val_mae: 29.5961\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4198.0758 - mse: 4198.0762 - mae: 34.9725 - val_loss: 2203.8081 - val_mse: 2203.8083 - val_mae: 29.6529\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4118.1025 - mse: 4118.1025 - mae: 33.1858 - val_loss: 2130.1360 - val_mse: 2130.1360 - val_mae: 29.2573\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4175.2352 - mse: 4175.2354 - mae: 34.7575 - val_loss: 2202.2520 - val_mse: 2202.2520 - val_mae: 29.6450\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4172.9103 - mse: 4172.9111 - mae: 33.7492 - val_loss: 2185.8087 - val_mse: 2185.8088 - val_mae: 29.5608\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4033.4734 - mse: 4033.4731 - mae: 33.9168 - val_loss: 2180.0777 - val_mse: 2180.0776 - val_mae: 29.5341\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4159.1026 - mse: 4159.1030 - mae: 34.2124 - val_loss: 2182.7695 - val_mse: 2182.7695 - val_mae: 29.5411\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 3937.6249 - mse: 3937.6243 - mae: 33.4051 - val_loss: 2167.4471 - val_mse: 2167.4473 - val_mae: 29.4505\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4037.7990 - mse: 4037.7991 - mae: 33.8689 - val_loss: 2170.9809 - val_mse: 2170.9810 - val_mae: 29.4438\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 697us/step - loss: 4026.0663 - mse: 4026.0669 - mae: 34.0032 - val_loss: 2192.5287 - val_mse: 2192.5288 - val_mae: 29.5331\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4093.3498 - mse: 4093.3494 - mae: 33.8179 - val_loss: 2240.5540 - val_mse: 2240.5542 - val_mae: 29.8091\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4033.5656 - mse: 4033.5657 - mae: 33.3454 - val_loss: 2155.7913 - val_mse: 2155.7915 - val_mae: 29.3043\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 3914.8695 - mse: 3914.8696 - mae: 32.9475 - val_loss: 2115.2965 - val_mse: 2115.2964 - val_mae: 29.0827\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 3878.2970 - mse: 3878.2966 - mae: 33.5527 - val_loss: 2127.7285 - val_mse: 2127.7285 - val_mae: 29.1457\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 3956.5437 - mse: 3956.5437 - mae: 33.6671 - val_loss: 2164.6285 - val_mse: 2164.6284 - val_mae: 29.3397\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 677us/step - loss: 4028.6136 - mse: 4028.6135 - mae: 34.3752 - val_loss: 2151.6243 - val_mse: 2151.6245 - val_mae: 29.2451\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 694us/step - loss: 4055.9566 - mse: 4055.9558 - mae: 33.9226 - val_loss: 2168.2008 - val_mse: 2168.2007 - val_mae: 29.3114\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 671us/step - loss: 4048.8250 - mse: 4048.8252 - mae: 34.0787 - val_loss: 2161.3299 - val_mse: 2161.3301 - val_mae: 29.2582\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4065.6998 - mse: 4065.6995 - mae: 34.1243 - val_loss: 2210.5985 - val_mse: 2210.5986 - val_mae: 29.5502\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4005.0342 - mse: 4005.0347 - mae: 33.4141 - val_loss: 2189.0369 - val_mse: 2189.0371 - val_mae: 29.4092\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4211.6059 - mse: 4211.6064 - mae: 34.6240 - val_loss: 2173.1705 - val_mse: 2173.1707 - val_mae: 29.3085\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 3938.3471 - mse: 3938.3467 - mae: 32.4610 - val_loss: 2124.0511 - val_mse: 2124.0510 - val_mae: 29.0151\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4095.0147 - mse: 4095.0144 - mae: 33.8205 - val_loss: 2188.4273 - val_mse: 2188.4272 - val_mae: 29.3982\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 3886.9491 - mse: 3886.9487 - mae: 33.0668 - val_loss: 2196.3567 - val_mse: 2196.3569 - val_mae: 29.4410\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4018.1455 - mse: 4018.1462 - mae: 34.3371 - val_loss: 2181.5700 - val_mse: 2181.5703 - val_mae: 29.3441\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4096.8195 - mse: 4096.8198 - mae: 33.7633 - val_loss: 2231.3276 - val_mse: 2231.3274 - val_mae: 29.6422\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 669us/step - loss: 4055.6428 - mse: 4055.6433 - mae: 34.0254 - val_loss: 2202.5389 - val_mse: 2202.5391 - val_mae: 29.4662\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 3991.5758 - mse: 3991.5757 - mae: 33.2712 - val_loss: 2202.6563 - val_mse: 2202.6562 - val_mae: 29.4710\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 3910.7140 - mse: 3910.7129 - mae: 33.8932 - val_loss: 2139.1190 - val_mse: 2139.1191 - val_mae: 29.0856\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4186.1967 - mse: 4186.1968 - mae: 34.2883 - val_loss: 2200.1146 - val_mse: 2200.1145 - val_mae: 29.4510\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 4114.2780 - mse: 4114.2778 - mae: 34.1713 - val_loss: 2189.2601 - val_mse: 2189.2603 - val_mae: 29.3823\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 3869.3489 - mse: 3869.3489 - mae: 32.1539 - val_loss: 2117.2358 - val_mse: 2117.2358 - val_mae: 28.9312\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 3811.4347 - mse: 3811.4346 - mae: 31.7431 - val_loss: 2141.5754 - val_mse: 2141.5754 - val_mae: 29.0734\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 529us/step - loss: 4013.3977 - mse: 4013.3975 - mae: 33.2118 - val_loss: 2231.9870 - val_mse: 2231.9868 - val_mae: 29.6410\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 3946.1732 - mse: 3946.1726 - mae: 33.8376 - val_loss: 2155.6628 - val_mse: 2155.6628 - val_mae: 29.1575\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 3834.7282 - mse: 3834.7280 - mae: 32.3310 - val_loss: 2145.4300 - val_mse: 2145.4299 - val_mae: 29.0945\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 532us/step - loss: 3862.2456 - mse: 3862.2458 - mae: 32.6620 - val_loss: 2090.7952 - val_mse: 2090.7952 - val_mae: 28.7611\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4089.8195 - mse: 4089.8198 - mae: 33.4472 - val_loss: 2146.6136 - val_mse: 2146.6135 - val_mae: 29.0926\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4034.9572 - mse: 4034.9568 - mae: 33.1421 - val_loss: 2118.6810 - val_mse: 2118.6809 - val_mae: 28.9151\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 3904.1479 - mse: 3904.1479 - mae: 32.8167 - val_loss: 2144.4419 - val_mse: 2144.4417 - val_mae: 29.0837\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 601us/step - loss: 3804.8060 - mse: 3804.8057 - mae: 32.7586 - val_loss: 2147.4307 - val_mse: 2147.4309 - val_mae: 29.1048\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4019.0344 - mse: 4019.0344 - mae: 33.0836 - val_loss: 2135.8194 - val_mse: 2135.8196 - val_mae: 29.0272\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4002.5418 - mse: 4002.5420 - mae: 32.7054 - val_loss: 2135.3669 - val_mse: 2135.3669 - val_mae: 29.0304\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 3849.4839 - mse: 3849.4836 - mae: 32.5206 - val_loss: 2152.9901 - val_mse: 2152.9900 - val_mae: 29.1364\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 3913.5586 - mse: 3913.5591 - mae: 33.2735 - val_loss: 2148.7272 - val_mse: 2148.7273 - val_mae: 29.1102\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 3930.9974 - mse: 3930.9973 - mae: 33.4022 - val_loss: 2144.9988 - val_mse: 2144.9988 - val_mae: 29.0775\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 0s 438us/step - loss: 3753.8847 - mse: 3753.8848 - mae: 33.1300 - val_loss: 2147.8855 - val_mse: 2147.8855 - val_mae: 29.0934\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 0s 474us/step - loss: 4100.9056 - mse: 4100.9058 - mae: 33.5924 - val_loss: 2161.9700 - val_mse: 2161.9700 - val_mae: 29.1775\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 3918.1837 - mse: 3918.1829 - mae: 32.1530 - val_loss: 2131.6168 - val_mse: 2131.6167 - val_mae: 28.9857\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 676us/step - loss: 3895.4943 - mse: 3895.4939 - mae: 32.6212 - val_loss: 2145.9980 - val_mse: 2145.9978 - val_mae: 29.0781\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 3837.1960 - mse: 3837.1958 - mae: 32.6794 - val_loss: 2095.6940 - val_mse: 2095.6941 - val_mae: 28.7699\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 3904.6818 - mse: 3904.6816 - mae: 32.7095 - val_loss: 2103.6759 - val_mse: 2103.6758 - val_mae: 28.8014\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 3881.3348 - mse: 3881.3350 - mae: 33.5461 - val_loss: 2118.0178 - val_mse: 2118.0178 - val_mae: 28.8852\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 725us/step - loss: 3974.1233 - mse: 3974.1233 - mae: 33.2845 - val_loss: 2137.9438 - val_mse: 2137.9438 - val_mae: 29.0289\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 3858.4314 - mse: 3858.4312 - mae: 32.6160 - val_loss: 2065.9386 - val_mse: 2065.9387 - val_mae: 28.6348\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 3988.2731 - mse: 3988.2727 - mae: 33.3366 - val_loss: 2147.9338 - val_mse: 2147.9338 - val_mae: 29.0962\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 3921.2530 - mse: 3921.2524 - mae: 33.3827 - val_loss: 2187.5529 - val_mse: 2187.5530 - val_mae: 29.3468\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4012.3960 - mse: 4012.3965 - mae: 32.7625 - val_loss: 2165.7543 - val_mse: 2165.7544 - val_mae: 29.1925\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 691us/step - loss: 3891.6974 - mse: 3891.6973 - mae: 33.6948 - val_loss: 2171.8861 - val_mse: 2171.8862 - val_mae: 29.2335\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 3922.9045 - mse: 3922.9048 - mae: 32.4680 - val_loss: 2137.2273 - val_mse: 2137.2275 - val_mae: 29.0079\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 3899.6497 - mse: 3899.6499 - mae: 32.2537 - val_loss: 2108.7100 - val_mse: 2108.7100 - val_mae: 28.8408\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 530us/step - loss: 3878.8206 - mse: 3878.8208 - mae: 32.1887 - val_loss: 2110.7016 - val_mse: 2110.7014 - val_mae: 28.8424\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4083.6844 - mse: 4083.6836 - mae: 33.0491 - val_loss: 2143.3701 - val_mse: 2143.3696 - val_mae: 29.0435\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 3789.0474 - mse: 3789.0471 - mae: 31.6856 - val_loss: 2100.4996 - val_mse: 2100.4995 - val_mae: 28.7841\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 4007.3644 - mse: 4007.3640 - mae: 32.7940 - val_loss: 2145.6892 - val_mse: 2145.6892 - val_mae: 29.0508\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 532us/step - loss: 3882.3216 - mse: 3882.3218 - mae: 32.8362 - val_loss: 2145.3947 - val_mse: 2145.3945 - val_mae: 29.0480\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 3648.3149 - mse: 3648.3145 - mae: 31.5231 - val_loss: 2109.8947 - val_mse: 2109.8948 - val_mae: 28.8317\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 4084.5860 - mse: 4084.5859 - mae: 33.2926 - val_loss: 2125.9886 - val_mse: 2125.9885 - val_mae: 28.9345\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4003.0301 - mse: 4003.0293 - mae: 32.8791 - val_loss: 2154.0387 - val_mse: 2154.0388 - val_mae: 29.1023\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 553us/step - loss: 3933.9361 - mse: 3933.9353 - mae: 32.2897 - val_loss: 2158.5357 - val_mse: 2158.5354 - val_mae: 29.1252\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 522us/step - loss: 3403.8020 - mse: 3403.8020 - mae: 33.8411 - val_loss: 1440.4587 - val_mse: 1440.4585 - val_mae: 25.7417\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3280.8690 - mse: 3280.8696 - mae: 32.6800 - val_loss: 1435.5692 - val_mse: 1435.5692 - val_mae: 25.5159\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3455.6921 - mse: 3455.6926 - mae: 33.1313 - val_loss: 1434.8609 - val_mse: 1434.8611 - val_mae: 25.8192\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3167.5033 - mse: 3167.5027 - mae: 31.6193 - val_loss: 1443.5166 - val_mse: 1443.5167 - val_mae: 26.5463\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3446.7439 - mse: 3446.7439 - mae: 32.9293 - val_loss: 1429.4181 - val_mse: 1429.4181 - val_mae: 25.7500\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3294.9185 - mse: 3294.9185 - mae: 31.7086 - val_loss: 1429.3977 - val_mse: 1429.3977 - val_mae: 25.9231\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3352.7672 - mse: 3352.7676 - mae: 32.2237 - val_loss: 1423.2440 - val_mse: 1423.2441 - val_mae: 25.3880\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3324.1711 - mse: 3324.1711 - mae: 32.6547 - val_loss: 1425.0860 - val_mse: 1425.0861 - val_mae: 25.8927\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3252.5473 - mse: 3252.5476 - mae: 31.9677 - val_loss: 1430.3806 - val_mse: 1430.3805 - val_mae: 26.3264\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3226.9315 - mse: 3226.9312 - mae: 31.6024 - val_loss: 1422.7303 - val_mse: 1422.7305 - val_mae: 25.8789\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3312.5458 - mse: 3312.5461 - mae: 32.1851 - val_loss: 1423.2235 - val_mse: 1423.2234 - val_mae: 25.9952\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3311.1199 - mse: 3311.1194 - mae: 31.7176 - val_loss: 1420.5629 - val_mse: 1420.5626 - val_mae: 25.7068\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3332.0821 - mse: 3332.0823 - mae: 32.5287 - val_loss: 1421.4115 - val_mse: 1421.4116 - val_mae: 25.8557\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3383.0732 - mse: 3383.0732 - mae: 32.0062 - val_loss: 1420.7220 - val_mse: 1420.7219 - val_mae: 25.8600\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3288.8864 - mse: 3288.8870 - mae: 32.3923 - val_loss: 1423.3921 - val_mse: 1423.3922 - val_mae: 26.1066\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3150.4306 - mse: 3150.4302 - mae: 31.3867 - val_loss: 1425.4420 - val_mse: 1425.4419 - val_mae: 26.2717\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3422.9748 - mse: 3422.9756 - mae: 32.8265 - val_loss: 1419.4101 - val_mse: 1419.4103 - val_mae: 25.8343\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3291.6885 - mse: 3291.6890 - mae: 32.3673 - val_loss: 1423.9258 - val_mse: 1423.9258 - val_mae: 26.1958\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3255.9337 - mse: 3255.9333 - mae: 31.8658 - val_loss: 1420.2767 - val_mse: 1420.2767 - val_mae: 25.9533\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3097.6217 - mse: 3097.6218 - mae: 31.1695 - val_loss: 1419.4096 - val_mse: 1419.4098 - val_mae: 25.9202\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 657us/step - loss: 3228.3756 - mse: 3228.3755 - mae: 31.7206 - val_loss: 1416.7413 - val_mse: 1416.7415 - val_mae: 25.5658\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3223.6853 - mse: 3223.6860 - mae: 31.2618 - val_loss: 1422.4849 - val_mse: 1422.4847 - val_mae: 26.0912\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3450.1869 - mse: 3450.1860 - mae: 32.6409 - val_loss: 1419.8394 - val_mse: 1419.8396 - val_mae: 25.9056\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 636us/step - loss: 3203.5641 - mse: 3203.5652 - mae: 31.5183 - val_loss: 1430.6887 - val_mse: 1430.6888 - val_mae: 26.5900\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 648us/step - loss: 3279.7423 - mse: 3279.7424 - mae: 31.7894 - val_loss: 1422.3240 - val_mse: 1422.3239 - val_mae: 26.0800\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 660us/step - loss: 3266.8486 - mse: 3266.8489 - mae: 31.1336 - val_loss: 1419.0959 - val_mse: 1419.0957 - val_mae: 25.7326\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3272.6612 - mse: 3272.6611 - mae: 32.0236 - val_loss: 1425.8097 - val_mse: 1425.8097 - val_mae: 26.2759\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3239.9256 - mse: 3239.9250 - mae: 32.0210 - val_loss: 1424.4953 - val_mse: 1424.4951 - val_mae: 26.2270\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3309.4794 - mse: 3309.4790 - mae: 32.1649 - val_loss: 1418.0999 - val_mse: 1418.1000 - val_mae: 25.6678\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3273.3988 - mse: 3273.3989 - mae: 31.9334 - val_loss: 1419.6232 - val_mse: 1419.6233 - val_mae: 25.8509\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3341.3046 - mse: 3341.3044 - mae: 32.2326 - val_loss: 1416.7318 - val_mse: 1416.7316 - val_mae: 25.1945\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3368.5340 - mse: 3368.5337 - mae: 32.2934 - val_loss: 1416.5866 - val_mse: 1416.5865 - val_mae: 25.3622\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3273.9194 - mse: 3273.9185 - mae: 31.4810 - val_loss: 1431.5937 - val_mse: 1431.5938 - val_mae: 26.6255\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3257.6031 - mse: 3257.6030 - mae: 31.1575 - val_loss: 1425.7448 - val_mse: 1425.7449 - val_mae: 26.2826\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 604us/step - loss: 3199.5057 - mse: 3199.5056 - mae: 32.2810 - val_loss: 1421.4030 - val_mse: 1421.4031 - val_mae: 25.9714\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3238.8205 - mse: 3238.8210 - mae: 31.5436 - val_loss: 1419.3778 - val_mse: 1419.3778 - val_mae: 25.7955\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3164.4209 - mse: 3164.4214 - mae: 31.9331 - val_loss: 1420.1149 - val_mse: 1420.1147 - val_mae: 25.9143\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 642us/step - loss: 3307.4218 - mse: 3307.4229 - mae: 31.4134 - val_loss: 1419.6581 - val_mse: 1419.6580 - val_mae: 25.8446\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3203.5278 - mse: 3203.5278 - mae: 31.4405 - val_loss: 1423.1795 - val_mse: 1423.1794 - val_mae: 26.1332\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3260.6119 - mse: 3260.6128 - mae: 31.2614 - val_loss: 1437.4733 - val_mse: 1437.4733 - val_mae: 26.8833\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3219.0262 - mse: 3219.0256 - mae: 31.4049 - val_loss: 1426.4146 - val_mse: 1426.4148 - val_mae: 26.3322\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3160.1851 - mse: 3160.1846 - mae: 31.7114 - val_loss: 1428.8372 - val_mse: 1428.8373 - val_mae: 26.4630\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3359.6755 - mse: 3359.6755 - mae: 31.7484 - val_loss: 1422.5851 - val_mse: 1422.5851 - val_mae: 26.0359\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3169.4780 - mse: 3169.4780 - mae: 30.7526 - val_loss: 1422.6980 - val_mse: 1422.6979 - val_mae: 26.0407\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3280.0002 - mse: 3280.0000 - mae: 31.7843 - val_loss: 1421.6507 - val_mse: 1421.6508 - val_mae: 25.9478\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 701us/step - loss: 3245.5461 - mse: 3245.5459 - mae: 31.3256 - val_loss: 1420.0555 - val_mse: 1420.0553 - val_mae: 25.8522\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3251.8308 - mse: 3251.8311 - mae: 31.4362 - val_loss: 1420.5758 - val_mse: 1420.5759 - val_mae: 25.8522\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 671us/step - loss: 3273.3517 - mse: 3273.3518 - mae: 31.4976 - val_loss: 1420.5100 - val_mse: 1420.5100 - val_mae: 25.8705\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3240.3589 - mse: 3240.3596 - mae: 31.4954 - val_loss: 1425.6327 - val_mse: 1425.6326 - val_mae: 26.2652\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 683us/step - loss: 3308.4379 - mse: 3308.4377 - mae: 31.8967 - val_loss: 1419.3571 - val_mse: 1419.3569 - val_mae: 25.6777\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3242.5277 - mse: 3242.5278 - mae: 31.5611 - val_loss: 1424.0846 - val_mse: 1424.0847 - val_mae: 26.1440\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 651us/step - loss: 3138.0027 - mse: 3138.0017 - mae: 30.8855 - val_loss: 1424.5388 - val_mse: 1424.5388 - val_mae: 26.2151\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3277.4973 - mse: 3277.4973 - mae: 31.3727 - val_loss: 1435.8810 - val_mse: 1435.8809 - val_mae: 26.8495\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3362.2350 - mse: 3362.2354 - mae: 31.4255 - val_loss: 1420.6947 - val_mse: 1420.6948 - val_mae: 25.9117\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 671us/step - loss: 3198.2202 - mse: 3198.2207 - mae: 31.4114 - val_loss: 1420.7753 - val_mse: 1420.7753 - val_mae: 25.9286\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3166.0780 - mse: 3166.0779 - mae: 30.9329 - val_loss: 1422.7712 - val_mse: 1422.7712 - val_mae: 26.0967\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3264.4402 - mse: 3264.4395 - mae: 31.9490 - val_loss: 1418.9560 - val_mse: 1418.9561 - val_mae: 25.5251\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3214.2992 - mse: 3214.2993 - mae: 31.3061 - val_loss: 1420.5537 - val_mse: 1420.5538 - val_mae: 25.8697\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3117.1795 - mse: 3117.1802 - mae: 31.0580 - val_loss: 1424.3275 - val_mse: 1424.3275 - val_mae: 26.1983\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 705us/step - loss: 3145.8790 - mse: 3145.8787 - mae: 31.6320 - val_loss: 1424.6602 - val_mse: 1424.6603 - val_mae: 26.1993\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3174.7299 - mse: 3174.7305 - mae: 31.3862 - val_loss: 1423.2086 - val_mse: 1423.2085 - val_mae: 26.0689\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3244.0758 - mse: 3244.0757 - mae: 31.5686 - val_loss: 1423.6749 - val_mse: 1423.6748 - val_mae: 26.1012\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3201.5063 - mse: 3201.5056 - mae: 31.1180 - val_loss: 1431.7558 - val_mse: 1431.7559 - val_mae: 26.5843\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3201.2385 - mse: 3201.2390 - mae: 31.8389 - val_loss: 1428.7711 - val_mse: 1428.7712 - val_mae: 26.3978\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3139.4240 - mse: 3139.4248 - mae: 30.9438 - val_loss: 1424.9812 - val_mse: 1424.9811 - val_mae: 26.1600\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3078.9293 - mse: 3078.9292 - mae: 30.8367 - val_loss: 1424.1137 - val_mse: 1424.1136 - val_mae: 26.1362\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3274.1873 - mse: 3274.1870 - mae: 31.1717 - val_loss: 1420.8244 - val_mse: 1420.8246 - val_mae: 25.8339\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 686us/step - loss: 3199.5870 - mse: 3199.5867 - mae: 31.3011 - val_loss: 1424.4654 - val_mse: 1424.4655 - val_mae: 26.1800\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3222.9521 - mse: 3222.9524 - mae: 31.3407 - val_loss: 1420.9338 - val_mse: 1420.9338 - val_mae: 25.8630\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3145.7634 - mse: 3145.7632 - mae: 31.0146 - val_loss: 1422.3908 - val_mse: 1422.3905 - val_mae: 25.9844\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3192.5872 - mse: 3192.5874 - mae: 31.0493 - val_loss: 1421.0045 - val_mse: 1421.0045 - val_mae: 25.7192\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3122.9524 - mse: 3122.9521 - mae: 30.4726 - val_loss: 1422.0243 - val_mse: 1422.0244 - val_mae: 25.9176\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3132.6734 - mse: 3132.6736 - mae: 30.5071 - val_loss: 1420.2032 - val_mse: 1420.2030 - val_mae: 25.7162\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3097.1533 - mse: 3097.1536 - mae: 30.3036 - val_loss: 1423.7026 - val_mse: 1423.7025 - val_mae: 26.1279\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3216.1565 - mse: 3216.1562 - mae: 31.1473 - val_loss: 1423.6018 - val_mse: 1423.6019 - val_mae: 26.1143\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3192.3606 - mse: 3192.3611 - mae: 30.9225 - val_loss: 1421.5627 - val_mse: 1421.5629 - val_mae: 25.8720\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 654us/step - loss: 3196.9132 - mse: 3196.9133 - mae: 30.5314 - val_loss: 1422.8376 - val_mse: 1422.8374 - val_mae: 26.0039\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3170.4709 - mse: 3170.4702 - mae: 31.4459 - val_loss: 1422.5432 - val_mse: 1422.5433 - val_mae: 26.0401\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3296.0427 - mse: 3296.0417 - mae: 31.6474 - val_loss: 1425.3587 - val_mse: 1425.3584 - val_mae: 26.2281\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3301.7561 - mse: 3301.7551 - mae: 31.8985 - val_loss: 1420.1471 - val_mse: 1420.1471 - val_mae: 25.7560\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2844.3966 - mse: 2844.3967 - mae: 30.7517 - val_loss: 1118.9715 - val_mse: 1118.9713 - val_mae: 23.2143\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2722.6054 - mse: 2722.6052 - mae: 30.3385 - val_loss: 1085.8198 - val_mse: 1085.8196 - val_mae: 23.6340\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 677us/step - loss: 2923.6826 - mse: 2923.6829 - mae: 30.8476 - val_loss: 1083.3476 - val_mse: 1083.3475 - val_mae: 23.7008\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 667us/step - loss: 2891.4744 - mse: 2891.4744 - mae: 30.5956 - val_loss: 1089.5392 - val_mse: 1089.5393 - val_mae: 23.4599\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2825.9795 - mse: 2825.9792 - mae: 30.1545 - val_loss: 1091.4241 - val_mse: 1091.4241 - val_mae: 23.4152\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 567us/step - loss: 2794.6058 - mse: 2794.6062 - mae: 30.7443 - val_loss: 1093.3172 - val_mse: 1093.3171 - val_mae: 23.3693\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2884.5695 - mse: 2884.5691 - mae: 30.8930 - val_loss: 1097.2531 - val_mse: 1097.2533 - val_mae: 23.2969\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 698us/step - loss: 2965.7319 - mse: 2965.7314 - mae: 30.8288 - val_loss: 1085.0723 - val_mse: 1085.0723 - val_mae: 23.4607\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2909.8472 - mse: 2909.8474 - mae: 30.8015 - val_loss: 1096.3619 - val_mse: 1096.3621 - val_mae: 23.2572\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2815.7522 - mse: 2815.7522 - mae: 30.5599 - val_loss: 1087.5693 - val_mse: 1087.5693 - val_mae: 23.3810\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2884.2146 - mse: 2884.2146 - mae: 30.8191 - val_loss: 1090.8273 - val_mse: 1090.8273 - val_mae: 23.2892\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 687us/step - loss: 2922.2644 - mse: 2922.2649 - mae: 30.7402 - val_loss: 1078.5918 - val_mse: 1078.5918 - val_mae: 23.5181\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 539us/step - loss: 2927.2542 - mse: 2927.2542 - mae: 30.9176 - val_loss: 1090.2339 - val_mse: 1090.2340 - val_mae: 23.2800\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2831.3718 - mse: 2831.3721 - mae: 31.0854 - val_loss: 1089.0380 - val_mse: 1089.0378 - val_mae: 23.2631\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2860.5405 - mse: 2860.5417 - mae: 30.4558 - val_loss: 1084.8109 - val_mse: 1084.8110 - val_mae: 23.3474\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 651us/step - loss: 2877.7427 - mse: 2877.7429 - mae: 30.8798 - val_loss: 1088.8623 - val_mse: 1088.8623 - val_mae: 23.2719\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2780.3202 - mse: 2780.3203 - mae: 29.8800 - val_loss: 1070.9681 - val_mse: 1070.9680 - val_mae: 23.7128\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2973.6620 - mse: 2973.6621 - mae: 31.1167 - val_loss: 1087.9965 - val_mse: 1087.9963 - val_mae: 23.2918\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 528us/step - loss: 2896.8439 - mse: 2896.8433 - mae: 30.8417 - val_loss: 1082.7648 - val_mse: 1082.7646 - val_mae: 23.3795\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2724.4629 - mse: 2724.4631 - mae: 30.3806 - val_loss: 1069.6649 - val_mse: 1069.6648 - val_mae: 23.7778\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2711.3508 - mse: 2711.3503 - mae: 30.6962 - val_loss: 1077.9848 - val_mse: 1077.9849 - val_mae: 23.5260\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2821.0040 - mse: 2821.0042 - mae: 30.3983 - val_loss: 1076.8285 - val_mse: 1076.8286 - val_mae: 23.5539\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2903.5708 - mse: 2903.5706 - mae: 30.8333 - val_loss: 1074.9845 - val_mse: 1074.9846 - val_mae: 23.6426\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 534us/step - loss: 2869.5708 - mse: 2869.5706 - mae: 30.6750 - val_loss: 1078.7689 - val_mse: 1078.7689 - val_mae: 23.4851\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2895.5337 - mse: 2895.5334 - mae: 30.7132 - val_loss: 1074.7834 - val_mse: 1074.7836 - val_mae: 23.5802\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2774.1199 - mse: 2774.1204 - mae: 29.9068 - val_loss: 1071.3077 - val_mse: 1071.3077 - val_mae: 23.7117\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2825.1746 - mse: 2825.1743 - mae: 30.5383 - val_loss: 1075.3354 - val_mse: 1075.3354 - val_mae: 23.5827\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 678us/step - loss: 2773.3601 - mse: 2773.3596 - mae: 30.1586 - val_loss: 1072.6805 - val_mse: 1072.6804 - val_mae: 23.6918\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2786.9468 - mse: 2786.9473 - mae: 30.3513 - val_loss: 1076.6508 - val_mse: 1076.6508 - val_mae: 23.5508\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2815.9127 - mse: 2815.9136 - mae: 30.2044 - val_loss: 1079.5729 - val_mse: 1079.5729 - val_mae: 23.4840\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2785.8488 - mse: 2785.8496 - mae: 30.4159 - val_loss: 1075.5502 - val_mse: 1075.5502 - val_mae: 23.5904\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2928.6357 - mse: 2928.6355 - mae: 30.8996 - val_loss: 1090.0090 - val_mse: 1090.0090 - val_mae: 23.2838\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2756.3953 - mse: 2756.3953 - mae: 29.9382 - val_loss: 1072.8457 - val_mse: 1072.8457 - val_mae: 23.6643\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 695us/step - loss: 2826.4927 - mse: 2826.4922 - mae: 30.3440 - val_loss: 1074.3153 - val_mse: 1074.3154 - val_mae: 23.6266\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 659us/step - loss: 2801.0372 - mse: 2801.0369 - mae: 30.7421 - val_loss: 1077.5133 - val_mse: 1077.5133 - val_mae: 23.5506\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2933.2988 - mse: 2933.2988 - mae: 30.9154 - val_loss: 1082.8865 - val_mse: 1082.8865 - val_mae: 23.4292\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2838.0044 - mse: 2838.0042 - mae: 30.2656 - val_loss: 1077.4366 - val_mse: 1077.4366 - val_mae: 23.5588\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2790.4204 - mse: 2790.4207 - mae: 30.4114 - val_loss: 1072.4699 - val_mse: 1072.4701 - val_mae: 23.7068\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2806.3428 - mse: 2806.3425 - mae: 29.9824 - val_loss: 1077.8098 - val_mse: 1077.8097 - val_mae: 23.5434\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2765.8256 - mse: 2765.8257 - mae: 30.1528 - val_loss: 1067.4811 - val_mse: 1067.4812 - val_mae: 23.9013\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2766.6626 - mse: 2766.6621 - mae: 29.9343 - val_loss: 1078.0300 - val_mse: 1078.0299 - val_mae: 23.5423\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2815.9136 - mse: 2815.9131 - mae: 30.5143 - val_loss: 1082.0649 - val_mse: 1082.0648 - val_mae: 23.4333\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2873.4471 - mse: 2873.4470 - mae: 30.1716 - val_loss: 1083.0161 - val_mse: 1083.0161 - val_mae: 23.4088\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 679us/step - loss: 2805.2800 - mse: 2805.2798 - mae: 30.1696 - val_loss: 1085.3840 - val_mse: 1085.3839 - val_mae: 23.3647\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2851.2855 - mse: 2851.2861 - mae: 30.6451 - val_loss: 1082.6790 - val_mse: 1082.6790 - val_mae: 23.4055\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2842.3433 - mse: 2842.3433 - mae: 30.2259 - val_loss: 1081.8793 - val_mse: 1081.8793 - val_mae: 23.4126\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2885.4317 - mse: 2885.4314 - mae: 30.5169 - val_loss: 1072.5572 - val_mse: 1072.5573 - val_mae: 23.6338\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2759.3357 - mse: 2759.3354 - mae: 30.4151 - val_loss: 1078.5755 - val_mse: 1078.5754 - val_mae: 23.4973\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2802.0052 - mse: 2802.0049 - mae: 30.1112 - val_loss: 1074.3920 - val_mse: 1074.3920 - val_mae: 23.5741\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 668us/step - loss: 2738.0823 - mse: 2738.0811 - mae: 29.8743 - val_loss: 1075.6055 - val_mse: 1075.6055 - val_mae: 23.5183\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2776.0387 - mse: 2776.0378 - mae: 30.1627 - val_loss: 1073.4509 - val_mse: 1073.4509 - val_mae: 23.5655\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2818.3128 - mse: 2818.3125 - mae: 30.0082 - val_loss: 1074.8938 - val_mse: 1074.8939 - val_mae: 23.5682\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2779.2732 - mse: 2779.2737 - mae: 29.7088 - val_loss: 1066.9560 - val_mse: 1066.9561 - val_mae: 23.8280\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2759.9985 - mse: 2759.9993 - mae: 30.1003 - val_loss: 1072.4906 - val_mse: 1072.4905 - val_mae: 23.6311\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2826.5703 - mse: 2826.5696 - mae: 30.0097 - val_loss: 1081.2795 - val_mse: 1081.2794 - val_mae: 23.4168\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 530us/step - loss: 2767.0375 - mse: 2767.0374 - mae: 30.6279 - val_loss: 1079.4812 - val_mse: 1079.4812 - val_mae: 23.4545\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2817.2729 - mse: 2817.2727 - mae: 29.6864 - val_loss: 1073.3955 - val_mse: 1073.3954 - val_mae: 23.5698\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2765.6777 - mse: 2765.6782 - mae: 29.8578 - val_loss: 1071.6098 - val_mse: 1071.6097 - val_mae: 23.6355\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2870.8199 - mse: 2870.8196 - mae: 30.8523 - val_loss: 1083.7142 - val_mse: 1083.7144 - val_mae: 23.3557\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2729.3809 - mse: 2729.3809 - mae: 29.4185 - val_loss: 1069.8962 - val_mse: 1069.8961 - val_mae: 23.6590\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2747.7556 - mse: 2747.7566 - mae: 30.2526 - val_loss: 1075.8745 - val_mse: 1075.8745 - val_mae: 23.5429\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2773.4521 - mse: 2773.4521 - mae: 29.9003 - val_loss: 1072.2006 - val_mse: 1072.2008 - val_mae: 23.6365\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2803.3413 - mse: 2803.3416 - mae: 29.6585 - val_loss: 1080.9198 - val_mse: 1080.9199 - val_mae: 23.4184\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 699us/step - loss: 2697.2146 - mse: 2697.2141 - mae: 29.9934 - val_loss: 1075.6926 - val_mse: 1075.6925 - val_mae: 23.4914\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2773.1605 - mse: 2773.1597 - mae: 29.9353 - val_loss: 1074.0656 - val_mse: 1074.0658 - val_mae: 23.5074\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2767.4670 - mse: 2767.4678 - mae: 29.7265 - val_loss: 1070.9948 - val_mse: 1070.9950 - val_mae: 23.5945\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2842.7135 - mse: 2842.7134 - mae: 30.1079 - val_loss: 1076.9463 - val_mse: 1076.9463 - val_mae: 23.4674\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2785.0310 - mse: 2785.0303 - mae: 29.7838 - val_loss: 1069.0800 - val_mse: 1069.0800 - val_mae: 23.6828\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2788.4452 - mse: 2788.4456 - mae: 30.1191 - val_loss: 1088.7642 - val_mse: 1088.7643 - val_mae: 23.2858\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2856.7012 - mse: 2856.7007 - mae: 30.2011 - val_loss: 1077.3601 - val_mse: 1077.3601 - val_mae: 23.4869\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2797.8436 - mse: 2797.8435 - mae: 30.6174 - val_loss: 1076.7786 - val_mse: 1076.7786 - val_mae: 23.4567\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 487us/step - loss: 2775.7622 - mse: 2775.7617 - mae: 30.2402 - val_loss: 1079.2108 - val_mse: 1079.2108 - val_mae: 23.3939\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2776.7520 - mse: 2776.7517 - mae: 30.0712 - val_loss: 1068.2082 - val_mse: 1068.2081 - val_mae: 23.7089\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2777.0133 - mse: 2777.0127 - mae: 29.7697 - val_loss: 1079.2635 - val_mse: 1079.2637 - val_mae: 23.4242\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2768.9025 - mse: 2768.9023 - mae: 29.8595 - val_loss: 1074.9710 - val_mse: 1074.9709 - val_mae: 23.5302\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2735.3729 - mse: 2735.3726 - mae: 29.7961 - val_loss: 1074.3854 - val_mse: 1074.3854 - val_mae: 23.5740\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 511us/step - loss: 2747.5855 - mse: 2747.5850 - mae: 30.4629 - val_loss: 1076.5716 - val_mse: 1076.5715 - val_mae: 23.5323\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 463us/step - loss: 2765.2985 - mse: 2765.2981 - mae: 29.9458 - val_loss: 1075.9847 - val_mse: 1075.9847 - val_mae: 23.5351\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 516us/step - loss: 2810.0996 - mse: 2810.0994 - mae: 30.1390 - val_loss: 1082.7595 - val_mse: 1082.7595 - val_mae: 23.3863\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 555us/step - loss: 2794.8023 - mse: 2794.8022 - mae: 30.1033 - val_loss: 1076.7845 - val_mse: 1076.7844 - val_mae: 23.4947\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2403.3986 - mse: 2403.3989 - mae: 29.0288 - val_loss: 1562.9497 - val_mse: 1562.9496 - val_mae: 26.7480\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2506.5733 - mse: 2506.5730 - mae: 29.7362 - val_loss: 1576.8251 - val_mse: 1576.8247 - val_mae: 26.6485\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2512.0657 - mse: 2512.0659 - mae: 29.8906 - val_loss: 1569.2378 - val_mse: 1569.2378 - val_mae: 26.6421\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 560us/step - loss: 2478.9261 - mse: 2478.9272 - mae: 29.4807 - val_loss: 1574.2539 - val_mse: 1574.2540 - val_mae: 26.5664\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2512.5994 - mse: 2512.6006 - mae: 29.8408 - val_loss: 1575.7737 - val_mse: 1575.7736 - val_mae: 26.5362\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2512.0082 - mse: 2512.0085 - mae: 29.2235 - val_loss: 1552.7121 - val_mse: 1552.7122 - val_mae: 26.6938\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2504.7836 - mse: 2504.7830 - mae: 29.3188 - val_loss: 1582.3890 - val_mse: 1582.3890 - val_mae: 26.4878\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2434.0233 - mse: 2434.0232 - mae: 29.0076 - val_loss: 1558.9243 - val_mse: 1558.9242 - val_mae: 26.6157\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2497.0123 - mse: 2497.0125 - mae: 29.8551 - val_loss: 1578.0683 - val_mse: 1578.0684 - val_mae: 26.4686\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2527.2668 - mse: 2527.2666 - mae: 29.7792 - val_loss: 1568.9352 - val_mse: 1568.9352 - val_mae: 26.5018\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2425.3741 - mse: 2425.3740 - mae: 29.0429 - val_loss: 1548.0960 - val_mse: 1548.0959 - val_mae: 26.6596\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2501.7337 - mse: 2501.7329 - mae: 29.7344 - val_loss: 1566.2724 - val_mse: 1566.2725 - val_mae: 26.4985\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2514.0977 - mse: 2514.0984 - mae: 29.4378 - val_loss: 1568.0799 - val_mse: 1568.0801 - val_mae: 26.4734\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2532.8764 - mse: 2532.8762 - mae: 29.2836 - val_loss: 1549.1496 - val_mse: 1549.1495 - val_mae: 26.5762\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 553us/step - loss: 2406.4166 - mse: 2406.4160 - mae: 29.2999 - val_loss: 1566.7705 - val_mse: 1566.7704 - val_mae: 26.4577\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 656us/step - loss: 2394.2378 - mse: 2394.2373 - mae: 28.7660 - val_loss: 1563.1158 - val_mse: 1563.1158 - val_mae: 26.4677\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2470.9636 - mse: 2470.9631 - mae: 29.6405 - val_loss: 1569.8176 - val_mse: 1569.8176 - val_mae: 26.4307\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2441.4736 - mse: 2441.4731 - mae: 29.5873 - val_loss: 1556.3828 - val_mse: 1556.3826 - val_mae: 26.4824\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2532.9025 - mse: 2532.9021 - mae: 29.9527 - val_loss: 1557.5277 - val_mse: 1557.5278 - val_mae: 26.4803\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2570.3517 - mse: 2570.3516 - mae: 29.9477 - val_loss: 1564.3162 - val_mse: 1564.3162 - val_mae: 26.4006\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2451.7134 - mse: 2451.7136 - mae: 29.3327 - val_loss: 1561.2152 - val_mse: 1561.2152 - val_mae: 26.4131\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2442.2213 - mse: 2442.2212 - mae: 29.0333 - val_loss: 1547.3141 - val_mse: 1547.3143 - val_mae: 26.5034\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 533us/step - loss: 2483.6654 - mse: 2483.6653 - mae: 29.3270 - val_loss: 1548.8464 - val_mse: 1548.8464 - val_mae: 26.4637\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2447.8187 - mse: 2447.8188 - mae: 29.3147 - val_loss: 1543.2222 - val_mse: 1543.2220 - val_mae: 26.4621\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2469.0283 - mse: 2469.0283 - mae: 29.6805 - val_loss: 1556.9140 - val_mse: 1556.9139 - val_mae: 26.3631\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 570us/step - loss: 2524.5564 - mse: 2524.5562 - mae: 29.5085 - val_loss: 1548.8564 - val_mse: 1548.8563 - val_mae: 26.4249\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 669us/step - loss: 2463.0853 - mse: 2463.0842 - mae: 29.2032 - val_loss: 1539.6307 - val_mse: 1539.6309 - val_mae: 26.4823\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2495.8396 - mse: 2495.8398 - mae: 29.3384 - val_loss: 1549.5399 - val_mse: 1549.5400 - val_mae: 26.3993\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2479.7975 - mse: 2479.7974 - mae: 29.1601 - val_loss: 1529.8242 - val_mse: 1529.8241 - val_mae: 26.5329\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2431.2433 - mse: 2431.2439 - mae: 29.2337 - val_loss: 1528.3910 - val_mse: 1528.3909 - val_mae: 26.5261\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2465.6186 - mse: 2465.6177 - mae: 29.1735 - val_loss: 1542.7897 - val_mse: 1542.7897 - val_mae: 26.3780\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2506.9543 - mse: 2506.9541 - mae: 29.0171 - val_loss: 1543.2623 - val_mse: 1543.2625 - val_mae: 26.3645\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2506.0047 - mse: 2506.0049 - mae: 29.1747 - val_loss: 1558.5182 - val_mse: 1558.5182 - val_mae: 26.2720\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 474us/step - loss: 2412.4848 - mse: 2412.4846 - mae: 28.7805 - val_loss: 1537.6868 - val_mse: 1537.6866 - val_mae: 26.3848\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2532.6469 - mse: 2532.6477 - mae: 29.7941 - val_loss: 1541.9564 - val_mse: 1541.9565 - val_mae: 26.3532\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 499us/step - loss: 2518.3679 - mse: 2518.3677 - mae: 29.4833 - val_loss: 1527.3009 - val_mse: 1527.3009 - val_mae: 26.4816\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2380.8445 - mse: 2380.8445 - mae: 28.6196 - val_loss: 1548.7165 - val_mse: 1548.7163 - val_mae: 26.3158\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2479.1274 - mse: 2479.1277 - mae: 29.4890 - val_loss: 1555.7164 - val_mse: 1555.7164 - val_mae: 26.2419\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 543us/step - loss: 2425.4135 - mse: 2425.4128 - mae: 28.7071 - val_loss: 1537.9028 - val_mse: 1537.9028 - val_mae: 26.3181\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 528us/step - loss: 2450.8247 - mse: 2450.8252 - mae: 29.1616 - val_loss: 1560.1011 - val_mse: 1560.1011 - val_mae: 26.2055\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2459.5418 - mse: 2459.5410 - mae: 29.4454 - val_loss: 1568.8224 - val_mse: 1568.8221 - val_mae: 26.1512\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2418.1484 - mse: 2418.1479 - mae: 28.8154 - val_loss: 1539.0675 - val_mse: 1539.0675 - val_mae: 26.2757\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2496.7347 - mse: 2496.7346 - mae: 29.4895 - val_loss: 1529.3009 - val_mse: 1529.3009 - val_mae: 26.3260\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2468.2761 - mse: 2468.2761 - mae: 28.6781 - val_loss: 1539.8041 - val_mse: 1539.8040 - val_mae: 26.2455\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2447.0398 - mse: 2447.0396 - mae: 29.2958 - val_loss: 1533.9570 - val_mse: 1533.9570 - val_mae: 26.2590\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 463us/step - loss: 2384.6881 - mse: 2384.6885 - mae: 28.7176 - val_loss: 1543.3293 - val_mse: 1543.3292 - val_mae: 26.1693\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 547us/step - loss: 2414.2970 - mse: 2414.2969 - mae: 28.8192 - val_loss: 1522.9368 - val_mse: 1522.9366 - val_mae: 26.3080\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2404.4341 - mse: 2404.4355 - mae: 28.8896 - val_loss: 1515.8616 - val_mse: 1515.8615 - val_mae: 26.3701\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2412.5435 - mse: 2412.5435 - mae: 28.9404 - val_loss: 1530.6130 - val_mse: 1530.6129 - val_mae: 26.2402\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 537us/step - loss: 2520.4234 - mse: 2520.4236 - mae: 29.5605 - val_loss: 1540.5717 - val_mse: 1540.5718 - val_mae: 26.1684\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 508us/step - loss: 2448.7924 - mse: 2448.7922 - mae: 28.7823 - val_loss: 1522.2040 - val_mse: 1522.2041 - val_mae: 26.2954\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 511us/step - loss: 2428.6715 - mse: 2428.6716 - mae: 28.6416 - val_loss: 1526.8898 - val_mse: 1526.8898 - val_mae: 26.2550\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2470.8764 - mse: 2470.8762 - mae: 29.4436 - val_loss: 1532.3082 - val_mse: 1532.3083 - val_mae: 26.2328\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2427.2971 - mse: 2427.2969 - mae: 28.7953 - val_loss: 1528.5153 - val_mse: 1528.5154 - val_mae: 26.2726\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2445.9060 - mse: 2445.9062 - mae: 29.1143 - val_loss: 1523.2193 - val_mse: 1523.2194 - val_mae: 26.3081\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2490.1367 - mse: 2490.1362 - mae: 29.0559 - val_loss: 1531.1214 - val_mse: 1531.1213 - val_mae: 26.2230\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2457.0829 - mse: 2457.0828 - mae: 29.1247 - val_loss: 1512.6296 - val_mse: 1512.6298 - val_mae: 26.3733\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 690us/step - loss: 2513.1143 - mse: 2513.1143 - mae: 29.6854 - val_loss: 1531.7531 - val_mse: 1531.7531 - val_mae: 26.2154\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2454.1664 - mse: 2454.1658 - mae: 29.2787 - val_loss: 1539.2554 - val_mse: 1539.2555 - val_mae: 26.1591\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2373.6308 - mse: 2373.6306 - mae: 29.1657 - val_loss: 1535.7877 - val_mse: 1535.7878 - val_mae: 26.1848\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 631us/step - loss: 2425.0636 - mse: 2425.0637 - mae: 29.1573 - val_loss: 1535.1697 - val_mse: 1535.1698 - val_mae: 26.1418\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2393.9406 - mse: 2393.9402 - mae: 28.5006 - val_loss: 1527.8953 - val_mse: 1527.8951 - val_mae: 26.1626\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2445.6949 - mse: 2445.6953 - mae: 28.9060 - val_loss: 1538.4764 - val_mse: 1538.4766 - val_mae: 26.0921\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 485us/step - loss: 2418.3306 - mse: 2418.3301 - mae: 28.5575 - val_loss: 1511.5995 - val_mse: 1511.5994 - val_mae: 26.3029\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 462us/step - loss: 2414.3612 - mse: 2414.3613 - mae: 28.8700 - val_loss: 1523.0232 - val_mse: 1523.0231 - val_mae: 26.1847\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 603us/step - loss: 2435.5755 - mse: 2435.5752 - mae: 29.0355 - val_loss: 1532.4406 - val_mse: 1532.4406 - val_mae: 26.1116\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2392.4117 - mse: 2392.4111 - mae: 28.6002 - val_loss: 1503.9126 - val_mse: 1503.9125 - val_mae: 26.3217\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2400.4735 - mse: 2400.4731 - mae: 29.1218 - val_loss: 1517.6264 - val_mse: 1517.6263 - val_mae: 26.2164\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2443.2691 - mse: 2443.2695 - mae: 29.0666 - val_loss: 1542.3848 - val_mse: 1542.3848 - val_mae: 26.0440\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2407.1913 - mse: 2407.1907 - mae: 28.5515 - val_loss: 1504.3940 - val_mse: 1504.3939 - val_mae: 26.3041\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2386.4071 - mse: 2386.4070 - mae: 28.4982 - val_loss: 1511.1759 - val_mse: 1511.1758 - val_mae: 26.2306\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2406.3998 - mse: 2406.3992 - mae: 28.3851 - val_loss: 1502.6167 - val_mse: 1502.6168 - val_mae: 26.2472\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 700us/step - loss: 2359.8276 - mse: 2359.8274 - mae: 28.4567 - val_loss: 1518.1914 - val_mse: 1518.1912 - val_mae: 26.0906\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 671us/step - loss: 2330.3768 - mse: 2330.3767 - mae: 28.6231 - val_loss: 1520.1308 - val_mse: 1520.1306 - val_mae: 26.0530\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2432.7791 - mse: 2432.7783 - mae: 29.3311 - val_loss: 1501.3218 - val_mse: 1501.3218 - val_mae: 26.1996\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2426.3791 - mse: 2426.3789 - mae: 28.9138 - val_loss: 1525.6705 - val_mse: 1525.6707 - val_mae: 26.0025\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2443.9727 - mse: 2443.9727 - mae: 28.6588 - val_loss: 1510.0741 - val_mse: 1510.0740 - val_mae: 26.1150\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2431.5273 - mse: 2431.5276 - mae: 28.5887 - val_loss: 1523.3673 - val_mse: 1523.3673 - val_mae: 25.9982\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2439.9303 - mse: 2439.9297 - mae: 29.1683 - val_loss: 1512.9498 - val_mse: 1512.9496 - val_mae: 26.0529\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2397.2441 - mse: 2397.2437 - mae: 28.5134 - val_loss: 1509.3325 - val_mse: 1509.3324 - val_mae: 26.0494\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 634us/step - loss: 2372.2031 - mse: 2372.2041 - mae: 29.4116 - val_loss: 3634.1096 - val_mse: 3634.1091 - val_mae: 22.8483\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2313.3940 - mse: 2313.3938 - mae: 29.2094 - val_loss: 3637.7326 - val_mse: 3637.7319 - val_mae: 22.5300\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2373.0569 - mse: 2373.0571 - mae: 29.4153 - val_loss: 3637.3044 - val_mse: 3637.3040 - val_mae: 22.7066\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 557us/step - loss: 2373.5612 - mse: 2373.5613 - mae: 29.6116 - val_loss: 3637.6778 - val_mse: 3637.6790 - val_mae: 22.4913\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2343.1776 - mse: 2343.1785 - mae: 29.4857 - val_loss: 3637.1710 - val_mse: 3637.1709 - val_mae: 22.9175\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2304.8947 - mse: 2304.8950 - mae: 29.3904 - val_loss: 3638.0869 - val_mse: 3638.0869 - val_mae: 23.3825\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2306.3626 - mse: 2306.3625 - mae: 29.1463 - val_loss: 3637.5048 - val_mse: 3637.5044 - val_mae: 23.0600\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2236.0585 - mse: 2236.0579 - mae: 28.9145 - val_loss: 3636.7624 - val_mse: 3636.7625 - val_mae: 23.2983\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2294.3330 - mse: 2294.3330 - mae: 29.1129 - val_loss: 3637.6352 - val_mse: 3637.6355 - val_mae: 23.0117\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 678us/step - loss: 2283.1056 - mse: 2283.1062 - mae: 28.8842 - val_loss: 3636.7270 - val_mse: 3636.7268 - val_mae: 23.0102\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2314.3844 - mse: 2314.3843 - mae: 28.9367 - val_loss: 3637.6918 - val_mse: 3637.6921 - val_mae: 23.3842\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2323.8573 - mse: 2323.8564 - mae: 29.4801 - val_loss: 3637.8025 - val_mse: 3637.8032 - val_mae: 22.9179\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2268.1186 - mse: 2268.1179 - mae: 28.8406 - val_loss: 3637.4240 - val_mse: 3637.4241 - val_mae: 22.6741\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2282.1557 - mse: 2282.1560 - mae: 28.8186 - val_loss: 3636.9268 - val_mse: 3636.9270 - val_mae: 22.4880\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2322.8088 - mse: 2322.8076 - mae: 29.3444 - val_loss: 3635.2785 - val_mse: 3635.2778 - val_mae: 22.6439\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2311.7423 - mse: 2311.7419 - mae: 28.9755 - val_loss: 3635.3436 - val_mse: 3635.3433 - val_mae: 22.8971\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2291.3375 - mse: 2291.3379 - mae: 29.0614 - val_loss: 3636.4669 - val_mse: 3636.4663 - val_mae: 22.5197\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2301.8244 - mse: 2301.8242 - mae: 29.1979 - val_loss: 3637.2238 - val_mse: 3637.2246 - val_mae: 22.4896\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2294.5836 - mse: 2294.5828 - mae: 28.9040 - val_loss: 3636.4779 - val_mse: 3636.4780 - val_mae: 22.8141\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 1s 471us/step - loss: 2327.7149 - mse: 2327.7153 - mae: 29.3980 - val_loss: 3635.9922 - val_mse: 3635.9927 - val_mae: 22.7035\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 532us/step - loss: 2339.7333 - mse: 2339.7332 - mae: 29.5945 - val_loss: 3635.9279 - val_mse: 3635.9277 - val_mae: 22.6948\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2215.0436 - mse: 2215.0425 - mae: 28.4565 - val_loss: 3635.2976 - val_mse: 3635.2986 - val_mae: 23.2890\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2304.9281 - mse: 2304.9280 - mae: 29.0805 - val_loss: 3635.3450 - val_mse: 3635.3447 - val_mae: 22.7910\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2240.5096 - mse: 2240.5100 - mae: 29.1144 - val_loss: 3636.1693 - val_mse: 3636.1697 - val_mae: 22.5578\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2278.2194 - mse: 2278.2195 - mae: 29.2805 - val_loss: 3635.4985 - val_mse: 3635.4976 - val_mae: 22.7240\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2288.0530 - mse: 2288.0544 - mae: 28.9749 - val_loss: 3636.7452 - val_mse: 3636.7454 - val_mae: 22.6285\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2327.7972 - mse: 2327.7971 - mae: 29.5391 - val_loss: 3637.1116 - val_mse: 3637.1118 - val_mae: 23.2912\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2351.8455 - mse: 2351.8459 - mae: 29.2189 - val_loss: 3638.0614 - val_mse: 3638.0605 - val_mae: 22.4623\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2285.3121 - mse: 2285.3120 - mae: 28.8721 - val_loss: 3636.5615 - val_mse: 3636.5623 - val_mae: 22.4611\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2297.8763 - mse: 2297.8767 - mae: 28.8061 - val_loss: 3635.9301 - val_mse: 3635.9297 - val_mae: 23.3181\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2322.1265 - mse: 2322.1270 - mae: 28.9344 - val_loss: 3635.1211 - val_mse: 3635.1218 - val_mae: 23.0946\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2305.1861 - mse: 2305.1860 - mae: 28.7712 - val_loss: 3637.1438 - val_mse: 3637.1438 - val_mae: 23.3763\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2296.9737 - mse: 2296.9731 - mae: 28.8382 - val_loss: 3636.4704 - val_mse: 3636.4702 - val_mae: 22.9308\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2337.2225 - mse: 2337.2231 - mae: 29.1062 - val_loss: 3637.1549 - val_mse: 3637.1553 - val_mae: 22.9209\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2301.4751 - mse: 2301.4756 - mae: 28.4744 - val_loss: 3636.8302 - val_mse: 3636.8303 - val_mae: 23.0347\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2282.7911 - mse: 2282.7913 - mae: 29.0953 - val_loss: 3637.0907 - val_mse: 3637.0906 - val_mae: 23.2658\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2261.8244 - mse: 2261.8245 - mae: 28.6810 - val_loss: 3636.5426 - val_mse: 3636.5422 - val_mae: 23.4152\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2193.9896 - mse: 2193.9907 - mae: 28.5762 - val_loss: 3637.1158 - val_mse: 3637.1162 - val_mae: 22.8248\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2234.8834 - mse: 2234.8835 - mae: 28.6493 - val_loss: 3637.0982 - val_mse: 3637.0981 - val_mae: 23.2823\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 619us/step - loss: 2293.0177 - mse: 2293.0181 - mae: 29.0686 - val_loss: 3637.3443 - val_mse: 3637.3445 - val_mae: 22.8337\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2291.0844 - mse: 2291.0845 - mae: 28.4794 - val_loss: 3637.1687 - val_mse: 3637.1694 - val_mae: 23.1083\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2293.4755 - mse: 2293.4753 - mae: 28.9230 - val_loss: 3638.7961 - val_mse: 3638.7961 - val_mae: 23.1587\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2282.6347 - mse: 2282.6350 - mae: 28.8076 - val_loss: 3638.2704 - val_mse: 3638.2705 - val_mae: 23.1857\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2313.1930 - mse: 2313.1934 - mae: 29.0942 - val_loss: 3638.8824 - val_mse: 3638.8821 - val_mae: 22.9966\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2334.2372 - mse: 2334.2371 - mae: 29.1964 - val_loss: 3638.6889 - val_mse: 3638.6887 - val_mae: 23.0057\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2334.5631 - mse: 2334.5627 - mae: 29.0685 - val_loss: 3639.1188 - val_mse: 3639.1189 - val_mae: 23.2286\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2296.0923 - mse: 2296.0918 - mae: 28.8201 - val_loss: 3639.4957 - val_mse: 3639.4954 - val_mae: 22.8659\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 1s 493us/step - loss: 2296.1791 - mse: 2296.1794 - mae: 29.0546 - val_loss: 3640.4177 - val_mse: 3640.4177 - val_mae: 22.4552\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2348.3704 - mse: 2348.3704 - mae: 28.8801 - val_loss: 3638.1931 - val_mse: 3638.1931 - val_mae: 22.9962\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2279.5979 - mse: 2279.5969 - mae: 28.7990 - val_loss: 3638.0788 - val_mse: 3638.0793 - val_mae: 23.1463\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 558us/step - loss: 2235.1791 - mse: 2235.1790 - mae: 28.7412 - val_loss: 3638.4137 - val_mse: 3638.4150 - val_mae: 23.0091\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 548us/step - loss: 2313.3253 - mse: 2313.3257 - mae: 29.2592 - val_loss: 3640.0443 - val_mse: 3640.0442 - val_mae: 22.6105\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2258.5951 - mse: 2258.5955 - mae: 28.7653 - val_loss: 3639.3659 - val_mse: 3639.3655 - val_mae: 22.6429\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2236.2512 - mse: 2236.2520 - mae: 28.2837 - val_loss: 3638.2426 - val_mse: 3638.2427 - val_mae: 22.8180\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2275.5409 - mse: 2275.5405 - mae: 28.6710 - val_loss: 3638.9086 - val_mse: 3638.9094 - val_mae: 22.9709\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2250.5419 - mse: 2250.5420 - mae: 28.8540 - val_loss: 3638.2753 - val_mse: 3638.2754 - val_mae: 22.8700\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2281.0690 - mse: 2281.0688 - mae: 29.0104 - val_loss: 3639.2247 - val_mse: 3639.2253 - val_mae: 22.5741\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2280.0526 - mse: 2280.0522 - mae: 28.7670 - val_loss: 3638.7251 - val_mse: 3638.7249 - val_mae: 22.6268\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2263.1283 - mse: 2263.1274 - mae: 29.0212 - val_loss: 3639.1024 - val_mse: 3639.1021 - val_mae: 22.6897\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2286.9921 - mse: 2286.9929 - mae: 28.4545 - val_loss: 3638.8800 - val_mse: 3638.8801 - val_mae: 23.0054\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2314.5082 - mse: 2314.5083 - mae: 29.0215 - val_loss: 3639.7659 - val_mse: 3639.7654 - val_mae: 23.0446\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2283.4212 - mse: 2283.4216 - mae: 28.7965 - val_loss: 3640.8666 - val_mse: 3640.8672 - val_mae: 23.3101\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2225.6857 - mse: 2225.6865 - mae: 28.2254 - val_loss: 3640.7681 - val_mse: 3640.7678 - val_mae: 23.4043\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 681us/step - loss: 2255.6808 - mse: 2255.6794 - mae: 28.7149 - val_loss: 3640.4464 - val_mse: 3640.4465 - val_mae: 23.2358\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2291.0911 - mse: 2291.0911 - mae: 28.5886 - val_loss: 3641.6522 - val_mse: 3641.6516 - val_mae: 23.5750\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2266.3840 - mse: 2266.3838 - mae: 28.6513 - val_loss: 3640.3536 - val_mse: 3640.3538 - val_mae: 22.8505\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2314.0513 - mse: 2314.0518 - mae: 28.8385 - val_loss: 3640.7879 - val_mse: 3640.7878 - val_mae: 22.8586\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2239.7683 - mse: 2239.7683 - mae: 28.2704 - val_loss: 3641.1570 - val_mse: 3641.1567 - val_mae: 23.4239\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 1s 470us/step - loss: 2268.7412 - mse: 2268.7422 - mae: 28.8995 - val_loss: 3641.2198 - val_mse: 3641.2200 - val_mae: 23.3021\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2280.3035 - mse: 2280.3025 - mae: 28.6918 - val_loss: 3639.8393 - val_mse: 3639.8394 - val_mae: 23.0339\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2272.9198 - mse: 2272.9194 - mae: 28.8137 - val_loss: 3638.9844 - val_mse: 3638.9839 - val_mae: 22.9883\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2262.4436 - mse: 2262.4431 - mae: 28.8885 - val_loss: 3640.5540 - val_mse: 3640.5544 - val_mae: 23.1560\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 531us/step - loss: 2275.9895 - mse: 2275.9907 - mae: 28.6057 - val_loss: 3640.6982 - val_mse: 3640.6982 - val_mae: 23.1728\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 563us/step - loss: 2256.7109 - mse: 2256.7114 - mae: 28.3499 - val_loss: 3640.9673 - val_mse: 3640.9673 - val_mae: 23.0441\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2262.2544 - mse: 2262.2546 - mae: 28.5465 - val_loss: 3641.5916 - val_mse: 3641.5918 - val_mae: 22.9393\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 1s 474us/step - loss: 2251.9736 - mse: 2251.9736 - mae: 28.4228 - val_loss: 3640.7725 - val_mse: 3640.7727 - val_mae: 23.0693\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2252.7739 - mse: 2252.7739 - mae: 28.4055 - val_loss: 3640.0669 - val_mse: 3640.0669 - val_mae: 23.1241\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 666us/step - loss: 2258.2239 - mse: 2258.2241 - mae: 28.6718 - val_loss: 3640.3660 - val_mse: 3640.3657 - val_mae: 23.2287\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2283.3839 - mse: 2283.3840 - mae: 28.8229 - val_loss: 3641.2684 - val_mse: 3641.2673 - val_mae: 23.1010\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2235.3205 - mse: 2235.3215 - mae: 28.1600 - val_loss: 3641.7079 - val_mse: 3641.7083 - val_mae: 23.4880\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2638.3900 - mse: 2638.3896 - mae: 28.2792 - val_loss: 2245.1141 - val_mse: 2245.1138 - val_mae: 25.7920\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2583.0419 - mse: 2583.0430 - mae: 28.0089 - val_loss: 2240.6130 - val_mse: 2240.6128 - val_mae: 25.5909\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2672.8081 - mse: 2672.8086 - mae: 28.5069 - val_loss: 2236.4668 - val_mse: 2236.4668 - val_mae: 25.7353\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2662.6573 - mse: 2662.6575 - mae: 28.2023 - val_loss: 2232.0563 - val_mse: 2232.0564 - val_mae: 25.7682\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2610.9358 - mse: 2610.9358 - mae: 28.0364 - val_loss: 2245.5830 - val_mse: 2245.5833 - val_mae: 25.6448\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2650.2801 - mse: 2650.2800 - mae: 28.2911 - val_loss: 2256.7547 - val_mse: 2256.7551 - val_mae: 25.5069\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2661.0826 - mse: 2661.0820 - mae: 28.1254 - val_loss: 2232.2380 - val_mse: 2232.2380 - val_mae: 25.9181\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 652us/step - loss: 2628.8487 - mse: 2628.8479 - mae: 28.0560 - val_loss: 2234.9991 - val_mse: 2234.9990 - val_mae: 25.6949\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2674.3254 - mse: 2674.3262 - mae: 28.0979 - val_loss: 2249.3695 - val_mse: 2249.3696 - val_mae: 25.4271\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2725.8928 - mse: 2725.8923 - mae: 28.6365 - val_loss: 2231.4919 - val_mse: 2231.4917 - val_mae: 25.8941\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2653.2018 - mse: 2653.2017 - mae: 28.0869 - val_loss: 2247.2853 - val_mse: 2247.2852 - val_mae: 25.5665\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2706.8907 - mse: 2706.8909 - mae: 28.5106 - val_loss: 2237.3115 - val_mse: 2237.3113 - val_mae: 25.8877\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2627.4838 - mse: 2627.4839 - mae: 27.9935 - val_loss: 2231.9979 - val_mse: 2231.9978 - val_mae: 25.8792\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2624.8157 - mse: 2624.8162 - mae: 28.3802 - val_loss: 2233.6853 - val_mse: 2233.6855 - val_mae: 25.7566\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2653.1332 - mse: 2653.1328 - mae: 28.1443 - val_loss: 2244.2412 - val_mse: 2244.2412 - val_mae: 25.5617\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2665.1451 - mse: 2665.1453 - mae: 28.4615 - val_loss: 2255.4997 - val_mse: 2255.4998 - val_mae: 25.3416\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2619.1631 - mse: 2619.1626 - mae: 27.9204 - val_loss: 2236.9045 - val_mse: 2236.9043 - val_mae: 25.6000\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2628.6917 - mse: 2628.6921 - mae: 28.0674 - val_loss: 2234.0477 - val_mse: 2234.0479 - val_mae: 25.7948\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2656.4302 - mse: 2656.4304 - mae: 28.0518 - val_loss: 2229.7905 - val_mse: 2229.7900 - val_mae: 25.6537\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2627.0664 - mse: 2627.0662 - mae: 28.0150 - val_loss: 2226.6846 - val_mse: 2226.6846 - val_mae: 25.5256\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2680.5117 - mse: 2680.5110 - mae: 28.3386 - val_loss: 2224.5473 - val_mse: 2224.5476 - val_mae: 25.4815\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2599.1040 - mse: 2599.1057 - mae: 27.5777 - val_loss: 2204.7883 - val_mse: 2204.7883 - val_mae: 25.8646\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2617.1842 - mse: 2617.1836 - mae: 28.0457 - val_loss: 2201.4430 - val_mse: 2201.4429 - val_mae: 25.8580\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 534us/step - loss: 2626.3971 - mse: 2626.3977 - mae: 27.9739 - val_loss: 2201.5002 - val_mse: 2201.4998 - val_mae: 25.9227\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 541us/step - loss: 2663.6477 - mse: 2663.6482 - mae: 28.2912 - val_loss: 2220.0223 - val_mse: 2220.0220 - val_mae: 25.5098\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2658.8339 - mse: 2658.8347 - mae: 27.8325 - val_loss: 2211.2257 - val_mse: 2211.2258 - val_mae: 25.5910\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2665.9465 - mse: 2665.9465 - mae: 28.2599 - val_loss: 2220.8956 - val_mse: 2220.8955 - val_mae: 25.5444\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2651.4650 - mse: 2651.4653 - mae: 28.4189 - val_loss: 2213.4967 - val_mse: 2213.4961 - val_mae: 25.6243\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 652us/step - loss: 2578.7947 - mse: 2578.7939 - mae: 27.6552 - val_loss: 2220.3139 - val_mse: 2220.3142 - val_mae: 25.9103\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2575.9656 - mse: 2575.9661 - mae: 28.1215 - val_loss: 2216.2744 - val_mse: 2216.2744 - val_mae: 25.9443\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2656.4459 - mse: 2656.4453 - mae: 28.4499 - val_loss: 2235.3389 - val_mse: 2235.3391 - val_mae: 25.4074\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 524us/step - loss: 2661.5508 - mse: 2661.5513 - mae: 28.2483 - val_loss: 2217.3822 - val_mse: 2217.3823 - val_mae: 25.8048\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2618.1176 - mse: 2618.1179 - mae: 28.0606 - val_loss: 2222.5213 - val_mse: 2222.5212 - val_mae: 25.5954\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2646.0316 - mse: 2646.0312 - mae: 28.0879 - val_loss: 2211.2662 - val_mse: 2211.2659 - val_mae: 25.7140\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2641.9735 - mse: 2641.9739 - mae: 27.9114 - val_loss: 2219.7409 - val_mse: 2219.7410 - val_mae: 25.7273\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2598.0288 - mse: 2598.0273 - mae: 27.8173 - val_loss: 2209.2755 - val_mse: 2209.2756 - val_mae: 25.5599\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 716us/step - loss: 2624.0686 - mse: 2624.0688 - mae: 27.9555 - val_loss: 2221.5031 - val_mse: 2221.5029 - val_mae: 25.4206\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2566.6457 - mse: 2566.6448 - mae: 27.7890 - val_loss: 2212.9732 - val_mse: 2212.9729 - val_mae: 25.7101\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2627.0022 - mse: 2627.0022 - mae: 28.2181 - val_loss: 2221.4452 - val_mse: 2221.4453 - val_mae: 25.5578\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2670.8604 - mse: 2670.8606 - mae: 28.3354 - val_loss: 2229.2537 - val_mse: 2229.2537 - val_mae: 25.3673\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2636.5426 - mse: 2636.5430 - mae: 27.7534 - val_loss: 2225.7965 - val_mse: 2225.7961 - val_mae: 25.5214\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2650.5146 - mse: 2650.5149 - mae: 28.2593 - val_loss: 2226.6398 - val_mse: 2226.6396 - val_mae: 25.5205\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2575.9720 - mse: 2575.9717 - mae: 28.0594 - val_loss: 2225.9511 - val_mse: 2225.9512 - val_mae: 25.4928\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 680us/step - loss: 2650.0961 - mse: 2650.0962 - mae: 28.0303 - val_loss: 2218.1209 - val_mse: 2218.1211 - val_mae: 25.8033\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2600.6284 - mse: 2600.6282 - mae: 28.1732 - val_loss: 2218.0828 - val_mse: 2218.0825 - val_mae: 25.7351\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 552us/step - loss: 2626.0753 - mse: 2626.0747 - mae: 28.1064 - val_loss: 2220.4911 - val_mse: 2220.4912 - val_mae: 25.5532\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2610.4075 - mse: 2610.4082 - mae: 27.9188 - val_loss: 2228.5538 - val_mse: 2228.5542 - val_mae: 25.4881\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2597.5163 - mse: 2597.5161 - mae: 27.7683 - val_loss: 2227.7440 - val_mse: 2227.7439 - val_mae: 25.1496\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2640.9769 - mse: 2640.9768 - mae: 27.7566 - val_loss: 2204.1657 - val_mse: 2204.1658 - val_mae: 25.6751\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2650.6885 - mse: 2650.6887 - mae: 28.1472 - val_loss: 2215.2080 - val_mse: 2215.2083 - val_mae: 25.9082\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2643.7209 - mse: 2643.7214 - mae: 28.1374 - val_loss: 2226.3309 - val_mse: 2226.3308 - val_mae: 25.3982\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2622.3613 - mse: 2622.3611 - mae: 28.0278 - val_loss: 2210.9196 - val_mse: 2210.9194 - val_mae: 25.6553\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2631.2569 - mse: 2631.2571 - mae: 28.0169 - val_loss: 2223.1312 - val_mse: 2223.1311 - val_mae: 25.4535\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2625.9319 - mse: 2625.9316 - mae: 28.1594 - val_loss: 2213.2088 - val_mse: 2213.2087 - val_mae: 25.6535\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2579.1004 - mse: 2579.1016 - mae: 28.2789 - val_loss: 2218.7540 - val_mse: 2218.7542 - val_mae: 25.3206\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2610.4388 - mse: 2610.4382 - mae: 27.7412 - val_loss: 2225.9074 - val_mse: 2225.9075 - val_mae: 25.2124\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 532us/step - loss: 2666.8619 - mse: 2666.8621 - mae: 28.3855 - val_loss: 2226.8582 - val_mse: 2226.8584 - val_mae: 25.2696\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 666us/step - loss: 2644.8269 - mse: 2644.8279 - mae: 27.9833 - val_loss: 2220.4669 - val_mse: 2220.4668 - val_mae: 25.5503\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2596.4476 - mse: 2596.4473 - mae: 27.8876 - val_loss: 2219.9562 - val_mse: 2219.9563 - val_mae: 25.9011\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2629.6408 - mse: 2629.6404 - mae: 28.1549 - val_loss: 2221.8267 - val_mse: 2221.8269 - val_mae: 25.6388\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2660.8075 - mse: 2660.8071 - mae: 27.7729 - val_loss: 2213.7760 - val_mse: 2213.7761 - val_mae: 25.7303\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2640.2093 - mse: 2640.2090 - mae: 28.1603 - val_loss: 2221.6564 - val_mse: 2221.6565 - val_mae: 25.4384\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2585.6836 - mse: 2585.6836 - mae: 27.8894 - val_loss: 2211.4864 - val_mse: 2211.4863 - val_mae: 25.7321\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 554us/step - loss: 2622.0293 - mse: 2622.0283 - mae: 28.0096 - val_loss: 2223.2670 - val_mse: 2223.2668 - val_mae: 25.5368\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2631.4847 - mse: 2631.4846 - mae: 28.0447 - val_loss: 2224.6547 - val_mse: 2224.6545 - val_mae: 25.2668\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2617.6737 - mse: 2617.6741 - mae: 27.9542 - val_loss: 2220.7328 - val_mse: 2220.7329 - val_mae: 25.3805\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2624.7097 - mse: 2624.7100 - mae: 27.6614 - val_loss: 2211.0825 - val_mse: 2211.0825 - val_mae: 25.5571\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2618.2259 - mse: 2618.2268 - mae: 27.5035 - val_loss: 2206.0968 - val_mse: 2206.0972 - val_mae: 25.6737\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 653us/step - loss: 2623.6341 - mse: 2623.6335 - mae: 28.0235 - val_loss: 2216.3465 - val_mse: 2216.3464 - val_mae: 25.2339\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2639.5741 - mse: 2639.5742 - mae: 27.8804 - val_loss: 2204.5295 - val_mse: 2204.5293 - val_mae: 25.6021\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 508us/step - loss: 2592.5668 - mse: 2592.5676 - mae: 27.7020 - val_loss: 2204.9998 - val_mse: 2204.9995 - val_mae: 25.5223\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2586.4556 - mse: 2586.4563 - mae: 27.5788 - val_loss: 2197.3943 - val_mse: 2197.3945 - val_mae: 25.8575\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2610.2749 - mse: 2610.2739 - mae: 27.9684 - val_loss: 2223.3959 - val_mse: 2223.3960 - val_mae: 25.3629\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2629.6979 - mse: 2629.6975 - mae: 27.4628 - val_loss: 2231.1794 - val_mse: 2231.1794 - val_mae: 25.3878\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2663.6004 - mse: 2663.6003 - mae: 28.0541 - val_loss: 2224.2590 - val_mse: 2224.2585 - val_mae: 25.3892\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2590.1033 - mse: 2590.1033 - mae: 27.5075 - val_loss: 2209.4253 - val_mse: 2209.4255 - val_mae: 25.7167\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2622.0008 - mse: 2622.0000 - mae: 27.9705 - val_loss: 2213.2787 - val_mse: 2213.2791 - val_mae: 25.4094\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2639.2297 - mse: 2639.2297 - mae: 27.8657 - val_loss: 2204.4385 - val_mse: 2204.4382 - val_mae: 25.6533\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2658.4143 - mse: 2658.4155 - mae: 28.0617 - val_loss: 2204.7169 - val_mse: 2204.7168 - val_mae: 25.7080\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2630.1047 - mse: 2630.1042 - mae: 27.6686 - val_loss: 2224.5280 - val_mse: 2224.5278 - val_mae: 25.3922\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 13353.6516 - mse: 13353.6514 - mae: 110.0145 - val_loss: 34674.4385 - val_mse: 34674.4375 - val_mae: 132.9389\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 13291.4848 - mse: 13291.4844 - mae: 109.7337 - val_loss: 34564.2852 - val_mse: 34564.2852 - val_mae: 132.5259\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 13124.6282 - mse: 13124.6289 - mae: 108.9760 - val_loss: 34247.9168 - val_mse: 34247.9141 - val_mae: 131.3365\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 701us/step - loss: 12701.7362 - mse: 12701.7363 - mae: 107.0459 - val_loss: 33389.5163 - val_mse: 33389.5156 - val_mae: 128.0624\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 642us/step - loss: 11569.9803 - mse: 11569.9805 - mae: 101.5555 - val_loss: 31085.7709 - val_mse: 31085.7715 - val_mae: 118.8536\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 8960.6656 - mse: 8960.6650 - mae: 87.4730 - val_loss: 25963.4738 - val_mse: 25963.4746 - val_mae: 95.2872\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 547us/step - loss: 4842.5779 - mse: 4842.5781 - mae: 57.5736 - val_loss: 18984.2553 - val_mse: 18984.2559 - val_mae: 47.7249\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 543us/step - loss: 2885.4766 - mse: 2885.4768 - mae: 38.0189 - val_loss: 17211.5662 - val_mse: 17211.5645 - val_mae: 37.9499\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 2975.1814 - mse: 2975.1812 - mae: 40.7123 - val_loss: 17535.2255 - val_mse: 17535.2246 - val_mae: 38.1138\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 688us/step - loss: 2763.3549 - mse: 2763.3550 - mae: 38.9840 - val_loss: 17509.8549 - val_mse: 17509.8535 - val_mae: 37.9851\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 723us/step - loss: 2730.7497 - mse: 2730.7498 - mae: 37.8363 - val_loss: 17362.1784 - val_mse: 17362.1758 - val_mae: 37.7386\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 657us/step - loss: 2634.3046 - mse: 2634.3044 - mae: 36.7172 - val_loss: 17456.8290 - val_mse: 17456.8281 - val_mae: 37.7723\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2717.3254 - mse: 2717.3252 - mae: 37.1617 - val_loss: 17430.7326 - val_mse: 17430.7324 - val_mae: 37.6743\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 693us/step - loss: 2475.9445 - mse: 2475.9448 - mae: 35.4081 - val_loss: 17483.8721 - val_mse: 17483.8711 - val_mae: 37.6884\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2456.8237 - mse: 2456.8237 - mae: 36.2124 - val_loss: 17427.5692 - val_mse: 17427.5703 - val_mae: 37.5567\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 638us/step - loss: 2342.4259 - mse: 2342.4260 - mae: 35.3025 - val_loss: 17380.6970 - val_mse: 17380.6953 - val_mae: 37.4754\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2610.2869 - mse: 2610.2869 - mae: 36.1192 - val_loss: 17419.3150 - val_mse: 17419.3145 - val_mae: 37.4569\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 543us/step - loss: 2948.1469 - mse: 2948.1467 - mae: 39.9814 - val_loss: 17597.4169 - val_mse: 17597.4160 - val_mae: 37.8515\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 2752.4883 - mse: 2752.4883 - mae: 38.7597 - val_loss: 17880.5201 - val_mse: 17880.5195 - val_mae: 38.7912\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 665us/step - loss: 2634.4407 - mse: 2634.4404 - mae: 36.5670 - val_loss: 17650.5530 - val_mse: 17650.5527 - val_mae: 37.9617\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 2708.9973 - mse: 2708.9973 - mae: 38.2776 - val_loss: 17495.0520 - val_mse: 17495.0527 - val_mae: 37.3601\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2288.9642 - mse: 2288.9641 - mae: 34.6116 - val_loss: 17569.6124 - val_mse: 17569.6133 - val_mae: 37.5737\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2374.8954 - mse: 2374.8955 - mae: 35.5161 - val_loss: 17439.6297 - val_mse: 17439.6309 - val_mae: 37.1704\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2608.7020 - mse: 2608.7019 - mae: 36.9028 - val_loss: 17539.8173 - val_mse: 17539.8184 - val_mae: 37.4062\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2448.1535 - mse: 2448.1531 - mae: 35.4584 - val_loss: 17452.9104 - val_mse: 17452.9102 - val_mae: 37.1025\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2658.4224 - mse: 2658.4224 - mae: 35.6543 - val_loss: 17315.3762 - val_mse: 17315.3789 - val_mae: 37.0103\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2298.0779 - mse: 2298.0776 - mae: 34.0193 - val_loss: 17334.4763 - val_mse: 17334.4766 - val_mae: 36.9783\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 2202.8527 - mse: 2202.8528 - mae: 32.8000 - val_loss: 17430.4890 - val_mse: 17430.4902 - val_mae: 36.9670\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 448us/step - loss: 2033.0492 - mse: 2033.0492 - mae: 32.0933 - val_loss: 17372.7933 - val_mse: 17372.7949 - val_mae: 36.9114\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2538.0587 - mse: 2538.0588 - mae: 35.1392 - val_loss: 17492.5825 - val_mse: 17492.5840 - val_mae: 37.0191\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2248.5004 - mse: 2248.5005 - mae: 33.7080 - val_loss: 17325.6010 - val_mse: 17325.6016 - val_mae: 36.8627\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 542us/step - loss: 2741.7832 - mse: 2741.7827 - mae: 37.6679 - val_loss: 17713.4389 - val_mse: 17713.4375 - val_mae: 37.7344\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 499us/step - loss: 2444.3598 - mse: 2444.3604 - mae: 35.2797 - val_loss: 17439.5586 - val_mse: 17439.5605 - val_mae: 36.8361\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 605us/step - loss: 2454.2900 - mse: 2454.2903 - mae: 35.3660 - val_loss: 17451.9486 - val_mse: 17451.9473 - val_mae: 36.8265\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2457.0281 - mse: 2457.0283 - mae: 34.3935 - val_loss: 17475.6283 - val_mse: 17475.6289 - val_mae: 36.8332\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2336.4601 - mse: 2336.4597 - mae: 34.2670 - val_loss: 17512.5091 - val_mse: 17512.5078 - val_mae: 36.8968\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2192.3347 - mse: 2192.3350 - mae: 33.6784 - val_loss: 17446.1466 - val_mse: 17446.1465 - val_mae: 36.7236\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 659us/step - loss: 2203.5527 - mse: 2203.5527 - mae: 32.8087 - val_loss: 17443.5042 - val_mse: 17443.5039 - val_mae: 36.6789\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2448.2561 - mse: 2448.2561 - mae: 35.5324 - val_loss: 17475.4283 - val_mse: 17475.4277 - val_mae: 36.7053\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 424us/step - loss: 2370.9458 - mse: 2370.9460 - mae: 35.0360 - val_loss: 17477.7379 - val_mse: 17477.7383 - val_mae: 36.6884\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 681us/step - loss: 2347.3700 - mse: 2347.3701 - mae: 34.6605 - val_loss: 17560.8032 - val_mse: 17560.8047 - val_mae: 36.8955\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 680us/step - loss: 2249.7867 - mse: 2249.7869 - mae: 33.2088 - val_loss: 17385.8157 - val_mse: 17385.8164 - val_mae: 36.5236\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 630us/step - loss: 2132.0308 - mse: 2132.0308 - mae: 33.2799 - val_loss: 17381.2126 - val_mse: 17381.2129 - val_mae: 36.5030\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2216.4749 - mse: 2216.4751 - mae: 33.6826 - val_loss: 17473.9600 - val_mse: 17473.9609 - val_mae: 36.6162\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 461us/step - loss: 2117.9004 - mse: 2117.9001 - mae: 32.5920 - val_loss: 17350.0864 - val_mse: 17350.0859 - val_mae: 36.4605\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2297.9481 - mse: 2297.9480 - mae: 34.1883 - val_loss: 17483.1982 - val_mse: 17483.1992 - val_mae: 36.6167\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 2132.8680 - mse: 2132.8682 - mae: 33.0463 - val_loss: 17395.2446 - val_mse: 17395.2441 - val_mae: 36.4436\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 526us/step - loss: 2166.9239 - mse: 2166.9238 - mae: 32.3870 - val_loss: 17408.7610 - val_mse: 17408.7617 - val_mae: 36.4566\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2390.9674 - mse: 2390.9678 - mae: 33.7489 - val_loss: 17577.2999 - val_mse: 17577.3008 - val_mae: 36.8100\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 423us/step - loss: 2287.8382 - mse: 2287.8384 - mae: 34.4250 - val_loss: 17537.2474 - val_mse: 17537.2461 - val_mae: 36.6793\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 471us/step - loss: 2041.0747 - mse: 2041.0746 - mae: 32.2887 - val_loss: 17290.9181 - val_mse: 17290.9180 - val_mae: 36.4076\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 2137.3588 - mse: 2137.3589 - mae: 32.68 - 0s 511us/step - loss: 2081.1564 - mse: 2081.1565 - mae: 32.2732 - val_loss: 17574.2849 - val_mse: 17574.2852 - val_mae: 36.7628\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 545us/step - loss: 2262.6768 - mse: 2262.6765 - mae: 33.5419 - val_loss: 17469.0181 - val_mse: 17469.0176 - val_mae: 36.5003\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 429us/step - loss: 2457.1272 - mse: 2457.1274 - mae: 34.8632 - val_loss: 17558.0799 - val_mse: 17558.0781 - val_mae: 36.6847\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 552us/step - loss: 2229.9362 - mse: 2229.9360 - mae: 33.2264 - val_loss: 17462.8165 - val_mse: 17462.8184 - val_mae: 36.4753\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 535us/step - loss: 2192.2365 - mse: 2192.2366 - mae: 32.2371 - val_loss: 17450.8009 - val_mse: 17450.8027 - val_mae: 36.4383\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 2104.8863 - mse: 2104.8862 - mae: 31.4863 - val_loss: 17412.1561 - val_mse: 17412.1562 - val_mae: 36.3610\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 428us/step - loss: 2080.7130 - mse: 2080.7129 - mae: 32.3477 - val_loss: 17507.4355 - val_mse: 17507.4355 - val_mae: 36.5075\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 525us/step - loss: 2065.9880 - mse: 2065.9880 - mae: 31.5182 - val_loss: 17368.9744 - val_mse: 17368.9746 - val_mae: 36.2795\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 429us/step - loss: 2157.8436 - mse: 2157.8438 - mae: 32.4722 - val_loss: 17568.3855 - val_mse: 17568.3867 - val_mae: 36.6101\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 489us/step - loss: 2020.6028 - mse: 2020.6029 - mae: 30.6671 - val_loss: 17397.5335 - val_mse: 17397.5332 - val_mae: 36.3012\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 434us/step - loss: 2113.7283 - mse: 2113.7285 - mae: 31.9371 - val_loss: 17591.1902 - val_mse: 17591.1895 - val_mae: 36.6630\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 462us/step - loss: 1985.1385 - mse: 1985.1385 - mae: 30.7609 - val_loss: 17419.1926 - val_mse: 17419.1914 - val_mae: 36.3115\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 431us/step - loss: 2016.1178 - mse: 2016.1178 - mae: 31.4296 - val_loss: 17537.5610 - val_mse: 17537.5625 - val_mae: 36.5109\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 458us/step - loss: 2091.5177 - mse: 2091.5178 - mae: 32.3282 - val_loss: 17434.6011 - val_mse: 17434.6016 - val_mae: 36.3207\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 491us/step - loss: 2121.8722 - mse: 2121.8726 - mae: 31.8414 - val_loss: 17531.7952 - val_mse: 17531.7949 - val_mae: 36.4667\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 441us/step - loss: 2173.5265 - mse: 2173.5266 - mae: 32.1191 - val_loss: 17499.2872 - val_mse: 17499.2852 - val_mae: 36.3738\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 491us/step - loss: 2042.0633 - mse: 2042.0631 - mae: 31.2785 - val_loss: 17461.8914 - val_mse: 17461.8926 - val_mae: 36.3224\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 485us/step - loss: 2189.2812 - mse: 2189.2812 - mae: 32.0342 - val_loss: 17568.4527 - val_mse: 17568.4531 - val_mae: 36.5454\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 1986.8823 - mse: 1986.8824 - mae: 30.9328 - val_loss: 17298.1446 - val_mse: 17298.1465 - val_mae: 36.1613\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 433us/step - loss: 2026.4258 - mse: 2026.4259 - mae: 31.2237 - val_loss: 17410.4401 - val_mse: 17410.4395 - val_mae: 36.2519\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 435us/step - loss: 2122.8740 - mse: 2122.8740 - mae: 32.2851 - val_loss: 17509.8282 - val_mse: 17509.8281 - val_mae: 36.3738\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 1978.8731 - mse: 1978.8733 - mae: 30.4008 - val_loss: 17693.0292 - val_mse: 17693.0293 - val_mae: 37.0590\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 455us/step - loss: 1985.9397 - mse: 1985.9396 - mae: 30.3061 - val_loss: 17505.4733 - val_mse: 17505.4727 - val_mae: 36.3839\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2048.7473 - mse: 2048.7471 - mae: 31.3491 - val_loss: 17287.4515 - val_mse: 17287.4512 - val_mae: 36.1611\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2027.9872 - mse: 2027.9873 - mae: 31.6844 - val_loss: 17664.2525 - val_mse: 17664.2520 - val_mae: 36.9688\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2044.3610 - mse: 2044.3610 - mae: 31.5332 - val_loss: 17479.4888 - val_mse: 17479.4883 - val_mae: 36.3918\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 623us/step - loss: 2161.3005 - mse: 2161.3008 - mae: 32.2308 - val_loss: 17596.5592 - val_mse: 17596.5605 - val_mae: 36.7117\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 565us/step - loss: 2063.6519 - mse: 2063.6519 - mae: 31.2888 - val_loss: 17515.8830 - val_mse: 17515.8828 - val_mae: 36.4537\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 1886.9408 - mse: 1886.9410 - mae: 30.3400 - val_loss: 17638.9824 - val_mse: 17638.9844 - val_mae: 36.8592\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4170.5151 - mse: 4170.5146 - mae: 34.8085 - val_loss: 2169.6799 - val_mse: 2169.6797 - val_mae: 30.5277\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 4125.4216 - mse: 4125.4214 - mae: 35.0236 - val_loss: 2261.7934 - val_mse: 2261.7932 - val_mae: 30.8867\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 4291.9038 - mse: 4291.9038 - mae: 35.4996 - val_loss: 2340.3727 - val_mse: 2340.3728 - val_mae: 31.2840\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4448.8043 - mse: 4448.8052 - mae: 35.7280 - val_loss: 2347.4545 - val_mse: 2347.4546 - val_mae: 31.3040\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4254.7488 - mse: 4254.7485 - mae: 35.5080 - val_loss: 2249.6748 - val_mse: 2249.6746 - val_mae: 30.8019\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4203.1930 - mse: 4203.1924 - mae: 34.4727 - val_loss: 2354.4372 - val_mse: 2354.4373 - val_mae: 31.3247\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 699us/step - loss: 4120.0003 - mse: 4120.0005 - mae: 35.1154 - val_loss: 2295.3443 - val_mse: 2295.3442 - val_mae: 31.0097\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 721us/step - loss: 4373.8062 - mse: 4373.8066 - mae: 36.0559 - val_loss: 2356.0417 - val_mse: 2356.0415 - val_mae: 31.3056\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 614us/step - loss: 4338.6273 - mse: 4338.6279 - mae: 36.0570 - val_loss: 2369.2052 - val_mse: 2369.2053 - val_mae: 31.3631\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 663us/step - loss: 4269.0693 - mse: 4269.0703 - mae: 34.8186 - val_loss: 2269.2795 - val_mse: 2269.2795 - val_mae: 30.8460\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4089.0948 - mse: 4089.0942 - mae: 34.1536 - val_loss: 2290.5958 - val_mse: 2290.5957 - val_mae: 30.9475\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 633us/step - loss: 4235.7135 - mse: 4235.7139 - mae: 35.6490 - val_loss: 2323.1992 - val_mse: 2323.1990 - val_mae: 31.1177\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4288.4108 - mse: 4288.4111 - mae: 35.5292 - val_loss: 2305.7508 - val_mse: 2305.7505 - val_mae: 31.0174\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 4347.7417 - mse: 4347.7417 - mae: 35.1116 - val_loss: 2393.3165 - val_mse: 2393.3164 - val_mae: 31.4503\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4309.6197 - mse: 4309.6196 - mae: 35.4421 - val_loss: 2301.6823 - val_mse: 2301.6826 - val_mae: 30.9751\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4357.5044 - mse: 4357.5039 - mae: 35.0509 - val_loss: 2357.4941 - val_mse: 2357.4939 - val_mae: 31.2463\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 633us/step - loss: 3989.2823 - mse: 3989.2820 - mae: 34.5316 - val_loss: 2297.0855 - val_mse: 2297.0854 - val_mae: 30.9336\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4148.7235 - mse: 4148.7236 - mae: 34.3871 - val_loss: 2275.0518 - val_mse: 2275.0518 - val_mae: 30.8170\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 643us/step - loss: 4179.7410 - mse: 4179.7417 - mae: 34.8278 - val_loss: 2254.0022 - val_mse: 2254.0022 - val_mae: 30.7049\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 3929.0264 - mse: 3929.0269 - mae: 34.0078 - val_loss: 2234.2632 - val_mse: 2234.2629 - val_mae: 30.6100\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4097.2204 - mse: 4097.2202 - mae: 34.9837 - val_loss: 2325.7981 - val_mse: 2325.7981 - val_mae: 31.0626\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4242.4197 - mse: 4242.4199 - mae: 34.3090 - val_loss: 2406.2092 - val_mse: 2406.2095 - val_mae: 31.4831\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4200.0735 - mse: 4200.0732 - mae: 34.2726 - val_loss: 2304.8441 - val_mse: 2304.8440 - val_mae: 30.9468\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4276.2000 - mse: 4276.1997 - mae: 35.0994 - val_loss: 2372.1419 - val_mse: 2372.1418 - val_mae: 31.2861\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 636us/step - loss: 3907.2831 - mse: 3907.2827 - mae: 34.1624 - val_loss: 2261.0263 - val_mse: 2261.0264 - val_mae: 30.7001\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 704us/step - loss: 4228.3661 - mse: 4228.3662 - mae: 35.0315 - val_loss: 2322.4063 - val_mse: 2322.4062 - val_mae: 31.0227\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4098.9178 - mse: 4098.9170 - mae: 33.9392 - val_loss: 2299.5717 - val_mse: 2299.5718 - val_mae: 30.8985\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 661us/step - loss: 4216.6042 - mse: 4216.6045 - mae: 34.5951 - val_loss: 2334.8429 - val_mse: 2334.8430 - val_mae: 31.0763\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 4140.5605 - mse: 4140.5605 - mae: 33.4935 - val_loss: 2315.3785 - val_mse: 2315.3784 - val_mae: 30.9678\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 738us/step - loss: 4126.7810 - mse: 4126.7812 - mae: 34.9560 - val_loss: 2296.2068 - val_mse: 2296.2070 - val_mae: 30.8557\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 675us/step - loss: 4125.6596 - mse: 4125.6597 - mae: 34.4619 - val_loss: 2360.8078 - val_mse: 2360.8081 - val_mae: 31.1845\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 702us/step - loss: 4124.9844 - mse: 4124.9849 - mae: 34.5249 - val_loss: 2315.9186 - val_mse: 2315.9185 - val_mae: 30.9482\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4157.0034 - mse: 4157.0039 - mae: 34.3432 - val_loss: 2290.7580 - val_mse: 2290.7578 - val_mae: 30.8109\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 552us/step - loss: 4304.3905 - mse: 4304.3906 - mae: 35.3416 - val_loss: 2259.9849 - val_mse: 2259.9849 - val_mae: 30.6441\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 4102.7157 - mse: 4102.7158 - mae: 34.6651 - val_loss: 2377.7842 - val_mse: 2377.7842 - val_mae: 31.2679\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 4230.0056 - mse: 4230.0049 - mae: 35.0349 - val_loss: 2379.7824 - val_mse: 2379.7825 - val_mae: 31.2749\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 649us/step - loss: 4151.9727 - mse: 4151.9722 - mae: 35.1013 - val_loss: 2301.3642 - val_mse: 2301.3643 - val_mae: 30.8541\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4175.9578 - mse: 4175.9580 - mae: 33.7673 - val_loss: 2319.5180 - val_mse: 2319.5181 - val_mae: 30.9471\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 674us/step - loss: 4078.0993 - mse: 4078.0991 - mae: 34.5639 - val_loss: 2371.5100 - val_mse: 2371.5103 - val_mae: 31.2222\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4140.4242 - mse: 4140.4238 - mae: 34.5075 - val_loss: 2293.9497 - val_mse: 2293.9497 - val_mae: 30.8155\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4038.9895 - mse: 4038.9897 - mae: 34.8789 - val_loss: 2309.0708 - val_mse: 2309.0710 - val_mae: 30.8940\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4062.2243 - mse: 4062.2244 - mae: 33.8815 - val_loss: 2308.3352 - val_mse: 2308.3354 - val_mae: 30.8853\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 533us/step - loss: 4098.8696 - mse: 4098.8696 - mae: 35.0839 - val_loss: 2340.2247 - val_mse: 2340.2249 - val_mae: 31.0414\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 0s 495us/step - loss: 4150.8278 - mse: 4150.8276 - mae: 34.6789 - val_loss: 2309.2632 - val_mse: 2309.2632 - val_mae: 30.8841\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 4285.2072 - mse: 4285.2070 - mae: 35.7636 - val_loss: 2381.5963 - val_mse: 2381.5962 - val_mae: 31.2660\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 4057.7420 - mse: 4057.7424 - mae: 33.6327 - val_loss: 2330.3819 - val_mse: 2330.3821 - val_mae: 30.9812\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 703us/step - loss: 4027.8404 - mse: 4027.8408 - mae: 33.9937 - val_loss: 2297.7953 - val_mse: 2297.7952 - val_mae: 30.8042\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 3957.9730 - mse: 3957.9729 - mae: 32.9206 - val_loss: 2223.7141 - val_mse: 2223.7141 - val_mae: 30.4462\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 548us/step - loss: 4064.4174 - mse: 4064.4175 - mae: 34.1080 - val_loss: 2258.9952 - val_mse: 2258.9951 - val_mae: 30.5970\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 626us/step - loss: 4126.0072 - mse: 4126.0068 - mae: 33.7655 - val_loss: 2337.8178 - val_mse: 2337.8179 - val_mae: 30.9971\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 3964.3866 - mse: 3964.3875 - mae: 32.8309 - val_loss: 2246.6014 - val_mse: 2246.6018 - val_mae: 30.5295\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4160.9664 - mse: 4160.9668 - mae: 34.8302 - val_loss: 2314.0157 - val_mse: 2314.0154 - val_mae: 30.8619\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 664us/step - loss: 4240.1895 - mse: 4240.1895 - mae: 34.1739 - val_loss: 2284.3882 - val_mse: 2284.3882 - val_mae: 30.7132\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4195.6804 - mse: 4195.6802 - mae: 33.9110 - val_loss: 2264.4433 - val_mse: 2264.4431 - val_mae: 30.6144\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 661us/step - loss: 4239.0563 - mse: 4239.0557 - mae: 34.2389 - val_loss: 2285.2340 - val_mse: 2285.2339 - val_mae: 30.7209\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4164.1338 - mse: 4164.1333 - mae: 34.6757 - val_loss: 2275.3825 - val_mse: 2275.3826 - val_mae: 30.6738\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 675us/step - loss: 4186.6248 - mse: 4186.6250 - mae: 34.6236 - val_loss: 2306.8489 - val_mse: 2306.8491 - val_mae: 30.8335\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 677us/step - loss: 4104.3834 - mse: 4104.3838 - mae: 33.3004 - val_loss: 2292.0803 - val_mse: 2292.0801 - val_mae: 30.7587\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 3882.9746 - mse: 3882.9753 - mae: 33.0328 - val_loss: 2331.5662 - val_mse: 2331.5662 - val_mae: 30.9632\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 4162.8980 - mse: 4162.8984 - mae: 34.1627 - val_loss: 2312.7067 - val_mse: 2312.7068 - val_mae: 30.8612\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 4199.6824 - mse: 4199.6821 - mae: 33.6898 - val_loss: 2295.3682 - val_mse: 2295.3682 - val_mae: 30.7733\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 3996.1627 - mse: 3996.1631 - mae: 33.3631 - val_loss: 2329.7962 - val_mse: 2329.7961 - val_mae: 30.9459\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 4151.6808 - mse: 4151.6812 - mae: 33.9869 - val_loss: 2382.5920 - val_mse: 2382.5918 - val_mae: 31.2353\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 549us/step - loss: 3983.0972 - mse: 3983.0974 - mae: 33.1489 - val_loss: 2279.2810 - val_mse: 2279.2810 - val_mae: 30.6915\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 3914.5515 - mse: 3914.5518 - mae: 33.1288 - val_loss: 2261.8242 - val_mse: 2261.8242 - val_mae: 30.5994\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4148.3019 - mse: 4148.3027 - mae: 34.5088 - val_loss: 2286.3616 - val_mse: 2286.3618 - val_mae: 30.7137\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 3943.0576 - mse: 3943.0579 - mae: 33.5085 - val_loss: 2308.8351 - val_mse: 2308.8352 - val_mae: 30.8230\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4275.2225 - mse: 4275.2231 - mae: 34.2315 - val_loss: 2383.8738 - val_mse: 2383.8738 - val_mae: 31.2275\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 699us/step - loss: 3983.4726 - mse: 3983.4731 - mae: 34.0678 - val_loss: 2256.9491 - val_mse: 2256.9490 - val_mae: 30.5645\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4297.3227 - mse: 4297.3223 - mae: 35.3669 - val_loss: 2376.5322 - val_mse: 2376.5320 - val_mae: 31.1786\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4118.6294 - mse: 4118.6294 - mae: 32.8506 - val_loss: 2328.7703 - val_mse: 2328.7703 - val_mae: 30.9117\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 3936.4386 - mse: 3936.4385 - mae: 31.9352 - val_loss: 2348.5748 - val_mse: 2348.5750 - val_mae: 31.0201\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 569us/step - loss: 4065.5840 - mse: 4065.5833 - mae: 32.7306 - val_loss: 2301.4015 - val_mse: 2301.4016 - val_mae: 30.7697\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4081.2876 - mse: 4081.2876 - mae: 33.8342 - val_loss: 2311.6207 - val_mse: 2311.6206 - val_mae: 30.8212\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 609us/step - loss: 4170.8093 - mse: 4170.8091 - mae: 34.3837 - val_loss: 2418.1534 - val_mse: 2418.1533 - val_mae: 31.3990\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4018.6688 - mse: 4018.6692 - mae: 33.0853 - val_loss: 2292.4952 - val_mse: 2292.4956 - val_mae: 30.7274\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4001.6374 - mse: 4001.6372 - mae: 32.7171 - val_loss: 2287.2190 - val_mse: 2287.2188 - val_mae: 30.7031\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4039.5184 - mse: 4039.5181 - mae: 32.9684 - val_loss: 2327.5310 - val_mse: 2327.5310 - val_mae: 30.8952\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 3922.6576 - mse: 3922.6580 - mae: 32.1439 - val_loss: 2250.1732 - val_mse: 2250.1733 - val_mae: 30.5184\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4003.1280 - mse: 4003.1279 - mae: 33.0718 - val_loss: 2304.7982 - val_mse: 2304.7983 - val_mae: 30.7677\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3426.5289 - mse: 3426.5295 - mae: 32.3842 - val_loss: 1451.3098 - val_mse: 1451.3098 - val_mae: 24.8208\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 502us/step - loss: 3478.3556 - mse: 3478.3552 - mae: 33.5003 - val_loss: 1453.8539 - val_mse: 1453.8541 - val_mae: 24.7674\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3340.9479 - mse: 3340.9485 - mae: 33.2259 - val_loss: 1456.5801 - val_mse: 1456.5802 - val_mae: 24.6423\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 484us/step - loss: 3439.2017 - mse: 3439.2024 - mae: 32.9274 - val_loss: 1457.9974 - val_mse: 1457.9973 - val_mae: 24.6286\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3362.4961 - mse: 3362.4961 - mae: 32.1981 - val_loss: 1451.6255 - val_mse: 1451.6255 - val_mae: 24.9038\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3419.4703 - mse: 3419.4702 - mae: 32.9591 - val_loss: 1450.6392 - val_mse: 1450.6392 - val_mae: 24.9322\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3460.6105 - mse: 3460.6106 - mae: 33.7716 - val_loss: 1451.4395 - val_mse: 1451.4395 - val_mae: 24.9264\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3349.7572 - mse: 3349.7578 - mae: 32.7765 - val_loss: 1460.7874 - val_mse: 1460.7872 - val_mae: 24.6274\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3420.1719 - mse: 3420.1721 - mae: 33.0656 - val_loss: 1454.2401 - val_mse: 1454.2401 - val_mae: 24.9053\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 674us/step - loss: 3305.5133 - mse: 3305.5127 - mae: 32.8835 - val_loss: 1452.4149 - val_mse: 1452.4148 - val_mae: 25.2015\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3452.4431 - mse: 3452.4438 - mae: 33.1793 - val_loss: 1457.2485 - val_mse: 1457.2485 - val_mae: 24.9337\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 687us/step - loss: 3401.2230 - mse: 3401.2234 - mae: 33.4387 - val_loss: 1452.6179 - val_mse: 1452.6182 - val_mae: 25.2847\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 702us/step - loss: 3293.6213 - mse: 3293.6221 - mae: 31.7987 - val_loss: 1454.9127 - val_mse: 1454.9127 - val_mae: 25.1964\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3469.4784 - mse: 3469.4790 - mae: 33.2968 - val_loss: 1462.5427 - val_mse: 1462.5430 - val_mae: 24.7896\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3424.2681 - mse: 3424.2678 - mae: 32.9084 - val_loss: 1459.8903 - val_mse: 1459.8900 - val_mae: 24.9551\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3259.9822 - mse: 3259.9829 - mae: 32.3376 - val_loss: 1461.5026 - val_mse: 1461.5026 - val_mae: 24.9061\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3313.3230 - mse: 3313.3245 - mae: 32.5427 - val_loss: 1463.5683 - val_mse: 1463.5682 - val_mae: 24.8575\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3399.0734 - mse: 3399.0732 - mae: 33.2913 - val_loss: 1464.8630 - val_mse: 1464.8632 - val_mae: 24.8307\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3265.9661 - mse: 3265.9668 - mae: 32.2869 - val_loss: 1458.1096 - val_mse: 1458.1096 - val_mae: 25.1494\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3368.3408 - mse: 3368.3406 - mae: 33.4974 - val_loss: 1461.1760 - val_mse: 1461.1760 - val_mae: 24.9694\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 687us/step - loss: 3266.7193 - mse: 3266.7197 - mae: 32.3681 - val_loss: 1460.2049 - val_mse: 1460.2048 - val_mae: 25.0135\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 563us/step - loss: 3403.6002 - mse: 3403.6006 - mae: 33.7696 - val_loss: 1464.4855 - val_mse: 1464.4855 - val_mae: 24.8528\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 673us/step - loss: 3433.1448 - mse: 3433.1440 - mae: 31.9846 - val_loss: 1456.6544 - val_mse: 1456.6543 - val_mae: 25.3559\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3496.1372 - mse: 3496.1372 - mae: 33.7705 - val_loss: 1470.8185 - val_mse: 1470.8184 - val_mae: 24.7230\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3479.8650 - mse: 3479.8645 - mae: 32.7380 - val_loss: 1471.3309 - val_mse: 1471.3311 - val_mae: 24.7152\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 633us/step - loss: 3427.3153 - mse: 3427.3147 - mae: 32.6168 - val_loss: 1461.4573 - val_mse: 1461.4574 - val_mae: 25.1720\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3377.2481 - mse: 3377.2476 - mae: 32.1673 - val_loss: 1464.7985 - val_mse: 1464.7986 - val_mae: 25.0937\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 546us/step - loss: 3383.6368 - mse: 3383.6367 - mae: 32.9213 - val_loss: 1465.1538 - val_mse: 1465.1537 - val_mae: 25.1016\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 543us/step - loss: 3473.6700 - mse: 3473.6707 - mae: 33.0862 - val_loss: 1461.4568 - val_mse: 1461.4567 - val_mae: 25.2603\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3412.9265 - mse: 3412.9270 - mae: 32.3745 - val_loss: 1459.2254 - val_mse: 1459.2255 - val_mae: 25.6650\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3345.5112 - mse: 3345.5115 - mae: 32.9335 - val_loss: 1469.4173 - val_mse: 1469.4172 - val_mae: 24.9054\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3362.8434 - mse: 3362.8435 - mae: 31.8595 - val_loss: 1461.1707 - val_mse: 1461.1708 - val_mae: 25.4020\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 693us/step - loss: 3338.1505 - mse: 3338.1504 - mae: 32.4603 - val_loss: 1471.3567 - val_mse: 1471.3566 - val_mae: 24.8597\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3191.4570 - mse: 3191.4573 - mae: 31.6392 - val_loss: 1473.2239 - val_mse: 1473.2238 - val_mae: 24.8595\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3334.2881 - mse: 3334.2878 - mae: 31.9354 - val_loss: 1466.4486 - val_mse: 1466.4485 - val_mae: 25.1476\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3382.5557 - mse: 3382.5562 - mae: 32.3324 - val_loss: 1475.2289 - val_mse: 1475.2290 - val_mae: 24.8323\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3305.1690 - mse: 3305.1685 - mae: 33.0612 - val_loss: 1478.0923 - val_mse: 1478.0924 - val_mae: 24.7744\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3291.8024 - mse: 3291.8037 - mae: 32.2814 - val_loss: 1466.1105 - val_mse: 1466.1105 - val_mae: 25.2661\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3302.8163 - mse: 3302.8164 - mae: 32.5484 - val_loss: 1473.6836 - val_mse: 1473.6837 - val_mae: 24.9627\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - ETA: 0s - loss: 3456.4190 - mse: 3456.4197 - mae: 32.38 - 1s 689us/step - loss: 3352.2670 - mse: 3352.2678 - mae: 32.1840 - val_loss: 1466.1315 - val_mse: 1466.1313 - val_mae: 25.4065\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3357.8216 - mse: 3357.8218 - mae: 32.9790 - val_loss: 1472.1158 - val_mse: 1472.1158 - val_mae: 25.0264\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3264.3416 - mse: 3264.3418 - mae: 31.8857 - val_loss: 1469.3105 - val_mse: 1469.3105 - val_mae: 25.2396\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3424.7752 - mse: 3424.7759 - mae: 32.3092 - val_loss: 1470.7856 - val_mse: 1470.7858 - val_mae: 25.1833\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3341.4725 - mse: 3341.4722 - mae: 31.9621 - val_loss: 1470.7063 - val_mse: 1470.7064 - val_mae: 25.2979\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3400.6831 - mse: 3400.6829 - mae: 32.2760 - val_loss: 1473.2537 - val_mse: 1473.2537 - val_mae: 25.1394\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3381.4918 - mse: 3381.4922 - mae: 32.4566 - val_loss: 1474.3615 - val_mse: 1474.3615 - val_mae: 25.0867\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3271.7783 - mse: 3271.7778 - mae: 31.9381 - val_loss: 1469.1375 - val_mse: 1469.1376 - val_mae: 25.3908\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3327.1566 - mse: 3327.1575 - mae: 32.2396 - val_loss: 1468.0151 - val_mse: 1468.0150 - val_mae: 25.6216\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3353.5637 - mse: 3353.5635 - mae: 32.3856 - val_loss: 1468.4479 - val_mse: 1468.4479 - val_mae: 25.6358\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3287.5711 - mse: 3287.5706 - mae: 31.8382 - val_loss: 1471.2998 - val_mse: 1471.2997 - val_mae: 25.3555\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3294.7831 - mse: 3294.7832 - mae: 31.8070 - val_loss: 1474.5762 - val_mse: 1474.5762 - val_mae: 25.1579\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 617us/step - loss: 3298.7848 - mse: 3298.7849 - mae: 32.0926 - val_loss: 1473.3091 - val_mse: 1473.3092 - val_mae: 25.1881\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 641us/step - loss: 3155.8169 - mse: 3155.8171 - mae: 31.5590 - val_loss: 1469.3299 - val_mse: 1469.3298 - val_mae: 25.5962\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 653us/step - loss: 3303.0603 - mse: 3303.0598 - mae: 31.6832 - val_loss: 1469.3565 - val_mse: 1469.3568 - val_mae: 25.5211\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 664us/step - loss: 3349.6529 - mse: 3349.6526 - mae: 32.0102 - val_loss: 1468.8844 - val_mse: 1468.8846 - val_mae: 25.5923\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 716us/step - loss: 3370.0595 - mse: 3370.0586 - mae: 33.0361 - val_loss: 1470.8192 - val_mse: 1470.8193 - val_mae: 25.3659\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3278.4492 - mse: 3278.4485 - mae: 31.8776 - val_loss: 1477.2002 - val_mse: 1477.2002 - val_mae: 25.0442\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3297.6163 - mse: 3297.6157 - mae: 31.5121 - val_loss: 1469.9830 - val_mse: 1469.9828 - val_mae: 25.5112\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3308.9636 - mse: 3308.9636 - mae: 31.7527 - val_loss: 1468.9260 - val_mse: 1468.9260 - val_mae: 25.5760\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 637us/step - loss: 3319.3397 - mse: 3319.3406 - mae: 32.0376 - val_loss: 1473.7076 - val_mse: 1473.7074 - val_mae: 25.2134\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 664us/step - loss: 3306.1029 - mse: 3306.1038 - mae: 31.9113 - val_loss: 1470.9612 - val_mse: 1470.9612 - val_mae: 25.3819\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 669us/step - loss: 3235.1591 - mse: 3235.1602 - mae: 32.0258 - val_loss: 1471.9494 - val_mse: 1471.9496 - val_mae: 25.2776\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 666us/step - loss: 3093.5510 - mse: 3093.5515 - mae: 31.4386 - val_loss: 1470.7358 - val_mse: 1470.7356 - val_mae: 25.4175\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3212.9552 - mse: 3212.9551 - mae: 31.5161 - val_loss: 1469.8378 - val_mse: 1469.8379 - val_mae: 25.6533\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3322.4079 - mse: 3322.4077 - mae: 32.6146 - val_loss: 1473.9822 - val_mse: 1473.9822 - val_mae: 25.3232\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3194.2331 - mse: 3194.2334 - mae: 31.4663 - val_loss: 1471.8266 - val_mse: 1471.8264 - val_mae: 25.4881\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 698us/step - loss: 3177.3423 - mse: 3177.3420 - mae: 31.9536 - val_loss: 1472.8223 - val_mse: 1472.8224 - val_mae: 25.3303\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3307.6216 - mse: 3307.6211 - mae: 32.4996 - val_loss: 1475.6094 - val_mse: 1475.6095 - val_mae: 25.1521\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3298.6172 - mse: 3298.6165 - mae: 31.5557 - val_loss: 1469.6925 - val_mse: 1469.6925 - val_mae: 25.5869\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 633us/step - loss: 3263.6597 - mse: 3263.6599 - mae: 31.5616 - val_loss: 1471.3949 - val_mse: 1471.3949 - val_mae: 25.3925\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 522us/step - loss: 3303.2956 - mse: 3303.2964 - mae: 31.4563 - val_loss: 1477.2906 - val_mse: 1477.2904 - val_mae: 25.1000\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3348.4771 - mse: 3348.4766 - mae: 32.1452 - val_loss: 1473.7152 - val_mse: 1473.7152 - val_mae: 25.3245\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3295.8337 - mse: 3295.8337 - mae: 31.6746 - val_loss: 1473.6347 - val_mse: 1473.6346 - val_mae: 25.3248\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 704us/step - loss: 3267.9626 - mse: 3267.9619 - mae: 31.8317 - val_loss: 1470.5625 - val_mse: 1470.5625 - val_mae: 25.5587\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 741us/step - loss: 3249.1929 - mse: 3249.1929 - mae: 31.3999 - val_loss: 1472.2839 - val_mse: 1472.2839 - val_mae: 25.4202\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 689us/step - loss: 3103.8005 - mse: 3103.8003 - mae: 31.2053 - val_loss: 1472.6569 - val_mse: 1472.6567 - val_mae: 25.5355\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3124.7522 - mse: 3124.7520 - mae: 30.7702 - val_loss: 1473.9113 - val_mse: 1473.9113 - val_mae: 25.6042\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3204.0493 - mse: 3204.0498 - mae: 32.1364 - val_loss: 1484.4262 - val_mse: 1484.4263 - val_mae: 25.0657\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3187.5654 - mse: 3187.5662 - mae: 31.3158 - val_loss: 1473.8178 - val_mse: 1473.8179 - val_mae: 25.5285\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 695us/step - loss: 3191.6078 - mse: 3191.6086 - mae: 31.2351 - val_loss: 1475.7702 - val_mse: 1475.7700 - val_mae: 25.4377\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2911.0379 - mse: 2911.0381 - mae: 31.0014 - val_loss: 1086.0735 - val_mse: 1086.0735 - val_mae: 24.0175\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2925.5777 - mse: 2925.5774 - mae: 31.9546 - val_loss: 1081.4774 - val_mse: 1081.4772 - val_mae: 24.6195\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2905.8273 - mse: 2905.8281 - mae: 30.8692 - val_loss: 1084.7113 - val_mse: 1084.7113 - val_mae: 23.8894\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2956.1236 - mse: 2956.1243 - mae: 31.4849 - val_loss: 1079.6937 - val_mse: 1079.6937 - val_mae: 24.2300\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2937.0256 - mse: 2937.0254 - mae: 31.2308 - val_loss: 1077.5881 - val_mse: 1077.5881 - val_mae: 24.3182\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2931.9927 - mse: 2931.9922 - mae: 30.7693 - val_loss: 1081.0090 - val_mse: 1081.0090 - val_mae: 23.7814\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 469us/step - loss: 2867.4335 - mse: 2867.4346 - mae: 31.2288 - val_loss: 1074.9447 - val_mse: 1074.9448 - val_mae: 24.1283\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2885.7547 - mse: 2885.7551 - mae: 31.3267 - val_loss: 1072.6330 - val_mse: 1072.6331 - val_mae: 24.1985\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2947.4756 - mse: 2947.4758 - mae: 31.3522 - val_loss: 1072.0736 - val_mse: 1072.0735 - val_mae: 24.0800\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2929.0362 - mse: 2929.0364 - mae: 31.2813 - val_loss: 1069.9713 - val_mse: 1069.9713 - val_mae: 24.1877\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2905.5498 - mse: 2905.5493 - mae: 30.6703 - val_loss: 1070.9684 - val_mse: 1070.9684 - val_mae: 23.9705\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 706us/step - loss: 2914.5963 - mse: 2914.5972 - mae: 31.1067 - val_loss: 1069.5576 - val_mse: 1069.5575 - val_mae: 24.1689\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2953.6640 - mse: 2953.6638 - mae: 31.2049 - val_loss: 1072.0571 - val_mse: 1072.0571 - val_mae: 23.9850\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 526us/step - loss: 2928.5849 - mse: 2928.5845 - mae: 31.2240 - val_loss: 1073.3230 - val_mse: 1073.3230 - val_mae: 23.9273\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 537us/step - loss: 2919.2366 - mse: 2919.2368 - mae: 30.7319 - val_loss: 1071.0069 - val_mse: 1071.0070 - val_mae: 24.2631\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2937.9289 - mse: 2937.9290 - mae: 31.1375 - val_loss: 1071.7289 - val_mse: 1071.7291 - val_mae: 24.2304\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 3018.7507 - mse: 3018.7510 - mae: 31.6353 - val_loss: 1073.1956 - val_mse: 1073.1956 - val_mae: 23.9544\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2866.2787 - mse: 2866.2781 - mae: 31.7562 - val_loss: 1072.5889 - val_mse: 1072.5889 - val_mae: 23.8558\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2883.7624 - mse: 2883.7612 - mae: 30.8636 - val_loss: 1072.2217 - val_mse: 1072.2216 - val_mae: 23.8303\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 687us/step - loss: 2903.7580 - mse: 2903.7581 - mae: 30.5681 - val_loss: 1068.4354 - val_mse: 1068.4354 - val_mae: 24.1124\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2933.8446 - mse: 2933.8435 - mae: 31.2469 - val_loss: 1067.5151 - val_mse: 1067.5151 - val_mae: 24.2897\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2886.8661 - mse: 2886.8660 - mae: 30.9384 - val_loss: 1067.0534 - val_mse: 1067.0532 - val_mae: 24.1874\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2831.1971 - mse: 2831.1970 - mae: 30.3831 - val_loss: 1069.3043 - val_mse: 1069.3042 - val_mae: 24.0814\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 653us/step - loss: 2810.9735 - mse: 2810.9734 - mae: 30.3740 - val_loss: 1068.1476 - val_mse: 1068.1476 - val_mae: 24.2629\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 664us/step - loss: 2844.9714 - mse: 2844.9712 - mae: 30.2043 - val_loss: 1067.9497 - val_mse: 1067.9498 - val_mae: 24.3052\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2814.1508 - mse: 2814.1511 - mae: 31.0936 - val_loss: 1070.8137 - val_mse: 1070.8136 - val_mae: 23.8578\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2874.9311 - mse: 2874.9312 - mae: 31.0795 - val_loss: 1067.4836 - val_mse: 1067.4836 - val_mae: 24.0979\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2920.8661 - mse: 2920.8655 - mae: 30.7445 - val_loss: 1075.1544 - val_mse: 1075.1544 - val_mae: 23.5598\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2795.8275 - mse: 2795.8276 - mae: 30.2254 - val_loss: 1065.9598 - val_mse: 1065.9597 - val_mae: 24.2978\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2839.1662 - mse: 2839.1663 - mae: 30.7165 - val_loss: 1065.9358 - val_mse: 1065.9358 - val_mae: 24.3908\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2889.5271 - mse: 2889.5256 - mae: 30.8366 - val_loss: 1065.4640 - val_mse: 1065.4639 - val_mae: 24.3245\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2850.4509 - mse: 2850.4497 - mae: 31.0509 - val_loss: 1065.0611 - val_mse: 1065.0613 - val_mae: 24.1472\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2872.2905 - mse: 2872.2908 - mae: 30.4555 - val_loss: 1066.1718 - val_mse: 1066.1719 - val_mae: 23.9908\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2873.4764 - mse: 2873.4771 - mae: 30.9031 - val_loss: 1063.6106 - val_mse: 1063.6106 - val_mae: 24.5020\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2928.2534 - mse: 2928.2537 - mae: 30.7334 - val_loss: 1064.3398 - val_mse: 1064.3398 - val_mae: 24.0296\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2853.2486 - mse: 2853.2485 - mae: 30.8281 - val_loss: 1062.4714 - val_mse: 1062.4713 - val_mae: 24.4034\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2792.8164 - mse: 2792.8152 - mae: 30.2078 - val_loss: 1062.5508 - val_mse: 1062.5509 - val_mae: 24.1942\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2801.1657 - mse: 2801.1655 - mae: 29.8176 - val_loss: 1061.6598 - val_mse: 1061.6598 - val_mae: 24.4531\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2850.0509 - mse: 2850.0503 - mae: 30.9109 - val_loss: 1061.2799 - val_mse: 1061.2797 - val_mae: 24.1413\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 626us/step - loss: 2878.0780 - mse: 2878.0789 - mae: 30.6317 - val_loss: 1061.0576 - val_mse: 1061.0576 - val_mae: 24.3511\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2812.7878 - mse: 2812.7881 - mae: 30.4434 - val_loss: 1061.2486 - val_mse: 1061.2487 - val_mae: 24.2896\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2899.4853 - mse: 2899.4856 - mae: 31.0201 - val_loss: 1061.6628 - val_mse: 1061.6628 - val_mae: 24.1577\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2758.6770 - mse: 2758.6765 - mae: 30.1713 - val_loss: 1061.4197 - val_mse: 1061.4197 - val_mae: 24.3004\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2831.2440 - mse: 2831.2444 - mae: 30.5350 - val_loss: 1060.5645 - val_mse: 1060.5643 - val_mae: 24.3666\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 609us/step - loss: 2796.9039 - mse: 2796.9038 - mae: 30.6329 - val_loss: 1061.2200 - val_mse: 1061.2200 - val_mae: 24.4315\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 682us/step - loss: 2797.8670 - mse: 2797.8665 - mae: 30.4117 - val_loss: 1060.3802 - val_mse: 1060.3801 - val_mae: 24.2647\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 710us/step - loss: 2916.2739 - mse: 2916.2742 - mae: 30.3534 - val_loss: 1059.9218 - val_mse: 1059.9218 - val_mae: 24.3746\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2853.8251 - mse: 2853.8257 - mae: 30.6900 - val_loss: 1060.0979 - val_mse: 1060.0978 - val_mae: 24.3353\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2827.9490 - mse: 2827.9482 - mae: 30.6826 - val_loss: 1062.4895 - val_mse: 1062.4895 - val_mae: 23.8580\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2851.7621 - mse: 2851.7632 - mae: 30.6819 - val_loss: 1061.5112 - val_mse: 1061.5111 - val_mae: 24.0017\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2870.9244 - mse: 2870.9253 - mae: 31.0752 - val_loss: 1059.8308 - val_mse: 1059.8308 - val_mae: 24.0815\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2864.8151 - mse: 2864.8152 - mae: 31.0724 - val_loss: 1059.0506 - val_mse: 1059.0507 - val_mae: 24.1731\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2894.6951 - mse: 2894.6958 - mae: 30.9445 - val_loss: 1058.5153 - val_mse: 1058.5154 - val_mae: 24.2106\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2818.5846 - mse: 2818.5840 - mae: 30.3476 - val_loss: 1058.5762 - val_mse: 1058.5762 - val_mae: 24.4115\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2791.2519 - mse: 2791.2510 - mae: 30.3872 - val_loss: 1057.8291 - val_mse: 1057.8292 - val_mae: 24.2292\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2825.7210 - mse: 2825.7207 - mae: 30.3614 - val_loss: 1057.9148 - val_mse: 1057.9148 - val_mae: 24.3575\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2879.7627 - mse: 2879.7632 - mae: 30.8021 - val_loss: 1058.2565 - val_mse: 1058.2563 - val_mae: 24.2090\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2824.1698 - mse: 2824.1702 - mae: 30.5255 - val_loss: 1059.1202 - val_mse: 1059.1201 - val_mae: 23.9804\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2801.0991 - mse: 2801.0989 - mae: 30.0831 - val_loss: 1057.2298 - val_mse: 1057.2300 - val_mae: 24.4100\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2857.0293 - mse: 2857.0286 - mae: 30.4250 - val_loss: 1059.4271 - val_mse: 1059.4271 - val_mae: 23.8066\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2819.0016 - mse: 2819.0015 - mae: 29.9438 - val_loss: 1057.0463 - val_mse: 1057.0464 - val_mae: 24.1693\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2920.3522 - mse: 2920.3518 - mae: 30.7936 - val_loss: 1057.5743 - val_mse: 1057.5743 - val_mae: 24.0024\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2861.7957 - mse: 2861.7959 - mae: 30.5588 - val_loss: 1057.9593 - val_mse: 1057.9595 - val_mae: 23.9944\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2842.3827 - mse: 2842.3828 - mae: 30.2527 - val_loss: 1057.5127 - val_mse: 1057.5126 - val_mae: 24.2447\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2814.4726 - mse: 2814.4717 - mae: 30.1464 - val_loss: 1058.4494 - val_mse: 1058.4493 - val_mae: 23.8483\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2908.7593 - mse: 2908.7588 - mae: 31.1385 - val_loss: 1060.6389 - val_mse: 1060.6389 - val_mae: 23.6905\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2826.2160 - mse: 2826.2158 - mae: 30.3550 - val_loss: 1056.7223 - val_mse: 1056.7225 - val_mae: 24.3156\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2788.5388 - mse: 2788.5381 - mae: 30.3243 - val_loss: 1058.8825 - val_mse: 1058.8826 - val_mae: 23.8128\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2786.3349 - mse: 2786.3357 - mae: 29.9945 - val_loss: 1058.7617 - val_mse: 1058.7616 - val_mae: 23.7668\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 689us/step - loss: 2844.5284 - mse: 2844.5281 - mae: 30.2969 - val_loss: 1055.5534 - val_mse: 1055.5535 - val_mae: 24.1521\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 669us/step - loss: 2850.8127 - mse: 2850.8123 - mae: 30.3045 - val_loss: 1057.4885 - val_mse: 1057.4886 - val_mae: 23.7442\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2767.9783 - mse: 2767.9790 - mae: 30.2673 - val_loss: 1053.7521 - val_mse: 1053.7520 - val_mae: 24.3061\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2846.8274 - mse: 2846.8276 - mae: 30.9305 - val_loss: 1053.1638 - val_mse: 1053.1638 - val_mae: 24.1457\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2727.0630 - mse: 2727.0635 - mae: 29.9855 - val_loss: 1053.7492 - val_mse: 1053.7491 - val_mae: 23.8779\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2737.7311 - mse: 2737.7305 - mae: 30.1040 - val_loss: 1051.8000 - val_mse: 1051.7999 - val_mae: 24.3704\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2760.7346 - mse: 2760.7351 - mae: 30.3695 - val_loss: 1050.8554 - val_mse: 1050.8553 - val_mae: 24.2054\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2805.1826 - mse: 2805.1826 - mae: 30.4215 - val_loss: 1051.9303 - val_mse: 1051.9301 - val_mae: 24.0701\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 578us/step - loss: 2829.7043 - mse: 2829.7053 - mae: 31.0983 - val_loss: 1052.2928 - val_mse: 1052.2928 - val_mae: 24.0578\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2827.7342 - mse: 2827.7341 - mae: 30.2291 - val_loss: 1054.7580 - val_mse: 1054.7581 - val_mae: 23.7762\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 559us/step - loss: 2812.3013 - mse: 2812.3010 - mae: 30.7953 - val_loss: 1053.7828 - val_mse: 1053.7828 - val_mae: 23.9765\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2584.1908 - mse: 2584.1914 - mae: 30.2246 - val_loss: 1563.7000 - val_mse: 1563.7000 - val_mae: 26.1901\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2530.2324 - mse: 2530.2324 - mae: 29.4946 - val_loss: 1528.2434 - val_mse: 1528.2434 - val_mae: 26.5800\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2496.4445 - mse: 2496.4441 - mae: 29.5725 - val_loss: 1522.7692 - val_mse: 1522.7693 - val_mae: 26.6277\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 664us/step - loss: 2526.4464 - mse: 2526.4468 - mae: 29.7168 - val_loss: 1535.7134 - val_mse: 1535.7135 - val_mae: 26.4179\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2482.9292 - mse: 2482.9287 - mae: 29.3969 - val_loss: 1529.7796 - val_mse: 1529.7795 - val_mae: 26.4575\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 668us/step - loss: 2559.5967 - mse: 2559.5964 - mae: 29.8874 - val_loss: 1535.6234 - val_mse: 1535.6235 - val_mae: 26.3420\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 666us/step - loss: 2454.4747 - mse: 2454.4746 - mae: 29.1771 - val_loss: 1529.9228 - val_mse: 1529.9226 - val_mae: 26.3896\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 644us/step - loss: 2473.9739 - mse: 2473.9734 - mae: 29.1628 - val_loss: 1541.3696 - val_mse: 1541.3695 - val_mae: 26.2163\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2620.7109 - mse: 2620.7117 - mae: 29.8846 - val_loss: 1509.4746 - val_mse: 1509.4744 - val_mae: 26.6916\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2527.2514 - mse: 2527.2515 - mae: 29.6659 - val_loss: 1524.2466 - val_mse: 1524.2466 - val_mae: 26.4435\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2427.2010 - mse: 2427.2007 - mae: 29.4301 - val_loss: 1520.4555 - val_mse: 1520.4553 - val_mae: 26.4918\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2539.7711 - mse: 2539.7710 - mae: 29.5865 - val_loss: 1509.8757 - val_mse: 1509.8756 - val_mae: 26.6405\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2586.9154 - mse: 2586.9160 - mae: 30.1407 - val_loss: 1521.5323 - val_mse: 1521.5323 - val_mae: 26.4304\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2614.9390 - mse: 2614.9377 - mae: 30.2498 - val_loss: 1525.3445 - val_mse: 1525.3444 - val_mae: 26.3618\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2539.1016 - mse: 2539.1023 - mae: 29.6757 - val_loss: 1523.3141 - val_mse: 1523.3143 - val_mae: 26.3739\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2523.1515 - mse: 2523.1506 - mae: 29.1542 - val_loss: 1506.2973 - val_mse: 1506.2974 - val_mae: 26.6419\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2483.7680 - mse: 2483.7683 - mae: 29.7007 - val_loss: 1527.6506 - val_mse: 1527.6506 - val_mae: 26.2873\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2442.4943 - mse: 2442.4937 - mae: 29.2388 - val_loss: 1516.4231 - val_mse: 1516.4232 - val_mae: 26.4059\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2493.7226 - mse: 2493.7231 - mae: 29.4605 - val_loss: 1499.9440 - val_mse: 1499.9443 - val_mae: 26.6507\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2520.2434 - mse: 2520.2439 - mae: 29.7018 - val_loss: 1530.4377 - val_mse: 1530.4377 - val_mae: 26.2240\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2557.3601 - mse: 2557.3604 - mae: 29.5901 - val_loss: 1492.2084 - val_mse: 1492.2085 - val_mae: 26.8097\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2491.1549 - mse: 2491.1555 - mae: 29.5239 - val_loss: 1519.1293 - val_mse: 1519.1294 - val_mae: 26.3390\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2513.0999 - mse: 2513.1003 - mae: 29.3415 - val_loss: 1521.2323 - val_mse: 1521.2323 - val_mae: 26.2857\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2528.4415 - mse: 2528.4426 - mae: 29.6959 - val_loss: 1535.4181 - val_mse: 1535.4181 - val_mae: 26.1088\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2449.6765 - mse: 2449.6763 - mae: 29.4246 - val_loss: 1518.9402 - val_mse: 1518.9402 - val_mae: 26.2811\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2494.3775 - mse: 2494.3774 - mae: 29.3551 - val_loss: 1524.2827 - val_mse: 1524.2826 - val_mae: 26.2104\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2473.3450 - mse: 2473.3447 - mae: 28.9892 - val_loss: 1506.2842 - val_mse: 1506.2842 - val_mae: 26.4608\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2565.4600 - mse: 2565.4597 - mae: 29.9254 - val_loss: 1523.3029 - val_mse: 1523.3027 - val_mae: 26.2243\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2506.2482 - mse: 2506.2483 - mae: 29.0317 - val_loss: 1530.7086 - val_mse: 1530.7086 - val_mae: 26.1243\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2452.0550 - mse: 2452.0547 - mae: 29.1524 - val_loss: 1503.4216 - val_mse: 1503.4214 - val_mae: 26.4466\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 652us/step - loss: 2526.1396 - mse: 2526.1396 - mae: 29.9781 - val_loss: 1534.4863 - val_mse: 1534.4862 - val_mae: 26.0424\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2454.2893 - mse: 2454.2893 - mae: 28.8878 - val_loss: 1507.8716 - val_mse: 1507.8716 - val_mae: 26.3291\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 545us/step - loss: 2584.1850 - mse: 2584.1838 - mae: 29.6326 - val_loss: 1514.5781 - val_mse: 1514.5781 - val_mae: 26.2388\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2476.1310 - mse: 2476.1309 - mae: 29.4922 - val_loss: 1525.7520 - val_mse: 1525.7520 - val_mae: 26.1113\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2512.4866 - mse: 2512.4866 - mae: 29.7554 - val_loss: 1521.9312 - val_mse: 1521.9314 - val_mae: 26.1258\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2454.8377 - mse: 2454.8379 - mae: 29.3058 - val_loss: 1517.0103 - val_mse: 1517.0101 - val_mae: 26.1572\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2512.8373 - mse: 2512.8376 - mae: 29.0386 - val_loss: 1518.5615 - val_mse: 1518.5614 - val_mae: 26.1779\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2493.4170 - mse: 2493.4182 - mae: 29.2554 - val_loss: 1502.5136 - val_mse: 1502.5137 - val_mae: 26.3844\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2434.1502 - mse: 2434.1499 - mae: 29.4469 - val_loss: 1498.9317 - val_mse: 1498.9316 - val_mae: 26.4063\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2513.0805 - mse: 2513.0803 - mae: 29.4582 - val_loss: 1511.0879 - val_mse: 1511.0881 - val_mae: 26.1983\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2500.4954 - mse: 2500.4949 - mae: 29.3185 - val_loss: 1509.1703 - val_mse: 1509.1702 - val_mae: 26.2278\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 665us/step - loss: 2483.9046 - mse: 2483.9038 - mae: 29.2715 - val_loss: 1515.7601 - val_mse: 1515.7600 - val_mae: 26.1451\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2431.1043 - mse: 2431.1050 - mae: 29.3010 - val_loss: 1510.7148 - val_mse: 1510.7146 - val_mae: 26.1838\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2517.2359 - mse: 2517.2356 - mae: 29.2443 - val_loss: 1508.2171 - val_mse: 1508.2169 - val_mae: 26.2014\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2487.2277 - mse: 2487.2283 - mae: 29.3940 - val_loss: 1506.6200 - val_mse: 1506.6200 - val_mae: 26.2478\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2469.8601 - mse: 2469.8604 - mae: 29.6064 - val_loss: 1495.5770 - val_mse: 1495.5769 - val_mae: 26.3803\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2490.6566 - mse: 2490.6567 - mae: 29.7406 - val_loss: 1500.1559 - val_mse: 1500.1559 - val_mae: 26.2673\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2443.2015 - mse: 2443.2012 - mae: 29.1290 - val_loss: 1497.7754 - val_mse: 1497.7755 - val_mae: 26.2608\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 550us/step - loss: 2495.1076 - mse: 2495.1079 - mae: 29.2722 - val_loss: 1508.9221 - val_mse: 1508.9221 - val_mae: 26.0666\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2477.6440 - mse: 2477.6443 - mae: 29.3940 - val_loss: 1506.7296 - val_mse: 1506.7297 - val_mae: 26.0626\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2534.1624 - mse: 2534.1616 - mae: 29.5171 - val_loss: 1503.9528 - val_mse: 1503.9529 - val_mae: 26.0864\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 489us/step - loss: 2492.0944 - mse: 2492.0940 - mae: 28.8658 - val_loss: 1486.0066 - val_mse: 1486.0066 - val_mae: 26.3157\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2480.3208 - mse: 2480.3210 - mae: 29.0472 - val_loss: 1486.8943 - val_mse: 1486.8944 - val_mae: 26.2585\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2487.2434 - mse: 2487.2437 - mae: 29.6749 - val_loss: 1488.5638 - val_mse: 1488.5636 - val_mae: 26.1889\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2439.7017 - mse: 2439.7019 - mae: 28.9689 - val_loss: 1481.4872 - val_mse: 1481.4873 - val_mae: 26.3020\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2486.6228 - mse: 2486.6233 - mae: 29.1792 - val_loss: 1492.7105 - val_mse: 1492.7104 - val_mae: 26.1195\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2500.2584 - mse: 2500.2573 - mae: 29.3210 - val_loss: 1490.9428 - val_mse: 1490.9429 - val_mae: 26.1026\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2460.9596 - mse: 2460.9592 - mae: 29.2571 - val_loss: 1496.6841 - val_mse: 1496.6838 - val_mae: 25.9716\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2508.0410 - mse: 2508.0408 - mae: 28.9441 - val_loss: 1494.1444 - val_mse: 1494.1444 - val_mae: 26.0139\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2493.1761 - mse: 2493.1763 - mae: 29.3636 - val_loss: 1489.7978 - val_mse: 1489.7976 - val_mae: 25.9833\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2489.3848 - mse: 2489.3848 - mae: 29.4575 - val_loss: 1491.2350 - val_mse: 1491.2349 - val_mae: 25.9401\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2417.0536 - mse: 2417.0537 - mae: 29.2227 - val_loss: 1492.6371 - val_mse: 1492.6371 - val_mae: 25.9028\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2446.5032 - mse: 2446.5034 - mae: 29.0328 - val_loss: 1470.4087 - val_mse: 1470.4086 - val_mae: 26.1621\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2428.5697 - mse: 2428.5696 - mae: 29.0732 - val_loss: 1484.9259 - val_mse: 1484.9259 - val_mae: 25.8985\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2474.3972 - mse: 2474.3977 - mae: 29.5069 - val_loss: 1492.0179 - val_mse: 1492.0182 - val_mae: 25.8313\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 543us/step - loss: 2408.7280 - mse: 2408.7280 - mae: 28.6912 - val_loss: 1487.4483 - val_mse: 1487.4484 - val_mae: 25.8813\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2436.1171 - mse: 2436.1172 - mae: 29.1590 - val_loss: 1487.9686 - val_mse: 1487.9688 - val_mae: 25.8299\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2501.6502 - mse: 2501.6504 - mae: 29.2367 - val_loss: 1504.8955 - val_mse: 1504.8955 - val_mae: 25.6638\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2427.0479 - mse: 2427.0481 - mae: 28.8542 - val_loss: 1507.8124 - val_mse: 1507.8126 - val_mae: 25.6181\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 591us/step - loss: 2358.5048 - mse: 2358.5042 - mae: 28.5014 - val_loss: 1482.7564 - val_mse: 1482.7565 - val_mae: 25.8695\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2453.5574 - mse: 2453.5571 - mae: 29.0780 - val_loss: 1486.9246 - val_mse: 1486.9246 - val_mae: 25.8023\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2493.8082 - mse: 2493.8088 - mae: 29.2202 - val_loss: 1499.8783 - val_mse: 1499.8784 - val_mae: 25.7167\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2396.4682 - mse: 2396.4685 - mae: 28.9044 - val_loss: 1499.6189 - val_mse: 1499.6188 - val_mae: 25.7000\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2536.9654 - mse: 2536.9656 - mae: 29.3927 - val_loss: 1478.4669 - val_mse: 1478.4669 - val_mae: 25.8934\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 545us/step - loss: 2472.4969 - mse: 2472.4976 - mae: 29.1664 - val_loss: 1483.8602 - val_mse: 1483.8600 - val_mae: 25.8205\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2558.5364 - mse: 2558.5369 - mae: 29.5603 - val_loss: 1494.4855 - val_mse: 1494.4855 - val_mae: 25.7517\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2413.8803 - mse: 2413.8804 - mae: 28.9178 - val_loss: 1485.3440 - val_mse: 1485.3441 - val_mae: 25.8290\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2348.1532 - mse: 2348.1533 - mae: 28.2542 - val_loss: 1469.0584 - val_mse: 1469.0585 - val_mae: 26.0418\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2443.6519 - mse: 2443.6521 - mae: 29.1144 - val_loss: 1477.6338 - val_mse: 1477.6339 - val_mae: 25.8754\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2453.1346 - mse: 2453.1350 - mae: 29.0776 - val_loss: 1466.5196 - val_mse: 1466.5195 - val_mae: 26.0554\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2410.6215 - mse: 2410.6211 - mae: 29.6895 - val_loss: 3651.6185 - val_mse: 3651.6182 - val_mae: 23.0382\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2358.8949 - mse: 2358.8953 - mae: 29.1599 - val_loss: 3651.9781 - val_mse: 3651.9783 - val_mae: 23.5755\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 668us/step - loss: 2378.8240 - mse: 2378.8240 - mae: 29.4819 - val_loss: 3650.6249 - val_mse: 3650.6245 - val_mae: 23.2107\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2398.3748 - mse: 2398.3750 - mae: 29.3104 - val_loss: 3649.6937 - val_mse: 3649.6941 - val_mae: 23.0074\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2354.8145 - mse: 2354.8142 - mae: 29.2911 - val_loss: 3650.1935 - val_mse: 3650.1934 - val_mae: 23.3695\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2372.8057 - mse: 2372.8052 - mae: 29.1075 - val_loss: 3650.8189 - val_mse: 3650.8201 - val_mae: 22.5859\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2283.1015 - mse: 2283.1008 - mae: 28.8883 - val_loss: 3648.6214 - val_mse: 3648.6221 - val_mae: 22.9351\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2361.5215 - mse: 2361.5212 - mae: 29.3973 - val_loss: 3649.0439 - val_mse: 3649.0442 - val_mae: 23.1198\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2359.3280 - mse: 2359.3276 - mae: 29.4012 - val_loss: 3649.5130 - val_mse: 3649.5134 - val_mae: 23.4099\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2339.3529 - mse: 2339.3538 - mae: 29.2263 - val_loss: 3648.1322 - val_mse: 3648.1311 - val_mae: 22.9451\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2310.9033 - mse: 2310.9028 - mae: 28.9500 - val_loss: 3648.4641 - val_mse: 3648.4631 - val_mae: 22.9557\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2311.2212 - mse: 2311.2209 - mae: 29.0593 - val_loss: 3648.6826 - val_mse: 3648.6838 - val_mae: 23.3130\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2327.9475 - mse: 2327.9478 - mae: 29.2476 - val_loss: 3648.9666 - val_mse: 3648.9658 - val_mae: 22.6893\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2366.8849 - mse: 2366.8850 - mae: 29.5003 - val_loss: 3648.0952 - val_mse: 3648.0945 - val_mae: 23.1515\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2338.4602 - mse: 2338.4604 - mae: 29.3062 - val_loss: 3648.8082 - val_mse: 3648.8093 - val_mae: 22.5459\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2338.6764 - mse: 2338.6765 - mae: 29.2991 - val_loss: 3648.7344 - val_mse: 3648.7354 - val_mae: 22.4326\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2271.5905 - mse: 2271.5903 - mae: 28.6209 - val_loss: 3647.8293 - val_mse: 3647.8293 - val_mae: 22.8420\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 529us/step - loss: 2345.1129 - mse: 2345.1130 - mae: 29.0608 - val_loss: 3647.9995 - val_mse: 3648.0002 - val_mae: 22.8709\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2354.7492 - mse: 2354.7493 - mae: 29.2475 - val_loss: 3647.5097 - val_mse: 3647.5093 - val_mae: 23.1530\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2301.9483 - mse: 2301.9485 - mae: 29.3318 - val_loss: 3650.2888 - val_mse: 3650.2886 - val_mae: 22.2365\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2316.2684 - mse: 2316.2683 - mae: 29.1560 - val_loss: 3648.7543 - val_mse: 3648.7534 - val_mae: 22.5243\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2339.2494 - mse: 2339.2498 - mae: 29.1859 - val_loss: 3647.7015 - val_mse: 3647.7012 - val_mae: 22.9588\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 1s 499us/step - loss: 2324.3510 - mse: 2324.3513 - mae: 28.9683 - val_loss: 3647.8742 - val_mse: 3647.8745 - val_mae: 22.7015\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2333.3819 - mse: 2333.3816 - mae: 28.9471 - val_loss: 3647.4695 - val_mse: 3647.4695 - val_mae: 22.9173\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2324.0155 - mse: 2324.0151 - mae: 29.1401 - val_loss: 3646.3305 - val_mse: 3646.3311 - val_mae: 22.8631\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2368.3295 - mse: 2368.3291 - mae: 29.5027 - val_loss: 3647.3791 - val_mse: 3647.3789 - val_mae: 22.6160\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2350.6324 - mse: 2350.6331 - mae: 29.0291 - val_loss: 3648.2376 - val_mse: 3648.2380 - val_mae: 22.4453\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2294.1670 - mse: 2294.1675 - mae: 29.2831 - val_loss: 3647.2164 - val_mse: 3647.2168 - val_mae: 22.7453\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2358.2674 - mse: 2358.2673 - mae: 29.2807 - val_loss: 3647.5569 - val_mse: 3647.5576 - val_mae: 23.2078\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2322.5403 - mse: 2322.5413 - mae: 29.5809 - val_loss: 3647.0996 - val_mse: 3647.0991 - val_mae: 22.7668\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2316.0843 - mse: 2316.0837 - mae: 28.9313 - val_loss: 3648.6612 - val_mse: 3648.6606 - val_mae: 23.5832\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2254.4910 - mse: 2254.4912 - mae: 29.0311 - val_loss: 3647.1297 - val_mse: 3647.1292 - val_mae: 22.9658\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2348.9099 - mse: 2348.9109 - mae: 29.3383 - val_loss: 3647.3285 - val_mse: 3647.3291 - val_mae: 22.6915\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2301.2356 - mse: 2301.2358 - mae: 28.9223 - val_loss: 3648.8728 - val_mse: 3648.8726 - val_mae: 22.5398\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 1s 485us/step - loss: 2314.5356 - mse: 2314.5354 - mae: 29.2213 - val_loss: 3649.9780 - val_mse: 3649.9780 - val_mae: 22.3218\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2324.2180 - mse: 2324.2173 - mae: 28.8982 - val_loss: 3647.4229 - val_mse: 3647.4226 - val_mae: 23.1036\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2325.4433 - mse: 2325.4438 - mae: 29.2335 - val_loss: 3647.4185 - val_mse: 3647.4185 - val_mae: 23.1226\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2266.8832 - mse: 2266.8835 - mae: 28.7060 - val_loss: 3648.1796 - val_mse: 3648.1797 - val_mae: 22.7009\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2310.1021 - mse: 2310.1030 - mae: 28.5956 - val_loss: 3648.1738 - val_mse: 3648.1736 - val_mae: 22.7238\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2292.9131 - mse: 2292.9126 - mae: 28.6619 - val_loss: 3647.4055 - val_mse: 3647.4060 - val_mae: 22.8327\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2317.9454 - mse: 2317.9448 - mae: 28.9532 - val_loss: 3648.9648 - val_mse: 3648.9648 - val_mae: 22.5783\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2296.1869 - mse: 2296.1873 - mae: 28.7617 - val_loss: 3648.3328 - val_mse: 3648.3328 - val_mae: 22.7864\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2361.9290 - mse: 2361.9292 - mae: 29.0548 - val_loss: 3647.8593 - val_mse: 3647.8594 - val_mae: 22.6282\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2286.6034 - mse: 2286.6033 - mae: 28.8890 - val_loss: 3648.3436 - val_mse: 3648.3438 - val_mae: 22.5506\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2333.9801 - mse: 2333.9802 - mae: 28.6852 - val_loss: 3648.7644 - val_mse: 3648.7644 - val_mae: 22.6338\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2321.8579 - mse: 2321.8584 - mae: 28.6873 - val_loss: 3648.2373 - val_mse: 3648.2363 - val_mae: 22.8683\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2299.1877 - mse: 2299.1870 - mae: 28.9377 - val_loss: 3652.6151 - val_mse: 3652.6152 - val_mae: 22.2675\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2283.2731 - mse: 2283.2734 - mae: 28.4439 - val_loss: 3649.4855 - val_mse: 3649.4856 - val_mae: 22.8331\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2307.0213 - mse: 2307.0210 - mae: 29.0339 - val_loss: 3650.1377 - val_mse: 3650.1387 - val_mae: 22.8385\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2287.6127 - mse: 2287.6135 - mae: 29.0765 - val_loss: 3652.4777 - val_mse: 3652.4783 - val_mae: 22.3880\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2309.2250 - mse: 2309.2251 - mae: 29.2020 - val_loss: 3652.0333 - val_mse: 3652.0337 - val_mae: 22.4429\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2318.5047 - mse: 2318.5044 - mae: 28.4280 - val_loss: 3651.1863 - val_mse: 3651.1868 - val_mae: 22.7347\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2315.5641 - mse: 2315.5630 - mae: 29.0976 - val_loss: 3652.2711 - val_mse: 3652.2708 - val_mae: 22.4549\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2337.9299 - mse: 2337.9299 - mae: 28.9198 - val_loss: 3649.4242 - val_mse: 3649.4233 - val_mae: 22.8278\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2345.8349 - mse: 2345.8352 - mae: 29.0935 - val_loss: 3650.5212 - val_mse: 3650.5210 - val_mae: 22.6064\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2344.7482 - mse: 2344.7488 - mae: 28.9595 - val_loss: 3648.4314 - val_mse: 3648.4312 - val_mae: 22.8769\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2297.5934 - mse: 2297.5942 - mae: 29.0086 - val_loss: 3650.0878 - val_mse: 3650.0886 - val_mae: 22.4083\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2289.3993 - mse: 2289.3989 - mae: 28.9158 - val_loss: 3649.1221 - val_mse: 3649.1226 - val_mae: 22.6987\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2305.4292 - mse: 2305.4292 - mae: 28.7991 - val_loss: 3651.5070 - val_mse: 3651.5063 - val_mae: 22.5058\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2320.3250 - mse: 2320.3252 - mae: 28.7252 - val_loss: 3651.0600 - val_mse: 3651.0605 - val_mae: 22.5721\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2277.0780 - mse: 2277.0781 - mae: 28.8527 - val_loss: 3649.3367 - val_mse: 3649.3367 - val_mae: 22.9768\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2256.9803 - mse: 2256.9800 - mae: 28.4933 - val_loss: 3649.2009 - val_mse: 3649.2004 - val_mae: 23.2601\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 565us/step - loss: 2288.0422 - mse: 2288.0417 - mae: 28.8795 - val_loss: 3648.9729 - val_mse: 3648.9729 - val_mae: 23.2258\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2332.8399 - mse: 2332.8391 - mae: 28.9250 - val_loss: 3649.0304 - val_mse: 3649.0310 - val_mae: 22.7243\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2294.4360 - mse: 2294.4353 - mae: 28.2909 - val_loss: 3648.6659 - val_mse: 3648.6658 - val_mae: 22.8665\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 680us/step - loss: 2278.1653 - mse: 2278.1646 - mae: 28.8786 - val_loss: 3649.1786 - val_mse: 3649.1787 - val_mae: 22.8075\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2252.1185 - mse: 2252.1189 - mae: 28.4769 - val_loss: 3648.8537 - val_mse: 3648.8545 - val_mae: 23.0129\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2322.5823 - mse: 2322.5818 - mae: 28.8326 - val_loss: 3648.7670 - val_mse: 3648.7673 - val_mae: 22.9455\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2299.6066 - mse: 2299.6060 - mae: 28.8977 - val_loss: 3647.5190 - val_mse: 3647.5195 - val_mae: 22.9701\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2283.4648 - mse: 2283.4651 - mae: 28.8558 - val_loss: 3648.1779 - val_mse: 3648.1777 - val_mae: 22.7746\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2260.3566 - mse: 2260.3564 - mae: 28.3123 - val_loss: 3649.0232 - val_mse: 3649.0227 - val_mae: 22.6330\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2313.9787 - mse: 2313.9785 - mae: 28.8751 - val_loss: 3651.1045 - val_mse: 3651.1045 - val_mae: 22.4085\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 585us/step - loss: 2298.5663 - mse: 2298.5659 - mae: 28.6151 - val_loss: 3649.6805 - val_mse: 3649.6799 - val_mae: 23.1149\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 694us/step - loss: 2321.3553 - mse: 2321.3557 - mae: 28.9620 - val_loss: 3650.6105 - val_mse: 3650.6104 - val_mae: 22.5352\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 632us/step - loss: 2329.1437 - mse: 2329.1433 - mae: 28.5796 - val_loss: 3651.1931 - val_mse: 3651.1931 - val_mae: 22.6785\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2334.7869 - mse: 2334.7866 - mae: 28.7798 - val_loss: 3650.3391 - val_mse: 3650.3396 - val_mae: 22.6710\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 670us/step - loss: 2309.1041 - mse: 2309.1045 - mae: 28.7307 - val_loss: 3649.3459 - val_mse: 3649.3455 - val_mae: 22.9879\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2301.1723 - mse: 2301.1721 - mae: 28.7618 - val_loss: 3651.3364 - val_mse: 3651.3357 - val_mae: 22.4765\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2305.1080 - mse: 2305.1089 - mae: 28.9921 - val_loss: 3651.0821 - val_mse: 3651.0820 - val_mae: 22.5525\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2279.3121 - mse: 2279.3123 - mae: 28.8337 - val_loss: 3651.4845 - val_mse: 3651.4849 - val_mae: 22.7388\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2659.8964 - mse: 2659.8967 - mae: 27.9168 - val_loss: 2073.6677 - val_mse: 2073.6677 - val_mae: 25.4499\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2681.2746 - mse: 2681.2751 - mae: 28.1382 - val_loss: 2048.9841 - val_mse: 2048.9839 - val_mae: 26.3435\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2636.1725 - mse: 2636.1724 - mae: 28.0837 - val_loss: 2057.4149 - val_mse: 2057.4148 - val_mae: 25.8552\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2693.1480 - mse: 2693.1487 - mae: 28.2610 - val_loss: 2071.0797 - val_mse: 2071.0796 - val_mae: 25.5399\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 665us/step - loss: 2696.6392 - mse: 2696.6396 - mae: 27.9624 - val_loss: 2077.1560 - val_mse: 2077.1562 - val_mae: 25.5501\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2652.4371 - mse: 2652.4373 - mae: 28.0705 - val_loss: 2067.7783 - val_mse: 2067.7788 - val_mae: 25.9459\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2714.2287 - mse: 2714.2285 - mae: 28.2002 - val_loss: 2073.7823 - val_mse: 2073.7825 - val_mae: 25.8292\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 685us/step - loss: 2683.1440 - mse: 2683.1445 - mae: 28.1847 - val_loss: 2073.9598 - val_mse: 2073.9595 - val_mae: 25.8672\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2623.7455 - mse: 2623.7454 - mae: 28.1504 - val_loss: 2071.0566 - val_mse: 2071.0564 - val_mae: 25.8390\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2711.0077 - mse: 2711.0085 - mae: 28.5173 - val_loss: 2081.5586 - val_mse: 2081.5588 - val_mae: 25.6569\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2706.0303 - mse: 2706.0308 - mae: 28.4862 - val_loss: 2085.0534 - val_mse: 2085.0532 - val_mae: 25.6682\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 665us/step - loss: 2695.2303 - mse: 2695.2307 - mae: 28.3615 - val_loss: 2075.0788 - val_mse: 2075.0791 - val_mae: 26.1474\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2676.7378 - mse: 2676.7375 - mae: 28.2904 - val_loss: 2080.6366 - val_mse: 2080.6367 - val_mae: 26.2015\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2738.1178 - mse: 2738.1177 - mae: 28.8697 - val_loss: 2097.6006 - val_mse: 2097.6006 - val_mae: 25.7072\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2628.2968 - mse: 2628.2974 - mae: 28.2222 - val_loss: 2076.9014 - val_mse: 2076.9016 - val_mae: 25.9668\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2652.9600 - mse: 2652.9592 - mae: 28.2740 - val_loss: 2076.3380 - val_mse: 2076.3384 - val_mae: 25.9654\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2634.0744 - mse: 2634.0752 - mae: 28.0566 - val_loss: 2073.4216 - val_mse: 2073.4216 - val_mae: 26.1005\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2661.1892 - mse: 2661.1895 - mae: 28.4121 - val_loss: 2079.5827 - val_mse: 2079.5825 - val_mae: 25.7443\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2607.6815 - mse: 2607.6812 - mae: 27.8100 - val_loss: 2071.8805 - val_mse: 2071.8806 - val_mae: 26.0717\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2690.9564 - mse: 2690.9556 - mae: 28.4468 - val_loss: 2075.5516 - val_mse: 2075.5518 - val_mae: 25.9504\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2640.5916 - mse: 2640.5911 - mae: 27.9744 - val_loss: 2084.6787 - val_mse: 2084.6785 - val_mae: 25.7797\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 661us/step - loss: 2658.6186 - mse: 2658.6189 - mae: 28.0635 - val_loss: 2071.4444 - val_mse: 2071.4443 - val_mae: 26.2110\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2636.7892 - mse: 2636.7903 - mae: 28.1633 - val_loss: 2067.9635 - val_mse: 2067.9636 - val_mae: 25.8047\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2652.9933 - mse: 2652.9934 - mae: 28.4348 - val_loss: 2071.2555 - val_mse: 2071.2554 - val_mae: 25.8118\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2662.7506 - mse: 2662.7495 - mae: 28.0535 - val_loss: 2077.1677 - val_mse: 2077.1677 - val_mae: 25.9244\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2694.6023 - mse: 2694.6023 - mae: 28.1646 - val_loss: 2078.5671 - val_mse: 2078.5674 - val_mae: 25.8601\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 654us/step - loss: 2714.3741 - mse: 2714.3738 - mae: 28.5622 - val_loss: 2088.0658 - val_mse: 2088.0659 - val_mae: 25.6075\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2689.9726 - mse: 2689.9722 - mae: 27.9990 - val_loss: 2090.6455 - val_mse: 2090.6455 - val_mae: 25.5546\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2726.8764 - mse: 2726.8767 - mae: 28.3185 - val_loss: 2094.1836 - val_mse: 2094.1836 - val_mae: 25.6051\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 657us/step - loss: 2686.5688 - mse: 2686.5686 - mae: 28.2354 - val_loss: 2086.6616 - val_mse: 2086.6616 - val_mae: 25.9599\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2629.1130 - mse: 2629.1128 - mae: 27.8626 - val_loss: 2078.8665 - val_mse: 2078.8669 - val_mae: 26.0087\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2677.3822 - mse: 2677.3823 - mae: 28.0832 - val_loss: 2078.3087 - val_mse: 2078.3088 - val_mae: 25.8129\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2616.8350 - mse: 2616.8345 - mae: 28.1475 - val_loss: 2071.6018 - val_mse: 2071.6018 - val_mae: 26.0323\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2728.3499 - mse: 2728.3503 - mae: 28.6433 - val_loss: 2080.2182 - val_mse: 2080.2185 - val_mae: 25.7018\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2680.4868 - mse: 2680.4878 - mae: 27.8742 - val_loss: 2080.7845 - val_mse: 2080.7847 - val_mae: 25.8067\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2650.0319 - mse: 2650.0317 - mae: 27.9057 - val_loss: 2076.3277 - val_mse: 2076.3279 - val_mae: 26.1432\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2681.5702 - mse: 2681.5698 - mae: 28.1806 - val_loss: 2077.8170 - val_mse: 2077.8169 - val_mae: 25.9659\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2667.5699 - mse: 2667.5706 - mae: 27.8651 - val_loss: 2072.2964 - val_mse: 2072.2964 - val_mae: 26.1527\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 652us/step - loss: 2604.2541 - mse: 2604.2546 - mae: 27.7769 - val_loss: 2085.1634 - val_mse: 2085.1633 - val_mae: 25.9843\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2686.5879 - mse: 2686.5879 - mae: 28.0153 - val_loss: 2086.4274 - val_mse: 2086.4275 - val_mae: 26.1207\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2635.0423 - mse: 2635.0425 - mae: 28.0190 - val_loss: 2077.1086 - val_mse: 2077.1089 - val_mae: 26.1470\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2648.2548 - mse: 2648.2551 - mae: 28.1699 - val_loss: 2062.2359 - val_mse: 2062.2366 - val_mae: 26.4222\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2623.0861 - mse: 2623.0857 - mae: 27.8235 - val_loss: 2071.5882 - val_mse: 2071.5884 - val_mae: 25.8891\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 645us/step - loss: 2644.8253 - mse: 2644.8252 - mae: 27.6761 - val_loss: 2069.8349 - val_mse: 2069.8350 - val_mae: 26.0229\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2669.8557 - mse: 2669.8557 - mae: 28.0984 - val_loss: 2068.9436 - val_mse: 2068.9431 - val_mae: 26.1428\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2659.5430 - mse: 2659.5427 - mae: 28.2587 - val_loss: 2084.8463 - val_mse: 2084.8462 - val_mae: 25.7305\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 554us/step - loss: 2631.9011 - mse: 2631.9014 - mae: 27.7037 - val_loss: 2082.2581 - val_mse: 2082.2581 - val_mae: 25.9745\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2631.2034 - mse: 2631.2041 - mae: 27.8429 - val_loss: 2078.4430 - val_mse: 2078.4429 - val_mae: 26.1659\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2649.5172 - mse: 2649.5173 - mae: 27.9239 - val_loss: 2080.9699 - val_mse: 2080.9695 - val_mae: 26.0451\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2592.8972 - mse: 2592.8975 - mae: 28.0625 - val_loss: 2083.4253 - val_mse: 2083.4255 - val_mae: 26.0575\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2653.2650 - mse: 2653.2646 - mae: 27.9158 - val_loss: 2080.6849 - val_mse: 2080.6848 - val_mae: 26.3282\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2591.6934 - mse: 2591.6934 - mae: 27.7215 - val_loss: 2074.7395 - val_mse: 2074.7397 - val_mae: 26.4146\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2629.3829 - mse: 2629.3833 - mae: 27.8588 - val_loss: 2074.4815 - val_mse: 2074.4812 - val_mae: 26.3094\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2636.1242 - mse: 2636.1233 - mae: 27.9912 - val_loss: 2072.8360 - val_mse: 2072.8364 - val_mae: 26.3351\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2649.2133 - mse: 2649.2129 - mae: 27.9040 - val_loss: 2068.9068 - val_mse: 2068.9072 - val_mae: 25.8271\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2646.8093 - mse: 2646.8098 - mae: 27.8658 - val_loss: 2074.1631 - val_mse: 2074.1626 - val_mae: 25.7912\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2651.9513 - mse: 2651.9521 - mae: 27.9757 - val_loss: 2081.7147 - val_mse: 2081.7148 - val_mae: 25.6921\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2636.0890 - mse: 2636.0884 - mae: 27.7111 - val_loss: 2069.5498 - val_mse: 2069.5498 - val_mae: 26.0476\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2655.6944 - mse: 2655.6938 - mae: 28.2115 - val_loss: 2080.4055 - val_mse: 2080.4053 - val_mae: 25.9914\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2626.7461 - mse: 2626.7463 - mae: 27.9204 - val_loss: 2082.8163 - val_mse: 2082.8169 - val_mae: 26.1087\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2613.5766 - mse: 2613.5771 - mae: 27.8048 - val_loss: 2083.2489 - val_mse: 2083.2488 - val_mae: 25.8238\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2617.8908 - mse: 2617.8909 - mae: 27.7001 - val_loss: 2080.1480 - val_mse: 2080.1477 - val_mae: 26.0880\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2703.1918 - mse: 2703.1929 - mae: 28.3255 - val_loss: 2095.1313 - val_mse: 2095.1309 - val_mae: 25.7542\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 647us/step - loss: 2609.4638 - mse: 2609.4636 - mae: 27.9502 - val_loss: 2081.0691 - val_mse: 2081.0688 - val_mae: 25.9798\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2652.6716 - mse: 2652.6716 - mae: 27.7422 - val_loss: 2073.5935 - val_mse: 2073.5935 - val_mae: 26.1087\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 546us/step - loss: 2647.1517 - mse: 2647.1519 - mae: 27.8667 - val_loss: 2067.8154 - val_mse: 2067.8154 - val_mae: 26.4832\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2646.4494 - mse: 2646.4497 - mae: 28.2264 - val_loss: 2075.5536 - val_mse: 2075.5540 - val_mae: 25.9201\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2683.6483 - mse: 2683.6479 - mae: 28.1233 - val_loss: 2073.4654 - val_mse: 2073.4653 - val_mae: 25.8928\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2676.1319 - mse: 2676.1323 - mae: 27.8414 - val_loss: 2075.0268 - val_mse: 2075.0269 - val_mae: 26.1072\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 517us/step - loss: 2610.0685 - mse: 2610.0686 - mae: 27.6665 - val_loss: 2082.6946 - val_mse: 2082.6948 - val_mae: 25.9247\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2658.9491 - mse: 2658.9492 - mae: 27.9136 - val_loss: 2071.9457 - val_mse: 2071.9456 - val_mae: 26.3298\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2684.3921 - mse: 2684.3923 - mae: 28.1889 - val_loss: 2078.8836 - val_mse: 2078.8835 - val_mae: 25.9664\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2626.7965 - mse: 2626.7974 - mae: 27.6502 - val_loss: 2072.5140 - val_mse: 2072.5142 - val_mae: 26.1882\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2612.1821 - mse: 2612.1821 - mae: 27.7643 - val_loss: 2073.2325 - val_mse: 2073.2324 - val_mae: 25.9523\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2629.2888 - mse: 2629.2881 - mae: 27.8020 - val_loss: 2082.6735 - val_mse: 2082.6736 - val_mae: 25.8654\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2665.7779 - mse: 2665.7773 - mae: 27.7444 - val_loss: 2070.4790 - val_mse: 2070.4788 - val_mae: 26.0825\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2653.5599 - mse: 2653.5601 - mae: 27.9761 - val_loss: 2071.4617 - val_mse: 2071.4619 - val_mae: 26.2575\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2671.6487 - mse: 2671.6494 - mae: 28.0012 - val_loss: 2087.0401 - val_mse: 2087.0400 - val_mae: 25.5400\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 650us/step - loss: 2666.6763 - mse: 2666.6765 - mae: 28.1671 - val_loss: 2075.8245 - val_mse: 2075.8245 - val_mae: 25.9244\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 663us/step - loss: 2624.6166 - mse: 2624.6165 - mae: 27.7913 - val_loss: 2068.5489 - val_mse: 2068.5488 - val_mae: 26.1544\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 13310.0276 - mse: 13310.0273 - mae: 109.8195 - val_loss: 34567.4735 - val_mse: 34567.4727 - val_mae: 132.5423\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 13083.4616 - mse: 13083.4619 - mae: 108.7831 - val_loss: 34093.0371 - val_mse: 34093.0352 - val_mae: 130.7644\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 12443.5209 - mse: 12443.5215 - mae: 105.7877 - val_loss: 32687.3454 - val_mse: 32687.3457 - val_mae: 125.3549\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 546us/step - loss: 10737.4462 - mse: 10737.4463 - mae: 97.0931 - val_loss: 28934.7621 - val_mse: 28934.7617 - val_mae: 109.6316\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 6812.4844 - mse: 6812.4844 - mae: 72.5534 - val_loss: 21340.0924 - val_mse: 21340.0938 - val_mae: 67.4469\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 3269.7568 - mse: 3269.7573 - mae: 41.7271 - val_loss: 17007.7458 - val_mse: 17007.7461 - val_mae: 35.5135\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2697.3946 - mse: 2697.3948 - mae: 37.9817 - val_loss: 17373.0843 - val_mse: 17373.0840 - val_mae: 35.5333\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 689us/step - loss: 2727.2767 - mse: 2727.2766 - mae: 38.1499 - val_loss: 17470.8923 - val_mse: 17470.8926 - val_mae: 35.6890\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 2602.3618 - mse: 2602.3621 - mae: 36.9492 - val_loss: 17304.2568 - val_mse: 17304.2559 - val_mae: 35.4790\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 2872.2822 - mse: 2872.2822 - mae: 38.9118 - val_loss: 17386.5057 - val_mse: 17386.5039 - val_mae: 35.5396\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 2711.5959 - mse: 2711.5959 - mae: 37.0464 - val_loss: 17653.1049 - val_mse: 17653.1035 - val_mae: 36.2659\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 666us/step - loss: 2821.2782 - mse: 2821.2783 - mae: 38.6028 - val_loss: 17487.4813 - val_mse: 17487.4824 - val_mae: 35.6931\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2745.3290 - mse: 2745.3293 - mae: 38.0526 - val_loss: 17315.6130 - val_mse: 17315.6133 - val_mae: 35.5106\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2553.9905 - mse: 2553.9905 - mae: 36.2988 - val_loss: 17542.3780 - val_mse: 17542.3770 - val_mae: 35.8302\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 411us/step - loss: 2612.2695 - mse: 2612.2695 - mae: 37.0889 - val_loss: 17427.3983 - val_mse: 17427.3984 - val_mae: 35.5856\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 273us/step - loss: 2867.9047 - mse: 2867.9050 - mae: 38.2763 - val_loss: 17421.3655 - val_mse: 17421.3652 - val_mae: 35.5907\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 280us/step - loss: 2761.0437 - mse: 2761.0437 - mae: 36.7721 - val_loss: 17431.0916 - val_mse: 17431.0918 - val_mae: 35.5811\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 417us/step - loss: 2646.0901 - mse: 2646.0903 - mae: 36.9980 - val_loss: 17327.2446 - val_mse: 17327.2441 - val_mae: 35.4919\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 417us/step - loss: 2534.9143 - mse: 2534.9146 - mae: 36.1326 - val_loss: 17308.4353 - val_mse: 17308.4336 - val_mae: 35.4805\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 545us/step - loss: 2693.5571 - mse: 2693.5571 - mae: 35.9533 - val_loss: 17354.8421 - val_mse: 17354.8418 - val_mae: 35.4964\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 507us/step - loss: 2409.6674 - mse: 2409.6675 - mae: 35.6261 - val_loss: 17325.6509 - val_mse: 17325.6504 - val_mae: 35.4780\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 2286.5320 - mse: 2286.5317 - mae: 32.9825 - val_loss: 17092.3396 - val_mse: 17092.3398 - val_mae: 35.4984\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 2545.0437 - mse: 2545.0439 - mae: 35.4745 - val_loss: 17278.6790 - val_mse: 17278.6797 - val_mae: 35.4710\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 463us/step - loss: 2353.2681 - mse: 2353.2681 - mae: 34.5636 - val_loss: 17442.1792 - val_mse: 17442.1797 - val_mae: 35.6632\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 2441.3565 - mse: 2441.3567 - mae: 34.43 - 0s 522us/step - loss: 2340.1247 - mse: 2340.1250 - mae: 33.9626 - val_loss: 17376.0230 - val_mse: 17376.0234 - val_mae: 35.5643\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 455us/step - loss: 2544.5815 - mse: 2544.5815 - mae: 35.3391 - val_loss: 17377.6731 - val_mse: 17377.6738 - val_mae: 35.5646\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 352us/step - loss: 2536.6384 - mse: 2536.6379 - mae: 35.0804 - val_loss: 17594.6049 - val_mse: 17594.6055 - val_mae: 36.1020\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 389us/step - loss: 2582.5144 - mse: 2582.5142 - mae: 35.8267 - val_loss: 17367.1301 - val_mse: 17367.1289 - val_mae: 35.5283\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 464us/step - loss: 2523.4617 - mse: 2523.4617 - mae: 35.2161 - val_loss: 17520.0796 - val_mse: 17520.0801 - val_mae: 35.9127\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 331us/step - loss: 2259.3958 - mse: 2259.3955 - mae: 34.0485 - val_loss: 17154.3108 - val_mse: 17154.3125 - val_mae: 35.4796\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 461us/step - loss: 2330.2350 - mse: 2330.2351 - mae: 34.5863 - val_loss: 17398.0848 - val_mse: 17398.0840 - val_mae: 35.6543\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 473us/step - loss: 2257.7411 - mse: 2257.7412 - mae: 33.5625 - val_loss: 17164.8181 - val_mse: 17164.8184 - val_mae: 35.4869\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 407us/step - loss: 2395.8721 - mse: 2395.8723 - mae: 34.8571 - val_loss: 17375.3184 - val_mse: 17375.3184 - val_mae: 35.6555\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 437us/step - loss: 2518.9432 - mse: 2518.9431 - mae: 35.0325 - val_loss: 17433.4432 - val_mse: 17433.4414 - val_mae: 35.8296\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 451us/step - loss: 2343.1342 - mse: 2343.1343 - mae: 33.6646 - val_loss: 17644.4908 - val_mse: 17644.4922 - val_mae: 36.5143\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 488us/step - loss: 2306.5442 - mse: 2306.5442 - mae: 32.8562 - val_loss: 17278.0390 - val_mse: 17278.0391 - val_mae: 35.5464\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 540us/step - loss: 2336.3964 - mse: 2336.3960 - mae: 33.6231 - val_loss: 17379.4142 - val_mse: 17379.4141 - val_mae: 35.7494\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2170.0105 - mse: 2170.0105 - mae: 32.9882 - val_loss: 17311.2809 - val_mse: 17311.2793 - val_mae: 35.6503\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 418us/step - loss: 2380.2907 - mse: 2380.2908 - mae: 34.5201 - val_loss: 17410.5367 - val_mse: 17410.5352 - val_mae: 35.8605\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 347us/step - loss: 2343.5302 - mse: 2343.5300 - mae: 34.5851 - val_loss: 17479.6074 - val_mse: 17479.6074 - val_mae: 36.0318\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 2210.5132 - mse: 2210.5127 - mae: 33.2062 - val_loss: 17359.5807 - val_mse: 17359.5801 - val_mae: 35.7939\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2109.5172 - mse: 2109.5173 - mae: 31.9456 - val_loss: 17355.9734 - val_mse: 17355.9727 - val_mae: 35.8021\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 445us/step - loss: 2238.0643 - mse: 2238.0647 - mae: 33.0634 - val_loss: 17478.3169 - val_mse: 17478.3164 - val_mae: 36.0999\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2289.7423 - mse: 2289.7419 - mae: 33.1813 - val_loss: 17373.7474 - val_mse: 17373.7480 - val_mae: 35.8756\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 2286.4247 - mse: 2286.4246 - mae: 33.4991 - val_loss: 17277.5037 - val_mse: 17277.5039 - val_mae: 35.7105\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 469us/step - loss: 2199.2187 - mse: 2199.2185 - mae: 32.7130 - val_loss: 17501.0042 - val_mse: 17501.0039 - val_mae: 36.2097\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 529us/step - loss: 2265.4104 - mse: 2265.4104 - mae: 32.7874 - val_loss: 17555.3565 - val_mse: 17555.3574 - val_mae: 36.4199\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 531us/step - loss: 2189.4486 - mse: 2189.4485 - mae: 32.5857 - val_loss: 17406.8680 - val_mse: 17406.8672 - val_mae: 35.9927\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2087.0593 - mse: 2087.0596 - mae: 31.2798 - val_loss: 17292.6103 - val_mse: 17292.6074 - val_mae: 35.7949\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2058.7353 - mse: 2058.7354 - mae: 31.5735 - val_loss: 17486.1045 - val_mse: 17486.1035 - val_mae: 36.2284\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 618us/step - loss: 2137.3185 - mse: 2137.3188 - mae: 31.8042 - val_loss: 17273.7420 - val_mse: 17273.7441 - val_mae: 35.7910\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 2090.4111 - mse: 2090.4111 - mae: 31.4450 - val_loss: 17395.2871 - val_mse: 17395.2852 - val_mae: 36.0467\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 560us/step - loss: 2154.8921 - mse: 2154.8921 - mae: 31.9072 - val_loss: 17410.2966 - val_mse: 17410.2969 - val_mae: 36.1117\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2147.1693 - mse: 2147.1694 - mae: 33.0837 - val_loss: 17620.6551 - val_mse: 17620.6543 - val_mae: 36.9413\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 575us/step - loss: 2124.6984 - mse: 2124.6985 - mae: 31.9296 - val_loss: 17514.9787 - val_mse: 17514.9785 - val_mae: 36.4686\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2141.5703 - mse: 2141.5701 - mae: 32.0674 - val_loss: 17447.3290 - val_mse: 17447.3281 - val_mae: 36.2828\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 2368.3997 - mse: 2368.3997 - mae: 33.1177 - val_loss: 17546.5378 - val_mse: 17546.5371 - val_mae: 36.6303\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 744us/step - loss: 2072.4483 - mse: 2072.4482 - mae: 32.0966 - val_loss: 17314.9525 - val_mse: 17314.9512 - val_mae: 36.0000\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 627us/step - loss: 2152.8195 - mse: 2152.8196 - mae: 32.5921 - val_loss: 17418.4763 - val_mse: 17418.4766 - val_mae: 36.2268\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 661us/step - loss: 2141.1129 - mse: 2141.1128 - mae: 32.1499 - val_loss: 17309.6706 - val_mse: 17309.6719 - val_mae: 36.0233\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 666us/step - loss: 2213.0999 - mse: 2213.1001 - mae: 32.5092 - val_loss: 17452.4141 - val_mse: 17452.4141 - val_mae: 36.3451\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 628us/step - loss: 1988.9469 - mse: 1988.9471 - mae: 31.2411 - val_loss: 17256.0707 - val_mse: 17256.0723 - val_mae: 35.9917\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 2130.5385 - mse: 2130.5386 - mae: 32.3448 - val_loss: 17534.9068 - val_mse: 17534.9062 - val_mae: 36.6998\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 1816.1238 - mse: 1816.1238 - mae: 29.2544 - val_loss: 17394.1536 - val_mse: 17394.1523 - val_mae: 36.2843\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 1965.3710 - mse: 1965.3712 - mae: 30.6166 - val_loss: 17590.3001 - val_mse: 17590.3008 - val_mae: 37.0343\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 644us/step - loss: 2105.9302 - mse: 2105.9299 - mae: 30.9504 - val_loss: 17321.7096 - val_mse: 17321.7109 - val_mae: 36.2149\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2026.7981 - mse: 2026.7979 - mae: 30.7873 - val_loss: 17347.1603 - val_mse: 17347.1602 - val_mae: 36.2561\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2016.4280 - mse: 2016.4281 - mae: 31.6703 - val_loss: 17314.7495 - val_mse: 17314.7500 - val_mae: 36.2246\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 548us/step - loss: 1879.9867 - mse: 1879.9868 - mae: 29.8316 - val_loss: 17319.7089 - val_mse: 17319.7070 - val_mae: 36.3048\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2118.2352 - mse: 2118.2356 - mae: 31.9753 - val_loss: 17489.5054 - val_mse: 17489.5059 - val_mae: 36.7653\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 520us/step - loss: 1991.8461 - mse: 1991.8459 - mae: 30.8163 - val_loss: 17300.8126 - val_mse: 17300.8145 - val_mae: 36.3081\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 533us/step - loss: 1839.1412 - mse: 1839.1412 - mae: 29.4658 - val_loss: 17399.0190 - val_mse: 17399.0176 - val_mae: 36.5404\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 579us/step - loss: 2040.2783 - mse: 2040.2783 - mae: 30.9558 - val_loss: 17454.4718 - val_mse: 17454.4727 - val_mae: 36.6827\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 1948.5811 - mse: 1948.5808 - mae: 30.2790 - val_loss: 17425.9733 - val_mse: 17425.9727 - val_mae: 36.6131\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 1971.6678 - mse: 1971.6680 - mae: 31.3099 - val_loss: 17444.7603 - val_mse: 17444.7598 - val_mae: 36.7141\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 713us/step - loss: 2026.9827 - mse: 2026.9827 - mae: 30.8381 - val_loss: 17594.4513 - val_mse: 17594.4492 - val_mae: 37.3861\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 1895.8025 - mse: 1895.8026 - mae: 31.0577 - val_loss: 17553.6793 - val_mse: 17553.6797 - val_mae: 37.1934\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 1885.2043 - mse: 1885.2045 - mae: 29.3296 - val_loss: 17586.7495 - val_mse: 17586.7480 - val_mae: 37.3686\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2030.8118 - mse: 2030.8118 - mae: 30.8739 - val_loss: 17560.2224 - val_mse: 17560.2227 - val_mae: 37.2315\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 492us/step - loss: 2020.3967 - mse: 2020.3967 - mae: 30.6941 - val_loss: 17445.4544 - val_mse: 17445.4531 - val_mae: 36.7716\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 4320.5443 - mse: 4320.5444 - mae: 34.7957 - val_loss: 2014.0792 - val_mse: 2014.0791 - val_mae: 29.3966\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 526us/step - loss: 4193.4664 - mse: 4193.4668 - mae: 36.0327 - val_loss: 2141.0668 - val_mse: 2141.0669 - val_mae: 29.8737\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 0s 490us/step - loss: 4115.4642 - mse: 4115.4639 - mae: 35.1878 - val_loss: 2261.5707 - val_mse: 2261.5708 - val_mae: 30.4506\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 3987.5564 - mse: 3987.5566 - mae: 33.7834 - val_loss: 2193.9255 - val_mse: 2193.9255 - val_mae: 30.1081\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4481.4698 - mse: 4481.4702 - mae: 36.3153 - val_loss: 2265.5972 - val_mse: 2265.5972 - val_mae: 30.4748\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 4360.3841 - mse: 4360.3838 - mae: 34.9284 - val_loss: 2262.5270 - val_mse: 2262.5271 - val_mae: 30.4518\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4311.3133 - mse: 4311.3130 - mae: 35.3837 - val_loss: 2229.7957 - val_mse: 2229.7959 - val_mae: 30.2633\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4403.6628 - mse: 4403.6631 - mae: 35.5974 - val_loss: 2187.5936 - val_mse: 2187.5938 - val_mae: 30.0417\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4304.0305 - mse: 4304.0303 - mae: 34.9699 - val_loss: 2222.3253 - val_mse: 2222.3257 - val_mae: 30.2001\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4207.6633 - mse: 4207.6631 - mae: 35.0328 - val_loss: 2247.7066 - val_mse: 2247.7065 - val_mae: 30.3318\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4235.1310 - mse: 4235.1309 - mae: 35.3364 - val_loss: 2238.5731 - val_mse: 2238.5732 - val_mae: 30.2888\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 4145.4431 - mse: 4145.4429 - mae: 34.6977 - val_loss: 2239.5418 - val_mse: 2239.5417 - val_mae: 30.2832\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4418.3507 - mse: 4418.3501 - mae: 35.5803 - val_loss: 2327.8566 - val_mse: 2327.8564 - val_mae: 30.7503\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 653us/step - loss: 4061.0043 - mse: 4061.0044 - mae: 34.6622 - val_loss: 2162.3583 - val_mse: 2162.3584 - val_mae: 29.8759\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 612us/step - loss: 4223.8346 - mse: 4223.8350 - mae: 34.7988 - val_loss: 2218.2269 - val_mse: 2218.2268 - val_mae: 30.1364\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 513us/step - loss: 4176.7277 - mse: 4176.7271 - mae: 34.8259 - val_loss: 2184.5298 - val_mse: 2184.5298 - val_mae: 29.9713\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 702us/step - loss: 4066.1300 - mse: 4066.1299 - mae: 34.2941 - val_loss: 2198.4435 - val_mse: 2198.4436 - val_mae: 30.0354\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4051.0140 - mse: 4051.0144 - mae: 34.8916 - val_loss: 2265.8272 - val_mse: 2265.8269 - val_mae: 30.3810\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4219.4857 - mse: 4219.4854 - mae: 35.2034 - val_loss: 2207.5003 - val_mse: 2207.5002 - val_mae: 30.0715\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4028.6110 - mse: 4028.6116 - mae: 34.1448 - val_loss: 2264.3303 - val_mse: 2264.3303 - val_mae: 30.3673\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4184.4363 - mse: 4184.4365 - mae: 34.5178 - val_loss: 2241.0936 - val_mse: 2241.0935 - val_mae: 30.2384\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 616us/step - loss: 4120.6916 - mse: 4120.6919 - mae: 34.4799 - val_loss: 2267.2824 - val_mse: 2267.2822 - val_mae: 30.3804\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 4046.1556 - mse: 4046.1565 - mae: 35.3963 - val_loss: 2249.3576 - val_mse: 2249.3574 - val_mae: 30.2822\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4152.2423 - mse: 4152.2417 - mae: 34.9214 - val_loss: 2179.8008 - val_mse: 2179.8010 - val_mae: 29.9331\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 569us/step - loss: 4208.0218 - mse: 4208.0225 - mae: 35.3158 - val_loss: 2288.6560 - val_mse: 2288.6560 - val_mae: 30.5008\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4071.2295 - mse: 4071.2297 - mae: 34.8881 - val_loss: 2279.6230 - val_mse: 2279.6230 - val_mae: 30.4450\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4058.7366 - mse: 4058.7371 - mae: 34.5291 - val_loss: 2254.5480 - val_mse: 2254.5481 - val_mae: 30.2981\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 633us/step - loss: 3954.9844 - mse: 3954.9841 - mae: 35.0878 - val_loss: 2186.2624 - val_mse: 2186.2622 - val_mae: 29.9577\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 548us/step - loss: 4242.3616 - mse: 4242.3623 - mae: 35.2582 - val_loss: 2272.5612 - val_mse: 2272.5613 - val_mae: 30.3983\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4074.1392 - mse: 4074.1389 - mae: 34.4310 - val_loss: 2199.0415 - val_mse: 2199.0413 - val_mae: 29.9979\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 636us/step - loss: 4148.2644 - mse: 4148.2642 - mae: 34.6037 - val_loss: 2167.5216 - val_mse: 2167.5217 - val_mae: 29.8516\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 689us/step - loss: 4127.7537 - mse: 4127.7534 - mae: 34.8841 - val_loss: 2213.8159 - val_mse: 2213.8157 - val_mae: 30.0759\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 518us/step - loss: 4166.2961 - mse: 4166.2964 - mae: 33.7403 - val_loss: 2248.5111 - val_mse: 2248.5112 - val_mae: 30.2584\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 555us/step - loss: 4232.8153 - mse: 4232.8159 - mae: 34.5284 - val_loss: 2271.4281 - val_mse: 2271.4277 - val_mae: 30.3901\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 503us/step - loss: 4192.8483 - mse: 4192.8486 - mae: 35.0763 - val_loss: 2222.7685 - val_mse: 2222.7688 - val_mae: 30.1339\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 551us/step - loss: 3979.4176 - mse: 3979.4175 - mae: 34.4563 - val_loss: 2182.0978 - val_mse: 2182.0979 - val_mae: 29.9376\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4020.1286 - mse: 4020.1287 - mae: 34.4516 - val_loss: 2262.9620 - val_mse: 2262.9622 - val_mae: 30.3484\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 568us/step - loss: 4163.7770 - mse: 4163.7764 - mae: 34.2122 - val_loss: 2275.1424 - val_mse: 2275.1423 - val_mae: 30.4237\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4165.7992 - mse: 4165.7993 - mae: 35.1394 - val_loss: 2229.1110 - val_mse: 2229.1111 - val_mae: 30.1799\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4198.9479 - mse: 4198.9482 - mae: 34.2405 - val_loss: 2230.9118 - val_mse: 2230.9119 - val_mae: 30.1941\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 574us/step - loss: 4132.2662 - mse: 4132.2666 - mae: 34.2307 - val_loss: 2265.5721 - val_mse: 2265.5720 - val_mae: 30.3688\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 548us/step - loss: 3907.2574 - mse: 3907.2573 - mae: 32.9622 - val_loss: 2192.5011 - val_mse: 2192.5010 - val_mae: 29.9988\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 664us/step - loss: 3863.1936 - mse: 3863.1929 - mae: 31.9028 - val_loss: 2212.9124 - val_mse: 2212.9124 - val_mae: 30.0857\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4195.1503 - mse: 4195.1509 - mae: 34.5610 - val_loss: 2222.5758 - val_mse: 2222.5757 - val_mae: 30.1356\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4071.4055 - mse: 4071.4060 - mae: 34.3400 - val_loss: 2207.9748 - val_mse: 2207.9751 - val_mae: 30.0707\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 531us/step - loss: 4137.0179 - mse: 4137.0176 - mae: 34.0021 - val_loss: 2213.4683 - val_mse: 2213.4685 - val_mae: 30.1004\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4005.7115 - mse: 4005.7112 - mae: 33.8074 - val_loss: 2220.8777 - val_mse: 2220.8774 - val_mae: 30.1355\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 4219.0891 - mse: 4219.0889 - mae: 33.8534 - val_loss: 2291.6094 - val_mse: 2291.6096 - val_mae: 30.5091\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 621us/step - loss: 4008.8143 - mse: 4008.8142 - mae: 33.4351 - val_loss: 2187.1158 - val_mse: 2187.1157 - val_mae: 29.9876\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 689us/step - loss: 4007.3534 - mse: 4007.3530 - mae: 33.2943 - val_loss: 2178.1190 - val_mse: 2178.1189 - val_mae: 29.9333\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 707us/step - loss: 4160.8630 - mse: 4160.8633 - mae: 35.0254 - val_loss: 2275.6057 - val_mse: 2275.6057 - val_mae: 30.4100\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4191.8868 - mse: 4191.8872 - mae: 34.4852 - val_loss: 2300.0044 - val_mse: 2300.0044 - val_mae: 30.5466\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4176.1705 - mse: 4176.1704 - mae: 34.2251 - val_loss: 2235.7974 - val_mse: 2235.7974 - val_mae: 30.2004\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 3977.0502 - mse: 3977.0500 - mae: 32.9670 - val_loss: 2183.9365 - val_mse: 2183.9365 - val_mae: 29.9619\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 4091.4773 - mse: 4091.4773 - mae: 34.2928 - val_loss: 2203.7310 - val_mse: 2203.7312 - val_mae: 30.0537\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4036.6007 - mse: 4036.6006 - mae: 33.7351 - val_loss: 2279.9318 - val_mse: 2279.9316 - val_mae: 30.4338\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4000.1254 - mse: 4000.1257 - mae: 34.6383 - val_loss: 2241.2765 - val_mse: 2241.2769 - val_mae: 30.2284\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 511us/step - loss: 4129.9312 - mse: 4129.9316 - mae: 33.6704 - val_loss: 2308.7224 - val_mse: 2308.7222 - val_mae: 30.5927\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 527us/step - loss: 4100.1715 - mse: 4100.1709 - mae: 33.4053 - val_loss: 2281.8551 - val_mse: 2281.8552 - val_mae: 30.4471\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 4022.1560 - mse: 4022.1565 - mae: 33.4634 - val_loss: 2223.2444 - val_mse: 2223.2444 - val_mae: 30.1410\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 538us/step - loss: 4144.7445 - mse: 4144.7451 - mae: 33.7443 - val_loss: 2243.4552 - val_mse: 2243.4553 - val_mae: 30.2284\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 4011.0218 - mse: 4011.0210 - mae: 33.5692 - val_loss: 2293.9760 - val_mse: 2293.9763 - val_mae: 30.5090\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 706us/step - loss: 4077.0996 - mse: 4077.1001 - mae: 33.3837 - val_loss: 2229.2763 - val_mse: 2229.2764 - val_mae: 30.1702\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 677us/step - loss: 4027.7747 - mse: 4027.7749 - mae: 33.9082 - val_loss: 2227.1576 - val_mse: 2227.1575 - val_mae: 30.1692\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4038.0400 - mse: 4038.0403 - mae: 32.7203 - val_loss: 2214.7359 - val_mse: 2214.7358 - val_mae: 30.1136\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 3959.8725 - mse: 3959.8726 - mae: 32.6844 - val_loss: 2250.8656 - val_mse: 2250.8655 - val_mae: 30.2766\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4263.8791 - mse: 4263.8784 - mae: 34.1904 - val_loss: 2315.6973 - val_mse: 2315.6975 - val_mae: 30.6296\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 3998.3224 - mse: 3998.3225 - mae: 33.1723 - val_loss: 2259.3778 - val_mse: 2259.3777 - val_mae: 30.3297\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 566us/step - loss: 4190.0534 - mse: 4190.0532 - mae: 35.2402 - val_loss: 2234.8981 - val_mse: 2234.8977 - val_mae: 30.2274\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4090.3775 - mse: 4090.3777 - mae: 33.5909 - val_loss: 2241.9682 - val_mse: 2241.9680 - val_mae: 30.2653\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 668us/step - loss: 4114.8118 - mse: 4114.8125 - mae: 33.6358 - val_loss: 2253.4524 - val_mse: 2253.4521 - val_mae: 30.3197\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 3932.3375 - mse: 3932.3381 - mae: 33.0512 - val_loss: 2174.1076 - val_mse: 2174.1074 - val_mae: 29.9752\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4061.4243 - mse: 4061.4246 - mae: 33.1363 - val_loss: 2168.9530 - val_mse: 2168.9529 - val_mae: 29.9611\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 628us/step - loss: 4041.4565 - mse: 4041.4563 - mae: 33.3257 - val_loss: 2214.8208 - val_mse: 2214.8210 - val_mae: 30.1481\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 4291.9779 - mse: 4291.9785 - mae: 34.2639 - val_loss: 2267.8626 - val_mse: 2267.8625 - val_mae: 30.4000\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 600us/step - loss: 4042.0002 - mse: 4042.0000 - mae: 32.9402 - val_loss: 2237.0531 - val_mse: 2237.0532 - val_mae: 30.2488\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 665us/step - loss: 4057.5378 - mse: 4057.5378 - mae: 34.1483 - val_loss: 2306.2260 - val_mse: 2306.2263 - val_mae: 30.5931\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 663us/step - loss: 4131.7323 - mse: 4131.7324 - mae: 32.2744 - val_loss: 2228.0694 - val_mse: 2228.0696 - val_mae: 30.1889\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 0s 499us/step - loss: 3897.0696 - mse: 3897.0688 - mae: 33.0859 - val_loss: 2191.7590 - val_mse: 2191.7590 - val_mae: 30.0217\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 529us/step - loss: 3908.0664 - mse: 3908.0664 - mae: 32.9915 - val_loss: 2260.0703 - val_mse: 2260.0703 - val_mae: 30.3338\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3436.0441 - mse: 3436.0437 - mae: 33.0437 - val_loss: 1466.6309 - val_mse: 1466.6309 - val_mae: 24.8802\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3309.0045 - mse: 3309.0046 - mae: 32.7626 - val_loss: 1460.3138 - val_mse: 1460.3137 - val_mae: 25.0961\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3348.4470 - mse: 3348.4473 - mae: 32.6477 - val_loss: 1476.6251 - val_mse: 1476.6250 - val_mae: 24.5848\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3380.9645 - mse: 3380.9648 - mae: 32.6880 - val_loss: 1456.7240 - val_mse: 1456.7238 - val_mae: 25.2797\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 704us/step - loss: 3284.8603 - mse: 3284.8611 - mae: 32.5942 - val_loss: 1455.1358 - val_mse: 1455.1360 - val_mae: 25.2491\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3382.3546 - mse: 3382.3545 - mae: 32.2222 - val_loss: 1458.4067 - val_mse: 1458.4066 - val_mae: 25.1082\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3305.6139 - mse: 3305.6147 - mae: 33.1475 - val_loss: 1459.3971 - val_mse: 1459.3971 - val_mae: 25.0567\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3453.4107 - mse: 3453.4111 - mae: 33.6355 - val_loss: 1459.5030 - val_mse: 1459.5032 - val_mae: 24.9792\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3413.6036 - mse: 3413.6038 - mae: 32.6864 - val_loss: 1455.2873 - val_mse: 1455.2872 - val_mae: 25.1993\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 615us/step - loss: 3369.5986 - mse: 3369.5979 - mae: 32.3572 - val_loss: 1462.3265 - val_mse: 1462.3265 - val_mae: 24.9777\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3258.7435 - mse: 3258.7439 - mae: 32.2482 - val_loss: 1464.3608 - val_mse: 1464.3608 - val_mae: 24.9385\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3213.2570 - mse: 3213.2576 - mae: 32.0866 - val_loss: 1458.7548 - val_mse: 1458.7550 - val_mae: 25.1703\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3388.5636 - mse: 3388.5630 - mae: 33.2472 - val_loss: 1467.6250 - val_mse: 1467.6250 - val_mae: 24.8890\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3471.4316 - mse: 3471.4314 - mae: 33.2389 - val_loss: 1471.2817 - val_mse: 1471.2817 - val_mae: 24.8382\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3268.8195 - mse: 3268.8188 - mae: 32.0164 - val_loss: 1460.7103 - val_mse: 1460.7102 - val_mae: 25.1700\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 570us/step - loss: 3391.3237 - mse: 3391.3237 - mae: 33.1903 - val_loss: 1459.2323 - val_mse: 1459.2324 - val_mae: 25.2265\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3182.2469 - mse: 3182.2468 - mae: 31.3468 - val_loss: 1455.5438 - val_mse: 1455.5438 - val_mae: 25.7533\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3436.5279 - mse: 3436.5273 - mae: 32.9511 - val_loss: 1461.5433 - val_mse: 1461.5435 - val_mae: 25.1484\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3359.1502 - mse: 3359.1511 - mae: 33.0557 - val_loss: 1459.3297 - val_mse: 1459.3296 - val_mae: 25.2341\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3247.6139 - mse: 3247.6145 - mae: 31.7324 - val_loss: 1464.2343 - val_mse: 1464.2343 - val_mae: 24.9495\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3234.3947 - mse: 3234.3945 - mae: 32.6640 - val_loss: 1459.0375 - val_mse: 1459.0375 - val_mae: 25.1170\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3322.1373 - mse: 3322.1375 - mae: 32.9086 - val_loss: 1465.2373 - val_mse: 1465.2373 - val_mae: 24.8385\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 663us/step - loss: 3346.7633 - mse: 3346.7629 - mae: 32.6626 - val_loss: 1457.7543 - val_mse: 1457.7544 - val_mae: 25.0833\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3304.0874 - mse: 3304.0879 - mae: 32.8150 - val_loss: 1454.7967 - val_mse: 1454.7968 - val_mae: 25.2028\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3469.5862 - mse: 3469.5869 - mae: 33.8251 - val_loss: 1458.0657 - val_mse: 1458.0660 - val_mae: 25.0447\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3321.6488 - mse: 3321.6492 - mae: 31.8282 - val_loss: 1459.0132 - val_mse: 1459.0131 - val_mae: 25.0691\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 556us/step - loss: 3277.5200 - mse: 3277.5200 - mae: 31.8673 - val_loss: 1460.1191 - val_mse: 1460.1190 - val_mae: 25.1157\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 625us/step - loss: 3301.8724 - mse: 3301.8723 - mae: 32.2951 - val_loss: 1454.7042 - val_mse: 1454.7042 - val_mae: 25.5864\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3328.8714 - mse: 3328.8713 - mae: 32.4554 - val_loss: 1459.9859 - val_mse: 1459.9860 - val_mae: 25.2038\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3295.7996 - mse: 3295.7993 - mae: 32.4216 - val_loss: 1460.1369 - val_mse: 1460.1371 - val_mae: 25.3176\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 536us/step - loss: 3429.0571 - mse: 3429.0571 - mae: 33.1482 - val_loss: 1463.6151 - val_mse: 1463.6151 - val_mae: 25.0686\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3365.8783 - mse: 3365.8787 - mae: 33.0140 - val_loss: 1463.2999 - val_mse: 1463.2999 - val_mae: 25.0696\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3388.4765 - mse: 3388.4766 - mae: 33.0246 - val_loss: 1459.7975 - val_mse: 1459.7976 - val_mae: 25.3892\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3186.7712 - mse: 3186.7710 - mae: 31.6870 - val_loss: 1463.5271 - val_mse: 1463.5271 - val_mae: 25.1412\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 529us/step - loss: 3391.4205 - mse: 3391.4211 - mae: 32.9206 - val_loss: 1459.5715 - val_mse: 1459.5714 - val_mae: 25.3346\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 549us/step - loss: 3333.4752 - mse: 3333.4758 - mae: 32.2416 - val_loss: 1461.1285 - val_mse: 1461.1285 - val_mae: 25.1408\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3300.6475 - mse: 3300.6477 - mae: 32.1185 - val_loss: 1456.9908 - val_mse: 1456.9908 - val_mae: 25.5910\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3358.5905 - mse: 3358.5906 - mae: 31.8887 - val_loss: 1458.9530 - val_mse: 1458.9530 - val_mae: 25.3457\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3301.6748 - mse: 3301.6748 - mae: 32.7541 - val_loss: 1462.8887 - val_mse: 1462.8887 - val_mae: 25.2092\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3364.6085 - mse: 3364.6094 - mae: 32.6126 - val_loss: 1463.7639 - val_mse: 1463.7639 - val_mae: 25.1635\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 446us/step - loss: 3479.9385 - mse: 3479.9397 - mae: 32.7865 - val_loss: 1460.5558 - val_mse: 1460.5558 - val_mae: 25.4045\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3184.6325 - mse: 3184.6328 - mae: 32.0684 - val_loss: 1463.0870 - val_mse: 1463.0869 - val_mae: 25.1845\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 534us/step - loss: 3272.6336 - mse: 3272.6340 - mae: 31.4391 - val_loss: 1459.4138 - val_mse: 1459.4139 - val_mae: 25.7467\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 671us/step - loss: 3268.8288 - mse: 3268.8284 - mae: 31.6689 - val_loss: 1459.7083 - val_mse: 1459.7084 - val_mae: 25.8830\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3374.8022 - mse: 3374.8020 - mae: 32.4668 - val_loss: 1466.0798 - val_mse: 1466.0798 - val_mae: 25.0912\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 702us/step - loss: 3344.8263 - mse: 3344.8262 - mae: 32.9357 - val_loss: 1467.3347 - val_mse: 1467.3347 - val_mae: 25.0602\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3263.1605 - mse: 3263.1606 - mae: 32.0418 - val_loss: 1463.2593 - val_mse: 1463.2594 - val_mae: 25.4160\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3305.1251 - mse: 3305.1248 - mae: 32.1641 - val_loss: 1465.8890 - val_mse: 1465.8890 - val_mae: 25.2171\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 697us/step - loss: 3247.1959 - mse: 3247.1963 - mae: 32.4219 - val_loss: 1465.2764 - val_mse: 1465.2764 - val_mae: 25.2144\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3346.4522 - mse: 3346.4519 - mae: 32.4808 - val_loss: 1466.1660 - val_mse: 1466.1659 - val_mae: 25.1716\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3160.5979 - mse: 3160.5986 - mae: 31.5586 - val_loss: 1460.7391 - val_mse: 1460.7391 - val_mae: 25.5858\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3254.0496 - mse: 3254.0496 - mae: 31.9696 - val_loss: 1461.8746 - val_mse: 1461.8745 - val_mae: 25.4970\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3325.2843 - mse: 3325.2839 - mae: 31.7939 - val_loss: 1467.0087 - val_mse: 1467.0087 - val_mae: 25.1733\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3189.3402 - mse: 3189.3406 - mae: 31.5207 - val_loss: 1461.5600 - val_mse: 1461.5598 - val_mae: 25.6832\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3308.0933 - mse: 3308.0938 - mae: 31.9033 - val_loss: 1466.6842 - val_mse: 1466.6842 - val_mae: 25.3338\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3249.5886 - mse: 3249.5889 - mae: 31.7567 - val_loss: 1466.2822 - val_mse: 1466.2821 - val_mae: 25.4068\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3169.7910 - mse: 3169.7905 - mae: 31.0369 - val_loss: 1466.1645 - val_mse: 1466.1646 - val_mae: 25.5262\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3257.9382 - mse: 3257.9382 - mae: 32.3726 - val_loss: 1465.0309 - val_mse: 1465.0308 - val_mae: 25.3821\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3266.5418 - mse: 3266.5415 - mae: 31.8300 - val_loss: 1466.3499 - val_mse: 1466.3500 - val_mae: 25.2706\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 553us/step - loss: 3267.4551 - mse: 3267.4548 - mae: 31.4384 - val_loss: 1461.8544 - val_mse: 1461.8542 - val_mae: 25.5414\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3165.1856 - mse: 3165.1853 - mae: 31.8788 - val_loss: 1462.0262 - val_mse: 1462.0261 - val_mae: 25.5349\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 482us/step - loss: 3167.4718 - mse: 3167.4727 - mae: 31.4355 - val_loss: 1464.6826 - val_mse: 1464.6826 - val_mae: 25.3968\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 537us/step - loss: 3222.1829 - mse: 3222.1829 - mae: 31.7202 - val_loss: 1467.3324 - val_mse: 1467.3323 - val_mae: 25.1348\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 560us/step - loss: 3315.1555 - mse: 3315.1558 - mae: 32.4282 - val_loss: 1461.0549 - val_mse: 1461.0549 - val_mae: 25.4997\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3250.4623 - mse: 3250.4626 - mae: 31.8023 - val_loss: 1464.3208 - val_mse: 1464.3207 - val_mae: 25.1992\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 3173.5042 - mse: 3173.5044 - mae: 31.4727 - val_loss: 1463.5684 - val_mse: 1463.5685 - val_mae: 25.3315\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3306.5712 - mse: 3306.5713 - mae: 31.8616 - val_loss: 1460.7641 - val_mse: 1460.7640 - val_mae: 25.5222\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3289.0374 - mse: 3289.0378 - mae: 32.2441 - val_loss: 1461.1662 - val_mse: 1461.1665 - val_mae: 25.4627\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3229.1325 - mse: 3229.1318 - mae: 31.7354 - val_loss: 1462.4858 - val_mse: 1462.4858 - val_mae: 25.4134\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3127.7993 - mse: 3127.7993 - mae: 30.9608 - val_loss: 1460.8077 - val_mse: 1460.8076 - val_mae: 25.7131\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3276.5712 - mse: 3276.5706 - mae: 32.3026 - val_loss: 1462.8590 - val_mse: 1462.8590 - val_mae: 25.4949\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3231.0535 - mse: 3231.0535 - mae: 31.6156 - val_loss: 1463.5564 - val_mse: 1463.5562 - val_mae: 25.3709\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3272.3416 - mse: 3272.3416 - mae: 31.8931 - val_loss: 1463.8817 - val_mse: 1463.8818 - val_mae: 25.4866\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3261.1589 - mse: 3261.1594 - mae: 31.2770 - val_loss: 1466.2127 - val_mse: 1466.2125 - val_mae: 25.3411\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 597us/step - loss: 3176.8871 - mse: 3176.8872 - mae: 31.2489 - val_loss: 1462.4739 - val_mse: 1462.4738 - val_mae: 26.0112\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3240.9219 - mse: 3240.9216 - mae: 32.0222 - val_loss: 1473.5272 - val_mse: 1473.5275 - val_mae: 24.9718\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3258.4697 - mse: 3258.4705 - mae: 32.1650 - val_loss: 1466.3902 - val_mse: 1466.3904 - val_mae: 25.3121\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3194.9265 - mse: 3194.9260 - mae: 31.1764 - val_loss: 1464.3447 - val_mse: 1464.3446 - val_mae: 25.9296\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 618us/step - loss: 3352.4221 - mse: 3352.4233 - mae: 32.5010 - val_loss: 1465.8585 - val_mse: 1465.8586 - val_mae: 25.4986\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3305.3489 - mse: 3305.3496 - mae: 32.0654 - val_loss: 1467.7781 - val_mse: 1467.7781 - val_mae: 25.3499\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2866.9239 - mse: 2866.9231 - mae: 31.0441 - val_loss: 1061.9003 - val_mse: 1061.9003 - val_mae: 23.9251\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2909.5089 - mse: 2909.5083 - mae: 31.0184 - val_loss: 1058.0757 - val_mse: 1058.0757 - val_mae: 24.1461\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2958.1154 - mse: 2958.1150 - mae: 31.0378 - val_loss: 1058.3114 - val_mse: 1058.3113 - val_mae: 23.8656\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2986.6838 - mse: 2986.6838 - mae: 31.7375 - val_loss: 1056.5859 - val_mse: 1056.5861 - val_mae: 23.7870\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2965.3863 - mse: 2965.3860 - mae: 31.2295 - val_loss: 1055.6371 - val_mse: 1055.6371 - val_mae: 23.5886\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2910.9466 - mse: 2910.9463 - mae: 30.9198 - val_loss: 1050.5330 - val_mse: 1050.5331 - val_mae: 23.8867\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2922.3890 - mse: 2922.3879 - mae: 31.4475 - val_loss: 1052.5124 - val_mse: 1052.5126 - val_mae: 23.5493\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2928.0807 - mse: 2928.0806 - mae: 30.7518 - val_loss: 1047.3014 - val_mse: 1047.3014 - val_mae: 24.1575\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2936.9221 - mse: 2936.9229 - mae: 30.7707 - val_loss: 1050.0980 - val_mse: 1050.0980 - val_mae: 23.5460\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2879.1797 - mse: 2879.1787 - mae: 30.8861 - val_loss: 1046.8275 - val_mse: 1046.8275 - val_mae: 23.8926\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 640us/step - loss: 2871.1016 - mse: 2871.1018 - mae: 30.6569 - val_loss: 1046.6764 - val_mse: 1046.6765 - val_mae: 23.7306\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 547us/step - loss: 2917.4361 - mse: 2917.4365 - mae: 30.2954 - val_loss: 1044.5927 - val_mse: 1044.5927 - val_mae: 23.9654\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2863.3913 - mse: 2863.3916 - mae: 30.7785 - val_loss: 1044.4564 - val_mse: 1044.4563 - val_mae: 24.1035\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 650us/step - loss: 2862.0234 - mse: 2862.0237 - mae: 30.3697 - val_loss: 1045.2126 - val_mse: 1045.2125 - val_mae: 23.9060\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2803.4097 - mse: 2803.4099 - mae: 30.7687 - val_loss: 1044.5817 - val_mse: 1044.5818 - val_mae: 23.9135\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 656us/step - loss: 2865.5592 - mse: 2865.5598 - mae: 31.3389 - val_loss: 1044.5014 - val_mse: 1044.5015 - val_mae: 23.9551\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2876.7672 - mse: 2876.7671 - mae: 31.0552 - val_loss: 1045.1899 - val_mse: 1045.1898 - val_mae: 23.7471\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2955.0697 - mse: 2955.0698 - mae: 31.3005 - val_loss: 1044.9812 - val_mse: 1044.9813 - val_mae: 23.8396\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2959.1479 - mse: 2959.1479 - mae: 30.8352 - val_loss: 1044.2533 - val_mse: 1044.2532 - val_mae: 24.0398\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2914.6887 - mse: 2914.6887 - mae: 30.8917 - val_loss: 1043.9558 - val_mse: 1043.9557 - val_mae: 23.8206\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2903.3651 - mse: 2903.3655 - mae: 30.5726 - val_loss: 1043.0239 - val_mse: 1043.0239 - val_mae: 24.0748\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2912.1610 - mse: 2912.1606 - mae: 31.0752 - val_loss: 1043.1980 - val_mse: 1043.1980 - val_mae: 23.7146\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2839.5162 - mse: 2839.5164 - mae: 30.3331 - val_loss: 1040.5311 - val_mse: 1040.5310 - val_mae: 24.0759\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2891.4330 - mse: 2891.4326 - mae: 31.1090 - val_loss: 1041.1147 - val_mse: 1041.1146 - val_mae: 23.7409\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 531us/step - loss: 2829.6417 - mse: 2829.6411 - mae: 30.4173 - val_loss: 1041.2821 - val_mse: 1041.2821 - val_mae: 24.3130\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2917.8313 - mse: 2917.8325 - mae: 31.1914 - val_loss: 1040.9243 - val_mse: 1040.9242 - val_mae: 24.0149\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2930.5276 - mse: 2930.5271 - mae: 31.1154 - val_loss: 1044.0946 - val_mse: 1044.0946 - val_mae: 23.4376\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 627us/step - loss: 2868.6952 - mse: 2868.6946 - mae: 30.9856 - val_loss: 1040.3036 - val_mse: 1040.3035 - val_mae: 23.9446\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2946.1768 - mse: 2946.1760 - mae: 30.9549 - val_loss: 1040.0660 - val_mse: 1040.0660 - val_mae: 23.9125\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2852.9826 - mse: 2852.9834 - mae: 30.4151 - val_loss: 1039.3751 - val_mse: 1039.3751 - val_mae: 24.0011\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2801.4558 - mse: 2801.4556 - mae: 30.5156 - val_loss: 1039.5321 - val_mse: 1039.5321 - val_mae: 24.1178\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2903.5737 - mse: 2903.5742 - mae: 30.7517 - val_loss: 1039.1452 - val_mse: 1039.1451 - val_mae: 24.0632\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2892.5541 - mse: 2892.5542 - mae: 31.1213 - val_loss: 1039.4823 - val_mse: 1039.4823 - val_mae: 24.0114\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2793.1257 - mse: 2793.1260 - mae: 29.8611 - val_loss: 1038.9623 - val_mse: 1038.9624 - val_mae: 23.9999\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2845.5082 - mse: 2845.5083 - mae: 30.7414 - val_loss: 1038.8520 - val_mse: 1038.8519 - val_mae: 24.0398\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2918.0982 - mse: 2918.0979 - mae: 30.8690 - val_loss: 1039.2180 - val_mse: 1039.2181 - val_mae: 23.7678\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 661us/step - loss: 2824.1194 - mse: 2824.1201 - mae: 30.8515 - val_loss: 1038.5324 - val_mse: 1038.5325 - val_mae: 24.0744\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 645us/step - loss: 2776.8921 - mse: 2776.8918 - mae: 30.4250 - val_loss: 1038.6119 - val_mse: 1038.6119 - val_mae: 23.7536\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 660us/step - loss: 2874.5119 - mse: 2874.5117 - mae: 30.6899 - val_loss: 1039.2563 - val_mse: 1039.2563 - val_mae: 24.3801\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2812.4157 - mse: 2812.4163 - mae: 30.3547 - val_loss: 1038.0554 - val_mse: 1038.0554 - val_mae: 24.1152\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2915.8986 - mse: 2915.8987 - mae: 30.7690 - val_loss: 1038.6550 - val_mse: 1038.6550 - val_mae: 24.3875\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2869.0294 - mse: 2869.0298 - mae: 30.7318 - val_loss: 1038.0858 - val_mse: 1038.0858 - val_mae: 23.7518\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 662us/step - loss: 2860.1728 - mse: 2860.1731 - mae: 30.7806 - val_loss: 1039.3937 - val_mse: 1039.3937 - val_mae: 24.4959\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 701us/step - loss: 2909.0746 - mse: 2909.0742 - mae: 30.7977 - val_loss: 1037.7311 - val_mse: 1037.7311 - val_mae: 23.7213\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2809.2740 - mse: 2809.2744 - mae: 30.5016 - val_loss: 1036.6359 - val_mse: 1036.6357 - val_mae: 24.1785\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2905.8763 - mse: 2905.8755 - mae: 30.9698 - val_loss: 1035.9340 - val_mse: 1035.9340 - val_mae: 24.0257\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 539us/step - loss: 2798.2607 - mse: 2798.2595 - mae: 30.2941 - val_loss: 1035.2286 - val_mse: 1035.2288 - val_mae: 24.1415\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2852.4724 - mse: 2852.4729 - mae: 31.1441 - val_loss: 1036.4517 - val_mse: 1036.4517 - val_mae: 24.4134\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2848.9382 - mse: 2848.9380 - mae: 30.5119 - val_loss: 1035.6335 - val_mse: 1035.6335 - val_mae: 24.1372\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2830.0342 - mse: 2830.0342 - mae: 30.3667 - val_loss: 1035.3967 - val_mse: 1035.3967 - val_mae: 23.6034\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 554us/step - loss: 2848.5323 - mse: 2848.5325 - mae: 30.3826 - val_loss: 1035.2425 - val_mse: 1035.2426 - val_mae: 24.2530\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2875.5961 - mse: 2875.5967 - mae: 30.5857 - val_loss: 1034.3200 - val_mse: 1034.3201 - val_mae: 24.0640\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2755.1838 - mse: 2755.1836 - mae: 29.8803 - val_loss: 1035.4849 - val_mse: 1035.4850 - val_mae: 23.5751\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2873.6624 - mse: 2873.6628 - mae: 30.4799 - val_loss: 1035.1257 - val_mse: 1035.1257 - val_mae: 24.1363\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 2764.3183 - mse: 2764.3176 - mae: 29.8691 - val_loss: 1035.8025 - val_mse: 1035.8024 - val_mae: 24.2228\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 579us/step - loss: 2876.9016 - mse: 2876.9026 - mae: 30.2194 - val_loss: 1037.1634 - val_mse: 1037.1632 - val_mae: 24.4380\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2637.4722 - mse: 2637.4724 - mae: 29.8495 - val_loss: 1035.1845 - val_mse: 1035.1846 - val_mae: 24.1806\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2790.3568 - mse: 2790.3572 - mae: 29.8701 - val_loss: 1037.1064 - val_mse: 1037.1063 - val_mae: 24.4246\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2837.1960 - mse: 2837.1958 - mae: 30.6339 - val_loss: 1034.4610 - val_mse: 1034.4612 - val_mae: 24.0054\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2838.8258 - mse: 2838.8262 - mae: 30.2161 - val_loss: 1033.7273 - val_mse: 1033.7274 - val_mae: 24.0625\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2855.8194 - mse: 2855.8193 - mae: 30.1976 - val_loss: 1033.4394 - val_mse: 1033.4393 - val_mae: 24.1501\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2808.9611 - mse: 2808.9602 - mae: 30.4527 - val_loss: 1032.1305 - val_mse: 1032.1305 - val_mae: 23.8956\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 546us/step - loss: 2804.2112 - mse: 2804.2112 - mae: 30.0629 - val_loss: 1031.5923 - val_mse: 1031.5924 - val_mae: 23.9496\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2841.8353 - mse: 2841.8350 - mae: 30.4757 - val_loss: 1032.1132 - val_mse: 1032.1132 - val_mae: 23.7350\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 535us/step - loss: 2848.0245 - mse: 2848.0244 - mae: 30.1969 - val_loss: 1033.9119 - val_mse: 1033.9119 - val_mae: 24.3010\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2832.8551 - mse: 2832.8552 - mae: 30.4968 - val_loss: 1032.0118 - val_mse: 1032.0120 - val_mae: 23.6013\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2761.8981 - mse: 2761.8984 - mae: 30.1576 - val_loss: 1032.2178 - val_mse: 1032.2178 - val_mae: 23.8171\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2806.2825 - mse: 2806.2827 - mae: 30.5818 - val_loss: 1032.1306 - val_mse: 1032.1305 - val_mae: 24.0092\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2747.1968 - mse: 2747.1975 - mae: 29.8698 - val_loss: 1033.9899 - val_mse: 1033.9901 - val_mae: 24.3458\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2803.5144 - mse: 2803.5146 - mae: 30.3039 - val_loss: 1032.9011 - val_mse: 1032.9011 - val_mae: 24.2361\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2776.4648 - mse: 2776.4644 - mae: 30.3642 - val_loss: 1031.0157 - val_mse: 1031.0157 - val_mae: 23.9966\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2822.8853 - mse: 2822.8855 - mae: 30.4473 - val_loss: 1030.2500 - val_mse: 1030.2499 - val_mae: 23.9529\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 679us/step - loss: 2844.4702 - mse: 2844.4707 - mae: 30.5225 - val_loss: 1029.7423 - val_mse: 1029.7422 - val_mae: 23.9824\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 624us/step - loss: 2806.4850 - mse: 2806.4858 - mae: 30.5148 - val_loss: 1029.1956 - val_mse: 1029.1956 - val_mae: 23.9209\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 630us/step - loss: 2744.1312 - mse: 2744.1311 - mae: 29.8645 - val_loss: 1029.2734 - val_mse: 1029.2733 - val_mae: 24.0375\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 654us/step - loss: 2840.3629 - mse: 2840.3628 - mae: 30.3410 - val_loss: 1028.3779 - val_mse: 1028.3778 - val_mae: 23.5645\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 622us/step - loss: 2855.6398 - mse: 2855.6399 - mae: 30.5025 - val_loss: 1029.6928 - val_mse: 1029.6929 - val_mae: 23.2044\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2832.1959 - mse: 2832.1956 - mae: 30.5671 - val_loss: 1027.4257 - val_mse: 1027.4259 - val_mae: 23.9471\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2733.6392 - mse: 2733.6394 - mae: 30.2341 - val_loss: 1028.2633 - val_mse: 1028.2633 - val_mae: 24.0387\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2856.5916 - mse: 2856.5908 - mae: 30.4931 - val_loss: 1026.8374 - val_mse: 1026.8374 - val_mae: 23.6978\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2628.5676 - mse: 2628.5674 - mae: 29.9860 - val_loss: 1442.4316 - val_mse: 1442.4316 - val_mae: 26.4484\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2458.3357 - mse: 2458.3357 - mae: 29.5154 - val_loss: 1419.9803 - val_mse: 1419.9805 - val_mae: 26.9404\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2462.6160 - mse: 2462.6160 - mae: 29.3897 - val_loss: 1440.8929 - val_mse: 1440.8929 - val_mae: 26.3407\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2541.0952 - mse: 2541.0955 - mae: 29.8090 - val_loss: 1433.1102 - val_mse: 1433.1102 - val_mae: 26.4407\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2453.3578 - mse: 2453.3579 - mae: 29.3538 - val_loss: 1437.2092 - val_mse: 1437.2092 - val_mae: 26.3223\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2470.9848 - mse: 2470.9846 - mae: 29.6311 - val_loss: 1430.4112 - val_mse: 1430.4113 - val_mae: 26.4201\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2521.2938 - mse: 2521.2935 - mae: 29.8180 - val_loss: 1437.1487 - val_mse: 1437.1488 - val_mae: 26.2771\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2512.4844 - mse: 2512.4849 - mae: 29.6082 - val_loss: 1435.8358 - val_mse: 1435.8358 - val_mae: 26.2812\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2463.8733 - mse: 2463.8738 - mae: 29.3208 - val_loss: 1421.6444 - val_mse: 1421.6444 - val_mae: 26.5689\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2524.2205 - mse: 2524.2205 - mae: 29.2286 - val_loss: 1431.7042 - val_mse: 1431.7042 - val_mae: 26.3562\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2512.6350 - mse: 2512.6357 - mae: 29.6448 - val_loss: 1440.8580 - val_mse: 1440.8582 - val_mae: 26.1337\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2524.4780 - mse: 2524.4778 - mae: 29.7918 - val_loss: 1415.6857 - val_mse: 1415.6859 - val_mae: 26.5869\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2525.6618 - mse: 2525.6619 - mae: 29.4141 - val_loss: 1429.3097 - val_mse: 1429.3094 - val_mae: 26.2441\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2529.6652 - mse: 2529.6655 - mae: 29.5069 - val_loss: 1427.0091 - val_mse: 1427.0093 - val_mae: 26.2723\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2433.3455 - mse: 2433.3457 - mae: 29.3261 - val_loss: 1406.4951 - val_mse: 1406.4952 - val_mae: 26.7358\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2531.6267 - mse: 2531.6265 - mae: 29.2262 - val_loss: 1425.0806 - val_mse: 1425.0806 - val_mae: 26.2916\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 613us/step - loss: 2475.2199 - mse: 2475.2197 - mae: 29.3422 - val_loss: 1419.9034 - val_mse: 1419.9033 - val_mae: 26.3734\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2532.5344 - mse: 2532.5352 - mae: 29.4479 - val_loss: 1419.8743 - val_mse: 1419.8743 - val_mae: 26.3119\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2448.0095 - mse: 2448.0090 - mae: 28.8733 - val_loss: 1416.1522 - val_mse: 1416.1522 - val_mae: 26.3934\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2516.2134 - mse: 2516.2134 - mae: 29.5278 - val_loss: 1442.9024 - val_mse: 1442.9026 - val_mae: 25.9074\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 634us/step - loss: 2406.9317 - mse: 2406.9314 - mae: 28.8421 - val_loss: 1420.6194 - val_mse: 1420.6195 - val_mae: 26.2582\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 2s 698us/step - loss: 2475.6759 - mse: 2475.6760 - mae: 29.5493 - val_loss: 1416.9357 - val_mse: 1416.9355 - val_mae: 26.3270\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2445.4745 - mse: 2445.4746 - mae: 29.1808 - val_loss: 1417.0958 - val_mse: 1417.0958 - val_mae: 26.3326\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2520.7034 - mse: 2520.7039 - mae: 29.9335 - val_loss: 1434.6443 - val_mse: 1434.6442 - val_mae: 25.9481\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2480.0561 - mse: 2480.0557 - mae: 29.1069 - val_loss: 1423.2364 - val_mse: 1423.2365 - val_mae: 26.1081\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2536.3464 - mse: 2536.3467 - mae: 29.2147 - val_loss: 1413.2411 - val_mse: 1413.2412 - val_mae: 26.2724\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2434.6096 - mse: 2434.6091 - mae: 28.9197 - val_loss: 1398.6668 - val_mse: 1398.6665 - val_mae: 26.6059\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 523us/step - loss: 2510.9253 - mse: 2510.9248 - mae: 29.7372 - val_loss: 1411.6873 - val_mse: 1411.6874 - val_mae: 26.3180\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2498.1754 - mse: 2498.1748 - mae: 29.3617 - val_loss: 1415.7966 - val_mse: 1415.7965 - val_mae: 26.2353\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2433.9963 - mse: 2433.9963 - mae: 29.5324 - val_loss: 1409.8981 - val_mse: 1409.8981 - val_mae: 26.3257\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2404.5816 - mse: 2404.5813 - mae: 28.7423 - val_loss: 1403.4062 - val_mse: 1403.4062 - val_mae: 26.4381\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2496.5945 - mse: 2496.5945 - mae: 29.2888 - val_loss: 1399.9616 - val_mse: 1399.9614 - val_mae: 26.5007\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 666us/step - loss: 2531.2655 - mse: 2531.2664 - mae: 29.8731 - val_loss: 1420.8975 - val_mse: 1420.8975 - val_mae: 25.9817\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2478.4231 - mse: 2478.4229 - mae: 28.7404 - val_loss: 1393.9897 - val_mse: 1393.9899 - val_mae: 26.5684\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2470.2085 - mse: 2470.2092 - mae: 29.0715 - val_loss: 1411.7592 - val_mse: 1411.7593 - val_mae: 26.0975\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2563.4889 - mse: 2563.4893 - mae: 29.5550 - val_loss: 1401.3562 - val_mse: 1401.3561 - val_mae: 26.2838\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2466.4751 - mse: 2466.4746 - mae: 29.0407 - val_loss: 1401.4390 - val_mse: 1401.4390 - val_mae: 26.2007\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 630us/step - loss: 2439.5694 - mse: 2439.5696 - mae: 29.2713 - val_loss: 1401.5882 - val_mse: 1401.5881 - val_mae: 26.1636\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 602us/step - loss: 2529.3977 - mse: 2529.3984 - mae: 29.5001 - val_loss: 1410.7342 - val_mse: 1410.7343 - val_mae: 25.8955\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 663us/step - loss: 2413.1486 - mse: 2413.1487 - mae: 29.0765 - val_loss: 1405.3915 - val_mse: 1405.3916 - val_mae: 26.0194\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2487.1019 - mse: 2487.1028 - mae: 29.4953 - val_loss: 1407.1515 - val_mse: 1407.1517 - val_mae: 26.0328\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 2s 663us/step - loss: 2449.5138 - mse: 2449.5142 - mae: 28.6197 - val_loss: 1394.4094 - val_mse: 1394.4093 - val_mae: 26.3034\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2472.4604 - mse: 2472.4604 - mae: 29.5451 - val_loss: 1407.4432 - val_mse: 1407.4431 - val_mae: 26.0073\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2448.6499 - mse: 2448.6494 - mae: 28.9572 - val_loss: 1395.5507 - val_mse: 1395.5504 - val_mae: 26.1863\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 674us/step - loss: 2467.7294 - mse: 2467.7295 - mae: 29.0824 - val_loss: 1398.6280 - val_mse: 1398.6281 - val_mae: 26.1437\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2440.7259 - mse: 2440.7258 - mae: 29.0698 - val_loss: 1407.1546 - val_mse: 1407.1545 - val_mae: 25.9611\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2531.3990 - mse: 2531.3999 - mae: 29.4679 - val_loss: 1395.9908 - val_mse: 1395.9908 - val_mae: 26.1869\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2515.4010 - mse: 2515.4014 - mae: 29.0815 - val_loss: 1406.4853 - val_mse: 1406.4855 - val_mae: 25.9748\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2532.9771 - mse: 2532.9768 - mae: 29.2383 - val_loss: 1405.1220 - val_mse: 1405.1218 - val_mae: 26.0365\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 1s 585us/step - loss: 2425.8819 - mse: 2425.8823 - mae: 28.5996 - val_loss: 1399.0775 - val_mse: 1399.0776 - val_mae: 26.1192\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 640us/step - loss: 2469.1274 - mse: 2469.1277 - mae: 28.8428 - val_loss: 1397.0016 - val_mse: 1397.0018 - val_mae: 26.1437\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2528.5822 - mse: 2528.5825 - mae: 28.9727 - val_loss: 1383.8299 - val_mse: 1383.8298 - val_mae: 26.5065\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 1s 536us/step - loss: 2474.5293 - mse: 2474.5291 - mae: 29.4360 - val_loss: 1404.5520 - val_mse: 1404.5521 - val_mae: 25.8643\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 554us/step - loss: 2435.3284 - mse: 2435.3286 - mae: 29.2805 - val_loss: 1397.6855 - val_mse: 1397.6854 - val_mae: 25.9392\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 678us/step - loss: 2451.2172 - mse: 2451.2173 - mae: 28.9361 - val_loss: 1388.5051 - val_mse: 1388.5050 - val_mae: 26.1282\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 658us/step - loss: 2477.5769 - mse: 2477.5767 - mae: 28.9771 - val_loss: 1391.7861 - val_mse: 1391.7861 - val_mae: 25.9882\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2448.0626 - mse: 2448.0625 - mae: 29.1287 - val_loss: 1395.6493 - val_mse: 1395.6492 - val_mae: 25.8875\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2465.0630 - mse: 2465.0632 - mae: 29.2411 - val_loss: 1388.0091 - val_mse: 1388.0093 - val_mae: 26.0690\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2436.5226 - mse: 2436.5222 - mae: 29.3419 - val_loss: 1395.3532 - val_mse: 1395.3531 - val_mae: 25.8673\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 723us/step - loss: 2457.9791 - mse: 2457.9790 - mae: 28.9533 - val_loss: 1391.2693 - val_mse: 1391.2693 - val_mae: 25.8999\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2478.5281 - mse: 2478.5288 - mae: 28.9895 - val_loss: 1392.0584 - val_mse: 1392.0587 - val_mae: 25.8593\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2424.9418 - mse: 2424.9417 - mae: 28.9318 - val_loss: 1390.4601 - val_mse: 1390.4602 - val_mae: 25.8272\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2441.7906 - mse: 2441.7900 - mae: 29.2447 - val_loss: 1388.3748 - val_mse: 1388.3748 - val_mae: 25.8479\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2447.4098 - mse: 2447.4089 - mae: 28.9131 - val_loss: 1376.9006 - val_mse: 1376.9006 - val_mae: 26.0774\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2444.7296 - mse: 2444.7302 - mae: 28.8958 - val_loss: 1388.1037 - val_mse: 1388.1039 - val_mae: 25.7819\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2398.8865 - mse: 2398.8865 - mae: 28.6565 - val_loss: 1395.7228 - val_mse: 1395.7227 - val_mae: 25.5621\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2453.5472 - mse: 2453.5471 - mae: 28.6208 - val_loss: 1378.6102 - val_mse: 1378.6101 - val_mae: 25.9144\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2427.1450 - mse: 2427.1438 - mae: 29.1478 - val_loss: 1378.3927 - val_mse: 1378.3927 - val_mae: 25.9617\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2407.1776 - mse: 2407.1775 - mae: 28.8222 - val_loss: 1380.3762 - val_mse: 1380.3761 - val_mae: 25.9473\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2420.8616 - mse: 2420.8625 - mae: 28.7570 - val_loss: 1383.7769 - val_mse: 1383.7767 - val_mae: 25.8195\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 657us/step - loss: 2426.8263 - mse: 2426.8264 - mae: 29.0014 - val_loss: 1377.8034 - val_mse: 1377.8032 - val_mae: 25.9812\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2387.1558 - mse: 2387.1562 - mae: 28.7477 - val_loss: 1375.1766 - val_mse: 1375.1766 - val_mae: 26.0190\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 543us/step - loss: 2404.8623 - mse: 2404.8633 - mae: 29.0025 - val_loss: 1374.5972 - val_mse: 1374.5972 - val_mae: 26.0286\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 562us/step - loss: 2362.7183 - mse: 2362.7180 - mae: 28.2109 - val_loss: 1382.3280 - val_mse: 1382.3280 - val_mae: 25.7150\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2445.8485 - mse: 2445.8489 - mae: 29.1402 - val_loss: 1388.4591 - val_mse: 1388.4591 - val_mae: 25.5916\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2485.7437 - mse: 2485.7427 - mae: 28.9734 - val_loss: 1369.6391 - val_mse: 1369.6392 - val_mae: 25.9703\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 642us/step - loss: 2426.8873 - mse: 2426.8879 - mae: 29.0708 - val_loss: 1381.3201 - val_mse: 1381.3201 - val_mae: 25.7057\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2472.3222 - mse: 2472.3230 - mae: 28.9985 - val_loss: 1383.3984 - val_mse: 1383.3983 - val_mae: 25.6530\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2438.7547 - mse: 2438.7544 - mae: 29.1524 - val_loss: 1378.6695 - val_mse: 1378.6693 - val_mae: 25.7099\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 552us/step - loss: 2479.9890 - mse: 2479.9890 - mae: 29.0243 - val_loss: 1384.9374 - val_mse: 1384.9374 - val_mae: 25.5233\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 567us/step - loss: 2332.9400 - mse: 2332.9402 - mae: 29.2676 - val_loss: 3664.9618 - val_mse: 3664.9619 - val_mae: 23.2980\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2320.2384 - mse: 2320.2385 - mae: 29.0712 - val_loss: 3670.1540 - val_mse: 3670.1541 - val_mae: 24.2066\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2311.0688 - mse: 2311.0693 - mae: 28.5089 - val_loss: 3670.1346 - val_mse: 3670.1355 - val_mae: 23.9616\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2341.3566 - mse: 2341.3562 - mae: 29.2626 - val_loss: 3668.0436 - val_mse: 3668.0435 - val_mae: 23.7011\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2425.1793 - mse: 2425.1799 - mae: 29.4913 - val_loss: 3666.0431 - val_mse: 3666.0425 - val_mae: 23.5240\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2284.6819 - mse: 2284.6821 - mae: 28.6578 - val_loss: 3666.1037 - val_mse: 3666.1045 - val_mae: 23.6384\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2307.6553 - mse: 2307.6550 - mae: 29.1236 - val_loss: 3668.9279 - val_mse: 3668.9277 - val_mae: 23.7185\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2313.4949 - mse: 2313.4944 - mae: 29.2635 - val_loss: 3669.1262 - val_mse: 3669.1265 - val_mae: 24.0004\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2312.2737 - mse: 2312.2734 - mae: 29.0740 - val_loss: 3667.3558 - val_mse: 3667.3557 - val_mae: 23.6208\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2313.9439 - mse: 2313.9443 - mae: 29.1290 - val_loss: 3673.1383 - val_mse: 3673.1389 - val_mae: 24.1332\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2378.5270 - mse: 2378.5269 - mae: 29.3237 - val_loss: 3670.0220 - val_mse: 3670.0220 - val_mae: 23.4566\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2357.3589 - mse: 2357.3584 - mae: 28.9264 - val_loss: 3670.1517 - val_mse: 3670.1523 - val_mae: 24.0467\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 643us/step - loss: 2258.8340 - mse: 2258.8333 - mae: 28.4942 - val_loss: 3669.7173 - val_mse: 3669.7173 - val_mae: 23.8519\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2342.5300 - mse: 2342.5300 - mae: 29.3448 - val_loss: 3671.3585 - val_mse: 3671.3582 - val_mae: 24.0894\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2341.2912 - mse: 2341.2913 - mae: 29.1788 - val_loss: 3666.7551 - val_mse: 3666.7544 - val_mae: 23.6992\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2411.8104 - mse: 2411.8101 - mae: 29.7880 - val_loss: 3669.3470 - val_mse: 3669.3464 - val_mae: 23.6390\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2259.2811 - mse: 2259.2808 - mae: 28.6159 - val_loss: 3674.0244 - val_mse: 3674.0244 - val_mae: 24.1580\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2332.3498 - mse: 2332.3494 - mae: 29.1760 - val_loss: 3675.0865 - val_mse: 3675.0869 - val_mae: 24.1554\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2319.3237 - mse: 2319.3232 - mae: 28.7929 - val_loss: 3672.4632 - val_mse: 3672.4636 - val_mae: 23.9482\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2267.6315 - mse: 2267.6326 - mae: 28.8629 - val_loss: 3674.1968 - val_mse: 3674.1970 - val_mae: 23.4936\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2266.4622 - mse: 2266.4629 - mae: 28.4455 - val_loss: 3671.9964 - val_mse: 3671.9966 - val_mae: 23.4267\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2267.6218 - mse: 2267.6230 - mae: 28.6521 - val_loss: 3675.8579 - val_mse: 3675.8574 - val_mae: 23.8548\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2300.6614 - mse: 2300.6614 - mae: 28.8135 - val_loss: 3673.8037 - val_mse: 3673.8044 - val_mae: 24.1105\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2301.7306 - mse: 2301.7300 - mae: 28.9643 - val_loss: 3671.4194 - val_mse: 3671.4185 - val_mae: 23.8525\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2252.1023 - mse: 2252.1033 - mae: 28.4108 - val_loss: 3671.8730 - val_mse: 3671.8728 - val_mae: 23.6540\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 635us/step - loss: 2293.9729 - mse: 2293.9736 - mae: 28.4114 - val_loss: 3670.0931 - val_mse: 3670.0938 - val_mae: 23.3546\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2320.6596 - mse: 2320.6599 - mae: 28.8783 - val_loss: 3678.6080 - val_mse: 3678.6072 - val_mae: 24.5874\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2240.0399 - mse: 2240.0396 - mae: 28.4675 - val_loss: 3671.2984 - val_mse: 3671.2979 - val_mae: 23.3106\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2331.4257 - mse: 2331.4253 - mae: 29.1339 - val_loss: 3671.2540 - val_mse: 3671.2537 - val_mae: 23.8525\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2282.5753 - mse: 2282.5752 - mae: 28.4617 - val_loss: 3677.1264 - val_mse: 3677.1255 - val_mae: 23.9317\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2358.9355 - mse: 2358.9355 - mae: 29.1747 - val_loss: 3675.0833 - val_mse: 3675.0833 - val_mae: 23.4196\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2308.9878 - mse: 2308.9875 - mae: 28.9272 - val_loss: 3673.7022 - val_mse: 3673.7029 - val_mae: 23.6512\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2293.3456 - mse: 2293.3455 - mae: 29.1963 - val_loss: 3671.8541 - val_mse: 3671.8538 - val_mae: 23.7863\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 687us/step - loss: 2326.1044 - mse: 2326.1040 - mae: 28.9949 - val_loss: 3671.8475 - val_mse: 3671.8477 - val_mae: 23.4947\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 682us/step - loss: 2204.6377 - mse: 2204.6375 - mae: 28.4171 - val_loss: 3675.8246 - val_mse: 3675.8247 - val_mae: 24.1803\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2311.4777 - mse: 2311.4783 - mae: 29.0806 - val_loss: 3672.9306 - val_mse: 3672.9299 - val_mae: 23.9364\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 648us/step - loss: 2277.3482 - mse: 2277.3477 - mae: 28.4150 - val_loss: 3671.9342 - val_mse: 3671.9341 - val_mae: 23.7424\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2311.5899 - mse: 2311.5901 - mae: 28.9142 - val_loss: 3671.6336 - val_mse: 3671.6343 - val_mae: 23.6429\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2291.1431 - mse: 2291.1428 - mae: 28.9432 - val_loss: 3672.7483 - val_mse: 3672.7483 - val_mae: 24.0397\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2319.3545 - mse: 2319.3545 - mae: 29.2428 - val_loss: 3671.6520 - val_mse: 3671.6514 - val_mae: 23.8475\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2301.5410 - mse: 2301.5408 - mae: 28.6273 - val_loss: 3671.5107 - val_mse: 3671.5103 - val_mae: 23.7994\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2310.0937 - mse: 2310.0942 - mae: 28.7080 - val_loss: 3670.8841 - val_mse: 3670.8843 - val_mae: 23.8443\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 675us/step - loss: 2346.5015 - mse: 2346.5015 - mae: 29.1012 - val_loss: 3672.5944 - val_mse: 3672.5947 - val_mae: 23.4197\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2241.2236 - mse: 2241.2231 - mae: 28.1830 - val_loss: 3675.5838 - val_mse: 3675.5840 - val_mae: 24.0742\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2358.4900 - mse: 2358.4897 - mae: 29.3666 - val_loss: 3674.4222 - val_mse: 3674.4221 - val_mae: 23.8487\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 553us/step - loss: 2228.5948 - mse: 2228.5938 - mae: 28.0800 - val_loss: 3680.6644 - val_mse: 3680.6650 - val_mae: 24.1665\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 532us/step - loss: 2283.1870 - mse: 2283.1873 - mae: 28.7559 - val_loss: 3681.2073 - val_mse: 3681.2073 - val_mae: 24.2682\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 682us/step - loss: 2334.6997 - mse: 2334.6992 - mae: 29.3758 - val_loss: 3678.3610 - val_mse: 3678.3613 - val_mae: 23.8002\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 686us/step - loss: 2191.7883 - mse: 2191.7883 - mae: 28.0293 - val_loss: 3684.4042 - val_mse: 3684.4033 - val_mae: 24.6237\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2278.5090 - mse: 2278.5088 - mae: 28.7863 - val_loss: 3683.4212 - val_mse: 3683.4214 - val_mae: 24.1862\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 540us/step - loss: 2296.4248 - mse: 2296.4241 - mae: 28.7536 - val_loss: 3682.3426 - val_mse: 3682.3428 - val_mae: 23.9056\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 622us/step - loss: 2297.1603 - mse: 2297.1604 - mae: 28.7255 - val_loss: 3683.1117 - val_mse: 3683.1113 - val_mae: 23.9056\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 678us/step - loss: 2248.9688 - mse: 2248.9690 - mae: 28.4507 - val_loss: 3681.7476 - val_mse: 3681.7480 - val_mae: 23.7308\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2301.0064 - mse: 2301.0071 - mae: 29.0265 - val_loss: 3683.2726 - val_mse: 3683.2725 - val_mae: 23.8836\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2316.8058 - mse: 2316.8062 - mae: 28.8796 - val_loss: 3680.3766 - val_mse: 3680.3774 - val_mae: 23.6557\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2261.5124 - mse: 2261.5117 - mae: 28.5423 - val_loss: 3681.6431 - val_mse: 3681.6426 - val_mae: 23.9171\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2295.3560 - mse: 2295.3557 - mae: 28.5415 - val_loss: 3679.6684 - val_mse: 3679.6685 - val_mae: 23.5805\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2339.0965 - mse: 2339.0964 - mae: 28.8776 - val_loss: 3678.7853 - val_mse: 3678.7852 - val_mae: 23.8704\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2323.7095 - mse: 2323.7102 - mae: 28.8207 - val_loss: 3683.2550 - val_mse: 3683.2556 - val_mae: 24.2907\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2321.5265 - mse: 2321.5264 - mae: 28.8620 - val_loss: 3680.0956 - val_mse: 3680.0959 - val_mae: 23.3048\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 657us/step - loss: 2263.0685 - mse: 2263.0688 - mae: 28.4776 - val_loss: 3684.3358 - val_mse: 3684.3359 - val_mae: 23.8773\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2292.4756 - mse: 2292.4753 - mae: 28.7131 - val_loss: 3685.6308 - val_mse: 3685.6306 - val_mae: 24.0382\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2318.2187 - mse: 2318.2175 - mae: 28.6446 - val_loss: 3680.7447 - val_mse: 3680.7444 - val_mae: 23.5990\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2249.2041 - mse: 2249.2039 - mae: 28.7711 - val_loss: 3684.9542 - val_mse: 3684.9551 - val_mae: 23.7006\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 684us/step - loss: 2290.4658 - mse: 2290.4661 - mae: 28.5501 - val_loss: 3681.9989 - val_mse: 3681.9988 - val_mae: 23.6831\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2303.2657 - mse: 2303.2651 - mae: 28.8713 - val_loss: 3686.5441 - val_mse: 3686.5427 - val_mae: 24.2313\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 539us/step - loss: 2238.0604 - mse: 2238.0598 - mae: 28.5170 - val_loss: 3686.1434 - val_mse: 3686.1436 - val_mae: 24.0229\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2226.0500 - mse: 2226.0500 - mae: 28.2451 - val_loss: 3679.9292 - val_mse: 3679.9287 - val_mae: 23.3930\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2262.8976 - mse: 2262.8975 - mae: 28.5432 - val_loss: 3680.9317 - val_mse: 3680.9314 - val_mae: 23.6061\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2283.0579 - mse: 2283.0576 - mae: 28.5814 - val_loss: 3677.7255 - val_mse: 3677.7258 - val_mae: 23.6813\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2307.6644 - mse: 2307.6636 - mae: 28.7912 - val_loss: 3679.9302 - val_mse: 3679.9294 - val_mae: 23.7324\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 654us/step - loss: 2277.0038 - mse: 2277.0029 - mae: 28.4745 - val_loss: 3683.5575 - val_mse: 3683.5576 - val_mae: 24.1327\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2329.9162 - mse: 2329.9148 - mae: 28.9368 - val_loss: 3681.2473 - val_mse: 3681.2476 - val_mae: 23.8084\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2220.5939 - mse: 2220.5940 - mae: 28.2298 - val_loss: 3681.4003 - val_mse: 3681.4001 - val_mae: 23.9694\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2288.7347 - mse: 2288.7344 - mae: 28.7278 - val_loss: 3680.5098 - val_mse: 3680.5100 - val_mae: 23.9172\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 524us/step - loss: 2253.3141 - mse: 2253.3135 - mae: 28.3042 - val_loss: 3682.1355 - val_mse: 3682.1365 - val_mae: 23.8993\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2257.9033 - mse: 2257.9033 - mae: 28.3435 - val_loss: 3682.6105 - val_mse: 3682.6099 - val_mae: 24.1906\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2212.2942 - mse: 2212.2942 - mae: 28.1942 - val_loss: 3683.5568 - val_mse: 3683.5566 - val_mae: 24.0621\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2242.3295 - mse: 2242.3301 - mae: 28.3634 - val_loss: 3687.2674 - val_mse: 3687.2681 - val_mae: 24.0573\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 581us/step - loss: 2306.4657 - mse: 2306.4661 - mae: 28.7561 - val_loss: 3684.3149 - val_mse: 3684.3152 - val_mae: 23.9999\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2684.2839 - mse: 2684.2837 - mae: 28.6515 - val_loss: 2110.8481 - val_mse: 2110.8481 - val_mae: 26.6880\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2654.6044 - mse: 2654.6040 - mae: 28.1248 - val_loss: 2104.2725 - val_mse: 2104.2725 - val_mae: 26.7812\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2645.8772 - mse: 2645.8777 - mae: 27.6940 - val_loss: 2114.5468 - val_mse: 2114.5466 - val_mae: 26.4698\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2709.1073 - mse: 2709.1069 - mae: 28.4392 - val_loss: 2117.8160 - val_mse: 2117.8162 - val_mae: 26.5624\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2678.1777 - mse: 2678.1777 - mae: 28.3657 - val_loss: 2114.2471 - val_mse: 2114.2468 - val_mae: 26.5841\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2677.4507 - mse: 2677.4514 - mae: 28.2242 - val_loss: 2122.0613 - val_mse: 2122.0613 - val_mae: 26.4126\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2662.3821 - mse: 2662.3823 - mae: 27.8639 - val_loss: 2107.5853 - val_mse: 2107.5854 - val_mae: 26.9829\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 651us/step - loss: 2680.2138 - mse: 2680.2131 - mae: 28.3601 - val_loss: 2126.1692 - val_mse: 2126.1692 - val_mae: 26.5032\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2645.6367 - mse: 2645.6367 - mae: 28.3918 - val_loss: 2125.8355 - val_mse: 2125.8357 - val_mae: 26.5431\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2638.5267 - mse: 2638.5271 - mae: 28.1139 - val_loss: 2116.1579 - val_mse: 2116.1575 - val_mae: 26.9833\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2656.6346 - mse: 2656.6345 - mae: 28.2967 - val_loss: 2121.6673 - val_mse: 2121.6672 - val_mae: 26.8225\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 550us/step - loss: 2616.0512 - mse: 2616.0520 - mae: 28.1009 - val_loss: 2108.8447 - val_mse: 2108.8450 - val_mae: 27.2340\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2660.4495 - mse: 2660.4490 - mae: 28.1011 - val_loss: 2106.6152 - val_mse: 2106.6152 - val_mae: 26.7336\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2686.6887 - mse: 2686.6890 - mae: 27.9867 - val_loss: 2117.4097 - val_mse: 2117.4099 - val_mae: 26.6174\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2654.9270 - mse: 2654.9277 - mae: 28.1872 - val_loss: 2119.9492 - val_mse: 2119.9492 - val_mae: 26.8987\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2729.0844 - mse: 2729.0837 - mae: 28.5482 - val_loss: 2130.5261 - val_mse: 2130.5261 - val_mae: 26.4231\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2663.0777 - mse: 2663.0771 - mae: 27.8202 - val_loss: 2125.4465 - val_mse: 2125.4463 - val_mae: 26.5920\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2609.6661 - mse: 2609.6665 - mae: 27.8777 - val_loss: 2113.9284 - val_mse: 2113.9282 - val_mae: 26.7295\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 513us/step - loss: 2690.6045 - mse: 2690.6060 - mae: 28.1442 - val_loss: 2119.7289 - val_mse: 2119.7290 - val_mae: 26.5485\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 576us/step - loss: 2693.4958 - mse: 2693.4954 - mae: 28.3535 - val_loss: 2121.4913 - val_mse: 2121.4910 - val_mae: 26.4109\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2613.7478 - mse: 2613.7490 - mae: 28.1129 - val_loss: 2120.1693 - val_mse: 2120.1692 - val_mae: 26.4199\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2629.6766 - mse: 2629.6760 - mae: 28.2093 - val_loss: 2112.8784 - val_mse: 2112.8787 - val_mae: 26.7119\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 557us/step - loss: 2628.3038 - mse: 2628.3047 - mae: 27.8430 - val_loss: 2106.0802 - val_mse: 2106.0801 - val_mae: 26.9858\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2680.9635 - mse: 2680.9636 - mae: 28.2594 - val_loss: 2123.0397 - val_mse: 2123.0398 - val_mae: 26.5560\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2661.7492 - mse: 2661.7483 - mae: 28.0492 - val_loss: 2105.5671 - val_mse: 2105.5671 - val_mae: 27.0948\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2704.4308 - mse: 2704.4314 - mae: 28.0332 - val_loss: 2128.5265 - val_mse: 2128.5266 - val_mae: 26.1659\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 555us/step - loss: 2631.0371 - mse: 2631.0371 - mae: 27.8160 - val_loss: 2106.7779 - val_mse: 2106.7776 - val_mae: 26.9567\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2647.2970 - mse: 2647.2969 - mae: 27.8967 - val_loss: 2102.8867 - val_mse: 2102.8870 - val_mae: 27.0771\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2681.0438 - mse: 2681.0447 - mae: 28.0874 - val_loss: 2100.5057 - val_mse: 2100.5056 - val_mae: 27.2078\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2627.8984 - mse: 2627.8982 - mae: 28.2249 - val_loss: 2110.6216 - val_mse: 2110.6221 - val_mae: 26.4856\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 555us/step - loss: 2689.1678 - mse: 2689.1675 - mae: 28.2949 - val_loss: 2115.2943 - val_mse: 2115.2942 - val_mae: 26.7473\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2642.3990 - mse: 2642.3999 - mae: 27.6718 - val_loss: 2109.7697 - val_mse: 2109.7700 - val_mae: 26.8293\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2636.4117 - mse: 2636.4114 - mae: 28.5991 - val_loss: 2115.8472 - val_mse: 2115.8472 - val_mae: 26.7873\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 587us/step - loss: 2639.7566 - mse: 2639.7576 - mae: 27.7932 - val_loss: 2122.2985 - val_mse: 2122.2986 - val_mae: 26.2779\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2620.4067 - mse: 2620.4065 - mae: 28.0737 - val_loss: 2111.7468 - val_mse: 2111.7473 - val_mae: 26.8882\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2652.2311 - mse: 2652.2310 - mae: 28.3997 - val_loss: 2112.7431 - val_mse: 2112.7429 - val_mae: 26.7999\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2689.0005 - mse: 2689.0005 - mae: 28.3329 - val_loss: 2126.8444 - val_mse: 2126.8447 - val_mae: 26.3608\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2725.5215 - mse: 2725.5225 - mae: 28.2930 - val_loss: 2128.6402 - val_mse: 2128.6401 - val_mae: 26.2237\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2615.1430 - mse: 2615.1431 - mae: 27.8456 - val_loss: 2117.1522 - val_mse: 2117.1523 - val_mae: 26.7386\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2646.2989 - mse: 2646.2983 - mae: 28.2882 - val_loss: 2131.0132 - val_mse: 2131.0137 - val_mae: 26.3667\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2616.9807 - mse: 2616.9814 - mae: 28.2037 - val_loss: 2119.0374 - val_mse: 2119.0376 - val_mae: 26.5033\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2701.6267 - mse: 2701.6265 - mae: 28.0858 - val_loss: 2141.0158 - val_mse: 2141.0154 - val_mae: 26.1772\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2672.9076 - mse: 2672.9077 - mae: 28.1496 - val_loss: 2128.9005 - val_mse: 2128.9009 - val_mae: 26.5591\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2626.6224 - mse: 2626.6233 - mae: 27.9803 - val_loss: 2117.5804 - val_mse: 2117.5803 - val_mae: 26.7382\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 645us/step - loss: 2605.1514 - mse: 2605.1521 - mae: 27.7985 - val_loss: 2114.1434 - val_mse: 2114.1436 - val_mae: 26.5476\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2660.6581 - mse: 2660.6577 - mae: 27.9075 - val_loss: 2107.1131 - val_mse: 2107.1130 - val_mae: 26.7915\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2691.1962 - mse: 2691.1975 - mae: 28.0976 - val_loss: 2127.1288 - val_mse: 2127.1289 - val_mae: 26.2613\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2661.8523 - mse: 2661.8528 - mae: 27.7925 - val_loss: 2130.6371 - val_mse: 2130.6372 - val_mae: 26.4236\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2578.1612 - mse: 2578.1606 - mae: 27.9168 - val_loss: 2107.1276 - val_mse: 2107.1277 - val_mae: 27.3413\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2646.1258 - mse: 2646.1250 - mae: 28.1121 - val_loss: 2111.2716 - val_mse: 2111.2715 - val_mae: 26.8420\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2595.2320 - mse: 2595.2319 - mae: 27.9439 - val_loss: 2114.3359 - val_mse: 2114.3354 - val_mae: 26.7263\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2637.2958 - mse: 2637.2957 - mae: 27.6312 - val_loss: 2113.6786 - val_mse: 2113.6782 - val_mae: 26.9070\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2600.0064 - mse: 2600.0063 - mae: 27.7279 - val_loss: 2113.1472 - val_mse: 2113.1470 - val_mae: 26.8681\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2648.1076 - mse: 2648.1079 - mae: 27.8792 - val_loss: 2114.3530 - val_mse: 2114.3535 - val_mae: 26.8627\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2668.1441 - mse: 2668.1436 - mae: 28.0939 - val_loss: 2136.6981 - val_mse: 2136.6980 - val_mae: 26.2325\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 678us/step - loss: 2691.4639 - mse: 2691.4639 - mae: 27.9674 - val_loss: 2134.9977 - val_mse: 2134.9976 - val_mae: 26.5032\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2664.5984 - mse: 2664.5991 - mae: 28.2269 - val_loss: 2132.0225 - val_mse: 2132.0227 - val_mae: 26.4319\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2677.4860 - mse: 2677.4863 - mae: 28.0335 - val_loss: 2136.7601 - val_mse: 2136.7600 - val_mae: 26.2657\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2679.9144 - mse: 2679.9133 - mae: 28.3370 - val_loss: 2119.0318 - val_mse: 2119.0320 - val_mae: 27.0537\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 643us/step - loss: 2611.6169 - mse: 2611.6174 - mae: 27.8888 - val_loss: 2121.0062 - val_mse: 2121.0063 - val_mae: 26.8993\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2641.9096 - mse: 2641.9094 - mae: 27.7810 - val_loss: 2120.0662 - val_mse: 2120.0662 - val_mae: 26.9486\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2610.7865 - mse: 2610.7866 - mae: 27.6909 - val_loss: 2116.4456 - val_mse: 2116.4451 - val_mae: 26.9540\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 642us/step - loss: 2668.0495 - mse: 2668.0496 - mae: 27.7193 - val_loss: 2120.4520 - val_mse: 2120.4519 - val_mae: 26.6362\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2657.7899 - mse: 2657.7891 - mae: 27.9675 - val_loss: 2121.1667 - val_mse: 2121.1667 - val_mae: 26.5701\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 480us/step - loss: 2667.5543 - mse: 2667.5549 - mae: 28.1452 - val_loss: 2108.8332 - val_mse: 2108.8335 - val_mae: 26.9746\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 504us/step - loss: 2598.4145 - mse: 2598.4141 - mae: 27.7019 - val_loss: 2109.9322 - val_mse: 2109.9321 - val_mae: 26.9419\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2660.4278 - mse: 2660.4275 - mae: 27.9362 - val_loss: 2112.9134 - val_mse: 2112.9133 - val_mae: 26.7003\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 644us/step - loss: 2608.3634 - mse: 2608.3628 - mae: 28.0605 - val_loss: 2118.6106 - val_mse: 2118.6106 - val_mae: 26.7062\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2682.9203 - mse: 2682.9204 - mae: 28.1868 - val_loss: 2121.4260 - val_mse: 2121.4258 - val_mae: 26.6111\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2684.9352 - mse: 2684.9351 - mae: 28.3681 - val_loss: 2127.1390 - val_mse: 2127.1394 - val_mae: 26.5722\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2647.5101 - mse: 2647.5100 - mae: 27.9092 - val_loss: 2115.4800 - val_mse: 2115.4800 - val_mae: 26.8867\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 599us/step - loss: 2637.8937 - mse: 2637.8940 - mae: 27.7487 - val_loss: 2123.6985 - val_mse: 2123.6987 - val_mae: 26.6605\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 638us/step - loss: 2605.4273 - mse: 2605.4265 - mae: 27.7375 - val_loss: 2116.8459 - val_mse: 2116.8459 - val_mae: 26.4800\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2572.0287 - mse: 2572.0283 - mae: 27.7687 - val_loss: 2107.2852 - val_mse: 2107.2852 - val_mae: 26.9713\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 617us/step - loss: 2591.1257 - mse: 2591.1265 - mae: 27.3911 - val_loss: 2105.7847 - val_mse: 2105.7849 - val_mae: 26.8559\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2593.2613 - mse: 2593.2629 - mae: 27.6765 - val_loss: 2117.9059 - val_mse: 2117.9060 - val_mae: 26.5040\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2625.7594 - mse: 2625.7588 - mae: 27.7879 - val_loss: 2115.7209 - val_mse: 2115.7209 - val_mae: 26.7219\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2582.2836 - mse: 2582.2834 - mae: 27.5375 - val_loss: 2110.0303 - val_mse: 2110.0303 - val_mae: 26.8087\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2624.2377 - mse: 2624.2371 - mae: 27.7906 - val_loss: 2110.0939 - val_mse: 2110.0940 - val_mae: 27.0277\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2571.3141 - mse: 2571.3142 - mae: 27.7415 - val_loss: 2107.8145 - val_mse: 2107.8145 - val_mae: 26.9937\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 13340.3013 - mse: 13340.3008 - mae: 109.9527 - val_loss: 34638.8840 - val_mse: 34638.8789 - val_mae: 132.8064\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 394us/step - loss: 13219.3547 - mse: 13219.3564 - mae: 109.4157 - val_loss: 34394.3417 - val_mse: 34394.3398 - val_mae: 131.8922\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 249us/step - loss: 12859.1577 - mse: 12859.1582 - mae: 107.7423 - val_loss: 33612.9546 - val_mse: 33612.9570 - val_mae: 128.9346\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 489us/step - loss: 11889.1234 - mse: 11889.1230 - mae: 103.0560 - val_loss: 31443.5332 - val_mse: 31443.5312 - val_mae: 120.3551\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 496us/step - loss: 9276.9143 - mse: 9276.9150 - mae: 89.1153 - val_loss: 26181.9556 - val_mse: 26181.9531 - val_mae: 96.4618\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 468us/step - loss: 4963.6594 - mse: 4963.6587 - mae: 57.6480 - val_loss: 18704.1067 - val_mse: 18704.1055 - val_mae: 45.1704\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 492us/step - loss: 3012.1620 - mse: 3012.1621 - mae: 39.4119 - val_loss: 17232.9531 - val_mse: 17232.9531 - val_mae: 35.4084\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 432us/step - loss: 2905.6542 - mse: 2905.6538 - mae: 38.2211 - val_loss: 17744.8106 - val_mse: 17744.8105 - val_mae: 36.9743\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 470us/step - loss: 2831.1099 - mse: 2831.1099 - mae: 38.0870 - val_loss: 17518.2094 - val_mse: 17518.2070 - val_mae: 35.9381\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 462us/step - loss: 2978.9282 - mse: 2978.9282 - mae: 39.4292 - val_loss: 17518.2636 - val_mse: 17518.2637 - val_mae: 35.8995\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 479us/step - loss: 2789.7081 - mse: 2789.7083 - mae: 37.7354 - val_loss: 17346.3393 - val_mse: 17346.3398 - val_mae: 35.4263\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 474us/step - loss: 2894.3707 - mse: 2894.3706 - mae: 38.8767 - val_loss: 17528.9611 - val_mse: 17528.9629 - val_mae: 35.8998\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 434us/step - loss: 2573.0081 - mse: 2573.0083 - mae: 36.5129 - val_loss: 17613.2121 - val_mse: 17613.2129 - val_mae: 36.2021\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 306us/step - loss: 2548.4481 - mse: 2548.4482 - mae: 36.5231 - val_loss: 17509.6436 - val_mse: 17509.6445 - val_mae: 35.8036\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 467us/step - loss: 2698.3637 - mse: 2698.3638 - mae: 37.1264 - val_loss: 17497.9058 - val_mse: 17497.9082 - val_mae: 35.7399\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 454us/step - loss: 2688.3971 - mse: 2688.3970 - mae: 37.4823 - val_loss: 17373.3982 - val_mse: 17373.3965 - val_mae: 35.3971\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 478us/step - loss: 2832.3286 - mse: 2832.3286 - mae: 38.3749 - val_loss: 17476.9362 - val_mse: 17476.9336 - val_mae: 35.6475\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 421us/step - loss: 2761.3451 - mse: 2761.3452 - mae: 37.6611 - val_loss: 17575.0686 - val_mse: 17575.0703 - val_mae: 35.9565\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 414us/step - loss: 2572.4338 - mse: 2572.4341 - mae: 36.7652 - val_loss: 17392.6617 - val_mse: 17392.6621 - val_mae: 35.3798\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 660us/step - loss: 2540.1681 - mse: 2540.1685 - mae: 35.1153 - val_loss: 17319.2788 - val_mse: 17319.2773 - val_mae: 35.2661\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 393us/step - loss: 2605.0168 - mse: 2605.0168 - mae: 35.8233 - val_loss: 17459.4956 - val_mse: 17459.4961 - val_mae: 35.5278\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 467us/step - loss: 2286.3588 - mse: 2286.3586 - mae: 33.5845 - val_loss: 17439.5856 - val_mse: 17439.5859 - val_mae: 35.4538\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2454.5360 - mse: 2454.5359 - mae: 35.2472 - val_loss: 17394.7182 - val_mse: 17394.7188 - val_mae: 35.3318\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 577us/step - loss: 2545.6334 - mse: 2545.6331 - mae: 35.3774 - val_loss: 17297.3941 - val_mse: 17297.3926 - val_mae: 35.2325\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 597us/step - loss: 2591.6828 - mse: 2591.6831 - mae: 34.8300 - val_loss: 17408.7670 - val_mse: 17408.7676 - val_mae: 35.3731\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2534.9329 - mse: 2534.9333 - mae: 35.6864 - val_loss: 17352.2770 - val_mse: 17352.2773 - val_mae: 35.2927\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 699us/step - loss: 2367.1551 - mse: 2367.1550 - mae: 34.2900 - val_loss: 17476.0243 - val_mse: 17476.0215 - val_mae: 35.5007\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 701us/step - loss: 2451.5104 - mse: 2451.5107 - mae: 34.6863 - val_loss: 17496.4178 - val_mse: 17496.4160 - val_mae: 35.5315\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 628us/step - loss: 2488.3165 - mse: 2488.3164 - mae: 35.4220 - val_loss: 17364.6172 - val_mse: 17364.6152 - val_mae: 35.2771\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 2485.2954 - mse: 2485.2954 - mae: 34.8134 - val_loss: 17328.3681 - val_mse: 17328.3652 - val_mae: 35.2478\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2708.6877 - mse: 2708.6877 - mae: 37.6493 - val_loss: 17532.7939 - val_mse: 17532.7949 - val_mae: 35.6796\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 654us/step - loss: 2269.3352 - mse: 2269.3350 - mae: 34.0265 - val_loss: 17421.1134 - val_mse: 17421.1133 - val_mae: 35.4213\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 643us/step - loss: 2577.5905 - mse: 2577.5906 - mae: 36.7774 - val_loss: 17704.9351 - val_mse: 17704.9336 - val_mae: 36.3099\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 675us/step - loss: 2466.8213 - mse: 2466.8215 - mae: 34.5713 - val_loss: 17467.5394 - val_mse: 17467.5391 - val_mae: 35.5334\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2397.1950 - mse: 2397.1948 - mae: 34.3647 - val_loss: 17447.7656 - val_mse: 17447.7656 - val_mae: 35.4882\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 665us/step - loss: 2137.0335 - mse: 2137.0334 - mae: 32.9269 - val_loss: 17352.7173 - val_mse: 17352.7188 - val_mae: 35.3388\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 2317.9055 - mse: 2317.9055 - mae: 33.4580 - val_loss: 17227.1882 - val_mse: 17227.1875 - val_mae: 35.2325\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 675us/step - loss: 2125.3987 - mse: 2125.3984 - mae: 32.1997 - val_loss: 17528.5528 - val_mse: 17528.5547 - val_mae: 35.7539\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 524us/step - loss: 2290.1371 - mse: 2290.1372 - mae: 32.9995 - val_loss: 17405.0103 - val_mse: 17405.0098 - val_mae: 35.4777\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 524us/step - loss: 2344.1606 - mse: 2344.1604 - mae: 33.7040 - val_loss: 17399.8262 - val_mse: 17399.8281 - val_mae: 35.4819\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 734us/step - loss: 2134.9684 - mse: 2134.9685 - mae: 33.0589 - val_loss: 17364.0758 - val_mse: 17364.0762 - val_mae: 35.4343\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2320.4098 - mse: 2320.4099 - mae: 33.0900 - val_loss: 17565.7126 - val_mse: 17565.7129 - val_mae: 35.9299\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 2251.7827 - mse: 2251.7825 - mae: 32.2018 - val_loss: 17359.7653 - val_mse: 17359.7637 - val_mae: 35.4649\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2211.9959 - mse: 2211.9961 - mae: 32.5279 - val_loss: 17270.7988 - val_mse: 17270.8008 - val_mae: 35.3413\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2217.1393 - mse: 2217.1392 - mae: 32.6099 - val_loss: 17431.4236 - val_mse: 17431.4238 - val_mae: 35.6118\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 2226.1545 - mse: 2226.1545 - mae: 33.5031 - val_loss: 17503.4351 - val_mse: 17503.4355 - val_mae: 35.7982\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 545us/step - loss: 2279.7717 - mse: 2279.7717 - mae: 33.2081 - val_loss: 17312.6028 - val_mse: 17312.6035 - val_mae: 35.4384\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2254.7582 - mse: 2254.7583 - mae: 33.1150 - val_loss: 17552.3047 - val_mse: 17552.3047 - val_mae: 35.9924\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 2126.0723 - mse: 2126.0723 - mae: 32.3662 - val_loss: 17431.9692 - val_mse: 17431.9707 - val_mae: 35.7006\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2011.5225 - mse: 2011.5226 - mae: 32.3364 - val_loss: 17400.0464 - val_mse: 17400.0488 - val_mae: 35.6415\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 496us/step - loss: 2288.5246 - mse: 2288.5247 - mae: 33.1584 - val_loss: 17435.9648 - val_mse: 17435.9668 - val_mae: 35.7410\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 2100.7824 - mse: 2100.7825 - mae: 32.3308 - val_loss: 17385.8773 - val_mse: 17385.8789 - val_mae: 35.6674\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 2270.7708 - mse: 2270.7710 - mae: 33.3165 - val_loss: 17507.5920 - val_mse: 17507.5918 - val_mae: 35.9860\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 567us/step - loss: 2102.9169 - mse: 2102.9167 - mae: 32.0889 - val_loss: 17462.6015 - val_mse: 17462.6016 - val_mae: 35.8748\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2301.6463 - mse: 2301.6465 - mae: 32.8115 - val_loss: 17563.4912 - val_mse: 17563.4922 - val_mae: 36.2055\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 1999.5561 - mse: 1999.5560 - mae: 30.5561 - val_loss: 17461.7825 - val_mse: 17461.7812 - val_mae: 35.8842\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 1976.6374 - mse: 1976.6373 - mae: 30.4698 - val_loss: 17422.4154 - val_mse: 17422.4160 - val_mae: 35.8205\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2157.8994 - mse: 2157.8994 - mae: 32.6185 - val_loss: 17578.1063 - val_mse: 17578.1055 - val_mae: 36.4014\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2103.9705 - mse: 2103.9705 - mae: 31.1948 - val_loss: 17333.9488 - val_mse: 17333.9473 - val_mae: 35.6975\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 636us/step - loss: 1938.1049 - mse: 1938.1049 - mae: 31.1211 - val_loss: 17386.2695 - val_mse: 17386.2695 - val_mae: 35.7968\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2037.9463 - mse: 2037.9464 - mae: 31.2594 - val_loss: 17657.6993 - val_mse: 17657.6992 - val_mae: 36.8566\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 458us/step - loss: 2010.6013 - mse: 2010.6014 - mae: 31.8432 - val_loss: 17496.0375 - val_mse: 17496.0352 - val_mae: 36.1025\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 476us/step - loss: 2031.3528 - mse: 2031.3529 - mae: 30.7356 - val_loss: 17460.5508 - val_mse: 17460.5508 - val_mae: 36.0248\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 634us/step - loss: 2000.4547 - mse: 2000.4546 - mae: 31.5300 - val_loss: 17483.6879 - val_mse: 17483.6875 - val_mae: 36.0965\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 2250.5646 - mse: 2250.5645 - mae: 31.6094 - val_loss: 17557.6789 - val_mse: 17557.6777 - val_mae: 36.3872\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2091.4926 - mse: 2091.4927 - mae: 31.9184 - val_loss: 17413.7165 - val_mse: 17413.7168 - val_mae: 35.9394\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2010.4433 - mse: 2010.4432 - mae: 30.0640 - val_loss: 17423.0529 - val_mse: 17423.0527 - val_mae: 35.9699\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2057.2537 - mse: 2057.2534 - mae: 30.7872 - val_loss: 17579.8838 - val_mse: 17579.8867 - val_mae: 36.5965\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 435us/step - loss: 1723.3846 - mse: 1723.3846 - mae: 28.6241 - val_loss: 17399.3964 - val_mse: 17399.3965 - val_mae: 35.9425\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 668us/step - loss: 1984.8750 - mse: 1984.8749 - mae: 30.3769 - val_loss: 17542.0809 - val_mse: 17542.0820 - val_mae: 36.5265\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 1858.4103 - mse: 1858.4103 - mae: 29.0009 - val_loss: 17570.1372 - val_mse: 17570.1387 - val_mae: 36.6686\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 588us/step - loss: 1917.3124 - mse: 1917.3123 - mae: 30.1732 - val_loss: 17408.5080 - val_mse: 17408.5078 - val_mae: 36.0374\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 576us/step - loss: 2023.5233 - mse: 2023.5236 - mae: 31.3912 - val_loss: 17861.5257 - val_mse: 17861.5254 - val_mae: 38.4664\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 1964.9030 - mse: 1964.9028 - mae: 29.6720 - val_loss: 17479.1568 - val_mse: 17479.1562 - val_mae: 36.2973\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 763us/step - loss: 1930.7612 - mse: 1930.7610 - mae: 30.7015 - val_loss: 17532.7183 - val_mse: 17532.7188 - val_mae: 36.5312\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 1992.2714 - mse: 1992.2714 - mae: 30.4364 - val_loss: 17608.6776 - val_mse: 17608.6777 - val_mae: 36.8992\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 1988.3573 - mse: 1988.3574 - mae: 30.4522 - val_loss: 17293.7022 - val_mse: 17293.7012 - val_mae: 35.9499\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 1898.7031 - mse: 1898.7029 - mae: 29.8089 - val_loss: 17335.4220 - val_mse: 17335.4219 - val_mae: 36.0400\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 1941.2309 - mse: 1941.2310 - mae: 30.3812 - val_loss: 17560.5464 - val_mse: 17560.5469 - val_mae: 36.7114\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 609us/step - loss: 2101.4430 - mse: 2101.4429 - mae: 30.7246 - val_loss: 17796.6468 - val_mse: 17796.6484 - val_mae: 38.1419\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4283.9558 - mse: 4283.9565 - mae: 35.6208 - val_loss: 2174.8777 - val_mse: 2174.8777 - val_mae: 30.1141\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 4192.7017 - mse: 4192.7012 - mae: 35.6450 - val_loss: 2181.9756 - val_mse: 2181.9756 - val_mae: 30.1627\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 4120.3240 - mse: 4120.3237 - mae: 34.7571 - val_loss: 2228.4705 - val_mse: 2228.4707 - val_mae: 30.3790\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 644us/step - loss: 4136.9519 - mse: 4136.9517 - mae: 35.4441 - val_loss: 2211.2879 - val_mse: 2211.2881 - val_mae: 30.2863\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4101.0481 - mse: 4101.0483 - mae: 34.7559 - val_loss: 2234.1668 - val_mse: 2234.1667 - val_mae: 30.4046\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4335.8337 - mse: 4335.8335 - mae: 35.1238 - val_loss: 2351.2620 - val_mse: 2351.2620 - val_mae: 31.0178\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4222.2478 - mse: 4222.2466 - mae: 35.4316 - val_loss: 2309.6154 - val_mse: 2309.6155 - val_mae: 30.7932\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4064.3286 - mse: 4064.3279 - mae: 34.6543 - val_loss: 2162.1972 - val_mse: 2162.1970 - val_mae: 30.0239\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 4337.9687 - mse: 4337.9688 - mae: 35.4569 - val_loss: 2296.9939 - val_mse: 2296.9937 - val_mae: 30.7081\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4044.3452 - mse: 4044.3455 - mae: 35.1094 - val_loss: 2265.7583 - val_mse: 2265.7583 - val_mae: 30.5409\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 4078.3208 - mse: 4078.3213 - mae: 34.4401 - val_loss: 2253.5651 - val_mse: 2253.5652 - val_mae: 30.4679\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 575us/step - loss: 4306.1572 - mse: 4306.1567 - mae: 35.4380 - val_loss: 2280.5424 - val_mse: 2280.5425 - val_mae: 30.6029\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4180.5113 - mse: 4180.5112 - mae: 34.8006 - val_loss: 2286.9201 - val_mse: 2286.9202 - val_mae: 30.6311\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4023.6609 - mse: 4023.6606 - mae: 34.9985 - val_loss: 2232.7330 - val_mse: 2232.7329 - val_mae: 30.3461\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4168.3961 - mse: 4168.3960 - mae: 34.2276 - val_loss: 2224.1090 - val_mse: 2224.1089 - val_mae: 30.2943\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 518us/step - loss: 4331.0330 - mse: 4331.0332 - mae: 35.3179 - val_loss: 2302.4584 - val_mse: 2302.4587 - val_mae: 30.6999\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4205.2136 - mse: 4205.2139 - mae: 34.8137 - val_loss: 2214.0143 - val_mse: 2214.0144 - val_mae: 30.2292\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 520us/step - loss: 4190.5890 - mse: 4190.5894 - mae: 34.9537 - val_loss: 2211.9543 - val_mse: 2211.9546 - val_mae: 30.2130\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 660us/step - loss: 4280.0226 - mse: 4280.0229 - mae: 34.5293 - val_loss: 2284.6399 - val_mse: 2284.6399 - val_mae: 30.5957\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 690us/step - loss: 4081.7414 - mse: 4081.7417 - mae: 33.6473 - val_loss: 2243.2219 - val_mse: 2243.2219 - val_mae: 30.3778\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 4131.6944 - mse: 4131.6943 - mae: 33.4621 - val_loss: 2242.7797 - val_mse: 2242.7798 - val_mae: 30.3742\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4112.3994 - mse: 4112.3994 - mae: 34.3149 - val_loss: 2233.3562 - val_mse: 2233.3562 - val_mae: 30.3283\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 701us/step - loss: 4294.8815 - mse: 4294.8823 - mae: 35.0510 - val_loss: 2307.8631 - val_mse: 2307.8633 - val_mae: 30.7166\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 641us/step - loss: 4335.0893 - mse: 4335.0889 - mae: 35.1834 - val_loss: 2211.2264 - val_mse: 2211.2263 - val_mae: 30.2193\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4160.0955 - mse: 4160.0957 - mae: 35.1202 - val_loss: 2269.5166 - val_mse: 2269.5168 - val_mae: 30.5217\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 660us/step - loss: 4192.2530 - mse: 4192.2529 - mae: 34.3195 - val_loss: 2257.7714 - val_mse: 2257.7715 - val_mae: 30.4659\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 542us/step - loss: 4069.0403 - mse: 4069.0398 - mae: 33.4170 - val_loss: 2250.9533 - val_mse: 2250.9536 - val_mae: 30.4298\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4174.4123 - mse: 4174.4126 - mae: 35.9765 - val_loss: 2286.4973 - val_mse: 2286.4973 - val_mae: 30.6051\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 0s 497us/step - loss: 4334.6731 - mse: 4334.6724 - mae: 34.7917 - val_loss: 2282.3648 - val_mse: 2282.3645 - val_mae: 30.5835\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 541us/step - loss: 3875.3250 - mse: 3875.3242 - mae: 34.2944 - val_loss: 2259.5357 - val_mse: 2259.5354 - val_mae: 30.4697\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4126.6138 - mse: 4126.6138 - mae: 33.7636 - val_loss: 2225.9192 - val_mse: 2225.9194 - val_mae: 30.3157\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4060.1709 - mse: 4060.1719 - mae: 34.3568 - val_loss: 2279.3568 - val_mse: 2279.3569 - val_mae: 30.5588\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4018.2889 - mse: 4018.2886 - mae: 33.9930 - val_loss: 2204.0099 - val_mse: 2204.0100 - val_mae: 30.2001\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 485us/step - loss: 3986.3786 - mse: 3986.3784 - mae: 32.8509 - val_loss: 2234.7523 - val_mse: 2234.7522 - val_mae: 30.3434\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 0s 458us/step - loss: 4096.8546 - mse: 4096.8540 - mae: 34.2237 - val_loss: 2167.6685 - val_mse: 2167.6685 - val_mae: 30.0443\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 0s 485us/step - loss: 3977.1838 - mse: 3977.1831 - mae: 34.0192 - val_loss: 2261.1671 - val_mse: 2261.1670 - val_mae: 30.4676\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 507us/step - loss: 4251.4953 - mse: 4251.4951 - mae: 34.2667 - val_loss: 2246.5249 - val_mse: 2246.5249 - val_mae: 30.4052\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4260.0796 - mse: 4260.0801 - mae: 34.8349 - val_loss: 2350.9948 - val_mse: 2350.9946 - val_mae: 30.9181\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 657us/step - loss: 4162.1663 - mse: 4162.1665 - mae: 34.4397 - val_loss: 2304.3128 - val_mse: 2304.3130 - val_mae: 30.6707\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 3859.9644 - mse: 3859.9644 - mae: 32.8057 - val_loss: 2224.0998 - val_mse: 2224.0999 - val_mae: 30.2976\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4159.7219 - mse: 4159.7217 - mae: 33.4690 - val_loss: 2207.3230 - val_mse: 2207.3228 - val_mae: 30.2108\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 670us/step - loss: 4249.4990 - mse: 4249.4990 - mae: 33.6844 - val_loss: 2223.4608 - val_mse: 2223.4607 - val_mae: 30.2793\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 677us/step - loss: 4152.7117 - mse: 4152.7114 - mae: 33.8043 - val_loss: 2278.1073 - val_mse: 2278.1074 - val_mae: 30.5403\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4033.6230 - mse: 4033.6223 - mae: 32.8712 - val_loss: 2236.5061 - val_mse: 2236.5063 - val_mae: 30.3405\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4112.1013 - mse: 4112.1011 - mae: 33.7925 - val_loss: 2301.4200 - val_mse: 2301.4199 - val_mae: 30.6705\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 544us/step - loss: 4104.6474 - mse: 4104.6470 - mae: 34.4256 - val_loss: 2348.8224 - val_mse: 2348.8225 - val_mae: 30.9231\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 3934.7458 - mse: 3934.7456 - mae: 33.9386 - val_loss: 2279.6934 - val_mse: 2279.6936 - val_mae: 30.5539\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 652us/step - loss: 4224.2234 - mse: 4224.2231 - mae: 34.2181 - val_loss: 2274.5609 - val_mse: 2274.5608 - val_mae: 30.5219\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4280.4129 - mse: 4280.4126 - mae: 34.6446 - val_loss: 2273.6093 - val_mse: 2273.6091 - val_mae: 30.5150\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 4088.8456 - mse: 4088.8457 - mae: 34.2521 - val_loss: 2214.2471 - val_mse: 2214.2471 - val_mae: 30.2292\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 4030.3841 - mse: 4030.3838 - mae: 33.3829 - val_loss: 2209.1081 - val_mse: 2209.1082 - val_mae: 30.2043\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 681us/step - loss: 4068.2284 - mse: 4068.2285 - mae: 33.3027 - val_loss: 2214.0398 - val_mse: 2214.0396 - val_mae: 30.2275\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 622us/step - loss: 4080.9978 - mse: 4080.9980 - mae: 34.0055 - val_loss: 2277.9412 - val_mse: 2277.9414 - val_mae: 30.5320\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 561us/step - loss: 4017.4885 - mse: 4017.4890 - mae: 34.2666 - val_loss: 2201.6406 - val_mse: 2201.6409 - val_mae: 30.1567\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 3939.7165 - mse: 3939.7161 - mae: 33.3141 - val_loss: 2199.2319 - val_mse: 2199.2317 - val_mae: 30.1541\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 567us/step - loss: 4069.5011 - mse: 4069.5012 - mae: 33.1677 - val_loss: 2229.3967 - val_mse: 2229.3967 - val_mae: 30.2798\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 3963.6227 - mse: 3963.6228 - mae: 34.0451 - val_loss: 2202.5236 - val_mse: 2202.5237 - val_mae: 30.1386\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4045.7923 - mse: 4045.7920 - mae: 33.4770 - val_loss: 2211.6438 - val_mse: 2211.6443 - val_mae: 30.1930\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 603us/step - loss: 3899.6050 - mse: 3899.6050 - mae: 33.5236 - val_loss: 2264.6924 - val_mse: 2264.6924 - val_mae: 30.4617\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 3968.2382 - mse: 3968.2380 - mae: 33.8278 - val_loss: 2210.2597 - val_mse: 2210.2598 - val_mae: 30.1909\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4031.8166 - mse: 4031.8164 - mae: 33.3033 - val_loss: 2185.7493 - val_mse: 2185.7493 - val_mae: 30.0755\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 4146.4333 - mse: 4146.4331 - mae: 34.6318 - val_loss: 2258.0274 - val_mse: 2258.0273 - val_mae: 30.4245\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 3633.2722 - mse: 3633.2720 - mae: 32.8406 - val_loss: 2190.4260 - val_mse: 2190.4260 - val_mae: 30.1272\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 0s 446us/step - loss: 4135.5723 - mse: 4135.5723 - mae: 34.5248 - val_loss: 2293.3427 - val_mse: 2293.3425 - val_mae: 30.6117\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 0s 466us/step - loss: 3959.5074 - mse: 3959.5068 - mae: 33.4338 - val_loss: 2216.6871 - val_mse: 2216.6870 - val_mae: 30.2408\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 0s 480us/step - loss: 3854.6366 - mse: 3854.6365 - mae: 32.8341 - val_loss: 2234.2983 - val_mse: 2234.2983 - val_mae: 30.3262\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4126.3838 - mse: 4126.3838 - mae: 34.0526 - val_loss: 2287.6626 - val_mse: 2287.6628 - val_mae: 30.5795\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4208.7721 - mse: 4208.7720 - mae: 33.8661 - val_loss: 2320.6924 - val_mse: 2320.6929 - val_mae: 30.7527\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4171.8354 - mse: 4171.8345 - mae: 33.7557 - val_loss: 2320.0227 - val_mse: 2320.0227 - val_mae: 30.7522\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4067.5931 - mse: 4067.5933 - mae: 33.4007 - val_loss: 2253.8737 - val_mse: 2253.8738 - val_mae: 30.4176\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 602us/step - loss: 4005.5565 - mse: 4005.5564 - mae: 32.6867 - val_loss: 2196.4962 - val_mse: 2196.4966 - val_mae: 30.1397\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4063.4568 - mse: 4063.4565 - mae: 33.8173 - val_loss: 2279.3294 - val_mse: 2279.3293 - val_mae: 30.5412\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 3972.9679 - mse: 3972.9673 - mae: 33.2666 - val_loss: 2248.2366 - val_mse: 2248.2366 - val_mae: 30.4125\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 654us/step - loss: 4109.0682 - mse: 4109.0684 - mae: 33.6386 - val_loss: 2237.4418 - val_mse: 2237.4419 - val_mae: 30.3619\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4112.8968 - mse: 4112.8970 - mae: 33.7093 - val_loss: 2259.4726 - val_mse: 2259.4724 - val_mae: 30.4629\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 611us/step - loss: 3964.3946 - mse: 3964.3945 - mae: 33.4884 - val_loss: 2196.4599 - val_mse: 2196.4600 - val_mae: 30.1520\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 605us/step - loss: 4253.5110 - mse: 4253.5112 - mae: 33.9480 - val_loss: 2334.6084 - val_mse: 2334.6084 - val_mae: 30.8436\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 627us/step - loss: 4097.9860 - mse: 4097.9858 - mae: 32.7640 - val_loss: 2268.2936 - val_mse: 2268.2935 - val_mae: 30.5128\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 664us/step - loss: 3892.1287 - mse: 3892.1292 - mae: 33.5146 - val_loss: 2220.7775 - val_mse: 2220.7773 - val_mae: 30.2836\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 580us/step - loss: 4034.1116 - mse: 4034.1118 - mae: 32.8555 - val_loss: 2245.7624 - val_mse: 2245.7622 - val_mae: 30.4259\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3368.6857 - mse: 3368.6860 - mae: 33.3984 - val_loss: 1463.9762 - val_mse: 1463.9763 - val_mae: 24.6029\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3305.6596 - mse: 3305.6599 - mae: 32.4290 - val_loss: 1450.3473 - val_mse: 1450.3474 - val_mae: 24.7936\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3433.1142 - mse: 3433.1138 - mae: 32.9070 - val_loss: 1451.0777 - val_mse: 1451.0778 - val_mae: 24.7182\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3455.2970 - mse: 3455.2966 - mae: 32.9726 - val_loss: 1450.9344 - val_mse: 1450.9344 - val_mae: 24.6424\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3442.8868 - mse: 3442.8860 - mae: 32.1886 - val_loss: 1450.7472 - val_mse: 1450.7473 - val_mae: 24.6046\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 519us/step - loss: 3180.9010 - mse: 3180.9006 - mae: 32.4411 - val_loss: 1453.6729 - val_mse: 1453.6730 - val_mae: 24.5185\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3466.5606 - mse: 3466.5613 - mae: 33.0201 - val_loss: 1445.2776 - val_mse: 1445.2775 - val_mae: 24.7419\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 524us/step - loss: 3401.7246 - mse: 3401.7241 - mae: 33.4051 - val_loss: 1442.9223 - val_mse: 1442.9225 - val_mae: 24.7950\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 631us/step - loss: 3352.2599 - mse: 3352.2593 - mae: 32.9262 - val_loss: 1455.1783 - val_mse: 1455.1781 - val_mae: 24.4436\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3197.3146 - mse: 3197.3145 - mae: 31.8628 - val_loss: 1450.2508 - val_mse: 1450.2506 - val_mae: 24.6950\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3304.1699 - mse: 3304.1707 - mae: 32.0631 - val_loss: 1449.9252 - val_mse: 1449.9253 - val_mae: 24.6506\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3360.4931 - mse: 3360.4922 - mae: 32.2956 - val_loss: 1444.1963 - val_mse: 1444.1962 - val_mae: 24.8346\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3305.2666 - mse: 3305.2668 - mae: 32.6494 - val_loss: 1445.9186 - val_mse: 1445.9188 - val_mae: 24.7535\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 584us/step - loss: 3413.6446 - mse: 3413.6445 - mae: 33.0067 - val_loss: 1459.2674 - val_mse: 1459.2676 - val_mae: 24.3180\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3347.6102 - mse: 3347.6096 - mae: 32.3609 - val_loss: 1445.8454 - val_mse: 1445.8455 - val_mae: 24.7714\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3488.7700 - mse: 3488.7700 - mae: 33.2452 - val_loss: 1453.2832 - val_mse: 1453.2832 - val_mae: 24.5068\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 435us/step - loss: 3279.3359 - mse: 3279.3362 - mae: 31.8249 - val_loss: 1454.9626 - val_mse: 1454.9628 - val_mae: 24.5387\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3299.6957 - mse: 3299.6953 - mae: 32.5741 - val_loss: 1452.3768 - val_mse: 1452.3768 - val_mae: 24.6208\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3249.9684 - mse: 3249.9683 - mae: 32.3193 - val_loss: 1445.7993 - val_mse: 1445.7994 - val_mae: 24.9902\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 678us/step - loss: 3387.8592 - mse: 3387.8589 - mae: 32.3651 - val_loss: 1448.6719 - val_mse: 1448.6720 - val_mae: 24.7896\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 676us/step - loss: 3443.3525 - mse: 3443.3518 - mae: 32.4573 - val_loss: 1448.7681 - val_mse: 1448.7681 - val_mae: 24.7875\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 685us/step - loss: 3267.4447 - mse: 3267.4441 - mae: 31.7153 - val_loss: 1455.8267 - val_mse: 1455.8267 - val_mae: 24.6001\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 672us/step - loss: 3416.5849 - mse: 3416.5850 - mae: 32.3309 - val_loss: 1447.7428 - val_mse: 1447.7428 - val_mae: 24.8943\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3336.2520 - mse: 3336.2524 - mae: 32.4111 - val_loss: 1444.3541 - val_mse: 1444.3541 - val_mae: 25.1644\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3294.0058 - mse: 3294.0056 - mae: 32.3920 - val_loss: 1442.8041 - val_mse: 1442.8040 - val_mae: 25.3006\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3354.7755 - mse: 3354.7751 - mae: 32.2705 - val_loss: 1443.2659 - val_mse: 1443.2659 - val_mae: 25.3288\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3349.5846 - mse: 3349.5847 - mae: 32.6290 - val_loss: 1450.4107 - val_mse: 1450.4106 - val_mae: 24.8722\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3317.0991 - mse: 3317.0984 - mae: 32.1105 - val_loss: 1445.9978 - val_mse: 1445.9977 - val_mae: 25.1919\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 941us/step - loss: 3398.8362 - mse: 3398.8367 - mae: 33.2758 - val_loss: 1448.5642 - val_mse: 1448.5643 - val_mae: 24.8709\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3304.7202 - mse: 3304.7207 - mae: 31.7630 - val_loss: 1444.0097 - val_mse: 1444.0096 - val_mae: 25.0664\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3327.9599 - mse: 3327.9590 - mae: 32.8826 - val_loss: 1442.9362 - val_mse: 1442.9363 - val_mae: 25.0475\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 664us/step - loss: 3368.1407 - mse: 3368.1414 - mae: 32.8257 - val_loss: 1446.1718 - val_mse: 1446.1718 - val_mae: 24.8867\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 626us/step - loss: 3366.8488 - mse: 3366.8491 - mae: 32.1724 - val_loss: 1449.8750 - val_mse: 1449.8750 - val_mae: 24.7934\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 544us/step - loss: 3367.5378 - mse: 3367.5378 - mae: 32.2472 - val_loss: 1450.1559 - val_mse: 1450.1559 - val_mae: 24.8018\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 507us/step - loss: 3382.2660 - mse: 3382.2651 - mae: 32.3420 - val_loss: 1445.9108 - val_mse: 1445.9108 - val_mae: 25.0710\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 515us/step - loss: 3236.6578 - mse: 3236.6582 - mae: 32.0273 - val_loss: 1448.6225 - val_mse: 1448.6224 - val_mae: 24.9286\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 493us/step - loss: 3234.2954 - mse: 3234.2954 - mae: 32.4780 - val_loss: 1452.8759 - val_mse: 1452.8759 - val_mae: 24.8327\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3309.5839 - mse: 3309.5833 - mae: 32.0308 - val_loss: 1448.2717 - val_mse: 1448.2717 - val_mae: 25.0704\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3269.0087 - mse: 3269.0083 - mae: 32.1606 - val_loss: 1448.3142 - val_mse: 1448.3142 - val_mae: 25.1512\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3195.5298 - mse: 3195.5298 - mae: 31.5988 - val_loss: 1453.0115 - val_mse: 1453.0115 - val_mae: 24.9854\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3170.0463 - mse: 3170.0464 - mae: 31.4969 - val_loss: 1455.2611 - val_mse: 1455.2610 - val_mae: 24.9343\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3250.3072 - mse: 3250.3066 - mae: 31.7790 - val_loss: 1452.4044 - val_mse: 1452.4044 - val_mae: 25.1982\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3306.4940 - mse: 3306.4944 - mae: 32.5440 - val_loss: 1462.4910 - val_mse: 1462.4908 - val_mae: 24.7494\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 594us/step - loss: 3249.2336 - mse: 3249.2341 - mae: 31.8534 - val_loss: 1454.1848 - val_mse: 1454.1847 - val_mae: 25.0697\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 515us/step - loss: 3211.4888 - mse: 3211.4883 - mae: 31.7901 - val_loss: 1452.3852 - val_mse: 1452.3853 - val_mae: 25.1418\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 605us/step - loss: 3317.5619 - mse: 3317.5615 - mae: 32.0475 - val_loss: 1449.5927 - val_mse: 1449.5928 - val_mae: 25.3093\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 593us/step - loss: 3380.5136 - mse: 3380.5137 - mae: 32.2584 - val_loss: 1453.7251 - val_mse: 1453.7251 - val_mae: 25.0644\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3355.5257 - mse: 3355.5261 - mae: 32.4689 - val_loss: 1456.1889 - val_mse: 1456.1888 - val_mae: 24.8048\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 663us/step - loss: 3287.7115 - mse: 3287.7122 - mae: 31.8605 - val_loss: 1454.3007 - val_mse: 1454.3007 - val_mae: 24.9734\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 630us/step - loss: 3267.6307 - mse: 3267.6311 - mae: 31.7377 - val_loss: 1447.7835 - val_mse: 1447.7836 - val_mae: 25.2747\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 732us/step - loss: 3343.4023 - mse: 3343.4021 - mae: 32.8275 - val_loss: 1453.1311 - val_mse: 1453.1310 - val_mae: 24.8308\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3285.8014 - mse: 3285.8018 - mae: 32.1993 - val_loss: 1448.6608 - val_mse: 1448.6608 - val_mae: 25.2157\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3294.2250 - mse: 3294.2253 - mae: 32.2717 - val_loss: 1450.6051 - val_mse: 1450.6051 - val_mae: 25.1781\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 634us/step - loss: 3254.3349 - mse: 3254.3352 - mae: 31.6247 - val_loss: 1448.1685 - val_mse: 1448.1686 - val_mae: 25.4472\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3265.9891 - mse: 3265.9893 - mae: 31.9062 - val_loss: 1449.6786 - val_mse: 1449.6785 - val_mae: 25.2035\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 585us/step - loss: 3149.9701 - mse: 3149.9700 - mae: 32.5005 - val_loss: 1446.4094 - val_mse: 1446.4094 - val_mae: 25.3667\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3373.3627 - mse: 3373.3628 - mae: 32.0659 - val_loss: 1452.3110 - val_mse: 1452.3110 - val_mae: 24.9329\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 521us/step - loss: 3284.7045 - mse: 3284.7041 - mae: 31.7001 - val_loss: 1453.7659 - val_mse: 1453.7659 - val_mae: 24.9399\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3177.7954 - mse: 3177.7949 - mae: 31.7734 - val_loss: 1451.3244 - val_mse: 1451.3246 - val_mae: 25.2206\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3236.3035 - mse: 3236.3032 - mae: 31.4021 - val_loss: 1452.4455 - val_mse: 1452.4453 - val_mae: 25.1474\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 640us/step - loss: 3235.9007 - mse: 3235.9006 - mae: 32.0454 - val_loss: 1455.4143 - val_mse: 1455.4144 - val_mae: 24.9985\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3223.8241 - mse: 3223.8237 - mae: 31.6545 - val_loss: 1462.2993 - val_mse: 1462.2993 - val_mae: 24.7114\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 658us/step - loss: 3304.4047 - mse: 3304.4058 - mae: 32.2225 - val_loss: 1454.5306 - val_mse: 1454.5308 - val_mae: 25.0592\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3240.3323 - mse: 3240.3328 - mae: 30.8659 - val_loss: 1455.7353 - val_mse: 1455.7351 - val_mae: 25.0744\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3252.5360 - mse: 3252.5369 - mae: 31.9066 - val_loss: 1451.1990 - val_mse: 1451.1989 - val_mae: 25.5161\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 674us/step - loss: 3322.5163 - mse: 3322.5164 - mae: 31.5060 - val_loss: 1460.6377 - val_mse: 1460.6378 - val_mae: 24.8172\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3167.8016 - mse: 3167.8027 - mae: 31.9383 - val_loss: 1451.1217 - val_mse: 1451.1218 - val_mae: 25.2172\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 537us/step - loss: 3098.9308 - mse: 3098.9304 - mae: 30.6965 - val_loss: 1450.0377 - val_mse: 1450.0376 - val_mae: 25.3422\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 577us/step - loss: 3272.0909 - mse: 3272.0906 - mae: 31.5037 - val_loss: 1454.3884 - val_mse: 1454.3885 - val_mae: 25.0808\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3291.8555 - mse: 3291.8555 - mae: 31.8706 - val_loss: 1449.9489 - val_mse: 1449.9489 - val_mae: 25.6508\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3154.0754 - mse: 3154.0757 - mae: 31.5482 - val_loss: 1450.1568 - val_mse: 1450.1567 - val_mae: 25.4134\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3173.8055 - mse: 3173.8064 - mae: 31.7225 - val_loss: 1450.4690 - val_mse: 1450.4688 - val_mae: 25.3086\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3215.8537 - mse: 3215.8535 - mae: 31.2349 - val_loss: 1453.8407 - val_mse: 1453.8407 - val_mae: 25.1119\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 575us/step - loss: 3199.0907 - mse: 3199.0896 - mae: 31.3856 - val_loss: 1455.9889 - val_mse: 1455.9890 - val_mae: 24.9802\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3175.6319 - mse: 3175.6313 - mae: 31.3636 - val_loss: 1454.9342 - val_mse: 1454.9342 - val_mae: 25.0221\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 514us/step - loss: 3088.1034 - mse: 3088.1033 - mae: 30.8214 - val_loss: 1451.1912 - val_mse: 1451.1913 - val_mae: 25.2426\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3148.0619 - mse: 3148.0623 - mae: 30.8542 - val_loss: 1451.7956 - val_mse: 1451.7957 - val_mae: 25.1230\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3179.0497 - mse: 3179.0491 - mae: 31.4926 - val_loss: 1451.4364 - val_mse: 1451.4365 - val_mae: 25.2476\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3215.0942 - mse: 3215.0940 - mae: 31.9555 - val_loss: 1454.1737 - val_mse: 1454.1736 - val_mae: 25.1179\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 611us/step - loss: 3248.2507 - mse: 3248.2502 - mae: 31.5431 - val_loss: 1451.9380 - val_mse: 1451.9381 - val_mae: 25.3312\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2886.5407 - mse: 2886.5413 - mae: 31.0389 - val_loss: 1055.6195 - val_mse: 1055.6194 - val_mae: 23.7735\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2897.7838 - mse: 2897.7849 - mae: 31.1120 - val_loss: 1055.7592 - val_mse: 1055.7593 - val_mae: 23.5439\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2857.0720 - mse: 2857.0718 - mae: 30.5469 - val_loss: 1051.5280 - val_mse: 1051.5280 - val_mae: 23.6941\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 580us/step - loss: 2940.8027 - mse: 2940.8027 - mae: 31.2461 - val_loss: 1049.5577 - val_mse: 1049.5577 - val_mae: 23.5900\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2922.5514 - mse: 2922.5518 - mae: 31.2319 - val_loss: 1051.0259 - val_mse: 1051.0259 - val_mae: 23.4041\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2909.6164 - mse: 2909.6162 - mae: 30.5446 - val_loss: 1043.0458 - val_mse: 1043.0459 - val_mae: 23.9831\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2893.2753 - mse: 2893.2754 - mae: 31.2024 - val_loss: 1048.1695 - val_mse: 1048.1696 - val_mae: 23.5438\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2896.7602 - mse: 2896.7617 - mae: 31.3460 - val_loss: 1043.1178 - val_mse: 1043.1177 - val_mae: 23.9032\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2913.8184 - mse: 2913.8174 - mae: 31.5474 - val_loss: 1046.4857 - val_mse: 1046.4857 - val_mae: 23.4375\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2895.1787 - mse: 2895.1790 - mae: 30.9603 - val_loss: 1042.7710 - val_mse: 1042.7710 - val_mae: 23.6174\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2798.2150 - mse: 2798.2153 - mae: 30.8645 - val_loss: 1040.5935 - val_mse: 1040.5933 - val_mae: 23.7358\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2842.5071 - mse: 2842.5066 - mae: 30.1989 - val_loss: 1046.1018 - val_mse: 1046.1019 - val_mae: 23.4357\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2893.2813 - mse: 2893.2805 - mae: 30.6458 - val_loss: 1038.7109 - val_mse: 1038.7109 - val_mae: 23.7962\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2895.3347 - mse: 2895.3337 - mae: 31.3570 - val_loss: 1044.8974 - val_mse: 1044.8973 - val_mae: 23.3826\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2940.0361 - mse: 2940.0364 - mae: 31.4428 - val_loss: 1040.4223 - val_mse: 1040.4222 - val_mae: 23.6265\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2898.9144 - mse: 2898.9136 - mae: 30.2265 - val_loss: 1037.9435 - val_mse: 1037.9435 - val_mae: 23.6517\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2851.5507 - mse: 2851.5505 - mae: 30.2778 - val_loss: 1033.8038 - val_mse: 1033.8040 - val_mae: 24.2124\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2800.8429 - mse: 2800.8433 - mae: 30.2968 - val_loss: 1033.3786 - val_mse: 1033.3788 - val_mae: 23.9230\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 596us/step - loss: 2958.8651 - mse: 2958.8650 - mae: 31.5575 - val_loss: 1033.7160 - val_mse: 1033.7161 - val_mae: 23.6212\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 530us/step - loss: 2886.4064 - mse: 2886.4065 - mae: 30.9273 - val_loss: 1036.1692 - val_mse: 1036.1693 - val_mae: 23.3157\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 535us/step - loss: 2790.2582 - mse: 2790.2588 - mae: 30.2811 - val_loss: 1032.6482 - val_mse: 1032.6483 - val_mae: 23.4910\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 629us/step - loss: 2886.9927 - mse: 2886.9927 - mae: 30.2455 - val_loss: 1033.2245 - val_mse: 1033.2245 - val_mae: 23.4233\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2894.1291 - mse: 2894.1292 - mae: 30.7972 - val_loss: 1031.5835 - val_mse: 1031.5835 - val_mae: 23.5060\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 563us/step - loss: 2874.7573 - mse: 2874.7571 - mae: 31.0360 - val_loss: 1030.5927 - val_mse: 1030.5927 - val_mae: 23.5606\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 505us/step - loss: 2889.6263 - mse: 2889.6265 - mae: 31.0894 - val_loss: 1034.0979 - val_mse: 1034.0980 - val_mae: 23.2308\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2849.1165 - mse: 2849.1169 - mae: 30.1410 - val_loss: 1026.5157 - val_mse: 1026.5156 - val_mae: 23.8511\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2842.5876 - mse: 2842.5879 - mae: 30.8746 - val_loss: 1030.6150 - val_mse: 1030.6149 - val_mae: 23.3987\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2763.0224 - mse: 2763.0232 - mae: 29.9106 - val_loss: 1028.4364 - val_mse: 1028.4364 - val_mae: 23.5231\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2897.5931 - mse: 2897.5923 - mae: 30.9284 - val_loss: 1029.2426 - val_mse: 1029.2424 - val_mae: 23.4191\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 592us/step - loss: 2875.0123 - mse: 2875.0120 - mae: 30.3935 - val_loss: 1028.3421 - val_mse: 1028.3420 - val_mae: 23.4500\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2874.6780 - mse: 2874.6782 - mae: 30.9890 - val_loss: 1028.8596 - val_mse: 1028.8595 - val_mae: 23.4150\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2904.3291 - mse: 2904.3291 - mae: 31.0246 - val_loss: 1026.1547 - val_mse: 1026.1547 - val_mae: 23.8879\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 550us/step - loss: 2934.4710 - mse: 2934.4722 - mae: 31.0969 - val_loss: 1026.8192 - val_mse: 1026.8191 - val_mae: 23.7667\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2980.7987 - mse: 2980.7979 - mae: 31.0273 - val_loss: 1028.5901 - val_mse: 1028.5901 - val_mae: 23.5681\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2947.5960 - mse: 2947.5967 - mae: 30.7433 - val_loss: 1025.3407 - val_mse: 1025.3407 - val_mae: 23.7852\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 681us/step - loss: 2800.7788 - mse: 2800.7793 - mae: 30.1839 - val_loss: 1030.9789 - val_mse: 1030.9790 - val_mae: 23.2224\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 655us/step - loss: 2887.2599 - mse: 2887.2607 - mae: 30.5910 - val_loss: 1029.4407 - val_mse: 1029.4407 - val_mae: 23.2638\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2806.2973 - mse: 2806.2961 - mae: 30.4068 - val_loss: 1027.0793 - val_mse: 1027.0793 - val_mae: 23.2599\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2788.9446 - mse: 2788.9443 - mae: 30.0381 - val_loss: 1023.1456 - val_mse: 1023.1456 - val_mae: 24.0848\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 702us/step - loss: 2873.9544 - mse: 2873.9539 - mae: 30.7035 - val_loss: 1023.5540 - val_mse: 1023.5541 - val_mae: 23.5784\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2789.6493 - mse: 2789.6501 - mae: 29.6672 - val_loss: 1022.5247 - val_mse: 1022.5247 - val_mae: 23.6976\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2939.3544 - mse: 2939.3555 - mae: 30.8189 - val_loss: 1023.2360 - val_mse: 1023.2360 - val_mae: 23.5196\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 565us/step - loss: 2817.4837 - mse: 2817.4832 - mae: 30.7721 - val_loss: 1023.1963 - val_mse: 1023.1964 - val_mae: 23.3793\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2794.1212 - mse: 2794.1204 - mae: 29.9923 - val_loss: 1021.8983 - val_mse: 1021.8982 - val_mae: 23.5981\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 531us/step - loss: 2852.1321 - mse: 2852.1323 - mae: 30.7397 - val_loss: 1026.2347 - val_mse: 1026.2346 - val_mae: 23.2079\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2837.3987 - mse: 2837.3999 - mae: 29.8962 - val_loss: 1021.2678 - val_mse: 1021.2678 - val_mae: 23.7999\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2864.0115 - mse: 2864.0125 - mae: 30.4541 - val_loss: 1022.5186 - val_mse: 1022.5187 - val_mae: 23.5028\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2763.1272 - mse: 2763.1267 - mae: 29.9022 - val_loss: 1021.2130 - val_mse: 1021.2130 - val_mae: 23.5495\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2847.6433 - mse: 2847.6438 - mae: 29.8503 - val_loss: 1020.6408 - val_mse: 1020.6408 - val_mae: 23.4419\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 584us/step - loss: 2798.8777 - mse: 2798.8777 - mae: 29.5480 - val_loss: 1018.4370 - val_mse: 1018.4370 - val_mae: 23.5141\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 489us/step - loss: 2767.8918 - mse: 2767.8916 - mae: 30.2904 - val_loss: 1025.1269 - val_mse: 1025.1270 - val_mae: 22.9303\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2835.5390 - mse: 2835.5378 - mae: 30.0859 - val_loss: 1018.8243 - val_mse: 1018.8242 - val_mae: 23.4606\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 539us/step - loss: 2761.2616 - mse: 2761.2612 - mae: 30.1977 - val_loss: 1017.5797 - val_mse: 1017.5797 - val_mae: 23.8802\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2819.5432 - mse: 2819.5444 - mae: 30.2226 - val_loss: 1018.5653 - val_mse: 1018.5654 - val_mae: 23.5703\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2822.4213 - mse: 2822.4216 - mae: 29.9473 - val_loss: 1017.5103 - val_mse: 1017.5102 - val_mae: 23.7348\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2799.1156 - mse: 2799.1157 - mae: 30.2405 - val_loss: 1018.9113 - val_mse: 1018.9113 - val_mae: 23.3961\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2749.4681 - mse: 2749.4678 - mae: 29.9763 - val_loss: 1016.7521 - val_mse: 1016.7521 - val_mae: 23.7098\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2858.7356 - mse: 2858.7358 - mae: 30.6057 - val_loss: 1018.2138 - val_mse: 1018.2137 - val_mae: 23.3962\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2772.0736 - mse: 2772.0737 - mae: 30.5386 - val_loss: 1017.5470 - val_mse: 1017.5470 - val_mae: 23.3409\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 671us/step - loss: 2816.9635 - mse: 2816.9629 - mae: 29.9360 - val_loss: 1017.6176 - val_mse: 1017.6177 - val_mae: 23.2930\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2805.2561 - mse: 2805.2571 - mae: 30.3194 - val_loss: 1018.3069 - val_mse: 1018.3069 - val_mae: 23.3307\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2766.1521 - mse: 2766.1523 - mae: 30.0381 - val_loss: 1015.5702 - val_mse: 1015.5702 - val_mae: 23.6064\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2810.4569 - mse: 2810.4556 - mae: 30.2606 - val_loss: 1015.7214 - val_mse: 1015.7214 - val_mae: 23.3407\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2850.3817 - mse: 2850.3818 - mae: 30.2127 - val_loss: 1013.4802 - val_mse: 1013.4802 - val_mae: 23.7619\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2864.2212 - mse: 2864.2212 - mae: 30.4394 - val_loss: 1015.3372 - val_mse: 1015.3372 - val_mae: 23.2377\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 426us/step - loss: 2774.6669 - mse: 2774.6677 - mae: 29.9803 - val_loss: 1015.9667 - val_mse: 1015.9666 - val_mae: 23.2671\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2862.6718 - mse: 2862.6716 - mae: 29.9704 - val_loss: 1014.5430 - val_mse: 1014.5430 - val_mae: 23.5207\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2851.1161 - mse: 2851.1165 - mae: 30.5651 - val_loss: 1015.9291 - val_mse: 1015.9290 - val_mae: 23.1290\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2800.2911 - mse: 2800.2917 - mae: 29.3609 - val_loss: 1011.7629 - val_mse: 1011.7628 - val_mae: 23.5742\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2859.5787 - mse: 2859.5784 - mae: 30.8244 - val_loss: 1011.3568 - val_mse: 1011.3568 - val_mae: 23.3013\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2822.7074 - mse: 2822.7078 - mae: 29.9276 - val_loss: 1011.8339 - val_mse: 1011.8339 - val_mae: 23.2864\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 503us/step - loss: 2780.3010 - mse: 2780.3020 - mae: 29.9053 - val_loss: 1012.5259 - val_mse: 1012.5258 - val_mae: 23.1955\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2819.3802 - mse: 2819.3801 - mae: 30.0948 - val_loss: 1011.8194 - val_mse: 1011.8195 - val_mae: 23.3547\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2822.9604 - mse: 2822.9595 - mae: 29.8969 - val_loss: 1011.4180 - val_mse: 1011.4180 - val_mae: 23.7154\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 657us/step - loss: 2819.2141 - mse: 2819.2134 - mae: 30.1506 - val_loss: 1015.3850 - val_mse: 1015.3850 - val_mae: 23.1998\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 628us/step - loss: 2824.6644 - mse: 2824.6641 - mae: 30.0080 - val_loss: 1012.5670 - val_mse: 1012.5670 - val_mae: 23.5182\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 718us/step - loss: 2773.3761 - mse: 2773.3760 - mae: 30.0219 - val_loss: 1013.1431 - val_mse: 1013.1431 - val_mae: 23.3156\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 666us/step - loss: 2803.5175 - mse: 2803.5171 - mae: 30.2531 - val_loss: 1011.7444 - val_mse: 1011.7444 - val_mae: 23.8535\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2832.3646 - mse: 2832.3650 - mae: 30.3646 - val_loss: 1012.7005 - val_mse: 1012.7004 - val_mae: 23.2554\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 543us/step - loss: 2754.3978 - mse: 2754.3982 - mae: 29.6457 - val_loss: 1010.9807 - val_mse: 1010.9807 - val_mae: 23.6536\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 536us/step - loss: 2537.1473 - mse: 2537.1470 - mae: 29.7827 - val_loss: 1507.1253 - val_mse: 1507.1252 - val_mae: 26.5866\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2599.8772 - mse: 2599.8777 - mae: 29.6838 - val_loss: 1529.6982 - val_mse: 1529.6981 - val_mae: 26.2251\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2468.1573 - mse: 2468.1575 - mae: 29.0319 - val_loss: 1516.3613 - val_mse: 1516.3613 - val_mae: 26.3575\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2523.7369 - mse: 2523.7371 - mae: 29.6417 - val_loss: 1526.1067 - val_mse: 1526.1067 - val_mae: 26.2138\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2552.4163 - mse: 2552.4163 - mae: 29.6194 - val_loss: 1519.5181 - val_mse: 1519.5182 - val_mae: 26.2776\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2454.7984 - mse: 2454.7986 - mae: 29.1469 - val_loss: 1507.9950 - val_mse: 1507.9951 - val_mae: 26.3977\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2506.0116 - mse: 2506.0112 - mae: 29.0237 - val_loss: 1518.8866 - val_mse: 1518.8867 - val_mae: 26.2152\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2489.7107 - mse: 2489.7100 - mae: 29.3857 - val_loss: 1525.1361 - val_mse: 1525.1362 - val_mae: 26.1243\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2553.0600 - mse: 2553.0593 - mae: 29.2952 - val_loss: 1514.9731 - val_mse: 1514.9731 - val_mae: 26.2476\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2466.0056 - mse: 2466.0061 - mae: 29.0804 - val_loss: 1509.5058 - val_mse: 1509.5060 - val_mae: 26.3230\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2549.0841 - mse: 2549.0847 - mae: 29.4539 - val_loss: 1494.8247 - val_mse: 1494.8246 - val_mae: 26.5112\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 546us/step - loss: 2537.7086 - mse: 2537.7092 - mae: 29.3807 - val_loss: 1513.5058 - val_mse: 1513.5056 - val_mae: 26.1829\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2472.3768 - mse: 2472.3772 - mae: 29.0336 - val_loss: 1493.4759 - val_mse: 1493.4756 - val_mae: 26.4471\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2511.1354 - mse: 2511.1353 - mae: 29.3149 - val_loss: 1492.2360 - val_mse: 1492.2362 - val_mae: 26.4501\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2469.2921 - mse: 2469.2925 - mae: 29.0897 - val_loss: 1487.2838 - val_mse: 1487.2838 - val_mae: 26.5166\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 584us/step - loss: 2514.4531 - mse: 2514.4526 - mae: 29.3741 - val_loss: 1515.2292 - val_mse: 1515.2294 - val_mae: 26.0728\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2488.2053 - mse: 2488.2053 - mae: 29.3053 - val_loss: 1494.2557 - val_mse: 1494.2556 - val_mae: 26.3249\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2459.2993 - mse: 2459.2991 - mae: 28.8848 - val_loss: 1498.2743 - val_mse: 1498.2745 - val_mae: 26.2370\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 538us/step - loss: 2543.0152 - mse: 2543.0146 - mae: 29.2458 - val_loss: 1499.4174 - val_mse: 1499.4175 - val_mae: 26.1924\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 605us/step - loss: 2486.3480 - mse: 2486.3484 - mae: 28.8172 - val_loss: 1488.7914 - val_mse: 1488.7914 - val_mae: 26.3325\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2491.8834 - mse: 2491.8828 - mae: 29.4123 - val_loss: 1505.9780 - val_mse: 1505.9781 - val_mae: 26.0254\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2423.7816 - mse: 2423.7812 - mae: 29.2408 - val_loss: 1486.8403 - val_mse: 1486.8401 - val_mae: 26.2384\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 637us/step - loss: 2512.8603 - mse: 2512.8599 - mae: 29.1170 - val_loss: 1490.9090 - val_mse: 1490.9091 - val_mae: 26.1538\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2505.5714 - mse: 2505.5720 - mae: 29.3217 - val_loss: 1483.2589 - val_mse: 1483.2588 - val_mae: 26.2322\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2525.6382 - mse: 2525.6379 - mae: 29.5329 - val_loss: 1473.6243 - val_mse: 1473.6243 - val_mae: 26.3243\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2472.5530 - mse: 2472.5525 - mae: 28.9706 - val_loss: 1483.3379 - val_mse: 1483.3378 - val_mae: 26.1032\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2472.9881 - mse: 2472.9883 - mae: 29.0204 - val_loss: 1454.6971 - val_mse: 1454.6970 - val_mae: 26.5281\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2460.2810 - mse: 2460.2808 - mae: 29.4080 - val_loss: 1483.9365 - val_mse: 1483.9364 - val_mae: 26.0208\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 1s 577us/step - loss: 2453.2166 - mse: 2453.2156 - mae: 28.9941 - val_loss: 1484.4908 - val_mse: 1484.4907 - val_mae: 26.0035\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2445.9507 - mse: 2445.9504 - mae: 29.2021 - val_loss: 1471.1556 - val_mse: 1471.1556 - val_mae: 26.1945\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2501.3549 - mse: 2501.3557 - mae: 28.9586 - val_loss: 1469.6620 - val_mse: 1469.6619 - val_mae: 26.1916\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2490.0700 - mse: 2490.0701 - mae: 29.1300 - val_loss: 1479.8591 - val_mse: 1479.8593 - val_mae: 26.0107\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2437.7646 - mse: 2437.7654 - mae: 28.8299 - val_loss: 1472.7685 - val_mse: 1472.7682 - val_mae: 26.0773\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 537us/step - loss: 2384.7580 - mse: 2384.7581 - mae: 28.6255 - val_loss: 1456.0864 - val_mse: 1456.0864 - val_mae: 26.2893\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2483.7239 - mse: 2483.7246 - mae: 29.1441 - val_loss: 1473.4773 - val_mse: 1473.4774 - val_mae: 26.0107\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2551.2440 - mse: 2551.2441 - mae: 29.4598 - val_loss: 1468.2779 - val_mse: 1468.2778 - val_mae: 26.0668\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2488.0989 - mse: 2488.0989 - mae: 29.1428 - val_loss: 1461.1209 - val_mse: 1461.1207 - val_mae: 26.1456\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2436.8580 - mse: 2436.8579 - mae: 28.8625 - val_loss: 1464.3336 - val_mse: 1464.3336 - val_mae: 26.0175\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2458.8413 - mse: 2458.8411 - mae: 29.2126 - val_loss: 1463.0952 - val_mse: 1463.0955 - val_mae: 25.9809\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2417.6599 - mse: 2417.6594 - mae: 28.6927 - val_loss: 1449.0247 - val_mse: 1449.0247 - val_mae: 26.1998\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2457.3595 - mse: 2457.3594 - mae: 28.7867 - val_loss: 1463.0981 - val_mse: 1463.0979 - val_mae: 25.9673\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2493.2218 - mse: 2493.2212 - mae: 29.3626 - val_loss: 1467.7534 - val_mse: 1467.7534 - val_mae: 25.8552\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 524us/step - loss: 2432.1279 - mse: 2432.1284 - mae: 28.8510 - val_loss: 1478.5847 - val_mse: 1478.5847 - val_mae: 25.6847\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 497us/step - loss: 2448.0033 - mse: 2448.0037 - mae: 28.5550 - val_loss: 1444.6852 - val_mse: 1444.6849 - val_mae: 26.1775\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2428.1996 - mse: 2428.1987 - mae: 28.8028 - val_loss: 1467.7255 - val_mse: 1467.7255 - val_mae: 25.7641\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 1s 555us/step - loss: 2466.8390 - mse: 2466.8401 - mae: 29.3417 - val_loss: 1453.7750 - val_mse: 1453.7749 - val_mae: 25.9001\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2406.9966 - mse: 2406.9973 - mae: 28.7648 - val_loss: 1455.7659 - val_mse: 1455.7660 - val_mae: 25.7944\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 571us/step - loss: 2375.1208 - mse: 2375.1211 - mae: 28.7434 - val_loss: 1455.3698 - val_mse: 1455.3698 - val_mae: 25.7900\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2494.4217 - mse: 2494.4221 - mae: 29.1560 - val_loss: 1445.6336 - val_mse: 1445.6337 - val_mae: 25.9608\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2387.8501 - mse: 2387.8496 - mae: 28.8011 - val_loss: 1452.0951 - val_mse: 1452.0955 - val_mae: 25.8191\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2482.3374 - mse: 2482.3374 - mae: 29.0872 - val_loss: 1453.9897 - val_mse: 1453.9896 - val_mae: 25.8051\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 523us/step - loss: 2413.9287 - mse: 2413.9287 - mae: 28.5561 - val_loss: 1453.4723 - val_mse: 1453.4722 - val_mae: 25.8549\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - ETA: 0s - loss: 2474.0628 - mse: 2474.0625 - mae: 28.95 - 2s 617us/step - loss: 2456.2612 - mse: 2456.2607 - mae: 28.8833 - val_loss: 1442.4351 - val_mse: 1442.4349 - val_mae: 26.0149\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2395.4245 - mse: 2395.4238 - mae: 28.3254 - val_loss: 1457.6332 - val_mse: 1457.6332 - val_mae: 25.8105\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 649us/step - loss: 2413.9713 - mse: 2413.9714 - mae: 28.2503 - val_loss: 1441.4908 - val_mse: 1441.4907 - val_mae: 26.0818\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2436.2207 - mse: 2436.2214 - mae: 28.4768 - val_loss: 1447.4977 - val_mse: 1447.4978 - val_mae: 25.9341\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2471.8612 - mse: 2471.8613 - mae: 29.1985 - val_loss: 1440.0077 - val_mse: 1440.0079 - val_mae: 26.0184\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2454.5388 - mse: 2454.5398 - mae: 28.6042 - val_loss: 1456.9251 - val_mse: 1456.9253 - val_mae: 25.7411\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 534us/step - loss: 2437.5050 - mse: 2437.5049 - mae: 28.6807 - val_loss: 1469.8727 - val_mse: 1469.8727 - val_mae: 25.5942\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 635us/step - loss: 2455.4616 - mse: 2455.4619 - mae: 28.6387 - val_loss: 1447.9674 - val_mse: 1447.9674 - val_mae: 25.7916\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2423.5305 - mse: 2423.5312 - mae: 28.7285 - val_loss: 1446.6762 - val_mse: 1446.6763 - val_mae: 25.8194\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2504.9808 - mse: 2504.9807 - mae: 28.8670 - val_loss: 1442.6200 - val_mse: 1442.6199 - val_mae: 25.8872\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2445.4709 - mse: 2445.4705 - mae: 28.8968 - val_loss: 1450.9453 - val_mse: 1450.9453 - val_mae: 25.7025\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2457.1584 - mse: 2457.1582 - mae: 28.7842 - val_loss: 1448.1871 - val_mse: 1448.1871 - val_mae: 25.7189\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 697us/step - loss: 2469.9315 - mse: 2469.9309 - mae: 29.0129 - val_loss: 1432.8828 - val_mse: 1432.8828 - val_mae: 25.8935\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 639us/step - loss: 2433.8483 - mse: 2433.8474 - mae: 28.6482 - val_loss: 1431.6953 - val_mse: 1431.6953 - val_mae: 25.8561\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 679us/step - loss: 2439.8886 - mse: 2439.8887 - mae: 28.6107 - val_loss: 1417.1275 - val_mse: 1417.1276 - val_mae: 26.0088\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 2s 659us/step - loss: 2483.7055 - mse: 2483.7053 - mae: 28.9353 - val_loss: 1449.6004 - val_mse: 1449.6005 - val_mae: 25.5577\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 567us/step - loss: 2455.9777 - mse: 2455.9790 - mae: 29.1023 - val_loss: 1448.7371 - val_mse: 1448.7372 - val_mae: 25.5943\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 2s 676us/step - loss: 2496.2193 - mse: 2496.2195 - mae: 29.1281 - val_loss: 1446.0192 - val_mse: 1446.0193 - val_mae: 25.5868\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2464.7072 - mse: 2464.7080 - mae: 29.0315 - val_loss: 1440.2002 - val_mse: 1440.2003 - val_mae: 25.6398\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2462.1762 - mse: 2462.1772 - mae: 28.6234 - val_loss: 1433.4814 - val_mse: 1433.4812 - val_mae: 25.6605\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2465.6839 - mse: 2465.6846 - mae: 28.6709 - val_loss: 1443.5705 - val_mse: 1443.5707 - val_mae: 25.5180\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 1s 532us/step - loss: 2415.9862 - mse: 2415.9861 - mae: 28.2415 - val_loss: 1421.7929 - val_mse: 1421.7928 - val_mae: 25.7643\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 559us/step - loss: 2445.4415 - mse: 2445.4404 - mae: 28.6306 - val_loss: 1421.9232 - val_mse: 1421.9232 - val_mae: 25.8149\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 1s 490us/step - loss: 2388.8762 - mse: 2388.8762 - mae: 28.6888 - val_loss: 1436.5424 - val_mse: 1436.5422 - val_mae: 25.5736\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 527us/step - loss: 2459.4054 - mse: 2459.4055 - mae: 28.6812 - val_loss: 1441.0407 - val_mse: 1441.0406 - val_mae: 25.5492\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 551us/step - loss: 2436.4136 - mse: 2436.4133 - mae: 28.7147 - val_loss: 1438.1053 - val_mse: 1438.1052 - val_mae: 25.5808\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 679us/step - loss: 2427.7226 - mse: 2427.7227 - mae: 28.5476 - val_loss: 1431.4392 - val_mse: 1431.4392 - val_mae: 25.6343\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2408.5428 - mse: 2408.5432 - mae: 28.2935 - val_loss: 1418.4515 - val_mse: 1418.4515 - val_mae: 25.8099\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 659us/step - loss: 2297.9619 - mse: 2297.9614 - mae: 28.4757 - val_loss: 3639.0351 - val_mse: 3639.0359 - val_mae: 23.3565\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 614us/step - loss: 2274.4715 - mse: 2274.4714 - mae: 28.6102 - val_loss: 3638.2383 - val_mse: 3638.2380 - val_mae: 23.1566\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 638us/step - loss: 2377.4033 - mse: 2377.4033 - mae: 29.0995 - val_loss: 3638.3889 - val_mse: 3638.3892 - val_mae: 23.1703\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2387.0466 - mse: 2387.0474 - mae: 29.1355 - val_loss: 3640.1650 - val_mse: 3640.1650 - val_mae: 23.2861\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2330.2325 - mse: 2330.2319 - mae: 29.3005 - val_loss: 3640.0161 - val_mse: 3640.0168 - val_mae: 22.8947\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2332.0637 - mse: 2332.0640 - mae: 28.9302 - val_loss: 3642.2157 - val_mse: 3642.2151 - val_mae: 23.6128\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2267.9788 - mse: 2267.9780 - mae: 28.4973 - val_loss: 3638.9965 - val_mse: 3638.9963 - val_mae: 23.1944\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 630us/step - loss: 2337.0517 - mse: 2337.0510 - mae: 29.0246 - val_loss: 3639.4941 - val_mse: 3639.4939 - val_mae: 22.9204\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2317.7535 - mse: 2317.7539 - mae: 29.1708 - val_loss: 3641.1628 - val_mse: 3641.1631 - val_mae: 23.2124\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 554us/step - loss: 2311.2811 - mse: 2311.2812 - mae: 28.6671 - val_loss: 3640.3484 - val_mse: 3640.3486 - val_mae: 23.0979\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2377.6983 - mse: 2377.6982 - mae: 29.0338 - val_loss: 3640.3097 - val_mse: 3640.3103 - val_mae: 23.1557\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 688us/step - loss: 2361.7560 - mse: 2361.7563 - mae: 29.0931 - val_loss: 3639.8277 - val_mse: 3639.8274 - val_mae: 23.1786\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2378.5908 - mse: 2378.5901 - mae: 29.3106 - val_loss: 3642.7248 - val_mse: 3642.7241 - val_mae: 22.5423\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2320.0050 - mse: 2320.0049 - mae: 29.0073 - val_loss: 3642.7113 - val_mse: 3642.7114 - val_mae: 23.2233\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 563us/step - loss: 2330.4705 - mse: 2330.4705 - mae: 28.7964 - val_loss: 3645.1197 - val_mse: 3645.1201 - val_mae: 23.3944\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 596us/step - loss: 2346.7258 - mse: 2346.7261 - mae: 28.8960 - val_loss: 3644.5861 - val_mse: 3644.5854 - val_mae: 23.1371\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 649us/step - loss: 2313.0703 - mse: 2313.0706 - mae: 28.9897 - val_loss: 3643.9775 - val_mse: 3643.9775 - val_mae: 23.0900\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2339.1453 - mse: 2339.1453 - mae: 28.9437 - val_loss: 3644.8910 - val_mse: 3644.8901 - val_mae: 23.1571\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 539us/step - loss: 2235.2558 - mse: 2235.2561 - mae: 28.5604 - val_loss: 3643.8150 - val_mse: 3643.8152 - val_mae: 23.0370\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2285.1126 - mse: 2285.1121 - mae: 28.6271 - val_loss: 3643.9708 - val_mse: 3643.9700 - val_mae: 23.3678\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2334.3174 - mse: 2334.3176 - mae: 28.3951 - val_loss: 3644.1201 - val_mse: 3644.1199 - val_mae: 23.2810\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2320.2310 - mse: 2320.2310 - mae: 28.9984 - val_loss: 3645.6955 - val_mse: 3645.6958 - val_mae: 23.5672\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 504us/step - loss: 2303.5881 - mse: 2303.5891 - mae: 28.9956 - val_loss: 3644.5337 - val_mse: 3644.5332 - val_mae: 22.9371\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 571us/step - loss: 2308.2670 - mse: 2308.2668 - mae: 29.0085 - val_loss: 3643.5710 - val_mse: 3643.5713 - val_mae: 23.3577\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2308.8638 - mse: 2308.8638 - mae: 28.8157 - val_loss: 3643.4834 - val_mse: 3643.4832 - val_mae: 23.2206\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2253.1090 - mse: 2253.1094 - mae: 28.6002 - val_loss: 3644.8596 - val_mse: 3644.8599 - val_mae: 23.1711\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2312.4157 - mse: 2312.4163 - mae: 29.1912 - val_loss: 3645.1498 - val_mse: 3645.1497 - val_mae: 23.0014\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2333.1320 - mse: 2333.1321 - mae: 29.0181 - val_loss: 3643.8063 - val_mse: 3643.8059 - val_mae: 22.8787\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2290.5090 - mse: 2290.5085 - mae: 28.8306 - val_loss: 3643.6974 - val_mse: 3643.6975 - val_mae: 23.2104\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2287.3257 - mse: 2287.3252 - mae: 28.5222 - val_loss: 3645.1568 - val_mse: 3645.1567 - val_mae: 23.4523\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 542us/step - loss: 2307.6545 - mse: 2307.6541 - mae: 28.9275 - val_loss: 3646.5838 - val_mse: 3646.5842 - val_mae: 23.7426\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2275.7960 - mse: 2275.7961 - mae: 28.6571 - val_loss: 3647.6161 - val_mse: 3647.6167 - val_mae: 23.9594\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2331.4217 - mse: 2331.4214 - mae: 28.6959 - val_loss: 3643.6811 - val_mse: 3643.6807 - val_mae: 23.3990\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 547us/step - loss: 2323.9292 - mse: 2323.9290 - mae: 29.0844 - val_loss: 3644.6004 - val_mse: 3644.6001 - val_mae: 23.1887\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 590us/step - loss: 2293.8705 - mse: 2293.8711 - mae: 28.5980 - val_loss: 3647.3794 - val_mse: 3647.3792 - val_mae: 23.0948\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2313.7035 - mse: 2313.7034 - mae: 28.4621 - val_loss: 3644.9181 - val_mse: 3644.9177 - val_mae: 23.5638\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 615us/step - loss: 2264.5843 - mse: 2264.5837 - mae: 28.6399 - val_loss: 3645.0146 - val_mse: 3645.0129 - val_mae: 23.4663\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2330.0820 - mse: 2330.0813 - mae: 29.0311 - val_loss: 3644.3329 - val_mse: 3644.3328 - val_mae: 22.8530\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2233.3763 - mse: 2233.3770 - mae: 28.3969 - val_loss: 3643.8797 - val_mse: 3643.8799 - val_mae: 22.9206\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2300.1725 - mse: 2300.1736 - mae: 28.5646 - val_loss: 3644.6822 - val_mse: 3644.6831 - val_mae: 22.8739\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2297.3566 - mse: 2297.3562 - mae: 28.7824 - val_loss: 3644.4449 - val_mse: 3644.4451 - val_mae: 22.9361\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2320.3627 - mse: 2320.3630 - mae: 28.6803 - val_loss: 3645.5789 - val_mse: 3645.5786 - val_mae: 23.2899\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 699us/step - loss: 2339.5899 - mse: 2339.5896 - mae: 29.0323 - val_loss: 3647.3154 - val_mse: 3647.3157 - val_mae: 22.8017\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 564us/step - loss: 2211.4543 - mse: 2211.4551 - mae: 28.0769 - val_loss: 3649.2964 - val_mse: 3649.2966 - val_mae: 23.3856\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2241.3962 - mse: 2241.3962 - mae: 28.4393 - val_loss: 3647.8243 - val_mse: 3647.8237 - val_mae: 23.1090\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 606us/step - loss: 2257.6066 - mse: 2257.6062 - mae: 28.6824 - val_loss: 3647.7324 - val_mse: 3647.7319 - val_mae: 23.0400\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2281.2874 - mse: 2281.2864 - mae: 28.7123 - val_loss: 3647.8573 - val_mse: 3647.8582 - val_mae: 23.1624\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 537us/step - loss: 2272.8765 - mse: 2272.8772 - mae: 28.4090 - val_loss: 3646.8956 - val_mse: 3646.8955 - val_mae: 23.1777\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2271.0572 - mse: 2271.0576 - mae: 28.8071 - val_loss: 3649.3688 - val_mse: 3649.3689 - val_mae: 22.9975\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2293.5887 - mse: 2293.5889 - mae: 28.8760 - val_loss: 3650.2791 - val_mse: 3650.2791 - val_mae: 22.7354\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 523us/step - loss: 2303.1090 - mse: 2303.1094 - mae: 28.5682 - val_loss: 3652.3113 - val_mse: 3652.3108 - val_mae: 23.1843\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2280.0978 - mse: 2280.0969 - mae: 28.6177 - val_loss: 3651.0992 - val_mse: 3651.0986 - val_mae: 23.4699\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 578us/step - loss: 2289.6143 - mse: 2289.6138 - mae: 28.5859 - val_loss: 3649.8691 - val_mse: 3649.8694 - val_mae: 23.2562\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 672us/step - loss: 2260.4523 - mse: 2260.4524 - mae: 28.1901 - val_loss: 3652.8170 - val_mse: 3652.8167 - val_mae: 23.4109\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2285.6620 - mse: 2285.6609 - mae: 28.4924 - val_loss: 3653.7572 - val_mse: 3653.7563 - val_mae: 23.5222\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2280.1083 - mse: 2280.1082 - mae: 28.8086 - val_loss: 3653.9095 - val_mse: 3653.9089 - val_mae: 23.2585\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2281.9116 - mse: 2281.9114 - mae: 28.6009 - val_loss: 3652.0123 - val_mse: 3652.0120 - val_mae: 23.3263\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2285.2755 - mse: 2285.2759 - mae: 28.4831 - val_loss: 3651.2511 - val_mse: 3651.2512 - val_mae: 23.2296\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2267.0600 - mse: 2267.0603 - mae: 28.5373 - val_loss: 3652.8412 - val_mse: 3652.8413 - val_mae: 23.3535\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2286.5743 - mse: 2286.5742 - mae: 28.6937 - val_loss: 3651.9237 - val_mse: 3651.9231 - val_mae: 23.4024\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 663us/step - loss: 2313.2113 - mse: 2313.2117 - mae: 28.8014 - val_loss: 3649.9284 - val_mse: 3649.9287 - val_mae: 23.0947\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 530us/step - loss: 2284.0189 - mse: 2284.0183 - mae: 28.4712 - val_loss: 3647.8762 - val_mse: 3647.8762 - val_mae: 23.4264\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 691us/step - loss: 2272.8685 - mse: 2272.8687 - mae: 28.4791 - val_loss: 3648.0226 - val_mse: 3648.0220 - val_mae: 23.1581\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2289.8314 - mse: 2289.8315 - mae: 28.5615 - val_loss: 3650.0548 - val_mse: 3650.0549 - val_mae: 22.8165\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2193.6370 - mse: 2193.6372 - mae: 28.1964 - val_loss: 3651.1754 - val_mse: 3651.1753 - val_mae: 23.2623\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2239.9921 - mse: 2239.9912 - mae: 28.5480 - val_loss: 3651.6226 - val_mse: 3651.6226 - val_mae: 23.1880\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 664us/step - loss: 2266.1253 - mse: 2266.1252 - mae: 28.0825 - val_loss: 3652.0669 - val_mse: 3652.0669 - val_mae: 23.3457\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2248.8532 - mse: 2248.8540 - mae: 28.3135 - val_loss: 3652.4968 - val_mse: 3652.4971 - val_mae: 23.0818\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2160.5868 - mse: 2160.5876 - mae: 27.7500 - val_loss: 3652.3618 - val_mse: 3652.3613 - val_mae: 23.3238\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 532us/step - loss: 2257.0577 - mse: 2257.0571 - mae: 28.4656 - val_loss: 3652.1498 - val_mse: 3652.1499 - val_mae: 23.2499\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2267.9827 - mse: 2267.9827 - mae: 28.6292 - val_loss: 3652.4618 - val_mse: 3652.4619 - val_mae: 23.1986\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2271.4818 - mse: 2271.4829 - mae: 28.4131 - val_loss: 3654.1773 - val_mse: 3654.1770 - val_mae: 23.6610\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2288.8123 - mse: 2288.8125 - mae: 28.8052 - val_loss: 3655.5242 - val_mse: 3655.5254 - val_mae: 23.4368\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2250.4996 - mse: 2250.4993 - mae: 28.4717 - val_loss: 3656.3693 - val_mse: 3656.3699 - val_mae: 23.3180\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 531us/step - loss: 2233.4913 - mse: 2233.4910 - mae: 28.2425 - val_loss: 3656.5459 - val_mse: 3656.5459 - val_mae: 23.4261\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2244.3187 - mse: 2244.3191 - mae: 28.4463 - val_loss: 3656.0308 - val_mse: 3656.0300 - val_mae: 23.3594\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2256.4600 - mse: 2256.4597 - mae: 28.2976 - val_loss: 3655.6638 - val_mse: 3655.6633 - val_mae: 23.3376\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 566us/step - loss: 2245.3582 - mse: 2245.3584 - mae: 28.1444 - val_loss: 3654.6595 - val_mse: 3654.6599 - val_mae: 23.3497\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2273.6569 - mse: 2273.6572 - mae: 28.0574 - val_loss: 3653.7447 - val_mse: 3653.7444 - val_mae: 22.8769\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 526us/step - loss: 2265.2488 - mse: 2265.2488 - mae: 28.3656 - val_loss: 3656.4700 - val_mse: 3656.4702 - val_mae: 23.2723\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2640.5746 - mse: 2640.5742 - mae: 28.2079 - val_loss: 2128.0299 - val_mse: 2128.0300 - val_mae: 26.7742\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2663.9329 - mse: 2663.9331 - mae: 28.2589 - val_loss: 2144.9995 - val_mse: 2144.9993 - val_mae: 26.1860\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2686.2847 - mse: 2686.2847 - mae: 28.2734 - val_loss: 2139.9020 - val_mse: 2139.9019 - val_mae: 26.4061\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2611.7973 - mse: 2611.7966 - mae: 28.0711 - val_loss: 2149.8472 - val_mse: 2149.8469 - val_mae: 26.2281\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 549us/step - loss: 2657.0588 - mse: 2657.0601 - mae: 27.9576 - val_loss: 2146.7829 - val_mse: 2146.7832 - val_mae: 26.2662\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2611.2936 - mse: 2611.2942 - mae: 27.7190 - val_loss: 2155.9533 - val_mse: 2155.9534 - val_mae: 26.1282\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2683.2020 - mse: 2683.2009 - mae: 28.1109 - val_loss: 2150.1713 - val_mse: 2150.1714 - val_mae: 26.3482\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2615.3225 - mse: 2615.3228 - mae: 27.6051 - val_loss: 2151.0120 - val_mse: 2151.0125 - val_mae: 26.1822\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2646.8586 - mse: 2646.8577 - mae: 27.7053 - val_loss: 2135.6673 - val_mse: 2135.6670 - val_mae: 26.3345\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2677.4032 - mse: 2677.4026 - mae: 28.0766 - val_loss: 2137.6557 - val_mse: 2137.6553 - val_mae: 26.4492\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2683.1180 - mse: 2683.1177 - mae: 27.9989 - val_loss: 2142.1418 - val_mse: 2142.1418 - val_mae: 26.3407\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 600us/step - loss: 2685.9324 - mse: 2685.9326 - mae: 27.9291 - val_loss: 2138.7720 - val_mse: 2138.7717 - val_mae: 26.2310\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 567us/step - loss: 2639.1651 - mse: 2639.1643 - mae: 27.6882 - val_loss: 2138.4343 - val_mse: 2138.4346 - val_mae: 26.4772\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 694us/step - loss: 2625.5411 - mse: 2625.5408 - mae: 28.0157 - val_loss: 2139.3392 - val_mse: 2139.3394 - val_mae: 26.4371\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2667.0816 - mse: 2667.0808 - mae: 27.7905 - val_loss: 2148.7036 - val_mse: 2148.7036 - val_mae: 26.2638\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2724.2250 - mse: 2724.2249 - mae: 28.1926 - val_loss: 2148.0590 - val_mse: 2148.0588 - val_mae: 26.3870\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2675.4040 - mse: 2675.4041 - mae: 28.0134 - val_loss: 2144.3127 - val_mse: 2144.3125 - val_mae: 26.4294\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2658.3988 - mse: 2658.3989 - mae: 27.8962 - val_loss: 2139.8961 - val_mse: 2139.8962 - val_mae: 26.5180\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2613.7800 - mse: 2613.7800 - mae: 27.8617 - val_loss: 2145.7350 - val_mse: 2145.7356 - val_mae: 26.1807\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2676.0819 - mse: 2676.0815 - mae: 28.4865 - val_loss: 2150.1318 - val_mse: 2150.1318 - val_mae: 26.1797\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 690us/step - loss: 2657.1758 - mse: 2657.1753 - mae: 27.9352 - val_loss: 2153.1726 - val_mse: 2153.1724 - val_mae: 26.0838\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2649.2127 - mse: 2649.2124 - mae: 27.6459 - val_loss: 2143.8881 - val_mse: 2143.8882 - val_mae: 26.5220\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 680us/step - loss: 2632.6850 - mse: 2632.6848 - mae: 27.7904 - val_loss: 2143.4783 - val_mse: 2143.4788 - val_mae: 26.3468\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2627.0041 - mse: 2627.0037 - mae: 27.8495 - val_loss: 2153.8588 - val_mse: 2153.8586 - val_mae: 26.3941\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2655.8661 - mse: 2655.8665 - mae: 27.6537 - val_loss: 2138.0321 - val_mse: 2138.0320 - val_mae: 26.5580\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2644.9220 - mse: 2644.9229 - mae: 27.7324 - val_loss: 2145.3803 - val_mse: 2145.3804 - val_mae: 26.5156\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2632.1983 - mse: 2632.1982 - mae: 27.7013 - val_loss: 2162.0286 - val_mse: 2162.0286 - val_mae: 26.3070\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 630us/step - loss: 2637.4723 - mse: 2637.4717 - mae: 27.7201 - val_loss: 2148.3504 - val_mse: 2148.3499 - val_mae: 26.4357\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2656.1769 - mse: 2656.1770 - mae: 28.3600 - val_loss: 2145.9368 - val_mse: 2145.9370 - val_mae: 26.5155\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 553us/step - loss: 2660.0654 - mse: 2660.0662 - mae: 27.9559 - val_loss: 2146.3415 - val_mse: 2146.3418 - val_mae: 26.3534\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 544us/step - loss: 2667.3828 - mse: 2667.3828 - mae: 27.7713 - val_loss: 2156.9690 - val_mse: 2156.9690 - val_mae: 26.1018\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2634.0614 - mse: 2634.0618 - mae: 28.0162 - val_loss: 2159.4226 - val_mse: 2159.4226 - val_mae: 25.8443\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 563us/step - loss: 2671.6330 - mse: 2671.6333 - mae: 27.8324 - val_loss: 2138.9293 - val_mse: 2138.9294 - val_mae: 26.3383\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 548us/step - loss: 2618.9409 - mse: 2618.9404 - mae: 27.6087 - val_loss: 2134.3234 - val_mse: 2134.3230 - val_mae: 26.5496\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 645us/step - loss: 2649.8291 - mse: 2649.8291 - mae: 27.7243 - val_loss: 2143.4951 - val_mse: 2143.4951 - val_mae: 26.3583\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 566us/step - loss: 2638.9225 - mse: 2638.9229 - mae: 27.6173 - val_loss: 2139.7270 - val_mse: 2139.7266 - val_mae: 26.2119\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 659us/step - loss: 2592.4260 - mse: 2592.4260 - mae: 27.7351 - val_loss: 2138.0513 - val_mse: 2138.0513 - val_mae: 26.1857\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2617.5736 - mse: 2617.5732 - mae: 27.8265 - val_loss: 2134.8635 - val_mse: 2134.8635 - val_mae: 26.1541\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2684.9515 - mse: 2684.9509 - mae: 28.0856 - val_loss: 2123.9070 - val_mse: 2123.9067 - val_mae: 26.2766\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2603.5065 - mse: 2603.5071 - mae: 27.5681 - val_loss: 2121.1644 - val_mse: 2121.1643 - val_mae: 26.3713\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2654.2767 - mse: 2654.2764 - mae: 27.9605 - val_loss: 2132.3254 - val_mse: 2132.3252 - val_mae: 26.2230\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2662.9896 - mse: 2662.9897 - mae: 28.1970 - val_loss: 2134.4671 - val_mse: 2134.4673 - val_mae: 26.2791\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2588.8282 - mse: 2588.8281 - mae: 27.4196 - val_loss: 2133.6896 - val_mse: 2133.6895 - val_mae: 26.5079\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 666us/step - loss: 2641.6936 - mse: 2641.6941 - mae: 27.7897 - val_loss: 2148.3513 - val_mse: 2148.3513 - val_mae: 26.1025\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2683.8527 - mse: 2683.8525 - mae: 27.7393 - val_loss: 2135.5285 - val_mse: 2135.5286 - val_mae: 26.4341\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2649.9244 - mse: 2649.9250 - mae: 27.9700 - val_loss: 2130.3650 - val_mse: 2130.3650 - val_mae: 26.3583\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2654.9319 - mse: 2654.9316 - mae: 27.7583 - val_loss: 2129.2841 - val_mse: 2129.2839 - val_mae: 26.3729\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2623.9157 - mse: 2623.9163 - mae: 27.8833 - val_loss: 2123.9512 - val_mse: 2123.9514 - val_mae: 26.5867\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2677.7236 - mse: 2677.7239 - mae: 27.7822 - val_loss: 2138.8862 - val_mse: 2138.8862 - val_mae: 26.2952\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 607us/step - loss: 2636.9270 - mse: 2636.9275 - mae: 28.0899 - val_loss: 2137.9238 - val_mse: 2137.9238 - val_mae: 26.3566\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2653.2088 - mse: 2653.2083 - mae: 27.9630 - val_loss: 2145.2432 - val_mse: 2145.2432 - val_mae: 26.4141\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2569.2404 - mse: 2569.2395 - mae: 27.2743 - val_loss: 2142.8077 - val_mse: 2142.8079 - val_mae: 26.1457\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2620.8781 - mse: 2620.8779 - mae: 27.5173 - val_loss: 2138.3250 - val_mse: 2138.3250 - val_mae: 26.3107\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2667.6568 - mse: 2667.6570 - mae: 27.9103 - val_loss: 2128.3017 - val_mse: 2128.3015 - val_mae: 26.4134\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 649us/step - loss: 2654.4408 - mse: 2654.4419 - mae: 27.6195 - val_loss: 2141.5108 - val_mse: 2141.5112 - val_mae: 26.0034\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2621.3413 - mse: 2621.3416 - mae: 27.8188 - val_loss: 2118.8199 - val_mse: 2118.8198 - val_mae: 26.5081\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2627.1312 - mse: 2627.1313 - mae: 27.5232 - val_loss: 2130.9190 - val_mse: 2130.9189 - val_mae: 25.9693\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 561us/step - loss: 2609.5669 - mse: 2609.5674 - mae: 27.6022 - val_loss: 2127.3662 - val_mse: 2127.3662 - val_mae: 26.1748\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 526us/step - loss: 2620.7825 - mse: 2620.7817 - mae: 27.8570 - val_loss: 2128.0914 - val_mse: 2128.0911 - val_mae: 26.3164\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 606us/step - loss: 2608.1904 - mse: 2608.1904 - mae: 27.6415 - val_loss: 2132.6450 - val_mse: 2132.6450 - val_mae: 26.1296\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2622.7147 - mse: 2622.7148 - mae: 27.9620 - val_loss: 2137.3811 - val_mse: 2137.3811 - val_mae: 26.3239\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2684.1394 - mse: 2684.1392 - mae: 28.2744 - val_loss: 2130.7928 - val_mse: 2130.7925 - val_mae: 26.4377\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 564us/step - loss: 2595.2966 - mse: 2595.2976 - mae: 27.6381 - val_loss: 2136.5614 - val_mse: 2136.5613 - val_mae: 26.2440\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2602.4947 - mse: 2602.4944 - mae: 27.3786 - val_loss: 2120.0922 - val_mse: 2120.0923 - val_mae: 26.3935\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2578.7860 - mse: 2578.7861 - mae: 27.4259 - val_loss: 2121.6952 - val_mse: 2121.6956 - val_mae: 26.2575\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2595.4457 - mse: 2595.4458 - mae: 27.3138 - val_loss: 2107.5234 - val_mse: 2107.5232 - val_mae: 26.5997\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2646.2190 - mse: 2646.2180 - mae: 27.6869 - val_loss: 2108.0443 - val_mse: 2108.0442 - val_mae: 26.4511\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 560us/step - loss: 2613.1816 - mse: 2613.1807 - mae: 27.6649 - val_loss: 2109.5096 - val_mse: 2109.5095 - val_mae: 26.5167\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2628.0957 - mse: 2628.0950 - mae: 27.6659 - val_loss: 2103.9928 - val_mse: 2103.9929 - val_mae: 26.7006\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 580us/step - loss: 2597.9304 - mse: 2597.9299 - mae: 27.9775 - val_loss: 2109.3485 - val_mse: 2109.3481 - val_mae: 26.6337\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 556us/step - loss: 2574.2202 - mse: 2574.2200 - mae: 27.2688 - val_loss: 2112.8613 - val_mse: 2112.8613 - val_mae: 26.3119\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2646.9627 - mse: 2646.9626 - mae: 27.5633 - val_loss: 2131.6106 - val_mse: 2131.6106 - val_mae: 26.2091\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2607.7835 - mse: 2607.7842 - mae: 27.5294 - val_loss: 2124.2980 - val_mse: 2124.2981 - val_mae: 26.3814\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2653.0831 - mse: 2653.0830 - mae: 27.6710 - val_loss: 2127.3095 - val_mse: 2127.3096 - val_mae: 26.2254\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2621.9058 - mse: 2621.9050 - mae: 27.7249 - val_loss: 2116.4626 - val_mse: 2116.4631 - val_mae: 26.6237\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 602us/step - loss: 2626.2717 - mse: 2626.2727 - mae: 27.9932 - val_loss: 2121.3859 - val_mse: 2121.3860 - val_mae: 26.4751\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 593us/step - loss: 2624.7652 - mse: 2624.7659 - mae: 27.7433 - val_loss: 2126.4113 - val_mse: 2126.4111 - val_mae: 26.2565\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2602.8920 - mse: 2602.8926 - mae: 27.6879 - val_loss: 2111.1505 - val_mse: 2111.1506 - val_mae: 26.7999\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2620.4992 - mse: 2620.4993 - mae: 27.6869 - val_loss: 2116.7833 - val_mse: 2116.7832 - val_mae: 26.5947\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 521us/step - loss: 2583.5168 - mse: 2583.5171 - mae: 27.5183 - val_loss: 2118.8524 - val_mse: 2118.8521 - val_mae: 26.3508\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 13276.3843 - mse: 13276.3848 - mae: 109.6691 - val_loss: 34491.4159 - val_mse: 34491.4141 - val_mae: 132.2448\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 655us/step - loss: 12975.6885 - mse: 12975.6904 - mae: 108.2866 - val_loss: 33855.6834 - val_mse: 33855.6797 - val_mae: 129.8099\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 12081.0813 - mse: 12081.0801 - mae: 104.0298 - val_loss: 32032.3779 - val_mse: 32032.3789 - val_mae: 122.5612\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 605us/step - loss: 9682.9708 - mse: 9682.9707 - mae: 91.4118 - val_loss: 27434.5400 - val_mse: 27434.5391 - val_mae: 102.0185\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 625us/step - loss: 5549.2341 - mse: 5549.2334 - mae: 64.0519 - val_loss: 20198.0297 - val_mse: 20198.0293 - val_mae: 55.9815\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 605us/step - loss: 2988.0311 - mse: 2988.0312 - mae: 39.9005 - val_loss: 17580.4093 - val_mse: 17580.4082 - val_mae: 37.9000\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 2784.9780 - mse: 2784.9783 - mae: 38.2663 - val_loss: 18075.3434 - val_mse: 18075.3438 - val_mae: 38.5385\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2528.7491 - mse: 2528.7493 - mae: 35.9139 - val_loss: 17938.0008 - val_mse: 17938.0000 - val_mae: 38.0224\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2826.0806 - mse: 2826.0808 - mae: 38.3996 - val_loss: 17902.0964 - val_mse: 17902.0938 - val_mae: 37.9084\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 572us/step - loss: 2803.5502 - mse: 2803.5498 - mae: 38.5832 - val_loss: 18208.1625 - val_mse: 18208.1621 - val_mae: 39.1436\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 773us/step - loss: 2707.3010 - mse: 2707.3015 - mae: 37.6544 - val_loss: 17789.1924 - val_mse: 17789.1914 - val_mae: 37.6623\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2875.9436 - mse: 2875.9438 - mae: 38.3198 - val_loss: 17819.6120 - val_mse: 17819.6094 - val_mae: 37.7110\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 733us/step - loss: 2741.2596 - mse: 2741.2600 - mae: 38.1668 - val_loss: 17734.3671 - val_mse: 17734.3652 - val_mae: 37.5850\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 644us/step - loss: 2664.1975 - mse: 2664.1975 - mae: 37.2620 - val_loss: 17875.6517 - val_mse: 17875.6504 - val_mae: 37.8094\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 668us/step - loss: 2646.7910 - mse: 2646.7910 - mae: 37.0207 - val_loss: 18102.1795 - val_mse: 18102.1797 - val_mae: 38.5795\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 723us/step - loss: 2706.9300 - mse: 2706.9302 - mae: 37.7658 - val_loss: 17775.4019 - val_mse: 17775.4004 - val_mae: 37.5854\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 757us/step - loss: 2655.5467 - mse: 2655.5464 - mae: 37.7099 - val_loss: 17931.1874 - val_mse: 17931.1875 - val_mae: 37.8558\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 2737.7750 - mse: 2737.7749 - mae: 37.1454 - val_loss: 17887.1732 - val_mse: 17887.1719 - val_mae: 37.7298\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 680us/step - loss: 2451.5278 - mse: 2451.5276 - mae: 36.1874 - val_loss: 17957.1920 - val_mse: 17957.1914 - val_mae: 37.9219\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 677us/step - loss: 2721.1141 - mse: 2721.1140 - mae: 36.9863 - val_loss: 17959.8122 - val_mse: 17959.8125 - val_mae: 37.9342\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 584us/step - loss: 2689.4591 - mse: 2689.4590 - mae: 37.1354 - val_loss: 17755.7332 - val_mse: 17755.7344 - val_mae: 37.4920\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 573us/step - loss: 2518.2879 - mse: 2518.2881 - mae: 35.1026 - val_loss: 18124.2771 - val_mse: 18124.2773 - val_mae: 38.6078\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 680us/step - loss: 2604.3322 - mse: 2604.3325 - mae: 36.4992 - val_loss: 17880.3993 - val_mse: 17880.4004 - val_mae: 37.6599\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 570us/step - loss: 2601.9831 - mse: 2601.9829 - mae: 36.1908 - val_loss: 17938.5760 - val_mse: 17938.5762 - val_mae: 37.7857\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 2341.9425 - mse: 2341.9424 - mae: 34.4513 - val_loss: 17839.6231 - val_mse: 17839.6230 - val_mae: 37.5068\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 2332.8151 - mse: 2332.8147 - mae: 34.7869 - val_loss: 17781.3599 - val_mse: 17781.3574 - val_mae: 37.3916\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 604us/step - loss: 2634.8017 - mse: 2634.8018 - mae: 36.4309 - val_loss: 17913.2868 - val_mse: 17913.2852 - val_mae: 37.6783\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 522us/step - loss: 2472.5463 - mse: 2472.5459 - mae: 35.0604 - val_loss: 18125.5488 - val_mse: 18125.5488 - val_mae: 38.5977\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2450.5604 - mse: 2450.5603 - mae: 34.7224 - val_loss: 17824.0268 - val_mse: 17824.0254 - val_mae: 37.4509\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2518.9202 - mse: 2518.9202 - mae: 34.9632 - val_loss: 17913.0186 - val_mse: 17913.0195 - val_mae: 37.6513\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2381.5207 - mse: 2381.5208 - mae: 34.3888 - val_loss: 17721.6115 - val_mse: 17721.6113 - val_mae: 37.2921\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 602us/step - loss: 2408.4013 - mse: 2408.4016 - mae: 34.7862 - val_loss: 17617.5244 - val_mse: 17617.5234 - val_mae: 37.2556\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 689us/step - loss: 2219.1657 - mse: 2219.1658 - mae: 33.6043 - val_loss: 17865.6956 - val_mse: 17865.6953 - val_mae: 37.4958\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 646us/step - loss: 2369.6661 - mse: 2369.6660 - mae: 34.4495 - val_loss: 17824.7785 - val_mse: 17824.7773 - val_mae: 37.4109\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 538us/step - loss: 2247.5438 - mse: 2247.5435 - mae: 32.6159 - val_loss: 17732.2114 - val_mse: 17732.2109 - val_mae: 37.2896\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 633us/step - loss: 2316.0432 - mse: 2316.0432 - mae: 34.4271 - val_loss: 17857.8447 - val_mse: 17857.8457 - val_mae: 37.4860\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 713us/step - loss: 2401.4633 - mse: 2401.4631 - mae: 34.2674 - val_loss: 18019.8839 - val_mse: 18019.8848 - val_mae: 38.1301\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 656us/step - loss: 2382.0765 - mse: 2382.0767 - mae: 34.7689 - val_loss: 17866.8525 - val_mse: 17866.8516 - val_mae: 37.4852\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 2416.0341 - mse: 2416.0339 - mae: 34.6345 - val_loss: 17650.8638 - val_mse: 17650.8633 - val_mae: 37.1553\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 730us/step - loss: 2284.7766 - mse: 2284.7766 - mae: 34.2353 - val_loss: 17767.0229 - val_mse: 17767.0215 - val_mae: 37.2356\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 687us/step - loss: 2161.2247 - mse: 2161.2246 - mae: 32.4538 - val_loss: 17613.9670 - val_mse: 17613.9688 - val_mae: 37.0816\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 683us/step - loss: 2197.7922 - mse: 2197.7925 - mae: 33.2147 - val_loss: 17808.1848 - val_mse: 17808.1855 - val_mae: 37.2462\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 2199.9880 - mse: 2199.9878 - mae: 33.0374 - val_loss: 17668.5739 - val_mse: 17668.5762 - val_mae: 37.0635\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 754us/step - loss: 2097.9035 - mse: 2097.9036 - mae: 32.4794 - val_loss: 17751.8331 - val_mse: 17751.8320 - val_mae: 37.1427\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2205.8772 - mse: 2205.8772 - mae: 32.8792 - val_loss: 17715.9134 - val_mse: 17715.9141 - val_mae: 37.0910\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 554us/step - loss: 2324.8186 - mse: 2324.8188 - mae: 33.8500 - val_loss: 17649.8499 - val_mse: 17649.8496 - val_mae: 36.9909\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 569us/step - loss: 1956.9774 - mse: 1956.9775 - mae: 30.2578 - val_loss: 17670.3267 - val_mse: 17670.3262 - val_mae: 36.9993\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 612us/step - loss: 2116.7718 - mse: 2116.7717 - mae: 32.6042 - val_loss: 17674.2689 - val_mse: 17674.2676 - val_mae: 37.0149\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2249.7269 - mse: 2249.7271 - mae: 33.2412 - val_loss: 17771.5289 - val_mse: 17771.5273 - val_mae: 37.1374\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 664us/step - loss: 2268.3006 - mse: 2268.3005 - mae: 32.9278 - val_loss: 17693.1766 - val_mse: 17693.1777 - val_mae: 37.0346\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 2242.5143 - mse: 2242.5144 - mae: 33.0595 - val_loss: 17785.5167 - val_mse: 17785.5176 - val_mae: 37.1512\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2224.6032 - mse: 2224.6035 - mae: 33.1762 - val_loss: 17672.4652 - val_mse: 17672.4668 - val_mae: 37.0053\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 561us/step - loss: 2109.7327 - mse: 2109.7327 - mae: 31.8881 - val_loss: 17705.0198 - val_mse: 17705.0195 - val_mae: 37.0485\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 2167.6900 - mse: 2167.6902 - mae: 32.6294 - val_loss: 17777.6212 - val_mse: 17777.6211 - val_mae: 37.1378\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 558us/step - loss: 2255.2170 - mse: 2255.2170 - mae: 33.8680 - val_loss: 17783.3097 - val_mse: 17783.3105 - val_mae: 37.1629\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2241.0592 - mse: 2241.0593 - mae: 33.3230 - val_loss: 17704.5788 - val_mse: 17704.5781 - val_mae: 37.0170\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 613us/step - loss: 2100.6046 - mse: 2100.6042 - mae: 31.2882 - val_loss: 17486.9962 - val_mse: 17486.9961 - val_mae: 36.9045\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 662us/step - loss: 2277.4363 - mse: 2277.4365 - mae: 32.5390 - val_loss: 17824.5399 - val_mse: 17824.5391 - val_mae: 37.2842\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 713us/step - loss: 2233.2864 - mse: 2233.2866 - mae: 32.2606 - val_loss: 17710.5274 - val_mse: 17710.5254 - val_mae: 36.9743\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 2092.9144 - mse: 2092.9143 - mae: 31.7828 - val_loss: 17541.2552 - val_mse: 17541.2539 - val_mae: 36.8575\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 596us/step - loss: 2212.2417 - mse: 2212.2415 - mae: 32.6317 - val_loss: 17643.9014 - val_mse: 17643.9023 - val_mae: 36.9240\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 600us/step - loss: 2157.5541 - mse: 2157.5540 - mae: 32.7164 - val_loss: 17724.1135 - val_mse: 17724.1133 - val_mae: 37.0297\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 599us/step - loss: 1948.7487 - mse: 1948.7488 - mae: 30.1103 - val_loss: 17522.6972 - val_mse: 17522.6973 - val_mae: 36.8644\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2089.8068 - mse: 2089.8069 - mae: 31.9706 - val_loss: 17722.6547 - val_mse: 17722.6562 - val_mae: 37.0257\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 477us/step - loss: 2120.3089 - mse: 2120.3091 - mae: 31.9432 - val_loss: 17688.4167 - val_mse: 17688.4160 - val_mae: 37.0020\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 595us/step - loss: 2087.2651 - mse: 2087.2649 - mae: 31.8381 - val_loss: 17701.0704 - val_mse: 17701.0703 - val_mae: 37.0183\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 591us/step - loss: 1906.4333 - mse: 1906.4335 - mae: 31.4393 - val_loss: 17664.5610 - val_mse: 17664.5625 - val_mae: 36.9344\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 589us/step - loss: 2012.9619 - mse: 2012.9619 - mae: 30.7288 - val_loss: 17746.5319 - val_mse: 17746.5312 - val_mae: 37.1212\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 2354.8210 - mse: 2354.8208 - mae: 33.7181 - val_loss: 17857.4531 - val_mse: 17857.4512 - val_mae: 37.6377\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 653us/step - loss: 1961.5846 - mse: 1961.5847 - mae: 30.3328 - val_loss: 17665.4026 - val_mse: 17665.4023 - val_mae: 36.9142\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 2075.4879 - mse: 2075.4880 - mae: 31.7741 - val_loss: 17758.8209 - val_mse: 17758.8203 - val_mae: 37.1920\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 624us/step - loss: 2039.1796 - mse: 2039.1796 - mae: 31.9020 - val_loss: 17749.1712 - val_mse: 17749.1699 - val_mae: 37.1627\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 669us/step - loss: 1913.3601 - mse: 1913.3600 - mae: 30.1254 - val_loss: 17456.9701 - val_mse: 17456.9688 - val_mae: 36.7593\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 748us/step - loss: 1921.9718 - mse: 1921.9718 - mae: 30.1586 - val_loss: 17690.2988 - val_mse: 17690.3008 - val_mae: 37.0157\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 619us/step - loss: 1751.7652 - mse: 1751.7653 - mae: 29.4843 - val_loss: 17458.2090 - val_mse: 17458.2109 - val_mae: 36.7582\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 562us/step - loss: 2149.0899 - mse: 2149.0901 - mae: 32.2906 - val_loss: 17783.6531 - val_mse: 17783.6543 - val_mae: 37.3893\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 2015.9385 - mse: 2015.9384 - mae: 29.5420 - val_loss: 17612.2751 - val_mse: 17612.2734 - val_mae: 36.8495\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 1830.2960 - mse: 1830.2960 - mae: 29.1597 - val_loss: 17585.9571 - val_mse: 17585.9570 - val_mae: 36.8425\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 693us/step - loss: 2032.4375 - mse: 2032.4375 - mae: 31.6222 - val_loss: 17703.4796 - val_mse: 17703.4785 - val_mae: 37.1080\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 722us/step - loss: 1874.2786 - mse: 1874.2784 - mae: 29.3310 - val_loss: 17549.7052 - val_mse: 17549.7031 - val_mae: 36.8217\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4353.0749 - mse: 4353.0747 - mae: 35.5725 - val_loss: 2109.9109 - val_mse: 2109.9109 - val_mae: 30.0079\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 4350.0985 - mse: 4350.0986 - mae: 36.4160 - val_loss: 2243.5162 - val_mse: 2243.5166 - val_mae: 30.5045\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 694us/step - loss: 4241.2520 - mse: 4241.2515 - mae: 34.9666 - val_loss: 2258.0291 - val_mse: 2258.0291 - val_mae: 30.5685\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 639us/step - loss: 4088.6747 - mse: 4088.6748 - mae: 35.5153 - val_loss: 2191.8525 - val_mse: 2191.8525 - val_mae: 30.3206\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 633us/step - loss: 4319.4783 - mse: 4319.4780 - mae: 35.7681 - val_loss: 2250.5289 - val_mse: 2250.5291 - val_mae: 30.5378\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 695us/step - loss: 4437.9039 - mse: 4437.9038 - mae: 35.8729 - val_loss: 2293.8819 - val_mse: 2293.8818 - val_mae: 30.7183\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 693us/step - loss: 4080.9411 - mse: 4080.9409 - mae: 33.8103 - val_loss: 2248.5271 - val_mse: 2248.5273 - val_mae: 30.5350\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 697us/step - loss: 4390.8360 - mse: 4390.8359 - mae: 35.4329 - val_loss: 2418.5566 - val_mse: 2418.5569 - val_mae: 31.3023\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 1s 674us/step - loss: 4008.1530 - mse: 4008.1526 - mae: 34.6881 - val_loss: 2234.5600 - val_mse: 2234.5601 - val_mae: 30.4863\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 640us/step - loss: 4292.9171 - mse: 4292.9175 - mae: 35.0228 - val_loss: 2327.2202 - val_mse: 2327.2200 - val_mae: 30.8591\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 739us/step - loss: 4321.3384 - mse: 4321.3389 - mae: 35.1260 - val_loss: 2321.3805 - val_mse: 2321.3809 - val_mae: 30.8397\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4132.9041 - mse: 4132.9048 - mae: 34.8051 - val_loss: 2287.6043 - val_mse: 2287.6042 - val_mae: 30.6911\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 4115.4565 - mse: 4115.4556 - mae: 34.0332 - val_loss: 2350.8376 - val_mse: 2350.8376 - val_mae: 30.9922\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 671us/step - loss: 4171.5506 - mse: 4171.5503 - mae: 34.7391 - val_loss: 2228.8777 - val_mse: 2228.8777 - val_mae: 30.4563\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 687us/step - loss: 4137.7616 - mse: 4137.7622 - mae: 34.1612 - val_loss: 2240.7038 - val_mse: 2240.7039 - val_mae: 30.5115\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 629us/step - loss: 4222.9098 - mse: 4222.9092 - mae: 35.2646 - val_loss: 2283.0185 - val_mse: 2283.0183 - val_mae: 30.6882\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 643us/step - loss: 4203.2685 - mse: 4203.2690 - mae: 34.3359 - val_loss: 2292.1101 - val_mse: 2292.1104 - val_mae: 30.7280\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4096.3041 - mse: 4096.3042 - mae: 34.4964 - val_loss: 2337.3496 - val_mse: 2337.3496 - val_mae: 30.9573\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 594us/step - loss: 4189.9395 - mse: 4189.9399 - mae: 34.9532 - val_loss: 2290.1492 - val_mse: 2290.1492 - val_mae: 30.7263\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4388.8488 - mse: 4388.8486 - mae: 36.4568 - val_loss: 2374.8193 - val_mse: 2374.8193 - val_mae: 31.1602\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 732us/step - loss: 4076.7933 - mse: 4076.7927 - mae: 34.1015 - val_loss: 2249.2471 - val_mse: 2249.2473 - val_mae: 30.5621\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 3962.1709 - mse: 3962.1707 - mae: 34.7481 - val_loss: 2308.8599 - val_mse: 2308.8599 - val_mae: 30.8297\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 670us/step - loss: 4124.7091 - mse: 4124.7090 - mae: 34.7674 - val_loss: 2293.6689 - val_mse: 2293.6689 - val_mae: 30.7591\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4029.7245 - mse: 4029.7244 - mae: 34.8606 - val_loss: 2306.3290 - val_mse: 2306.3289 - val_mae: 30.8244\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 4073.1961 - mse: 4073.1963 - mae: 33.9928 - val_loss: 2261.8160 - val_mse: 2261.8159 - val_mae: 30.6107\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 578us/step - loss: 4209.8417 - mse: 4209.8418 - mae: 35.3061 - val_loss: 2334.1701 - val_mse: 2334.1699 - val_mae: 30.9767\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4115.5630 - mse: 4115.5635 - mae: 34.4527 - val_loss: 2307.2113 - val_mse: 2307.2114 - val_mae: 30.8363\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 593us/step - loss: 4078.3838 - mse: 4078.3838 - mae: 34.0727 - val_loss: 2328.0859 - val_mse: 2328.0857 - val_mae: 30.9399\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 615us/step - loss: 4105.1423 - mse: 4105.1421 - mae: 33.5150 - val_loss: 2294.8430 - val_mse: 2294.8430 - val_mae: 30.7658\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 520us/step - loss: 4086.8900 - mse: 4086.8896 - mae: 34.4087 - val_loss: 2280.2176 - val_mse: 2280.2175 - val_mae: 30.7010\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 651us/step - loss: 4005.0693 - mse: 4005.0698 - mae: 33.5022 - val_loss: 2292.4258 - val_mse: 2292.4258 - val_mae: 30.7605\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 692us/step - loss: 4018.0909 - mse: 4018.0908 - mae: 33.4446 - val_loss: 2277.4051 - val_mse: 2277.4050 - val_mae: 30.6835\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 4185.4625 - mse: 4185.4629 - mae: 33.9100 - val_loss: 2333.1399 - val_mse: 2333.1396 - val_mae: 30.9768\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 0s 469us/step - loss: 4048.5843 - mse: 4048.5840 - mae: 34.3630 - val_loss: 2319.1102 - val_mse: 2319.1104 - val_mae: 30.8932\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 524us/step - loss: 4121.9016 - mse: 4121.9014 - mae: 33.9510 - val_loss: 2333.1922 - val_mse: 2333.1921 - val_mae: 30.9581\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4178.5865 - mse: 4178.5869 - mae: 33.9381 - val_loss: 2304.3648 - val_mse: 2304.3650 - val_mae: 30.8129\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 648us/step - loss: 4004.1444 - mse: 4004.1443 - mae: 34.2140 - val_loss: 2239.6861 - val_mse: 2239.6860 - val_mae: 30.4947\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 696us/step - loss: 4154.2101 - mse: 4154.2104 - mae: 33.8340 - val_loss: 2293.2540 - val_mse: 2293.2539 - val_mae: 30.7701\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 543us/step - loss: 4149.2044 - mse: 4149.2046 - mae: 33.4273 - val_loss: 2240.6180 - val_mse: 2240.6179 - val_mae: 30.5024\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 4098.0262 - mse: 4098.0264 - mae: 33.8125 - val_loss: 2341.4660 - val_mse: 2341.4661 - val_mae: 31.0303\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4124.8109 - mse: 4124.8115 - mae: 33.7439 - val_loss: 2278.1403 - val_mse: 2278.1401 - val_mae: 30.7072\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 3828.3605 - mse: 3828.3594 - mae: 33.7347 - val_loss: 2259.5366 - val_mse: 2259.5361 - val_mae: 30.5976\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 592us/step - loss: 4027.8719 - mse: 4027.8716 - mae: 34.0105 - val_loss: 2315.8284 - val_mse: 2315.8284 - val_mae: 30.8964\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 536us/step - loss: 3905.1182 - mse: 3905.1177 - mae: 33.0497 - val_loss: 2336.1357 - val_mse: 2336.1357 - val_mae: 31.0100\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4090.5698 - mse: 4090.5696 - mae: 33.7531 - val_loss: 2325.9355 - val_mse: 2325.9353 - val_mae: 30.9617\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 3999.0246 - mse: 3999.0247 - mae: 33.5284 - val_loss: 2383.0262 - val_mse: 2383.0264 - val_mae: 31.2784\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 3952.8626 - mse: 3952.8625 - mae: 33.5796 - val_loss: 2288.8915 - val_mse: 2288.8916 - val_mae: 30.7638\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 659us/step - loss: 3982.1058 - mse: 3982.1062 - mae: 33.3601 - val_loss: 2357.8605 - val_mse: 2357.8604 - val_mae: 31.1547\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 550us/step - loss: 4130.3554 - mse: 4130.3550 - mae: 33.5325 - val_loss: 2345.5299 - val_mse: 2345.5300 - val_mae: 31.0891\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 3769.2996 - mse: 3769.2993 - mae: 32.2387 - val_loss: 2321.9435 - val_mse: 2321.9436 - val_mae: 30.9585\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4081.8411 - mse: 4081.8413 - mae: 33.1362 - val_loss: 2286.3382 - val_mse: 2286.3384 - val_mae: 30.7714\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 3905.8308 - mse: 3905.8303 - mae: 32.9216 - val_loss: 2266.2783 - val_mse: 2266.2786 - val_mae: 30.6912\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 548us/step - loss: 4106.8898 - mse: 4106.8906 - mae: 34.5125 - val_loss: 2379.7643 - val_mse: 2379.7642 - val_mae: 31.2954\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 683us/step - loss: 4019.4900 - mse: 4019.4900 - mae: 33.0706 - val_loss: 2356.5143 - val_mse: 2356.5142 - val_mae: 31.1658\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 3917.9470 - mse: 3917.9470 - mae: 33.1710 - val_loss: 2359.2781 - val_mse: 2359.2781 - val_mae: 31.1798\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 623us/step - loss: 4101.6899 - mse: 4101.6895 - mae: 33.0508 - val_loss: 2339.8709 - val_mse: 2339.8711 - val_mae: 31.0746\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4035.1917 - mse: 4035.1914 - mae: 33.7032 - val_loss: 2348.1267 - val_mse: 2348.1267 - val_mae: 31.1139\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 625us/step - loss: 3927.4319 - mse: 3927.4326 - mae: 32.7496 - val_loss: 2285.8890 - val_mse: 2285.8887 - val_mae: 30.7766\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 557us/step - loss: 4060.9597 - mse: 4060.9600 - mae: 33.9397 - val_loss: 2354.1700 - val_mse: 2354.1699 - val_mae: 31.1564\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 559us/step - loss: 4125.2122 - mse: 4125.2119 - mae: 34.2633 - val_loss: 2415.1713 - val_mse: 2415.1714 - val_mae: 31.4839\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 3988.7831 - mse: 3988.7830 - mae: 32.8673 - val_loss: 2385.7120 - val_mse: 2385.7119 - val_mae: 31.3266\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 589us/step - loss: 4015.0970 - mse: 4015.0979 - mae: 32.9031 - val_loss: 2348.8345 - val_mse: 2348.8345 - val_mae: 31.1258\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4007.2355 - mse: 4007.2356 - mae: 33.7704 - val_loss: 2404.4129 - val_mse: 2404.4126 - val_mae: 31.4325\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 620us/step - loss: 4113.9478 - mse: 4113.9478 - mae: 34.2826 - val_loss: 2401.3530 - val_mse: 2401.3530 - val_mae: 31.4135\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 573us/step - loss: 3995.8280 - mse: 3995.8276 - mae: 33.1594 - val_loss: 2369.6179 - val_mse: 2369.6179 - val_mae: 31.2473\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 590us/step - loss: 4141.1595 - mse: 4141.1592 - mae: 33.8420 - val_loss: 2409.0427 - val_mse: 2409.0425 - val_mae: 31.4628\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 576us/step - loss: 4014.1876 - mse: 4014.1877 - mae: 32.6282 - val_loss: 2324.2137 - val_mse: 2324.2139 - val_mae: 31.0036\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 670us/step - loss: 4070.6185 - mse: 4070.6189 - mae: 33.7257 - val_loss: 2377.4963 - val_mse: 2377.4963 - val_mae: 31.2975\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 4088.7832 - mse: 4088.7837 - mae: 33.6100 - val_loss: 2410.4364 - val_mse: 2410.4365 - val_mae: 31.4909\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 570us/step - loss: 4113.0242 - mse: 4113.0239 - mae: 32.5760 - val_loss: 2360.2395 - val_mse: 2360.2393 - val_mae: 31.2134\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 3937.2106 - mse: 3937.2107 - mae: 31.9345 - val_loss: 2345.1382 - val_mse: 2345.1382 - val_mae: 31.1157\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 554us/step - loss: 4092.0346 - mse: 4092.0349 - mae: 33.4487 - val_loss: 2379.1699 - val_mse: 2379.1697 - val_mae: 31.3046\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 515us/step - loss: 4052.5859 - mse: 4052.5859 - mae: 32.8511 - val_loss: 2344.8540 - val_mse: 2344.8540 - val_mae: 31.1046\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 3927.7811 - mse: 3927.7808 - mae: 33.2813 - val_loss: 2369.7984 - val_mse: 2369.7986 - val_mae: 31.2460\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 619us/step - loss: 4135.8141 - mse: 4135.8135 - mae: 33.0495 - val_loss: 2428.6081 - val_mse: 2428.6084 - val_mae: 31.5720\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 598us/step - loss: 3654.3435 - mse: 3654.3435 - mae: 32.1211 - val_loss: 2279.1739 - val_mse: 2279.1741 - val_mae: 30.7884\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 657us/step - loss: 4054.3518 - mse: 4054.3521 - mae: 33.4518 - val_loss: 2380.9157 - val_mse: 2380.9158 - val_mae: 31.3200\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 3922.4166 - mse: 3922.4170 - mae: 32.4619 - val_loss: 2376.8111 - val_mse: 2376.8113 - val_mae: 31.2974\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 617us/step - loss: 3770.0762 - mse: 3770.0762 - mae: 32.9969 - val_loss: 2372.2616 - val_mse: 2372.2617 - val_mae: 31.2687\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 618us/step - loss: 3930.0806 - mse: 3930.0811 - mae: 32.5422 - val_loss: 2386.3439 - val_mse: 2386.3438 - val_mae: 31.3398\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3309.7680 - mse: 3309.7686 - mae: 32.7730 - val_loss: 1466.1852 - val_mse: 1466.1852 - val_mae: 24.0672\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3454.7088 - mse: 3454.7087 - mae: 32.6799 - val_loss: 1456.9056 - val_mse: 1456.9054 - val_mae: 24.3222\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3255.3379 - mse: 3255.3384 - mae: 32.1933 - val_loss: 1449.2460 - val_mse: 1449.2460 - val_mae: 24.6097\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3324.9064 - mse: 3324.9062 - mae: 33.1536 - val_loss: 1457.4122 - val_mse: 1457.4122 - val_mae: 24.2619\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3377.1178 - mse: 3377.1182 - mae: 32.4969 - val_loss: 1452.3499 - val_mse: 1452.3502 - val_mae: 24.4707\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3238.6465 - mse: 3238.6465 - mae: 32.2560 - val_loss: 1454.4744 - val_mse: 1454.4745 - val_mae: 24.3947\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 622us/step - loss: 3249.2038 - mse: 3249.2046 - mae: 31.9326 - val_loss: 1451.5838 - val_mse: 1451.5837 - val_mae: 24.6160\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3334.1336 - mse: 3334.1338 - mae: 32.4134 - val_loss: 1451.6806 - val_mse: 1451.6807 - val_mae: 24.6356\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3406.9644 - mse: 3406.9646 - mae: 32.8685 - val_loss: 1453.6203 - val_mse: 1453.6201 - val_mae: 24.5826\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3303.0797 - mse: 3303.0798 - mae: 32.5929 - val_loss: 1451.3306 - val_mse: 1451.3306 - val_mae: 24.6954\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3354.5915 - mse: 3354.5916 - mae: 32.3184 - val_loss: 1456.0467 - val_mse: 1456.0466 - val_mae: 24.5160\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 672us/step - loss: 3285.5988 - mse: 3285.5996 - mae: 32.5079 - val_loss: 1449.5362 - val_mse: 1449.5363 - val_mae: 24.8864\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 661us/step - loss: 3526.1021 - mse: 3526.1016 - mae: 33.6008 - val_loss: 1466.0776 - val_mse: 1466.0778 - val_mae: 24.2279\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3356.1940 - mse: 3356.1936 - mae: 32.8950 - val_loss: 1448.8715 - val_mse: 1448.8715 - val_mae: 24.9598\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 664us/step - loss: 3287.8276 - mse: 3287.8274 - mae: 32.5224 - val_loss: 1448.7398 - val_mse: 1448.7397 - val_mae: 24.9772\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 671us/step - loss: 3314.9446 - mse: 3314.9441 - mae: 32.2596 - val_loss: 1447.4365 - val_mse: 1447.4363 - val_mae: 24.9887\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3306.6056 - mse: 3306.6057 - mae: 32.3769 - val_loss: 1447.8379 - val_mse: 1447.8379 - val_mae: 24.9162\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3321.0368 - mse: 3321.0369 - mae: 32.6591 - val_loss: 1456.2828 - val_mse: 1456.2827 - val_mae: 24.4992\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3373.4977 - mse: 3373.4973 - mae: 32.6529 - val_loss: 1447.4379 - val_mse: 1447.4379 - val_mae: 24.9421\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 557us/step - loss: 3338.1586 - mse: 3338.1584 - mae: 32.7721 - val_loss: 1448.3562 - val_mse: 1448.3563 - val_mae: 24.8330\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 581us/step - loss: 3308.0901 - mse: 3308.0903 - mae: 32.3539 - val_loss: 1448.6183 - val_mse: 1448.6182 - val_mae: 24.8136\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3234.3988 - mse: 3234.3984 - mae: 32.7631 - val_loss: 1448.0470 - val_mse: 1448.0470 - val_mae: 24.7617\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 600us/step - loss: 3256.3718 - mse: 3256.3723 - mae: 32.0237 - val_loss: 1450.9156 - val_mse: 1450.9158 - val_mae: 24.6219\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 572us/step - loss: 3365.1270 - mse: 3365.1274 - mae: 32.4449 - val_loss: 1447.0133 - val_mse: 1447.0135 - val_mae: 24.8463\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3214.5500 - mse: 3214.5500 - mae: 31.4220 - val_loss: 1445.4500 - val_mse: 1445.4501 - val_mae: 25.1031\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 502us/step - loss: 3259.8645 - mse: 3259.8638 - mae: 32.4107 - val_loss: 1451.6297 - val_mse: 1451.6296 - val_mae: 24.6513\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 530us/step - loss: 3423.1454 - mse: 3423.1453 - mae: 32.8424 - val_loss: 1449.7136 - val_mse: 1449.7136 - val_mae: 24.7689\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3379.2184 - mse: 3379.2185 - mae: 32.5348 - val_loss: 1448.5786 - val_mse: 1448.5784 - val_mae: 24.7640\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 650us/step - loss: 3389.7123 - mse: 3389.7122 - mae: 32.8707 - val_loss: 1449.7474 - val_mse: 1449.7474 - val_mae: 24.6632\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 598us/step - loss: 3390.4820 - mse: 3390.4827 - mae: 32.2932 - val_loss: 1450.6379 - val_mse: 1450.6378 - val_mae: 24.6980\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 547us/step - loss: 3357.0762 - mse: 3357.0759 - mae: 33.2156 - val_loss: 1448.1664 - val_mse: 1448.1665 - val_mae: 24.8362\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3323.0885 - mse: 3323.0884 - mae: 32.6515 - val_loss: 1454.4080 - val_mse: 1454.4080 - val_mae: 24.4553\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3249.1576 - mse: 3249.1580 - mae: 32.2303 - val_loss: 1450.3986 - val_mse: 1450.3986 - val_mae: 24.6257\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 663us/step - loss: 3380.2397 - mse: 3380.2405 - mae: 33.0479 - val_loss: 1455.0517 - val_mse: 1455.0515 - val_mae: 24.3763\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 627us/step - loss: 3414.2933 - mse: 3414.2930 - mae: 32.3376 - val_loss: 1445.6795 - val_mse: 1445.6794 - val_mae: 24.7862\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 692us/step - loss: 3225.5529 - mse: 3225.5525 - mae: 32.0469 - val_loss: 1451.5168 - val_mse: 1451.5168 - val_mae: 24.4527\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 699us/step - loss: 3275.7050 - mse: 3275.7051 - mae: 32.0172 - val_loss: 1447.8625 - val_mse: 1447.8627 - val_mae: 24.6587\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3406.3859 - mse: 3406.3853 - mae: 32.7282 - val_loss: 1447.1126 - val_mse: 1447.1128 - val_mae: 24.6805\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 716us/step - loss: 3274.9410 - mse: 3274.9419 - mae: 32.2777 - val_loss: 1442.0130 - val_mse: 1442.0128 - val_mae: 25.1753\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 665us/step - loss: 3282.0557 - mse: 3282.0549 - mae: 31.8331 - val_loss: 1450.0123 - val_mse: 1450.0122 - val_mae: 24.5568\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 652us/step - loss: 3216.5364 - mse: 3216.5364 - mae: 32.4685 - val_loss: 1448.8750 - val_mse: 1448.8749 - val_mae: 24.7091\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3323.1427 - mse: 3323.1426 - mae: 32.4100 - val_loss: 1453.1222 - val_mse: 1453.1222 - val_mae: 24.5521\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 613us/step - loss: 3326.1538 - mse: 3326.1538 - mae: 32.6434 - val_loss: 1450.9850 - val_mse: 1450.9850 - val_mae: 24.6724\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 562us/step - loss: 3284.4853 - mse: 3284.4849 - mae: 31.1929 - val_loss: 1445.8493 - val_mse: 1445.8492 - val_mae: 25.2342\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 662us/step - loss: 3267.8292 - mse: 3267.8289 - mae: 32.1185 - val_loss: 1450.4486 - val_mse: 1450.4484 - val_mae: 24.7105\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 647us/step - loss: 3372.4521 - mse: 3372.4521 - mae: 33.0794 - val_loss: 1451.5885 - val_mse: 1451.5884 - val_mae: 24.6655\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3178.9669 - mse: 3178.9668 - mae: 31.2134 - val_loss: 1448.9202 - val_mse: 1448.9202 - val_mae: 24.8213\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 587us/step - loss: 3258.8638 - mse: 3258.8638 - mae: 31.7095 - val_loss: 1447.6358 - val_mse: 1447.6359 - val_mae: 24.8408\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 579us/step - loss: 3309.3011 - mse: 3309.3000 - mae: 31.6390 - val_loss: 1446.2264 - val_mse: 1446.2264 - val_mae: 24.8932\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3208.2888 - mse: 3208.2886 - mae: 31.7916 - val_loss: 1449.2709 - val_mse: 1449.2709 - val_mae: 24.6739\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 628us/step - loss: 3354.1178 - mse: 3354.1174 - mae: 32.0097 - val_loss: 1446.6602 - val_mse: 1446.6604 - val_mae: 24.8153\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 635us/step - loss: 3341.8333 - mse: 3341.8335 - mae: 31.7270 - val_loss: 1448.8375 - val_mse: 1448.8375 - val_mae: 24.6338\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3321.7317 - mse: 3321.7324 - mae: 32.2119 - val_loss: 1445.6344 - val_mse: 1445.6344 - val_mae: 24.8520\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 674us/step - loss: 3237.6456 - mse: 3237.6455 - mae: 31.5484 - val_loss: 1442.4569 - val_mse: 1442.4570 - val_mae: 24.9210\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3284.0501 - mse: 3284.0498 - mae: 31.6429 - val_loss: 1447.6575 - val_mse: 1447.6575 - val_mae: 24.5782\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 646us/step - loss: 3282.6849 - mse: 3282.6851 - mae: 31.1480 - val_loss: 1447.2656 - val_mse: 1447.2654 - val_mae: 24.5637\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 620us/step - loss: 3271.4231 - mse: 3271.4231 - mae: 31.8341 - val_loss: 1446.8899 - val_mse: 1446.8896 - val_mae: 24.6760\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3279.1063 - mse: 3279.1060 - mae: 31.9322 - val_loss: 1443.4747 - val_mse: 1443.4747 - val_mae: 24.8857\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 621us/step - loss: 3266.4053 - mse: 3266.4060 - mae: 32.2454 - val_loss: 1444.4465 - val_mse: 1444.4465 - val_mae: 24.7249\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 583us/step - loss: 3223.1856 - mse: 3223.1865 - mae: 32.1365 - val_loss: 1442.7297 - val_mse: 1442.7296 - val_mae: 24.9160\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3408.8027 - mse: 3408.8030 - mae: 32.6704 - val_loss: 1450.5140 - val_mse: 1450.5140 - val_mae: 24.4312\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3208.7418 - mse: 3208.7407 - mae: 32.1155 - val_loss: 1444.1223 - val_mse: 1444.1223 - val_mae: 24.8460\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 555us/step - loss: 3149.9470 - mse: 3149.9460 - mae: 30.4088 - val_loss: 1443.2891 - val_mse: 1443.2891 - val_mae: 24.9959\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 545us/step - loss: 3302.5776 - mse: 3302.5769 - mae: 32.2137 - val_loss: 1448.6091 - val_mse: 1448.6091 - val_mae: 24.5807\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 608us/step - loss: 3105.0495 - mse: 3105.0498 - mae: 31.3013 - val_loss: 1443.9456 - val_mse: 1443.9453 - val_mae: 24.9939\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 623us/step - loss: 3233.6658 - mse: 3233.6658 - mae: 30.9514 - val_loss: 1446.2546 - val_mse: 1446.2545 - val_mae: 24.7381\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 590us/step - loss: 3232.1635 - mse: 3232.1636 - mae: 31.3296 - val_loss: 1447.2728 - val_mse: 1447.2727 - val_mae: 24.6778\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 702us/step - loss: 3347.6943 - mse: 3347.6951 - mae: 32.2731 - val_loss: 1448.3006 - val_mse: 1448.3004 - val_mae: 24.6135\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 639us/step - loss: 3292.9306 - mse: 3292.9314 - mae: 31.7087 - val_loss: 1444.8359 - val_mse: 1444.8359 - val_mae: 24.9812\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3270.8557 - mse: 3270.8560 - mae: 31.8601 - val_loss: 1449.7044 - val_mse: 1449.7042 - val_mae: 24.6420\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 722us/step - loss: 3247.5044 - mse: 3247.5034 - mae: 31.5749 - val_loss: 1449.5263 - val_mse: 1449.5262 - val_mae: 24.7200\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 667us/step - loss: 3181.8358 - mse: 3181.8352 - mae: 31.2967 - val_loss: 1452.1338 - val_mse: 1452.1337 - val_mae: 24.6750\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 523us/step - loss: 3336.2721 - mse: 3336.2722 - mae: 32.0341 - val_loss: 1452.7991 - val_mse: 1452.7993 - val_mae: 24.6959\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 466us/step - loss: 3292.4188 - mse: 3292.4187 - mae: 31.7508 - val_loss: 1449.1196 - val_mse: 1449.1196 - val_mae: 24.9655\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 536us/step - loss: 3336.0975 - mse: 3336.0974 - mae: 31.9497 - val_loss: 1449.2786 - val_mse: 1449.2786 - val_mae: 24.9971\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 595us/step - loss: 3322.3390 - mse: 3322.3394 - mae: 32.3079 - val_loss: 1450.1660 - val_mse: 1450.1661 - val_mae: 24.9401\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 717us/step - loss: 3281.1734 - mse: 3281.1733 - mae: 32.1782 - val_loss: 1454.7632 - val_mse: 1454.7632 - val_mae: 24.6205\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 645us/step - loss: 3156.0289 - mse: 3156.0291 - mae: 30.4583 - val_loss: 1449.3863 - val_mse: 1449.3862 - val_mae: 24.9215\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 566us/step - loss: 2995.0502 - mse: 2995.0508 - mae: 31.4883 - val_loss: 1449.9323 - val_mse: 1449.9324 - val_mae: 24.9714\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3207.2898 - mse: 3207.2900 - mae: 31.0907 - val_loss: 1450.8348 - val_mse: 1450.8347 - val_mae: 25.0508\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 512us/step - loss: 3002.3428 - mse: 3002.3425 - mae: 31.6209 - val_loss: 1053.4890 - val_mse: 1053.4891 - val_mae: 23.6368\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 607us/step - loss: 2853.9937 - mse: 2853.9929 - mae: 30.4694 - val_loss: 1049.4364 - val_mse: 1049.4364 - val_mae: 23.6574\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 573us/step - loss: 2872.1841 - mse: 2872.1836 - mae: 31.0730 - val_loss: 1050.7618 - val_mse: 1050.7617 - val_mae: 23.4567\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 680us/step - loss: 2907.1633 - mse: 2907.1631 - mae: 31.5402 - val_loss: 1049.7126 - val_mse: 1049.7125 - val_mae: 23.3783\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2925.6189 - mse: 2925.6182 - mae: 31.1608 - val_loss: 1046.4010 - val_mse: 1046.4011 - val_mae: 23.5537\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2900.1820 - mse: 2900.1821 - mae: 30.6744 - val_loss: 1046.4123 - val_mse: 1046.4122 - val_mae: 23.4277\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2841.2847 - mse: 2841.2847 - mae: 30.6451 - val_loss: 1043.5888 - val_mse: 1043.5886 - val_mae: 23.7922\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 570us/step - loss: 2724.6749 - mse: 2724.6743 - mae: 30.4614 - val_loss: 1041.8405 - val_mse: 1041.8405 - val_mae: 23.9554\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2789.8620 - mse: 2789.8618 - mae: 30.3710 - val_loss: 1041.1729 - val_mse: 1041.1729 - val_mae: 23.7453\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 734us/step - loss: 2848.0048 - mse: 2848.0039 - mae: 30.6823 - val_loss: 1040.0359 - val_mse: 1040.0359 - val_mae: 23.8708\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2938.0146 - mse: 2938.0149 - mae: 31.5071 - val_loss: 1039.6992 - val_mse: 1039.6991 - val_mae: 23.5146\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 642us/step - loss: 2830.8753 - mse: 2830.8748 - mae: 30.9371 - val_loss: 1038.9222 - val_mse: 1038.9222 - val_mae: 23.7170\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2938.5888 - mse: 2938.5891 - mae: 31.5195 - val_loss: 1038.9714 - val_mse: 1038.9713 - val_mae: 23.5727\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 663us/step - loss: 2864.2072 - mse: 2864.2075 - mae: 31.3012 - val_loss: 1039.5065 - val_mse: 1039.5065 - val_mae: 23.4826\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2780.3380 - mse: 2780.3389 - mae: 30.6980 - val_loss: 1038.6448 - val_mse: 1038.6448 - val_mae: 23.8512\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2891.6339 - mse: 2891.6328 - mae: 30.9073 - val_loss: 1039.2332 - val_mse: 1039.2330 - val_mae: 23.5506\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 533us/step - loss: 2939.9380 - mse: 2939.9377 - mae: 30.7473 - val_loss: 1039.5135 - val_mse: 1039.5135 - val_mae: 23.5252\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2905.4005 - mse: 2905.3999 - mae: 31.2390 - val_loss: 1038.7829 - val_mse: 1038.7828 - val_mae: 23.5146\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2854.4889 - mse: 2854.4883 - mae: 30.5384 - val_loss: 1037.7468 - val_mse: 1037.7467 - val_mae: 23.3744\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2925.0182 - mse: 2925.0186 - mae: 30.6317 - val_loss: 1036.5516 - val_mse: 1036.5515 - val_mae: 23.4340\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 560us/step - loss: 2852.1951 - mse: 2852.1953 - mae: 30.3719 - val_loss: 1037.7308 - val_mse: 1037.7310 - val_mae: 23.2621\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2903.1165 - mse: 2903.1169 - mae: 31.3788 - val_loss: 1038.4551 - val_mse: 1038.4552 - val_mae: 23.2642\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2814.7845 - mse: 2814.7849 - mae: 30.4324 - val_loss: 1037.1881 - val_mse: 1037.1880 - val_mae: 23.4953\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2853.2724 - mse: 2853.2729 - mae: 30.4883 - val_loss: 1036.6699 - val_mse: 1036.6698 - val_mae: 23.7434\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2887.0226 - mse: 2887.0227 - mae: 30.7994 - val_loss: 1036.4663 - val_mse: 1036.4663 - val_mae: 23.6823\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 644us/step - loss: 2881.5395 - mse: 2881.5393 - mae: 30.6700 - val_loss: 1036.9879 - val_mse: 1036.9879 - val_mae: 23.4187\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2981.1754 - mse: 2981.1753 - mae: 31.6146 - val_loss: 1038.5182 - val_mse: 1038.5182 - val_mae: 23.1471\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 612us/step - loss: 2811.4780 - mse: 2811.4771 - mae: 30.7201 - val_loss: 1035.6644 - val_mse: 1035.6644 - val_mae: 23.3825\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 488us/step - loss: 2905.1611 - mse: 2905.1611 - mae: 31.4263 - val_loss: 1034.3512 - val_mse: 1034.3513 - val_mae: 23.5653\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2894.9868 - mse: 2894.9871 - mae: 30.6272 - val_loss: 1037.2180 - val_mse: 1037.2179 - val_mae: 23.1279\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2817.4533 - mse: 2817.4529 - mae: 30.1295 - val_loss: 1034.7343 - val_mse: 1034.7344 - val_mae: 23.9523\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 576us/step - loss: 2857.7913 - mse: 2857.7917 - mae: 30.7473 - val_loss: 1035.8677 - val_mse: 1035.8676 - val_mae: 23.4907\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2830.8759 - mse: 2830.8757 - mae: 30.4340 - val_loss: 1035.3332 - val_mse: 1035.3331 - val_mae: 23.5540\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 662us/step - loss: 2776.1910 - mse: 2776.1909 - mae: 30.0955 - val_loss: 1033.8981 - val_mse: 1033.8981 - val_mae: 23.6816\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 557us/step - loss: 2787.0327 - mse: 2787.0322 - mae: 30.0127 - val_loss: 1033.2438 - val_mse: 1033.2439 - val_mae: 23.8079\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2785.0778 - mse: 2785.0789 - mae: 30.3710 - val_loss: 1032.8243 - val_mse: 1032.8245 - val_mae: 23.7408\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2825.1353 - mse: 2825.1355 - mae: 30.8338 - val_loss: 1033.2052 - val_mse: 1033.2052 - val_mae: 23.5954\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 581us/step - loss: 2768.5933 - mse: 2768.5933 - mae: 30.3250 - val_loss: 1034.1216 - val_mse: 1034.1216 - val_mae: 24.0115\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2846.6134 - mse: 2846.6133 - mae: 30.3671 - val_loss: 1032.9214 - val_mse: 1032.9214 - val_mae: 23.7593\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2786.2683 - mse: 2786.2686 - mae: 29.9819 - val_loss: 1032.6118 - val_mse: 1032.6119 - val_mae: 23.7893\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 542us/step - loss: 2852.9625 - mse: 2852.9626 - mae: 30.5673 - val_loss: 1034.6168 - val_mse: 1034.6168 - val_mae: 23.2699\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2836.3792 - mse: 2836.3796 - mae: 30.4559 - val_loss: 1033.5955 - val_mse: 1033.5953 - val_mae: 23.4295\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2812.6906 - mse: 2812.6909 - mae: 30.5075 - val_loss: 1034.6307 - val_mse: 1034.6306 - val_mae: 23.2385\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2865.8928 - mse: 2865.8931 - mae: 31.1065 - val_loss: 1033.1049 - val_mse: 1033.1049 - val_mae: 23.6731\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 524us/step - loss: 2852.3026 - mse: 2852.3032 - mae: 30.2966 - val_loss: 1033.9998 - val_mse: 1033.9999 - val_mae: 23.3989\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2832.0284 - mse: 2832.0278 - mae: 30.1627 - val_loss: 1033.7476 - val_mse: 1033.7474 - val_mae: 23.4592\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 564us/step - loss: 2809.2996 - mse: 2809.2993 - mae: 30.2049 - val_loss: 1033.5701 - val_mse: 1033.5702 - val_mae: 23.8503\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 604us/step - loss: 2799.0413 - mse: 2799.0417 - mae: 29.8258 - val_loss: 1032.5214 - val_mse: 1032.5212 - val_mae: 23.8861\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 693us/step - loss: 2883.1977 - mse: 2883.1980 - mae: 30.1618 - val_loss: 1032.3107 - val_mse: 1032.3108 - val_mae: 23.7604\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 577us/step - loss: 2773.1144 - mse: 2773.1147 - mae: 30.1073 - val_loss: 1031.9125 - val_mse: 1031.9126 - val_mae: 23.7593\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2753.2096 - mse: 2753.2083 - mae: 29.9328 - val_loss: 1031.4611 - val_mse: 1031.4611 - val_mae: 23.6828\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 605us/step - loss: 2767.8281 - mse: 2767.8286 - mae: 29.7806 - val_loss: 1030.1283 - val_mse: 1030.1284 - val_mae: 23.6073\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 671us/step - loss: 2783.0232 - mse: 2783.0232 - mae: 30.4446 - val_loss: 1030.0559 - val_mse: 1030.0559 - val_mae: 23.8313\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2771.1083 - mse: 2771.1086 - mae: 30.0547 - val_loss: 1029.5606 - val_mse: 1029.5607 - val_mae: 23.6479\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 520us/step - loss: 2812.0567 - mse: 2812.0569 - mae: 30.5736 - val_loss: 1029.0022 - val_mse: 1029.0022 - val_mae: 23.4590\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 637us/step - loss: 2853.2888 - mse: 2853.2888 - mae: 30.7479 - val_loss: 1029.4579 - val_mse: 1029.4580 - val_mae: 23.4018\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2782.0346 - mse: 2782.0344 - mae: 30.2847 - val_loss: 1028.8265 - val_mse: 1028.8264 - val_mae: 23.6197\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2831.8234 - mse: 2831.8232 - mae: 30.3969 - val_loss: 1027.7215 - val_mse: 1027.7214 - val_mae: 23.5450\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2797.8025 - mse: 2797.8022 - mae: 30.5399 - val_loss: 1027.7025 - val_mse: 1027.7024 - val_mae: 23.7639\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 618us/step - loss: 2863.5040 - mse: 2863.5054 - mae: 30.6140 - val_loss: 1027.1624 - val_mse: 1027.1624 - val_mae: 23.6104\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 617us/step - loss: 2780.2535 - mse: 2780.2537 - mae: 30.1342 - val_loss: 1027.7024 - val_mse: 1027.7025 - val_mae: 23.9520\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 665us/step - loss: 2734.6293 - mse: 2734.6289 - mae: 30.7600 - val_loss: 1026.4999 - val_mse: 1026.4999 - val_mae: 23.7031\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 663us/step - loss: 2808.6188 - mse: 2808.6194 - mae: 30.7450 - val_loss: 1026.3028 - val_mse: 1026.3027 - val_mae: 23.4938\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 633us/step - loss: 2791.5340 - mse: 2791.5339 - mae: 30.3306 - val_loss: 1026.2940 - val_mse: 1026.2939 - val_mae: 23.5605\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2840.9241 - mse: 2840.9236 - mae: 30.2981 - val_loss: 1026.2886 - val_mse: 1026.2887 - val_mae: 23.6033\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 619us/step - loss: 2831.8893 - mse: 2831.8889 - mae: 30.7478 - val_loss: 1026.6180 - val_mse: 1026.6180 - val_mae: 23.5177\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 583us/step - loss: 2749.5177 - mse: 2749.5176 - mae: 29.6656 - val_loss: 1027.0150 - val_mse: 1027.0150 - val_mae: 24.0040\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 621us/step - loss: 2767.4318 - mse: 2767.4312 - mae: 30.1544 - val_loss: 1026.0526 - val_mse: 1026.0526 - val_mae: 23.8723\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2831.3919 - mse: 2831.3928 - mae: 30.6663 - val_loss: 1025.8585 - val_mse: 1025.8585 - val_mae: 23.7582\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2722.0862 - mse: 2722.0859 - mae: 29.7153 - val_loss: 1025.0429 - val_mse: 1025.0428 - val_mae: 23.5944\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 620us/step - loss: 2831.9949 - mse: 2831.9946 - mae: 29.8383 - val_loss: 1024.5382 - val_mse: 1024.5382 - val_mae: 23.6010\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2896.5976 - mse: 2896.5972 - mae: 30.7037 - val_loss: 1025.1776 - val_mse: 1025.1776 - val_mae: 23.5657\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 568us/step - loss: 2853.7300 - mse: 2853.7310 - mae: 30.6731 - val_loss: 1025.5167 - val_mse: 1025.5167 - val_mae: 23.3436\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2813.0795 - mse: 2813.0796 - mae: 30.1675 - val_loss: 1024.7328 - val_mse: 1024.7328 - val_mae: 23.6147\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2800.1539 - mse: 2800.1543 - mae: 29.8992 - val_loss: 1024.7109 - val_mse: 1024.7109 - val_mae: 23.6609\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2768.8073 - mse: 2768.8066 - mae: 30.1253 - val_loss: 1024.1870 - val_mse: 1024.1870 - val_mae: 23.7111\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 569us/step - loss: 2750.1674 - mse: 2750.1675 - mae: 30.2629 - val_loss: 1024.2340 - val_mse: 1024.2340 - val_mae: 23.6645\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 551us/step - loss: 2765.0034 - mse: 2765.0024 - mae: 29.6913 - val_loss: 1023.7848 - val_mse: 1023.7849 - val_mae: 23.6173\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 639us/step - loss: 2849.7348 - mse: 2849.7351 - mae: 30.4318 - val_loss: 1024.3522 - val_mse: 1024.3522 - val_mae: 23.8559\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2861.6491 - mse: 2861.6489 - mae: 30.4690 - val_loss: 1023.4105 - val_mse: 1023.4105 - val_mae: 23.6717\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2509.3824 - mse: 2509.3823 - mae: 29.7667 - val_loss: 1492.8706 - val_mse: 1492.8707 - val_mae: 26.7614\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 593us/step - loss: 2491.1729 - mse: 2491.1733 - mae: 29.5434 - val_loss: 1473.6066 - val_mse: 1473.6067 - val_mae: 27.2051\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2528.3717 - mse: 2528.3716 - mae: 29.8273 - val_loss: 1487.0998 - val_mse: 1487.1000 - val_mae: 26.8561\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2512.6575 - mse: 2512.6572 - mae: 29.3296 - val_loss: 1476.4008 - val_mse: 1476.4010 - val_mae: 27.1399\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 2s 651us/step - loss: 2538.6028 - mse: 2538.6025 - mae: 29.8231 - val_loss: 1487.1147 - val_mse: 1487.1146 - val_mae: 26.8729\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2457.4406 - mse: 2457.4407 - mae: 29.1758 - val_loss: 1481.9154 - val_mse: 1481.9156 - val_mae: 26.9948\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2508.7004 - mse: 2508.7007 - mae: 29.6017 - val_loss: 1494.6409 - val_mse: 1494.6410 - val_mae: 26.7190\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2539.5121 - mse: 2539.5120 - mae: 29.6943 - val_loss: 1486.1776 - val_mse: 1486.1779 - val_mae: 26.8981\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 2s 609us/step - loss: 2453.2187 - mse: 2453.2185 - mae: 29.3405 - val_loss: 1489.2773 - val_mse: 1489.2772 - val_mae: 26.7949\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2519.8380 - mse: 2519.8374 - mae: 29.2870 - val_loss: 1493.8335 - val_mse: 1493.8335 - val_mae: 26.6514\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 1s 519us/step - loss: 2548.8556 - mse: 2548.8555 - mae: 29.2146 - val_loss: 1480.8287 - val_mse: 1480.8287 - val_mae: 26.8364\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 578us/step - loss: 2537.7166 - mse: 2537.7178 - mae: 29.6392 - val_loss: 1484.8173 - val_mse: 1484.8174 - val_mae: 26.7566\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 576us/step - loss: 2536.7082 - mse: 2536.7083 - mae: 29.3856 - val_loss: 1478.6639 - val_mse: 1478.6638 - val_mae: 26.8904\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 647us/step - loss: 2420.8830 - mse: 2420.8823 - mae: 28.9234 - val_loss: 1488.0434 - val_mse: 1488.0436 - val_mae: 26.6426\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 680us/step - loss: 2510.5523 - mse: 2510.5525 - mae: 29.2891 - val_loss: 1482.6972 - val_mse: 1482.6970 - val_mae: 26.7770\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 2s 638us/step - loss: 2554.8803 - mse: 2554.8806 - mae: 29.9830 - val_loss: 1482.6002 - val_mse: 1482.6003 - val_mae: 26.7466\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 1s 589us/step - loss: 2490.9223 - mse: 2490.9226 - mae: 28.8865 - val_loss: 1496.1911 - val_mse: 1496.1910 - val_mae: 26.4603\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2480.7869 - mse: 2480.7869 - mae: 28.7446 - val_loss: 1475.6361 - val_mse: 1475.6364 - val_mae: 26.8405\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2620.0887 - mse: 2620.0889 - mae: 29.9171 - val_loss: 1492.1668 - val_mse: 1492.1670 - val_mae: 26.5054\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 2s 606us/step - loss: 2558.1713 - mse: 2558.1707 - mae: 29.6662 - val_loss: 1486.7169 - val_mse: 1486.7169 - val_mae: 26.5872\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 497us/step - loss: 2432.3835 - mse: 2432.3835 - mae: 29.3467 - val_loss: 1487.2040 - val_mse: 1487.2041 - val_mae: 26.5624\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2423.8962 - mse: 2423.8960 - mae: 29.3174 - val_loss: 1485.6913 - val_mse: 1485.6913 - val_mae: 26.5797\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2550.2366 - mse: 2550.2363 - mae: 29.4748 - val_loss: 1486.5094 - val_mse: 1486.5094 - val_mae: 26.5399\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2461.0403 - mse: 2461.0403 - mae: 28.7709 - val_loss: 1474.3031 - val_mse: 1474.3031 - val_mae: 26.8045\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2482.2692 - mse: 2482.2695 - mae: 29.2047 - val_loss: 1488.9204 - val_mse: 1488.9203 - val_mae: 26.4971\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2518.3079 - mse: 2518.3076 - mae: 29.2442 - val_loss: 1482.0784 - val_mse: 1482.0784 - val_mae: 26.6125\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 618us/step - loss: 2502.0742 - mse: 2502.0745 - mae: 29.2149 - val_loss: 1476.3181 - val_mse: 1476.3180 - val_mae: 26.7315\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 1s 596us/step - loss: 2475.1228 - mse: 2475.1230 - mae: 28.7347 - val_loss: 1476.0871 - val_mse: 1476.0870 - val_mae: 26.7487\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 622us/step - loss: 2535.5447 - mse: 2535.5444 - mae: 29.5248 - val_loss: 1478.4032 - val_mse: 1478.4032 - val_mae: 26.6518\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 554us/step - loss: 2532.0835 - mse: 2532.0845 - mae: 29.8544 - val_loss: 1487.5046 - val_mse: 1487.5046 - val_mae: 26.4233\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - 1s 573us/step - loss: 2494.5553 - mse: 2494.5552 - mae: 29.4613 - val_loss: 1482.5756 - val_mse: 1482.5757 - val_mae: 26.4522\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 2s 626us/step - loss: 2479.3595 - mse: 2479.3599 - mae: 28.9954 - val_loss: 1470.6047 - val_mse: 1470.6050 - val_mae: 26.7455\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 653us/step - loss: 2450.3668 - mse: 2450.3667 - mae: 29.1602 - val_loss: 1466.4036 - val_mse: 1466.4037 - val_mae: 26.8097\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 526us/step - loss: 2550.0913 - mse: 2550.0906 - mae: 29.5291 - val_loss: 1467.8350 - val_mse: 1467.8351 - val_mae: 26.7713\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 1s 548us/step - loss: 2439.8368 - mse: 2439.8372 - mae: 29.0652 - val_loss: 1476.2230 - val_mse: 1476.2231 - val_mae: 26.6076\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2446.0500 - mse: 2446.0500 - mae: 28.9126 - val_loss: 1473.0600 - val_mse: 1473.0599 - val_mae: 26.6218\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2463.1973 - mse: 2463.1975 - mae: 28.8537 - val_loss: 1468.6602 - val_mse: 1468.6603 - val_mae: 26.6600\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 512us/step - loss: 2516.7083 - mse: 2516.7087 - mae: 29.5177 - val_loss: 1469.4353 - val_mse: 1469.4351 - val_mae: 26.6203\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2448.7088 - mse: 2448.7085 - mae: 28.8737 - val_loss: 1467.2411 - val_mse: 1467.2410 - val_mae: 26.6259\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2457.2028 - mse: 2457.2029 - mae: 28.9875 - val_loss: 1475.5845 - val_mse: 1475.5842 - val_mae: 26.4435\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2448.9330 - mse: 2448.9333 - mae: 29.1812 - val_loss: 1464.3648 - val_mse: 1464.3647 - val_mae: 26.6466\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 579us/step - loss: 2496.7262 - mse: 2496.7273 - mae: 29.2423 - val_loss: 1472.0802 - val_mse: 1472.0802 - val_mae: 26.5372\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 2s 664us/step - loss: 2582.5739 - mse: 2582.5735 - mae: 29.6441 - val_loss: 1468.9677 - val_mse: 1468.9677 - val_mae: 26.6083\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2415.1830 - mse: 2415.1838 - mae: 28.7745 - val_loss: 1469.8311 - val_mse: 1469.8311 - val_mae: 26.5626\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 2s 663us/step - loss: 2511.9760 - mse: 2511.9753 - mae: 29.0327 - val_loss: 1474.5935 - val_mse: 1474.5934 - val_mae: 26.5069\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2440.7516 - mse: 2440.7507 - mae: 29.2206 - val_loss: 1475.6357 - val_mse: 1475.6357 - val_mae: 26.4467\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2449.5466 - mse: 2449.5466 - mae: 28.7929 - val_loss: 1471.0721 - val_mse: 1471.0721 - val_mae: 26.5603\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 2s 627us/step - loss: 2439.6453 - mse: 2439.6448 - mae: 28.6959 - val_loss: 1470.1366 - val_mse: 1470.1366 - val_mae: 26.5276\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 648us/step - loss: 2484.0458 - mse: 2484.0461 - mae: 28.9380 - val_loss: 1469.1513 - val_mse: 1469.1514 - val_mae: 26.5281\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2433.9571 - mse: 2433.9575 - mae: 28.6543 - val_loss: 1456.8303 - val_mse: 1456.8304 - val_mae: 26.8313\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 1s 561us/step - loss: 2427.8328 - mse: 2427.8323 - mae: 28.9296 - val_loss: 1460.9802 - val_mse: 1460.9802 - val_mae: 26.7542\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 1s 581us/step - loss: 2428.4766 - mse: 2428.4763 - mae: 29.1547 - val_loss: 1461.8305 - val_mse: 1461.8302 - val_mae: 26.6734\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2492.1426 - mse: 2492.1428 - mae: 29.5928 - val_loss: 1464.0595 - val_mse: 1464.0594 - val_mae: 26.6045\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2433.5972 - mse: 2433.5977 - mae: 28.5008 - val_loss: 1447.0502 - val_mse: 1447.0504 - val_mae: 27.0101\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 1s 542us/step - loss: 2461.4107 - mse: 2461.4104 - mae: 28.9777 - val_loss: 1470.2403 - val_mse: 1470.2405 - val_mae: 26.4197\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2387.2717 - mse: 2387.2720 - mae: 28.7832 - val_loss: 1462.2983 - val_mse: 1462.2981 - val_mae: 26.5479\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2415.4684 - mse: 2415.4670 - mae: 28.5883 - val_loss: 1474.6575 - val_mse: 1474.6575 - val_mae: 26.2787\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 548us/step - loss: 2410.8455 - mse: 2410.8455 - mae: 28.8765 - val_loss: 1450.8268 - val_mse: 1450.8269 - val_mae: 26.7097\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 1s 574us/step - loss: 2392.5398 - mse: 2392.5400 - mae: 28.6532 - val_loss: 1463.3447 - val_mse: 1463.3448 - val_mae: 26.4275\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 1s 600us/step - loss: 2474.2897 - mse: 2474.2888 - mae: 29.1478 - val_loss: 1461.3718 - val_mse: 1461.3718 - val_mae: 26.4574\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2419.5750 - mse: 2419.5747 - mae: 28.3122 - val_loss: 1454.8413 - val_mse: 1454.8414 - val_mae: 26.5902\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 2s 652us/step - loss: 2418.5521 - mse: 2418.5522 - mae: 28.4451 - val_loss: 1467.8838 - val_mse: 1467.8840 - val_mae: 26.3487\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2423.7573 - mse: 2423.7573 - mae: 29.0148 - val_loss: 1460.1031 - val_mse: 1460.1031 - val_mae: 26.4995\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2417.1249 - mse: 2417.1240 - mae: 28.7747 - val_loss: 1449.4142 - val_mse: 1449.4143 - val_mae: 26.7645\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2498.3511 - mse: 2498.3513 - mae: 29.4751 - val_loss: 1462.8526 - val_mse: 1462.8527 - val_mae: 26.4228\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2443.6391 - mse: 2443.6387 - mae: 28.8102 - val_loss: 1465.8786 - val_mse: 1465.8785 - val_mae: 26.3656\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2465.6318 - mse: 2465.6321 - mae: 29.0373 - val_loss: 1462.2496 - val_mse: 1462.2496 - val_mae: 26.4169\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 601us/step - loss: 2478.9805 - mse: 2478.9800 - mae: 29.5509 - val_loss: 1478.4681 - val_mse: 1478.4680 - val_mae: 26.2125\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 1s 592us/step - loss: 2400.8684 - mse: 2400.8679 - mae: 28.2560 - val_loss: 1465.8924 - val_mse: 1465.8926 - val_mae: 26.3651\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2417.8137 - mse: 2417.8142 - mae: 28.2604 - val_loss: 1455.9677 - val_mse: 1455.9674 - val_mae: 26.5163\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2402.3294 - mse: 2402.3291 - mae: 28.5208 - val_loss: 1461.7105 - val_mse: 1461.7104 - val_mae: 26.4065\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 2s 647us/step - loss: 2443.9830 - mse: 2443.9824 - mae: 28.6941 - val_loss: 1464.7490 - val_mse: 1464.7491 - val_mae: 26.3466\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2455.7766 - mse: 2455.7766 - mae: 29.1861 - val_loss: 1488.9678 - val_mse: 1488.9677 - val_mae: 26.0271\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2421.7418 - mse: 2421.7412 - mae: 28.4162 - val_loss: 1461.4037 - val_mse: 1461.4039 - val_mae: 26.4214\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 1s 588us/step - loss: 2409.7024 - mse: 2409.7019 - mae: 28.3018 - val_loss: 1471.5502 - val_mse: 1471.5500 - val_mae: 26.2346\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 649us/step - loss: 2446.5534 - mse: 2446.5527 - mae: 29.0236 - val_loss: 1463.1315 - val_mse: 1463.1315 - val_mae: 26.3846\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 2s 619us/step - loss: 2392.4008 - mse: 2392.4011 - mae: 28.7755 - val_loss: 1462.0145 - val_mse: 1462.0144 - val_mae: 26.3720\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2450.3235 - mse: 2450.3230 - mae: 28.7918 - val_loss: 1455.1266 - val_mse: 1455.1267 - val_mae: 26.4834\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 2s 620us/step - loss: 2548.0173 - mse: 2548.0176 - mae: 29.5567 - val_loss: 1480.8090 - val_mse: 1480.8090 - val_mae: 26.0589\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 563us/step - loss: 2397.5397 - mse: 2397.5388 - mae: 29.0543 - val_loss: 1465.1681 - val_mse: 1465.1681 - val_mae: 26.3021\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2341.3615 - mse: 2341.3613 - mae: 29.0635 - val_loss: 3643.6691 - val_mse: 3643.6680 - val_mae: 23.2196\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2335.6943 - mse: 2335.6938 - mae: 29.5807 - val_loss: 3644.0303 - val_mse: 3644.0310 - val_mae: 22.9192\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2344.4602 - mse: 2344.4604 - mae: 29.5171 - val_loss: 3645.6783 - val_mse: 3645.6780 - val_mae: 22.9130\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 564us/step - loss: 2312.5468 - mse: 2312.5471 - mae: 28.9648 - val_loss: 3645.5895 - val_mse: 3645.5891 - val_mae: 23.5530\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 639us/step - loss: 2335.3622 - mse: 2335.3616 - mae: 29.2251 - val_loss: 3645.9947 - val_mse: 3645.9937 - val_mae: 23.1720\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2392.8506 - mse: 2392.8511 - mae: 29.2868 - val_loss: 3648.3242 - val_mse: 3648.3237 - val_mae: 22.6963\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 574us/step - loss: 2354.0369 - mse: 2354.0369 - mae: 29.2217 - val_loss: 3646.3967 - val_mse: 3646.3965 - val_mae: 23.1322\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2369.2285 - mse: 2369.2290 - mae: 29.5994 - val_loss: 3649.5359 - val_mse: 3649.5364 - val_mae: 23.6543\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2340.0146 - mse: 2340.0132 - mae: 28.9537 - val_loss: 3648.6395 - val_mse: 3648.6399 - val_mae: 23.3312\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 584us/step - loss: 2372.9853 - mse: 2372.9858 - mae: 29.4318 - val_loss: 3649.3729 - val_mse: 3649.3726 - val_mae: 22.7029\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2343.8711 - mse: 2343.8713 - mae: 29.0341 - val_loss: 3649.1153 - val_mse: 3649.1155 - val_mae: 23.1419\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 617us/step - loss: 2331.5121 - mse: 2331.5120 - mae: 29.1426 - val_loss: 3649.4956 - val_mse: 3649.4961 - val_mae: 23.2008\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2313.4619 - mse: 2313.4622 - mae: 28.8266 - val_loss: 3648.4105 - val_mse: 3648.4104 - val_mae: 23.2148\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2287.4147 - mse: 2287.4153 - mae: 29.3098 - val_loss: 3649.1313 - val_mse: 3649.1309 - val_mae: 23.0181\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2358.5508 - mse: 2358.5515 - mae: 29.3194 - val_loss: 3648.6438 - val_mse: 3648.6433 - val_mae: 23.2184\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2307.3773 - mse: 2307.3770 - mae: 28.9802 - val_loss: 3649.5152 - val_mse: 3649.5156 - val_mae: 23.0804\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2267.7186 - mse: 2267.7180 - mae: 28.9425 - val_loss: 3649.5084 - val_mse: 3649.5083 - val_mae: 23.0247\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2337.5873 - mse: 2337.5869 - mae: 29.5297 - val_loss: 3649.7372 - val_mse: 3649.7371 - val_mae: 22.8648\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 623us/step - loss: 2280.9670 - mse: 2280.9673 - mae: 28.9897 - val_loss: 3650.0181 - val_mse: 3650.0188 - val_mae: 22.6306\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 703us/step - loss: 2348.3593 - mse: 2348.3591 - mae: 29.0203 - val_loss: 3649.2876 - val_mse: 3649.2881 - val_mae: 22.6590\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 676us/step - loss: 2293.2087 - mse: 2293.2087 - mae: 28.8658 - val_loss: 3648.6866 - val_mse: 3648.6865 - val_mae: 23.0208\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2330.6715 - mse: 2330.6709 - mae: 29.2316 - val_loss: 3649.3735 - val_mse: 3649.3730 - val_mae: 23.2109\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2320.0300 - mse: 2320.0295 - mae: 28.9340 - val_loss: 3649.6820 - val_mse: 3649.6816 - val_mae: 23.0603\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2347.2556 - mse: 2347.2556 - mae: 29.0374 - val_loss: 3650.1343 - val_mse: 3650.1343 - val_mae: 23.0720\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2322.9080 - mse: 2322.9080 - mae: 29.0206 - val_loss: 3650.5155 - val_mse: 3650.5156 - val_mae: 23.3539\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2356.2275 - mse: 2356.2275 - mae: 29.2317 - val_loss: 3651.3192 - val_mse: 3651.3201 - val_mae: 22.9935\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 597us/step - loss: 2289.1655 - mse: 2289.1650 - mae: 28.9990 - val_loss: 3650.0191 - val_mse: 3650.0198 - val_mae: 23.1024\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2319.3476 - mse: 2319.3474 - mae: 28.9770 - val_loss: 3651.1826 - val_mse: 3651.1826 - val_mae: 22.8752\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 543us/step - loss: 2316.6739 - mse: 2316.6729 - mae: 28.5701 - val_loss: 3652.1658 - val_mse: 3652.1660 - val_mae: 22.8259\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 510us/step - loss: 2336.5667 - mse: 2336.5676 - mae: 28.9546 - val_loss: 3650.7721 - val_mse: 3650.7725 - val_mae: 22.8140\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2325.1066 - mse: 2325.1069 - mae: 28.8017 - val_loss: 3649.9642 - val_mse: 3649.9639 - val_mae: 22.8908\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2307.3411 - mse: 2307.3401 - mae: 28.7932 - val_loss: 3650.8978 - val_mse: 3650.8987 - val_mae: 23.5639\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2328.6270 - mse: 2328.6279 - mae: 29.3399 - val_loss: 3651.4713 - val_mse: 3651.4722 - val_mae: 23.3294\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2313.9099 - mse: 2313.9097 - mae: 28.9192 - val_loss: 3654.2870 - val_mse: 3654.2871 - val_mae: 23.3581\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2325.7928 - mse: 2325.7917 - mae: 28.9522 - val_loss: 3653.9427 - val_mse: 3653.9438 - val_mae: 22.7748\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2306.1714 - mse: 2306.1714 - mae: 28.8927 - val_loss: 3652.0856 - val_mse: 3652.0864 - val_mae: 23.0949\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2274.9027 - mse: 2274.9028 - mae: 28.9729 - val_loss: 3653.0169 - val_mse: 3653.0164 - val_mae: 22.7415\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 522us/step - loss: 2339.3508 - mse: 2339.3518 - mae: 29.1185 - val_loss: 3654.3861 - val_mse: 3654.3862 - val_mae: 23.2699\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 564us/step - loss: 2322.2075 - mse: 2322.2080 - mae: 28.7645 - val_loss: 3655.5489 - val_mse: 3655.5496 - val_mae: 23.4476\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 576us/step - loss: 2291.5738 - mse: 2291.5742 - mae: 28.9484 - val_loss: 3654.1763 - val_mse: 3654.1763 - val_mae: 22.7691\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2309.4095 - mse: 2309.4102 - mae: 28.6086 - val_loss: 3654.3619 - val_mse: 3654.3621 - val_mae: 23.8699\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 653us/step - loss: 2288.0489 - mse: 2288.0491 - mae: 28.9567 - val_loss: 3651.1428 - val_mse: 3651.1428 - val_mae: 22.7439\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 645us/step - loss: 2343.0586 - mse: 2343.0588 - mae: 28.7310 - val_loss: 3649.9503 - val_mse: 3649.9497 - val_mae: 22.7716\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 612us/step - loss: 2315.7556 - mse: 2315.7559 - mae: 29.0640 - val_loss: 3650.2024 - val_mse: 3650.2014 - val_mae: 22.6885\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2382.0007 - mse: 2382.0010 - mae: 29.1354 - val_loss: 3647.8761 - val_mse: 3647.8755 - val_mae: 22.9922\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 665us/step - loss: 2309.8904 - mse: 2309.8906 - mae: 28.8889 - val_loss: 3650.2313 - val_mse: 3650.2310 - val_mae: 22.9585\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2300.6289 - mse: 2300.6292 - mae: 29.1163 - val_loss: 3652.6247 - val_mse: 3652.6252 - val_mae: 23.1888\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2321.0542 - mse: 2321.0540 - mae: 28.9009 - val_loss: 3652.2232 - val_mse: 3652.2229 - val_mae: 23.2358\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 608us/step - loss: 2244.7966 - mse: 2244.7976 - mae: 28.2119 - val_loss: 3652.2585 - val_mse: 3652.2583 - val_mae: 23.6194\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 605us/step - loss: 2284.3775 - mse: 2284.3789 - mae: 28.9236 - val_loss: 3652.2688 - val_mse: 3652.2690 - val_mae: 23.1819\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2275.3562 - mse: 2275.3572 - mae: 28.4707 - val_loss: 3651.4718 - val_mse: 3651.4731 - val_mae: 23.2302\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2334.5222 - mse: 2334.5220 - mae: 28.9982 - val_loss: 3649.1199 - val_mse: 3649.1208 - val_mae: 23.0340\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2323.1029 - mse: 2323.1023 - mae: 28.8729 - val_loss: 3648.4788 - val_mse: 3648.4783 - val_mae: 22.7811\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 573us/step - loss: 2301.1882 - mse: 2301.1885 - mae: 28.8703 - val_loss: 3651.2379 - val_mse: 3651.2375 - val_mae: 22.7792\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 535us/step - loss: 2269.4808 - mse: 2269.4805 - mae: 28.5056 - val_loss: 3649.2827 - val_mse: 3649.2832 - val_mae: 22.8164\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 536us/step - loss: 2282.9971 - mse: 2282.9966 - mae: 28.6234 - val_loss: 3651.9716 - val_mse: 3651.9714 - val_mae: 23.3835\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2351.0915 - mse: 2351.0908 - mae: 29.1758 - val_loss: 3651.3881 - val_mse: 3651.3882 - val_mae: 22.9498\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 646us/step - loss: 2265.8327 - mse: 2265.8328 - mae: 28.6034 - val_loss: 3651.5210 - val_mse: 3651.5210 - val_mae: 23.3082\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2315.7947 - mse: 2315.7949 - mae: 29.0521 - val_loss: 3652.0652 - val_mse: 3652.0659 - val_mae: 23.0996\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 687us/step - loss: 2267.1870 - mse: 2267.1870 - mae: 28.6418 - val_loss: 3653.0787 - val_mse: 3653.0786 - val_mae: 22.9477\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 661us/step - loss: 2263.2789 - mse: 2263.2783 - mae: 28.2042 - val_loss: 3651.9194 - val_mse: 3651.9194 - val_mae: 23.1355\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 2s 670us/step - loss: 2269.5142 - mse: 2269.5142 - mae: 28.6002 - val_loss: 3650.6321 - val_mse: 3650.6326 - val_mae: 23.0762\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 550us/step - loss: 2309.2840 - mse: 2309.2839 - mae: 28.8540 - val_loss: 3652.3287 - val_mse: 3652.3286 - val_mae: 23.3911\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 1s 474us/step - loss: 2247.0201 - mse: 2247.0198 - mae: 28.3348 - val_loss: 3651.5489 - val_mse: 3651.5493 - val_mae: 23.2442\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 551us/step - loss: 2257.2960 - mse: 2257.2954 - mae: 28.8289 - val_loss: 3653.1536 - val_mse: 3653.1541 - val_mae: 23.4824\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2284.9314 - mse: 2284.9312 - mae: 28.9043 - val_loss: 3653.1112 - val_mse: 3653.1108 - val_mae: 23.0648\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2325.5998 - mse: 2325.6001 - mae: 28.6046 - val_loss: 3651.8603 - val_mse: 3651.8601 - val_mae: 23.2523\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2272.6553 - mse: 2272.6548 - mae: 28.4737 - val_loss: 3652.9820 - val_mse: 3652.9812 - val_mae: 23.2639\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2313.1793 - mse: 2313.1797 - mae: 28.8339 - val_loss: 3652.8486 - val_mse: 3652.8486 - val_mae: 23.1889\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 583us/step - loss: 2272.5933 - mse: 2272.5930 - mae: 28.2359 - val_loss: 3652.7217 - val_mse: 3652.7227 - val_mae: 23.2031\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 620us/step - loss: 2257.7734 - mse: 2257.7732 - mae: 28.6700 - val_loss: 3651.9501 - val_mse: 3651.9492 - val_mae: 22.9673\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2292.3378 - mse: 2292.3376 - mae: 28.9339 - val_loss: 3652.5236 - val_mse: 3652.5237 - val_mae: 22.8876\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2316.9431 - mse: 2316.9434 - mae: 28.7959 - val_loss: 3653.5515 - val_mse: 3653.5515 - val_mae: 22.9744\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 516us/step - loss: 2236.6877 - mse: 2236.6880 - mae: 28.3273 - val_loss: 3650.9841 - val_mse: 3650.9844 - val_mae: 22.9400\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2293.8125 - mse: 2293.8120 - mae: 28.6823 - val_loss: 3651.4434 - val_mse: 3651.4431 - val_mae: 22.9356\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 621us/step - loss: 2284.4552 - mse: 2284.4556 - mae: 28.1801 - val_loss: 3649.8606 - val_mse: 3649.8601 - val_mae: 23.2966\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 544us/step - loss: 2234.4076 - mse: 2234.4070 - mae: 28.3391 - val_loss: 3654.3163 - val_mse: 3654.3169 - val_mae: 24.2231\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2257.0061 - mse: 2257.0061 - mae: 28.7177 - val_loss: 3649.1741 - val_mse: 3649.1743 - val_mae: 23.3651\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2233.8177 - mse: 2233.8176 - mae: 28.3848 - val_loss: 3648.5351 - val_mse: 3648.5344 - val_mae: 23.0864\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 534us/step - loss: 2291.1639 - mse: 2291.1646 - mae: 28.5924 - val_loss: 3648.4829 - val_mse: 3648.4829 - val_mae: 23.4561\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2644.5512 - mse: 2644.5503 - mae: 27.9141 - val_loss: 2189.9914 - val_mse: 2189.9915 - val_mae: 25.6015\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2670.2893 - mse: 2670.2908 - mae: 28.0864 - val_loss: 2179.9082 - val_mse: 2179.9084 - val_mae: 25.7578\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2619.3267 - mse: 2619.3271 - mae: 28.0657 - val_loss: 2154.4947 - val_mse: 2154.4949 - val_mae: 26.4702\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 584us/step - loss: 2699.0849 - mse: 2699.0847 - mae: 28.4597 - val_loss: 2179.4272 - val_mse: 2179.4272 - val_mae: 25.8916\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2658.2240 - mse: 2658.2241 - mae: 28.2557 - val_loss: 2177.0238 - val_mse: 2177.0239 - val_mae: 25.9643\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 682us/step - loss: 2644.0787 - mse: 2644.0786 - mae: 28.2511 - val_loss: 2166.5803 - val_mse: 2166.5803 - val_mae: 26.2339\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2623.0253 - mse: 2623.0261 - mae: 28.0774 - val_loss: 2176.4457 - val_mse: 2176.4453 - val_mae: 25.9475\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 615us/step - loss: 2742.4484 - mse: 2742.4480 - mae: 28.8536 - val_loss: 2202.1593 - val_mse: 2202.1594 - val_mae: 25.6269\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2633.3474 - mse: 2633.3484 - mae: 28.0946 - val_loss: 2179.6021 - val_mse: 2179.6023 - val_mae: 26.1303\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2651.2307 - mse: 2651.2314 - mae: 28.0901 - val_loss: 2186.0389 - val_mse: 2186.0391 - val_mae: 26.0169\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2648.7185 - mse: 2648.7180 - mae: 28.3107 - val_loss: 2165.0965 - val_mse: 2165.0964 - val_mae: 26.5699\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2664.5376 - mse: 2664.5378 - mae: 28.0240 - val_loss: 2190.1834 - val_mse: 2190.1836 - val_mae: 25.9308\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2676.3231 - mse: 2676.3230 - mae: 27.6989 - val_loss: 2191.7136 - val_mse: 2191.7136 - val_mae: 26.0679\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 611us/step - loss: 2656.2507 - mse: 2656.2498 - mae: 27.8645 - val_loss: 2189.7391 - val_mse: 2189.7393 - val_mae: 26.1680\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 539us/step - loss: 2670.1698 - mse: 2670.1692 - mae: 28.1802 - val_loss: 2183.5598 - val_mse: 2183.5598 - val_mae: 26.3889\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2664.3166 - mse: 2664.3164 - mae: 28.2569 - val_loss: 2194.5940 - val_mse: 2194.5940 - val_mae: 25.8910\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2714.1780 - mse: 2714.1777 - mae: 28.2500 - val_loss: 2183.2146 - val_mse: 2183.2144 - val_mae: 26.1756\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2644.2882 - mse: 2644.2883 - mae: 27.6824 - val_loss: 2180.3627 - val_mse: 2180.3630 - val_mae: 26.2312\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2670.4471 - mse: 2670.4463 - mae: 28.0555 - val_loss: 2187.1494 - val_mse: 2187.1494 - val_mae: 26.0378\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2652.2397 - mse: 2652.2388 - mae: 28.1528 - val_loss: 2178.0865 - val_mse: 2178.0867 - val_mae: 26.1417\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2625.9951 - mse: 2625.9956 - mae: 27.9098 - val_loss: 2170.2241 - val_mse: 2170.2239 - val_mae: 26.2397\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2681.6822 - mse: 2681.6819 - mae: 28.1209 - val_loss: 2195.6821 - val_mse: 2195.6824 - val_mae: 25.7423\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2665.2625 - mse: 2665.2622 - mae: 28.0358 - val_loss: 2196.6924 - val_mse: 2196.6926 - val_mae: 25.7151\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2584.2682 - mse: 2584.2686 - mae: 27.5042 - val_loss: 2166.6636 - val_mse: 2166.6636 - val_mae: 26.4741\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 661us/step - loss: 2580.4254 - mse: 2580.4253 - mae: 28.0386 - val_loss: 2179.5085 - val_mse: 2179.5085 - val_mae: 26.1295\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 665us/step - loss: 2661.5887 - mse: 2661.5886 - mae: 28.4610 - val_loss: 2187.1149 - val_mse: 2187.1147 - val_mae: 25.8241\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2664.0838 - mse: 2664.0845 - mae: 28.2089 - val_loss: 2173.7504 - val_mse: 2173.7505 - val_mae: 26.0716\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2625.9921 - mse: 2625.9929 - mae: 27.8325 - val_loss: 2167.2201 - val_mse: 2167.2205 - val_mae: 26.1012\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 639us/step - loss: 2640.5387 - mse: 2640.5388 - mae: 28.5284 - val_loss: 2181.1525 - val_mse: 2181.1528 - val_mae: 25.8134\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 608us/step - loss: 2652.4896 - mse: 2652.4890 - mae: 28.2824 - val_loss: 2163.8375 - val_mse: 2163.8376 - val_mae: 26.3115\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 660us/step - loss: 2635.8211 - mse: 2635.8213 - mae: 27.8874 - val_loss: 2176.3842 - val_mse: 2176.3845 - val_mae: 25.9245\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 609us/step - loss: 2668.1603 - mse: 2668.1602 - mae: 28.3478 - val_loss: 2178.8997 - val_mse: 2178.8994 - val_mae: 25.9145\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2684.4718 - mse: 2684.4717 - mae: 28.0911 - val_loss: 2180.5427 - val_mse: 2180.5430 - val_mae: 26.0216\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2618.8414 - mse: 2618.8418 - mae: 28.0864 - val_loss: 2170.5334 - val_mse: 2170.5334 - val_mae: 26.1581\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2662.3466 - mse: 2662.3467 - mae: 28.0005 - val_loss: 2194.7003 - val_mse: 2194.7004 - val_mae: 25.9577\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 620us/step - loss: 2693.3404 - mse: 2693.3406 - mae: 28.0030 - val_loss: 2189.9965 - val_mse: 2189.9961 - val_mae: 26.1010\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 621us/step - loss: 2678.1890 - mse: 2678.1897 - mae: 27.9835 - val_loss: 2190.1296 - val_mse: 2190.1296 - val_mae: 25.9805\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2650.5689 - mse: 2650.5691 - mae: 28.2367 - val_loss: 2189.6272 - val_mse: 2189.6270 - val_mae: 26.1035\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 583us/step - loss: 2628.2782 - mse: 2628.2786 - mae: 28.1212 - val_loss: 2196.3785 - val_mse: 2196.3782 - val_mae: 26.1135\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 582us/step - loss: 2652.8236 - mse: 2652.8245 - mae: 27.9895 - val_loss: 2181.8582 - val_mse: 2181.8582 - val_mae: 26.2312\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2650.7764 - mse: 2650.7756 - mae: 28.1338 - val_loss: 2170.2283 - val_mse: 2170.2283 - val_mae: 26.2790\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 559us/step - loss: 2698.2989 - mse: 2698.2983 - mae: 28.2981 - val_loss: 2178.5840 - val_mse: 2178.5837 - val_mae: 25.8762\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2679.7195 - mse: 2679.7195 - mae: 28.0664 - val_loss: 2183.2711 - val_mse: 2183.2712 - val_mae: 25.8444\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 625us/step - loss: 2643.2511 - mse: 2643.2505 - mae: 27.7423 - val_loss: 2168.9663 - val_mse: 2168.9663 - val_mae: 26.1188\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2618.2431 - mse: 2618.2427 - mae: 27.9121 - val_loss: 2167.3020 - val_mse: 2167.3018 - val_mae: 25.9777\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2618.7521 - mse: 2618.7527 - mae: 27.9849 - val_loss: 2171.7130 - val_mse: 2171.7131 - val_mae: 26.1127\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2643.5499 - mse: 2643.5488 - mae: 28.0759 - val_loss: 2169.3002 - val_mse: 2169.3000 - val_mae: 26.1879\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 631us/step - loss: 2609.6266 - mse: 2609.6265 - mae: 27.7977 - val_loss: 2152.1682 - val_mse: 2152.1680 - val_mae: 26.7143\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 674us/step - loss: 2631.6782 - mse: 2631.6782 - mae: 28.2309 - val_loss: 2174.8316 - val_mse: 2174.8315 - val_mae: 25.9235\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 653us/step - loss: 2631.1592 - mse: 2631.1592 - mae: 27.8892 - val_loss: 2170.6445 - val_mse: 2170.6445 - val_mae: 25.9203\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 595us/step - loss: 2594.5746 - mse: 2594.5742 - mae: 27.9285 - val_loss: 2154.8024 - val_mse: 2154.8022 - val_mae: 26.3240\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 601us/step - loss: 2636.5931 - mse: 2636.5925 - mae: 27.9108 - val_loss: 2164.5930 - val_mse: 2164.5928 - val_mae: 26.2381\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2614.7249 - mse: 2614.7256 - mae: 27.8773 - val_loss: 2163.8191 - val_mse: 2163.8193 - val_mae: 26.2636\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2652.3188 - mse: 2652.3188 - mae: 27.8926 - val_loss: 2176.1793 - val_mse: 2176.1794 - val_mae: 25.9775\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 578us/step - loss: 2640.7805 - mse: 2640.7808 - mae: 27.6524 - val_loss: 2183.0935 - val_mse: 2183.0935 - val_mae: 25.9509\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2626.1911 - mse: 2626.1912 - mae: 27.7486 - val_loss: 2178.7259 - val_mse: 2178.7261 - val_mae: 26.1552\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2629.9957 - mse: 2629.9954 - mae: 28.1240 - val_loss: 2172.5898 - val_mse: 2172.5898 - val_mae: 26.0974\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 679us/step - loss: 2657.5165 - mse: 2657.5173 - mae: 27.8050 - val_loss: 2164.3339 - val_mse: 2164.3342 - val_mae: 26.3330\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 554us/step - loss: 2610.2635 - mse: 2610.2634 - mae: 27.7891 - val_loss: 2165.2276 - val_mse: 2165.2275 - val_mae: 26.2523\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 610us/step - loss: 2671.0044 - mse: 2671.0044 - mae: 28.3181 - val_loss: 2156.9197 - val_mse: 2156.9194 - val_mae: 26.2801\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2597.9672 - mse: 2597.9673 - mae: 27.7601 - val_loss: 2172.8255 - val_mse: 2172.8254 - val_mae: 26.1404\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 588us/step - loss: 2620.6657 - mse: 2620.6660 - mae: 27.9676 - val_loss: 2169.6239 - val_mse: 2169.6240 - val_mae: 26.1399\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 709us/step - loss: 2668.7973 - mse: 2668.7976 - mae: 28.3701 - val_loss: 2165.7051 - val_mse: 2165.7048 - val_mae: 26.1877\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 573us/step - loss: 2667.4987 - mse: 2667.4993 - mae: 28.0944 - val_loss: 2174.2357 - val_mse: 2174.2358 - val_mae: 26.1020\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 646us/step - loss: 2621.7337 - mse: 2621.7334 - mae: 27.8487 - val_loss: 2154.4365 - val_mse: 2154.4368 - val_mae: 26.5591\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 596us/step - loss: 2615.1489 - mse: 2615.1492 - mae: 27.9561 - val_loss: 2156.1651 - val_mse: 2156.1648 - val_mae: 26.5025\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 656us/step - loss: 2616.5289 - mse: 2616.5291 - mae: 27.9236 - val_loss: 2168.4798 - val_mse: 2168.4800 - val_mae: 26.1311\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 648us/step - loss: 2614.3756 - mse: 2614.3752 - mae: 27.7874 - val_loss: 2163.0971 - val_mse: 2163.0972 - val_mae: 26.1178\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 653us/step - loss: 2659.2983 - mse: 2659.2981 - mae: 28.2100 - val_loss: 2156.5715 - val_mse: 2156.5720 - val_mae: 26.4105\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2682.4752 - mse: 2682.4758 - mae: 28.0877 - val_loss: 2148.9941 - val_mse: 2148.9939 - val_mae: 26.5214\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 570us/step - loss: 2632.7477 - mse: 2632.7473 - mae: 28.1486 - val_loss: 2158.4236 - val_mse: 2158.4238 - val_mae: 26.1554\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2605.3270 - mse: 2605.3269 - mae: 27.5015 - val_loss: 2140.9474 - val_mse: 2140.9470 - val_mae: 26.6069\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2669.0185 - mse: 2669.0193 - mae: 27.9264 - val_loss: 2162.1441 - val_mse: 2162.1440 - val_mae: 26.1171\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 597us/step - loss: 2616.8537 - mse: 2616.8525 - mae: 27.8351 - val_loss: 2147.8576 - val_mse: 2147.8572 - val_mae: 26.2606\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - ETA: 0s - loss: 2638.4645 - mse: 2638.4634 - mae: 27.46 - 2s 568us/step - loss: 2627.6994 - mse: 2627.6982 - mae: 27.4518 - val_loss: 2158.7360 - val_mse: 2158.7358 - val_mae: 25.9954\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 674us/step - loss: 2651.8553 - mse: 2651.8550 - mae: 27.7047 - val_loss: 2147.4374 - val_mse: 2147.4375 - val_mae: 26.4689\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2678.0898 - mse: 2678.0894 - mae: 28.3779 - val_loss: 2160.5739 - val_mse: 2160.5740 - val_mae: 26.0553\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 2s 534us/step - loss: 2620.8894 - mse: 2620.8896 - mae: 27.8440 - val_loss: 2157.6395 - val_mse: 2157.6396 - val_mae: 26.2524\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 2s 494us/step - loss: 2613.5992 - mse: 2613.5991 - mae: 27.6128 - val_loss: 2161.7800 - val_mse: 2161.7800 - val_mae: 26.0697\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 2s 473us/step - loss: 2631.7950 - mse: 2631.7939 - mae: 28.0017 - val_loss: 2155.5089 - val_mse: 2155.5088 - val_mae: 26.2054\n",
      "Train on 500 samples, validate on 125 samples\n",
      "Epoch 1/80\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 13323.2319 - mse: 13323.2324 - mae: 109.8765 - val_loss: 34595.6571 - val_mse: 34595.6562 - val_mae: 132.6435\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 0s 536us/step - loss: 13150.0254 - mse: 13150.0254 - mae: 109.0950 - val_loss: 34263.6765 - val_mse: 34263.6758 - val_mae: 131.3898\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 12701.9724 - mse: 12701.9707 - mae: 107.0137 - val_loss: 33324.8884 - val_mse: 33324.8867 - val_mae: 127.7821\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 0s 664us/step - loss: 11491.2110 - mse: 11491.2090 - mae: 101.1613 - val_loss: 30844.0451 - val_mse: 30844.0449 - val_mae: 117.7133\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - 0s 761us/step - loss: 8685.8146 - mse: 8685.8154 - mae: 85.4586 - val_loss: 25329.0722 - val_mse: 25329.0723 - val_mae: 91.4310\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 4498.9456 - mse: 4498.9453 - mae: 53.6612 - val_loss: 18377.8643 - val_mse: 18377.8613 - val_mae: 39.7772\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 0s 660us/step - loss: 2941.7616 - mse: 2941.7615 - mae: 39.4692 - val_loss: 17540.7976 - val_mse: 17540.7969 - val_mae: 36.6057\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 0s 697us/step - loss: 2857.1982 - mse: 2857.1982 - mae: 38.8891 - val_loss: 17816.2463 - val_mse: 17816.2461 - val_mae: 36.6522\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 0s 640us/step - loss: 2633.6242 - mse: 2633.6240 - mae: 37.5687 - val_loss: 17611.0347 - val_mse: 17611.0332 - val_mae: 36.5545\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - 0s 564us/step - loss: 2766.9663 - mse: 2766.9663 - mae: 37.3862 - val_loss: 18001.1202 - val_mse: 18001.1191 - val_mae: 37.2468\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 0s 623us/step - loss: 2679.4783 - mse: 2679.4783 - mae: 37.6555 - val_loss: 17833.8873 - val_mse: 17833.8867 - val_mae: 36.6864\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 0s 690us/step - loss: 2530.2178 - mse: 2530.2175 - mae: 36.3090 - val_loss: 17938.7270 - val_mse: 17938.7266 - val_mae: 37.0194\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2537.2016 - mse: 2537.2017 - mae: 36.0702 - val_loss: 17805.2575 - val_mse: 17805.2598 - val_mae: 36.6889\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 0s 644us/step - loss: 2445.6292 - mse: 2445.6289 - mae: 36.0665 - val_loss: 17575.0495 - val_mse: 17575.0508 - val_mae: 36.6120\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - 0s 507us/step - loss: 2623.3360 - mse: 2623.3357 - mae: 37.2505 - val_loss: 17808.6371 - val_mse: 17808.6367 - val_mae: 36.7206\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 0s 532us/step - loss: 2729.4072 - mse: 2729.4072 - mae: 37.9866 - val_loss: 17846.3556 - val_mse: 17846.3555 - val_mae: 36.8145\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 0s 444us/step - loss: 2862.1872 - mse: 2862.1873 - mae: 38.2873 - val_loss: 17954.7414 - val_mse: 17954.7422 - val_mae: 37.1088\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 0s 447us/step - loss: 2721.7572 - mse: 2721.7571 - mae: 36.9057 - val_loss: 17791.6002 - val_mse: 17791.5996 - val_mae: 36.7189\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 0s 553us/step - loss: 2474.9495 - mse: 2474.9495 - mae: 35.2644 - val_loss: 17897.3091 - val_mse: 17897.3105 - val_mae: 37.0081\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - 0s 515us/step - loss: 2414.2090 - mse: 2414.2090 - mae: 35.7093 - val_loss: 17628.8076 - val_mse: 17628.8086 - val_mae: 36.6418\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 0s 585us/step - loss: 2678.4717 - mse: 2678.4714 - mae: 36.8055 - val_loss: 17882.2707 - val_mse: 17882.2695 - val_mae: 36.9760\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 0s 574us/step - loss: 2550.2032 - mse: 2550.2034 - mae: 36.7632 - val_loss: 18022.9723 - val_mse: 18022.9727 - val_mae: 37.5028\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 0s 590us/step - loss: 2555.8659 - mse: 2555.8660 - mae: 36.3562 - val_loss: 17602.5837 - val_mse: 17602.5840 - val_mae: 36.6641\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 0s 748us/step - loss: 2515.3884 - mse: 2515.3884 - mae: 36.0484 - val_loss: 17825.4471 - val_mse: 17825.4453 - val_mae: 36.8387\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 2488.8376 - mse: 2488.8376 - mae: 35.7590 - val_loss: 17867.8161 - val_mse: 17867.8184 - val_mae: 36.9392\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 0s 603us/step - loss: 2639.1854 - mse: 2639.1853 - mae: 35.8025 - val_loss: 17751.7991 - val_mse: 17751.8008 - val_mae: 36.7309\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 0s 687us/step - loss: 2485.7078 - mse: 2485.7078 - mae: 35.9315 - val_loss: 17624.4217 - val_mse: 17624.4219 - val_mae: 36.6935\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2368.7916 - mse: 2368.7917 - mae: 34.7851 - val_loss: 17724.8058 - val_mse: 17724.8066 - val_mae: 36.7241\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 0s 704us/step - loss: 2338.0347 - mse: 2338.0349 - mae: 34.3668 - val_loss: 17817.5208 - val_mse: 17817.5195 - val_mae: 36.9043\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - 0s 649us/step - loss: 2265.9626 - mse: 2265.9624 - mae: 33.4193 - val_loss: 17654.8208 - val_mse: 17654.8223 - val_mae: 36.7413\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 0s 631us/step - loss: 2401.2348 - mse: 2401.2349 - mae: 34.1000 - val_loss: 17844.8692 - val_mse: 17844.8691 - val_mae: 36.9919\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 0s 623us/step - loss: 2444.5702 - mse: 2444.5706 - mae: 34.3877 - val_loss: 17629.5373 - val_mse: 17629.5371 - val_mae: 36.7909\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 0s 641us/step - loss: 2187.3365 - mse: 2187.3367 - mae: 34.4245 - val_loss: 17578.3018 - val_mse: 17578.3047 - val_mae: 36.8430\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2420.3498 - mse: 2420.3496 - mae: 35.4612 - val_loss: 17797.1518 - val_mse: 17797.1504 - val_mae: 36.9129\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - 0s 521us/step - loss: 2323.7478 - mse: 2323.7480 - mae: 33.0034 - val_loss: 17696.1858 - val_mse: 17696.1855 - val_mae: 36.8049\n",
      "Epoch 36/80\n",
      "500/500 [==============================] - 0s 606us/step - loss: 2125.5616 - mse: 2125.5618 - mae: 33.3719 - val_loss: 17682.5005 - val_mse: 17682.5000 - val_mae: 36.8061\n",
      "Epoch 37/80\n",
      "500/500 [==============================] - 0s 568us/step - loss: 2366.4154 - mse: 2366.4155 - mae: 35.2936 - val_loss: 17996.8637 - val_mse: 17996.8613 - val_mae: 37.6299\n",
      "Epoch 38/80\n",
      "500/500 [==============================] - 0s 566us/step - loss: 2173.8384 - mse: 2173.8384 - mae: 32.9481 - val_loss: 17669.5263 - val_mse: 17669.5234 - val_mae: 36.8224\n",
      "Epoch 39/80\n",
      "500/500 [==============================] - 0s 543us/step - loss: 2310.1031 - mse: 2310.1033 - mae: 35.2215 - val_loss: 17594.6062 - val_mse: 17594.6074 - val_mae: 36.9114\n",
      "Epoch 40/80\n",
      "500/500 [==============================] - 0s 638us/step - loss: 2424.6641 - mse: 2424.6643 - mae: 34.2108 - val_loss: 17895.1144 - val_mse: 17895.1133 - val_mae: 37.2578\n",
      "Epoch 41/80\n",
      "500/500 [==============================] - 0s 639us/step - loss: 2253.8982 - mse: 2253.8982 - mae: 34.1479 - val_loss: 17627.9700 - val_mse: 17627.9707 - val_mae: 36.9200\n",
      "Epoch 42/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2309.8718 - mse: 2309.8718 - mae: 34.7199 - val_loss: 17891.9970 - val_mse: 17891.9980 - val_mae: 37.2552\n",
      "Epoch 43/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2270.8014 - mse: 2270.8015 - mae: 33.0445 - val_loss: 17664.4957 - val_mse: 17664.4961 - val_mae: 36.9072\n",
      "Epoch 44/80\n",
      "500/500 [==============================] - 0s 533us/step - loss: 2306.0573 - mse: 2306.0576 - mae: 34.1041 - val_loss: 17798.4935 - val_mse: 17798.4922 - val_mae: 37.0079\n",
      "Epoch 45/80\n",
      "500/500 [==============================] - 0s 708us/step - loss: 2195.7206 - mse: 2195.7207 - mae: 33.4741 - val_loss: 17887.7699 - val_mse: 17887.7695 - val_mae: 37.2911\n",
      "Epoch 46/80\n",
      "500/500 [==============================] - 0s 608us/step - loss: 2245.6841 - mse: 2245.6843 - mae: 33.4262 - val_loss: 17859.5149 - val_mse: 17859.5156 - val_mae: 37.2054\n",
      "Epoch 47/80\n",
      "500/500 [==============================] - 0s 610us/step - loss: 2120.2073 - mse: 2120.2070 - mae: 31.3412 - val_loss: 17721.7366 - val_mse: 17721.7363 - val_mae: 36.9960\n",
      "Epoch 48/80\n",
      "500/500 [==============================] - 0s 736us/step - loss: 1997.2898 - mse: 1997.2898 - mae: 31.7219 - val_loss: 17477.3820 - val_mse: 17477.3828 - val_mae: 37.2797\n",
      "Epoch 49/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2223.2757 - mse: 2223.2759 - mae: 32.6652 - val_loss: 17689.8109 - val_mse: 17689.8105 - val_mae: 37.0512\n",
      "Epoch 50/80\n",
      "500/500 [==============================] - 0s 736us/step - loss: 2177.4375 - mse: 2177.4375 - mae: 33.0100 - val_loss: 17761.5621 - val_mse: 17761.5625 - val_mae: 37.1078\n",
      "Epoch 51/80\n",
      "500/500 [==============================] - 0s 541us/step - loss: 2061.2950 - mse: 2061.2949 - mae: 31.8682 - val_loss: 17674.5826 - val_mse: 17674.5820 - val_mae: 37.1011\n",
      "Epoch 52/80\n",
      "500/500 [==============================] - 0s 578us/step - loss: 2074.0878 - mse: 2074.0879 - mae: 32.1690 - val_loss: 17688.1301 - val_mse: 17688.1309 - val_mae: 37.1193\n",
      "Epoch 53/80\n",
      "500/500 [==============================] - 0s 472us/step - loss: 2167.9626 - mse: 2167.9624 - mae: 31.7051 - val_loss: 17729.6360 - val_mse: 17729.6367 - val_mae: 37.1191\n",
      "Epoch 54/80\n",
      "500/500 [==============================] - 0s 503us/step - loss: 2274.9587 - mse: 2274.9587 - mae: 32.8659 - val_loss: 17717.1831 - val_mse: 17717.1836 - val_mae: 37.1131\n",
      "Epoch 55/80\n",
      "500/500 [==============================] - 0s 617us/step - loss: 2094.0503 - mse: 2094.0503 - mae: 31.3951 - val_loss: 17852.9484 - val_mse: 17852.9473 - val_mae: 37.4256\n",
      "Epoch 56/80\n",
      "500/500 [==============================] - 0s 529us/step - loss: 2226.7417 - mse: 2226.7415 - mae: 32.8699 - val_loss: 17681.0942 - val_mse: 17681.0938 - val_mae: 37.1592\n",
      "Epoch 57/80\n",
      "500/500 [==============================] - 0s 583us/step - loss: 2056.0617 - mse: 2056.0615 - mae: 31.1164 - val_loss: 17711.5523 - val_mse: 17711.5527 - val_mae: 37.1767\n",
      "Epoch 58/80\n",
      "500/500 [==============================] - 0s 670us/step - loss: 2003.4812 - mse: 2003.4813 - mae: 30.4545 - val_loss: 17659.2846 - val_mse: 17659.2852 - val_mae: 37.2236\n",
      "Epoch 59/80\n",
      "500/500 [==============================] - 0s 615us/step - loss: 2072.4594 - mse: 2072.4595 - mae: 31.5703 - val_loss: 18106.4179 - val_mse: 18106.4160 - val_mae: 38.9399\n",
      "Epoch 60/80\n",
      "500/500 [==============================] - 0s 598us/step - loss: 2031.3787 - mse: 2031.3788 - mae: 31.2456 - val_loss: 17764.8071 - val_mse: 17764.8066 - val_mae: 37.3493\n",
      "Epoch 61/80\n",
      "500/500 [==============================] - 0s 674us/step - loss: 2177.4817 - mse: 2177.4817 - mae: 32.9976 - val_loss: 17879.0014 - val_mse: 17879.0020 - val_mae: 37.6917\n",
      "Epoch 62/80\n",
      "500/500 [==============================] - 0s 673us/step - loss: 1990.3795 - mse: 1990.3795 - mae: 30.9142 - val_loss: 17811.7389 - val_mse: 17811.7402 - val_mae: 37.4629\n",
      "Epoch 63/80\n",
      "500/500 [==============================] - 0s 635us/step - loss: 2086.2114 - mse: 2086.2117 - mae: 30.9361 - val_loss: 17826.0812 - val_mse: 17826.0820 - val_mae: 37.5418\n",
      "Epoch 64/80\n",
      "500/500 [==============================] - 0s 668us/step - loss: 1964.8751 - mse: 1964.8751 - mae: 30.6008 - val_loss: 17742.2794 - val_mse: 17742.2793 - val_mae: 37.3848\n",
      "Epoch 65/80\n",
      "500/500 [==============================] - 0s 645us/step - loss: 2036.5253 - mse: 2036.5253 - mae: 32.0161 - val_loss: 17751.9705 - val_mse: 17751.9727 - val_mae: 37.4398\n",
      "Epoch 66/80\n",
      "500/500 [==============================] - 0s 622us/step - loss: 1966.8294 - mse: 1966.8293 - mae: 32.2412 - val_loss: 17858.5045 - val_mse: 17858.5039 - val_mae: 37.7773\n",
      "Epoch 67/80\n",
      "500/500 [==============================] - 0s 626us/step - loss: 2096.5766 - mse: 2096.5767 - mae: 32.1215 - val_loss: 17847.3923 - val_mse: 17847.3926 - val_mae: 37.7851\n",
      "Epoch 68/80\n",
      "500/500 [==============================] - 0s 616us/step - loss: 2060.5143 - mse: 2060.5144 - mae: 32.2756 - val_loss: 17919.0851 - val_mse: 17919.0859 - val_mae: 38.1741\n",
      "Epoch 69/80\n",
      "500/500 [==============================] - 0s 563us/step - loss: 1997.6253 - mse: 1997.6252 - mae: 31.0287 - val_loss: 17705.0127 - val_mse: 17705.0137 - val_mae: 37.5433\n",
      "Epoch 70/80\n",
      "500/500 [==============================] - 0s 611us/step - loss: 1939.8096 - mse: 1939.8096 - mae: 31.0427 - val_loss: 17780.6846 - val_mse: 17780.6836 - val_mae: 37.7319\n",
      "Epoch 71/80\n",
      "500/500 [==============================] - 0s 621us/step - loss: 1950.9569 - mse: 1950.9570 - mae: 31.6574 - val_loss: 17734.3298 - val_mse: 17734.3301 - val_mae: 37.6417\n",
      "Epoch 72/80\n",
      "500/500 [==============================] - 0s 593us/step - loss: 2217.8539 - mse: 2217.8540 - mae: 31.5477 - val_loss: 17809.9652 - val_mse: 17809.9668 - val_mae: 37.8592\n",
      "Epoch 73/80\n",
      "500/500 [==============================] - 0s 648us/step - loss: 1968.1342 - mse: 1968.1344 - mae: 30.3271 - val_loss: 17701.7984 - val_mse: 17701.8008 - val_mae: 37.6354\n",
      "Epoch 74/80\n",
      "500/500 [==============================] - 0s 652us/step - loss: 1977.1331 - mse: 1977.1334 - mae: 30.9333 - val_loss: 17757.0063 - val_mse: 17757.0059 - val_mae: 37.8028\n",
      "Epoch 75/80\n",
      "500/500 [==============================] - 0s 694us/step - loss: 1945.4410 - mse: 1945.4410 - mae: 31.2226 - val_loss: 17757.3712 - val_mse: 17757.3691 - val_mae: 37.8266\n",
      "Epoch 76/80\n",
      "500/500 [==============================] - 0s 664us/step - loss: 1877.5534 - mse: 1877.5535 - mae: 30.4584 - val_loss: 17716.2492 - val_mse: 17716.2480 - val_mae: 37.7596\n",
      "Epoch 77/80\n",
      "500/500 [==============================] - 0s 580us/step - loss: 2000.0466 - mse: 2000.0466 - mae: 31.2111 - val_loss: 17867.4585 - val_mse: 17867.4609 - val_mae: 38.2249\n",
      "Epoch 78/80\n",
      "500/500 [==============================] - 0s 582us/step - loss: 2077.9136 - mse: 2077.9136 - mae: 31.2443 - val_loss: 17660.9247 - val_mse: 17660.9258 - val_mae: 37.7105\n",
      "Epoch 79/80\n",
      "500/500 [==============================] - 0s 581us/step - loss: 1773.8635 - mse: 1773.8635 - mae: 28.8300 - val_loss: 17678.6397 - val_mse: 17678.6406 - val_mae: 37.7466\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 0s 559us/step - loss: 1972.1047 - mse: 1972.1047 - mae: 30.9135 - val_loss: 17997.6011 - val_mse: 17997.6016 - val_mae: 38.9539\n",
      "Train on 997 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "997/997 [==============================] - 1s 597us/step - loss: 4226.1557 - mse: 4226.1558 - mae: 34.8704 - val_loss: 2114.8180 - val_mse: 2114.8181 - val_mae: 30.8492\n",
      "Epoch 2/80\n",
      "997/997 [==============================] - 1s 638us/step - loss: 4303.8394 - mse: 4303.8394 - mae: 36.4451 - val_loss: 2240.9318 - val_mse: 2240.9319 - val_mae: 31.1930\n",
      "Epoch 3/80\n",
      "997/997 [==============================] - 1s 663us/step - loss: 4245.4952 - mse: 4245.4956 - mae: 34.7276 - val_loss: 2258.7137 - val_mse: 2258.7134 - val_mae: 31.2562\n",
      "Epoch 4/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4272.3936 - mse: 4272.3936 - mae: 35.2371 - val_loss: 2267.4215 - val_mse: 2267.4214 - val_mae: 31.2841\n",
      "Epoch 5/80\n",
      "997/997 [==============================] - 1s 606us/step - loss: 4321.0521 - mse: 4321.0518 - mae: 35.9358 - val_loss: 2305.3372 - val_mse: 2305.3369 - val_mae: 31.4221\n",
      "Epoch 6/80\n",
      "997/997 [==============================] - 1s 636us/step - loss: 4058.7683 - mse: 4058.7676 - mae: 34.7802 - val_loss: 2223.1749 - val_mse: 2223.1748 - val_mae: 31.1085\n",
      "Epoch 7/80\n",
      "997/997 [==============================] - 1s 560us/step - loss: 4228.3333 - mse: 4228.3335 - mae: 35.2049 - val_loss: 2330.3273 - val_mse: 2330.3274 - val_mae: 31.4856\n",
      "Epoch 8/80\n",
      "997/997 [==============================] - 1s 645us/step - loss: 4151.9574 - mse: 4151.9570 - mae: 34.6930 - val_loss: 2331.1335 - val_mse: 2331.1335 - val_mae: 31.4693\n",
      "Epoch 9/80\n",
      "997/997 [==============================] - 0s 460us/step - loss: 4182.1880 - mse: 4182.1875 - mae: 35.1353 - val_loss: 2299.0520 - val_mse: 2299.0522 - val_mae: 31.3357\n",
      "Epoch 10/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4265.6686 - mse: 4265.6675 - mae: 35.3100 - val_loss: 2303.1566 - val_mse: 2303.1565 - val_mae: 31.3567\n",
      "Epoch 11/80\n",
      "997/997 [==============================] - 1s 630us/step - loss: 4242.0274 - mse: 4242.0283 - mae: 35.7176 - val_loss: 2291.6166 - val_mse: 2291.6165 - val_mae: 31.2946\n",
      "Epoch 12/80\n",
      "997/997 [==============================] - 1s 660us/step - loss: 4071.0595 - mse: 4071.0596 - mae: 34.7432 - val_loss: 2241.1762 - val_mse: 2241.1765 - val_mae: 31.1013\n",
      "Epoch 13/80\n",
      "997/997 [==============================] - 1s 682us/step - loss: 3966.9888 - mse: 3966.9890 - mae: 34.0285 - val_loss: 2244.2488 - val_mse: 2244.2490 - val_mae: 31.1124\n",
      "Epoch 14/80\n",
      "997/997 [==============================] - 1s 649us/step - loss: 4158.0997 - mse: 4158.1001 - mae: 34.7638 - val_loss: 2219.2389 - val_mse: 2219.2388 - val_mae: 31.0187\n",
      "Epoch 15/80\n",
      "997/997 [==============================] - 1s 696us/step - loss: 4208.0567 - mse: 4208.0566 - mae: 35.7270 - val_loss: 2324.8127 - val_mse: 2324.8127 - val_mae: 31.3904\n",
      "Epoch 16/80\n",
      "997/997 [==============================] - 1s 572us/step - loss: 4139.0952 - mse: 4139.0957 - mae: 34.6121 - val_loss: 2300.9819 - val_mse: 2300.9822 - val_mae: 31.2833\n",
      "Epoch 17/80\n",
      "997/997 [==============================] - 1s 571us/step - loss: 4052.5968 - mse: 4052.5967 - mae: 33.2715 - val_loss: 2328.6809 - val_mse: 2328.6807 - val_mae: 31.3828\n",
      "Epoch 18/80\n",
      "997/997 [==============================] - 1s 540us/step - loss: 4112.3410 - mse: 4112.3408 - mae: 35.4843 - val_loss: 2310.7788 - val_mse: 2310.7791 - val_mae: 31.3073\n",
      "Epoch 19/80\n",
      "997/997 [==============================] - 1s 584us/step - loss: 4018.2879 - mse: 4018.2874 - mae: 33.8577 - val_loss: 2214.3626 - val_mse: 2214.3628 - val_mae: 30.9297\n",
      "Epoch 20/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 4299.4365 - mse: 4299.4370 - mae: 35.5622 - val_loss: 2284.8628 - val_mse: 2284.8628 - val_mae: 31.1904\n",
      "Epoch 21/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 4081.2881 - mse: 4081.2881 - mae: 34.3073 - val_loss: 2226.1070 - val_mse: 2226.1069 - val_mae: 30.9520\n",
      "Epoch 22/80\n",
      "997/997 [==============================] - 1s 586us/step - loss: 4195.8451 - mse: 4195.8457 - mae: 35.2062 - val_loss: 2297.0830 - val_mse: 2297.0828 - val_mae: 31.2199\n",
      "Epoch 23/80\n",
      "997/997 [==============================] - 1s 744us/step - loss: 4166.6017 - mse: 4166.6011 - mae: 33.9876 - val_loss: 2269.5641 - val_mse: 2269.5640 - val_mae: 31.1050\n",
      "Epoch 24/80\n",
      "997/997 [==============================] - 1s 678us/step - loss: 4046.9042 - mse: 4046.9043 - mae: 34.1036 - val_loss: 2246.6960 - val_mse: 2246.6960 - val_mae: 31.0048\n",
      "Epoch 25/80\n",
      "997/997 [==============================] - 1s 664us/step - loss: 4237.6480 - mse: 4237.6484 - mae: 35.1483 - val_loss: 2261.0596 - val_mse: 2261.0596 - val_mae: 31.0582\n",
      "Epoch 26/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4255.3770 - mse: 4255.3765 - mae: 34.5767 - val_loss: 2340.4490 - val_mse: 2340.4492 - val_mae: 31.3392\n",
      "Epoch 27/80\n",
      "997/997 [==============================] - 1s 595us/step - loss: 4259.4007 - mse: 4259.4009 - mae: 34.5720 - val_loss: 2321.7194 - val_mse: 2321.7195 - val_mae: 31.2718\n",
      "Epoch 28/80\n",
      "997/997 [==============================] - 1s 624us/step - loss: 4318.3723 - mse: 4318.3726 - mae: 35.1672 - val_loss: 2239.8837 - val_mse: 2239.8838 - val_mae: 30.9501\n",
      "Epoch 29/80\n",
      "997/997 [==============================] - 1s 676us/step - loss: 4339.5007 - mse: 4339.5005 - mae: 35.0968 - val_loss: 2340.1904 - val_mse: 2340.1904 - val_mae: 31.3093\n",
      "Epoch 30/80\n",
      "997/997 [==============================] - 1s 568us/step - loss: 4150.9580 - mse: 4150.9585 - mae: 33.8478 - val_loss: 2252.3444 - val_mse: 2252.3445 - val_mae: 30.9764\n",
      "Epoch 31/80\n",
      "997/997 [==============================] - 1s 672us/step - loss: 4172.0995 - mse: 4172.0996 - mae: 34.4631 - val_loss: 2314.4286 - val_mse: 2314.4285 - val_mae: 31.2026\n",
      "Epoch 32/80\n",
      "997/997 [==============================] - 1s 556us/step - loss: 4048.2315 - mse: 4048.2319 - mae: 33.4824 - val_loss: 2241.6211 - val_mse: 2241.6213 - val_mae: 30.9233\n",
      "Epoch 33/80\n",
      "997/997 [==============================] - 1s 562us/step - loss: 4106.2817 - mse: 4106.2817 - mae: 33.0549 - val_loss: 2215.4761 - val_mse: 2215.4763 - val_mae: 30.8237\n",
      "Epoch 34/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 4203.1081 - mse: 4203.1084 - mae: 34.4567 - val_loss: 2279.2278 - val_mse: 2279.2278 - val_mae: 31.0606\n",
      "Epoch 35/80\n",
      "997/997 [==============================] - 1s 671us/step - loss: 4160.6236 - mse: 4160.6240 - mae: 34.6003 - val_loss: 2242.0098 - val_mse: 2242.0098 - val_mae: 30.9086\n",
      "Epoch 36/80\n",
      "997/997 [==============================] - 1s 613us/step - loss: 4038.4721 - mse: 4038.4724 - mae: 33.8179 - val_loss: 2218.6017 - val_mse: 2218.6016 - val_mae: 30.8064\n",
      "Epoch 37/80\n",
      "997/997 [==============================] - 1s 647us/step - loss: 4150.5520 - mse: 4150.5522 - mae: 34.0389 - val_loss: 2301.7679 - val_mse: 2301.7678 - val_mae: 31.1117\n",
      "Epoch 38/80\n",
      "997/997 [==============================] - 1s 546us/step - loss: 4087.4024 - mse: 4087.4019 - mae: 34.3276 - val_loss: 2248.6863 - val_mse: 2248.6865 - val_mae: 30.9258\n",
      "Epoch 39/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 4248.0768 - mse: 4248.0767 - mae: 35.2617 - val_loss: 2323.4646 - val_mse: 2323.4646 - val_mae: 31.2144\n",
      "Epoch 40/80\n",
      "997/997 [==============================] - 1s 583us/step - loss: 4154.7871 - mse: 4154.7876 - mae: 34.9934 - val_loss: 2279.5132 - val_mse: 2279.5132 - val_mae: 31.0430\n",
      "Epoch 41/80\n",
      "997/997 [==============================] - 1s 675us/step - loss: 4036.5677 - mse: 4036.5671 - mae: 33.4438 - val_loss: 2216.8402 - val_mse: 2216.8401 - val_mae: 30.7872\n",
      "Epoch 42/80\n",
      "997/997 [==============================] - 1s 528us/step - loss: 4163.3076 - mse: 4163.3071 - mae: 33.9963 - val_loss: 2254.9595 - val_mse: 2254.9595 - val_mae: 30.9181\n",
      "Epoch 43/80\n",
      "997/997 [==============================] - 1s 518us/step - loss: 4001.5154 - mse: 4001.5146 - mae: 33.4190 - val_loss: 2213.3879 - val_mse: 2213.3879 - val_mae: 30.7572\n",
      "Epoch 44/80\n",
      "997/997 [==============================] - 1s 579us/step - loss: 4150.4134 - mse: 4150.4136 - mae: 34.1454 - val_loss: 2356.5737 - val_mse: 2356.5737 - val_mae: 31.2921\n",
      "Epoch 45/80\n",
      "997/997 [==============================] - 1s 581us/step - loss: 4177.1381 - mse: 4177.1377 - mae: 34.2062 - val_loss: 2358.1773 - val_mse: 2358.1775 - val_mae: 31.2973\n",
      "Epoch 46/80\n",
      "997/997 [==============================] - 1s 710us/step - loss: 4078.4243 - mse: 4078.4241 - mae: 33.5985 - val_loss: 2282.5063 - val_mse: 2282.5063 - val_mae: 31.0086\n",
      "Epoch 47/80\n",
      "997/997 [==============================] - 1s 673us/step - loss: 3895.1809 - mse: 3895.1809 - mae: 32.3891 - val_loss: 2256.5469 - val_mse: 2256.5471 - val_mae: 30.9030\n",
      "Epoch 48/80\n",
      "997/997 [==============================] - 1s 604us/step - loss: 4161.5700 - mse: 4161.5698 - mae: 34.9733 - val_loss: 2344.6512 - val_mse: 2344.6509 - val_mae: 31.2255\n",
      "Epoch 49/80\n",
      "997/997 [==============================] - 1s 587us/step - loss: 3807.0518 - mse: 3807.0518 - mae: 32.5968 - val_loss: 2224.0489 - val_mse: 2224.0491 - val_mae: 30.7620\n",
      "Epoch 50/80\n",
      "997/997 [==============================] - 1s 517us/step - loss: 4098.2573 - mse: 4098.2573 - mae: 33.7177 - val_loss: 2310.8144 - val_mse: 2310.8142 - val_mae: 31.0712\n",
      "Epoch 51/80\n",
      "997/997 [==============================] - 1s 512us/step - loss: 4109.3036 - mse: 4109.3042 - mae: 33.9223 - val_loss: 2241.4930 - val_mse: 2241.4929 - val_mae: 30.8240\n",
      "Epoch 52/80\n",
      "997/997 [==============================] - 1s 637us/step - loss: 3958.1335 - mse: 3958.1335 - mae: 32.9634 - val_loss: 2281.0374 - val_mse: 2281.0376 - val_mae: 30.9554\n",
      "Epoch 53/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 4093.7280 - mse: 4093.7280 - mae: 33.3848 - val_loss: 2247.9002 - val_mse: 2247.9001 - val_mae: 30.8265\n",
      "Epoch 54/80\n",
      "997/997 [==============================] - 1s 642us/step - loss: 4226.8893 - mse: 4226.8896 - mae: 33.7312 - val_loss: 2269.8856 - val_mse: 2269.8857 - val_mae: 30.8940\n",
      "Epoch 55/80\n",
      "997/997 [==============================] - 1s 692us/step - loss: 4141.5802 - mse: 4141.5801 - mae: 33.2849 - val_loss: 2236.9641 - val_mse: 2236.9641 - val_mae: 30.7573\n",
      "Epoch 56/80\n",
      "997/997 [==============================] - 1s 596us/step - loss: 4103.1853 - mse: 4103.1860 - mae: 33.9887 - val_loss: 2278.0289 - val_mse: 2278.0288 - val_mae: 30.8973\n",
      "Epoch 57/80\n",
      "997/997 [==============================] - 1s 610us/step - loss: 4075.7399 - mse: 4075.7402 - mae: 34.0123 - val_loss: 2309.4524 - val_mse: 2309.4524 - val_mae: 31.0119\n",
      "Epoch 58/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 4074.8456 - mse: 4074.8457 - mae: 33.6702 - val_loss: 2276.6840 - val_mse: 2276.6841 - val_mae: 30.8869\n",
      "Epoch 59/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4040.8924 - mse: 4040.8921 - mae: 32.8024 - val_loss: 2255.4490 - val_mse: 2255.4487 - val_mae: 30.8073\n",
      "Epoch 60/80\n",
      "997/997 [==============================] - 1s 622us/step - loss: 4062.8519 - mse: 4062.8523 - mae: 33.3561 - val_loss: 2268.8661 - val_mse: 2268.8660 - val_mae: 30.8604\n",
      "Epoch 61/80\n",
      "997/997 [==============================] - 1s 599us/step - loss: 4214.8930 - mse: 4214.8926 - mae: 33.4697 - val_loss: 2322.3452 - val_mse: 2322.3452 - val_mae: 31.0611\n",
      "Epoch 62/80\n",
      "997/997 [==============================] - 1s 591us/step - loss: 4168.5909 - mse: 4168.5908 - mae: 33.6266 - val_loss: 2258.3818 - val_mse: 2258.3818 - val_mae: 30.8496\n",
      "Epoch 63/80\n",
      "997/997 [==============================] - 1s 655us/step - loss: 4149.6661 - mse: 4149.6660 - mae: 33.3099 - val_loss: 2293.5858 - val_mse: 2293.5857 - val_mae: 30.9823\n",
      "Epoch 64/80\n",
      "997/997 [==============================] - 1s 585us/step - loss: 3998.2948 - mse: 3998.2952 - mae: 33.4651 - val_loss: 2258.0526 - val_mse: 2258.0525 - val_mae: 30.8557\n",
      "Epoch 65/80\n",
      "997/997 [==============================] - 1s 537us/step - loss: 4001.0466 - mse: 4001.0461 - mae: 33.8044 - val_loss: 2266.4110 - val_mse: 2266.4109 - val_mae: 30.8895\n",
      "Epoch 66/80\n",
      "997/997 [==============================] - 1s 565us/step - loss: 4059.2786 - mse: 4059.2791 - mae: 33.6640 - val_loss: 2333.1907 - val_mse: 2333.1904 - val_mae: 31.1253\n",
      "Epoch 67/80\n",
      "997/997 [==============================] - 1s 631us/step - loss: 4203.2483 - mse: 4203.2485 - mae: 33.8078 - val_loss: 2348.8827 - val_mse: 2348.8828 - val_mae: 31.1759\n",
      "Epoch 68/80\n",
      "997/997 [==============================] - 1s 667us/step - loss: 3905.4072 - mse: 3905.4072 - mae: 32.3085 - val_loss: 2235.0308 - val_mse: 2235.0308 - val_mae: 30.7430\n",
      "Epoch 69/80\n",
      "997/997 [==============================] - 1s 692us/step - loss: 4156.2080 - mse: 4156.2080 - mae: 33.9789 - val_loss: 2236.7619 - val_mse: 2236.7617 - val_mae: 30.7300\n",
      "Epoch 70/80\n",
      "997/997 [==============================] - 1s 582us/step - loss: 4010.0221 - mse: 4010.0222 - mae: 33.3552 - val_loss: 2319.1521 - val_mse: 2319.1523 - val_mae: 31.0260\n",
      "Epoch 71/80\n",
      "997/997 [==============================] - 1s 685us/step - loss: 4087.3785 - mse: 4087.3789 - mae: 33.3878 - val_loss: 2324.2322 - val_mse: 2324.2322 - val_mae: 31.0404\n",
      "Epoch 72/80\n",
      "997/997 [==============================] - 1s 656us/step - loss: 4060.8638 - mse: 4060.8638 - mae: 33.1075 - val_loss: 2266.9130 - val_mse: 2266.9131 - val_mae: 30.8360\n",
      "Epoch 73/80\n",
      "997/997 [==============================] - 1s 563us/step - loss: 4026.5742 - mse: 4026.5742 - mae: 33.6807 - val_loss: 2302.7218 - val_mse: 2302.7214 - val_mae: 30.9555\n",
      "Epoch 74/80\n",
      "997/997 [==============================] - 1s 634us/step - loss: 3891.9107 - mse: 3891.9111 - mae: 32.9812 - val_loss: 2230.2042 - val_mse: 2230.2043 - val_mae: 30.6792\n",
      "Epoch 75/80\n",
      "997/997 [==============================] - 1s 632us/step - loss: 3904.7285 - mse: 3904.7288 - mae: 33.3363 - val_loss: 2257.9340 - val_mse: 2257.9343 - val_mae: 30.7645\n",
      "Epoch 76/80\n",
      "997/997 [==============================] - 1s 646us/step - loss: 3943.9933 - mse: 3943.9934 - mae: 32.5070 - val_loss: 2226.5289 - val_mse: 2226.5291 - val_mae: 30.6607\n",
      "Epoch 77/80\n",
      "997/997 [==============================] - 1s 588us/step - loss: 3952.5364 - mse: 3952.5359 - mae: 33.0439 - val_loss: 2238.9788 - val_mse: 2238.9788 - val_mae: 30.7042\n",
      "Epoch 78/80\n",
      "997/997 [==============================] - 1s 607us/step - loss: 4025.0245 - mse: 4025.0249 - mae: 33.5336 - val_loss: 2295.1726 - val_mse: 2295.1729 - val_mae: 30.9000\n",
      "Epoch 79/80\n",
      "997/997 [==============================] - 1s 577us/step - loss: 4101.1377 - mse: 4101.1367 - mae: 33.2957 - val_loss: 2320.7580 - val_mse: 2320.7578 - val_mae: 30.9809\n",
      "Epoch 80/80\n",
      "997/997 [==============================] - 1s 608us/step - loss: 4063.9845 - mse: 4063.9844 - mae: 33.3301 - val_loss: 2323.6147 - val_mse: 2323.6147 - val_mae: 30.9894\n",
      "Train on 1495 samples, validate on 374 samples\n",
      "Epoch 1/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3359.4886 - mse: 3359.4893 - mae: 33.3482 - val_loss: 1492.7885 - val_mse: 1492.7886 - val_mae: 24.9655\n",
      "Epoch 2/80\n",
      "1495/1495 [==============================] - 1s 686us/step - loss: 3422.7363 - mse: 3422.7361 - mae: 33.0269 - val_loss: 1482.4368 - val_mse: 1482.4369 - val_mae: 25.1796\n",
      "Epoch 3/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3284.6086 - mse: 3284.6094 - mae: 33.3826 - val_loss: 1472.8632 - val_mse: 1472.8632 - val_mae: 25.5861\n",
      "Epoch 4/80\n",
      "1495/1495 [==============================] - 1s 654us/step - loss: 3287.4424 - mse: 3287.4431 - mae: 32.4250 - val_loss: 1481.7246 - val_mse: 1481.7246 - val_mae: 25.1263\n",
      "Epoch 5/80\n",
      "1495/1495 [==============================] - 1s 551us/step - loss: 3350.1273 - mse: 3350.1272 - mae: 32.2194 - val_loss: 1474.9110 - val_mse: 1474.9111 - val_mae: 25.3932\n",
      "Epoch 6/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3237.7156 - mse: 3237.7151 - mae: 32.5970 - val_loss: 1478.4615 - val_mse: 1478.4614 - val_mae: 25.3434\n",
      "Epoch 7/80\n",
      "1495/1495 [==============================] - 1s 568us/step - loss: 3229.3785 - mse: 3229.3789 - mae: 32.9773 - val_loss: 1476.6364 - val_mse: 1476.6364 - val_mae: 25.4210\n",
      "Epoch 8/80\n",
      "1495/1495 [==============================] - 1s 558us/step - loss: 3421.5522 - mse: 3421.5525 - mae: 32.7015 - val_loss: 1483.4618 - val_mse: 1483.4618 - val_mae: 25.1815\n",
      "Epoch 9/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3287.2062 - mse: 3287.2068 - mae: 32.7553 - val_loss: 1478.3702 - val_mse: 1478.3701 - val_mae: 25.5635\n",
      "Epoch 10/80\n",
      "1495/1495 [==============================] - 1s 599us/step - loss: 3376.3143 - mse: 3376.3135 - mae: 32.7040 - val_loss: 1479.5976 - val_mse: 1479.5978 - val_mae: 25.5914\n",
      "Epoch 11/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3308.1258 - mse: 3308.1265 - mae: 31.5456 - val_loss: 1483.6134 - val_mse: 1483.6135 - val_mae: 25.4668\n",
      "Epoch 12/80\n",
      "1495/1495 [==============================] - 1s 554us/step - loss: 3374.8797 - mse: 3374.8804 - mae: 32.9291 - val_loss: 1490.5410 - val_mse: 1490.5409 - val_mae: 25.2021\n",
      "Epoch 13/80\n",
      "1495/1495 [==============================] - 1s 574us/step - loss: 3229.8026 - mse: 3229.8030 - mae: 32.5802 - val_loss: 1484.6044 - val_mse: 1484.6046 - val_mae: 25.4172\n",
      "Epoch 14/80\n",
      "1495/1495 [==============================] - 1s 580us/step - loss: 3245.7784 - mse: 3245.7786 - mae: 31.4700 - val_loss: 1478.7212 - val_mse: 1478.7213 - val_mae: 25.6748\n",
      "Epoch 15/80\n",
      "1495/1495 [==============================] - 1s 602us/step - loss: 3313.9775 - mse: 3313.9775 - mae: 33.0580 - val_loss: 1494.9950 - val_mse: 1494.9951 - val_mae: 25.0381\n",
      "Epoch 16/80\n",
      "1495/1495 [==============================] - 1s 668us/step - loss: 3197.7185 - mse: 3197.7188 - mae: 31.3896 - val_loss: 1482.3948 - val_mse: 1482.3949 - val_mae: 25.3823\n",
      "Epoch 17/80\n",
      "1495/1495 [==============================] - 1s 606us/step - loss: 3366.2394 - mse: 3366.2390 - mae: 32.6634 - val_loss: 1480.4144 - val_mse: 1480.4144 - val_mae: 25.4710\n",
      "Epoch 18/80\n",
      "1495/1495 [==============================] - 1s 548us/step - loss: 3321.1819 - mse: 3321.1824 - mae: 32.4663 - val_loss: 1491.2199 - val_mse: 1491.2197 - val_mae: 25.1451\n",
      "Epoch 19/80\n",
      "1495/1495 [==============================] - 1s 541us/step - loss: 3277.3714 - mse: 3277.3713 - mae: 32.0124 - val_loss: 1480.8851 - val_mse: 1480.8849 - val_mae: 25.6407\n",
      "Epoch 20/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3333.9714 - mse: 3333.9717 - mae: 32.5403 - val_loss: 1482.8034 - val_mse: 1482.8036 - val_mae: 25.4486\n",
      "Epoch 21/80\n",
      "1495/1495 [==============================] - 1s 632us/step - loss: 3205.4329 - mse: 3205.4331 - mae: 32.0872 - val_loss: 1481.4349 - val_mse: 1481.4348 - val_mae: 25.4118\n",
      "Epoch 22/80\n",
      "1495/1495 [==============================] - 1s 569us/step - loss: 3353.3803 - mse: 3353.3806 - mae: 32.8986 - val_loss: 1489.3198 - val_mse: 1489.3198 - val_mae: 25.2570\n",
      "Epoch 23/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3389.4095 - mse: 3389.4089 - mae: 32.4286 - val_loss: 1491.2685 - val_mse: 1491.2687 - val_mae: 25.2115\n",
      "Epoch 24/80\n",
      "1495/1495 [==============================] - 1s 543us/step - loss: 3351.9413 - mse: 3351.9414 - mae: 32.5690 - val_loss: 1480.9502 - val_mse: 1480.9501 - val_mae: 25.5450\n",
      "Epoch 25/80\n",
      "1495/1495 [==============================] - 1s 653us/step - loss: 3251.8089 - mse: 3251.8091 - mae: 31.8968 - val_loss: 1479.1398 - val_mse: 1479.1399 - val_mae: 25.6243\n",
      "Epoch 26/80\n",
      "1495/1495 [==============================] - 1s 638us/step - loss: 3277.3724 - mse: 3277.3723 - mae: 31.5085 - val_loss: 1479.0748 - val_mse: 1479.0747 - val_mae: 25.4774\n",
      "Epoch 27/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3303.3569 - mse: 3303.3560 - mae: 32.6127 - val_loss: 1494.9096 - val_mse: 1494.9098 - val_mae: 25.0193\n",
      "Epoch 28/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3335.4546 - mse: 3335.4541 - mae: 32.0359 - val_loss: 1479.6293 - val_mse: 1479.6294 - val_mae: 25.5477\n",
      "Epoch 29/80\n",
      "1495/1495 [==============================] - 1s 619us/step - loss: 3301.9900 - mse: 3301.9902 - mae: 31.6051 - val_loss: 1482.4386 - val_mse: 1482.4385 - val_mae: 25.5201\n",
      "Epoch 30/80\n",
      "1495/1495 [==============================] - 1s 564us/step - loss: 3229.4165 - mse: 3229.4170 - mae: 32.3417 - val_loss: 1483.2344 - val_mse: 1483.2343 - val_mae: 25.5767\n",
      "Epoch 31/80\n",
      "1495/1495 [==============================] - 1s 550us/step - loss: 3367.3370 - mse: 3367.3374 - mae: 32.6502 - val_loss: 1487.1024 - val_mse: 1487.1027 - val_mae: 25.4727\n",
      "Epoch 32/80\n",
      "1495/1495 [==============================] - 1s 542us/step - loss: 3335.9884 - mse: 3335.9873 - mae: 32.0859 - val_loss: 1484.2178 - val_mse: 1484.2178 - val_mae: 25.6482\n",
      "Epoch 33/80\n",
      "1495/1495 [==============================] - 1s 533us/step - loss: 3346.9324 - mse: 3346.9321 - mae: 32.7120 - val_loss: 1483.5665 - val_mse: 1483.5667 - val_mae: 25.7744\n",
      "Epoch 34/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3341.5444 - mse: 3341.5444 - mae: 32.1000 - val_loss: 1480.8981 - val_mse: 1480.8982 - val_mae: 26.0187\n",
      "Epoch 35/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3258.1192 - mse: 3258.1194 - mae: 32.3127 - val_loss: 1481.0679 - val_mse: 1481.0679 - val_mae: 25.7514\n",
      "Epoch 36/80\n",
      "1495/1495 [==============================] - 1s 684us/step - loss: 3240.8420 - mse: 3240.8418 - mae: 31.4015 - val_loss: 1479.2885 - val_mse: 1479.2886 - val_mae: 26.0012\n",
      "Epoch 37/80\n",
      "1495/1495 [==============================] - 1s 663us/step - loss: 3234.8004 - mse: 3234.8003 - mae: 31.1859 - val_loss: 1481.5659 - val_mse: 1481.5658 - val_mae: 25.6361\n",
      "Epoch 38/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3219.8770 - mse: 3219.8772 - mae: 31.2073 - val_loss: 1481.3349 - val_mse: 1481.3348 - val_mae: 25.6032\n",
      "Epoch 39/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3348.8051 - mse: 3348.8057 - mae: 32.2439 - val_loss: 1482.5059 - val_mse: 1482.5060 - val_mae: 25.6015\n",
      "Epoch 40/80\n",
      "1495/1495 [==============================] - 1s 582us/step - loss: 3384.2563 - mse: 3384.2559 - mae: 32.6622 - val_loss: 1486.5033 - val_mse: 1486.5032 - val_mae: 25.4476\n",
      "Epoch 41/80\n",
      "1495/1495 [==============================] - 1s 596us/step - loss: 3278.4977 - mse: 3278.4973 - mae: 31.8839 - val_loss: 1487.3363 - val_mse: 1487.3362 - val_mae: 25.5258\n",
      "Epoch 42/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3265.3574 - mse: 3265.3579 - mae: 31.9571 - val_loss: 1484.6908 - val_mse: 1484.6908 - val_mae: 25.5036\n",
      "Epoch 43/80\n",
      "1495/1495 [==============================] - 1s 588us/step - loss: 3291.7559 - mse: 3291.7556 - mae: 31.6200 - val_loss: 1483.4368 - val_mse: 1483.4366 - val_mae: 25.6730\n",
      "Epoch 44/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3271.9049 - mse: 3271.9053 - mae: 31.8455 - val_loss: 1481.4559 - val_mse: 1481.4559 - val_mae: 25.8653\n",
      "Epoch 45/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3327.6225 - mse: 3327.6233 - mae: 32.3957 - val_loss: 1482.5659 - val_mse: 1482.5658 - val_mae: 25.3661\n",
      "Epoch 46/80\n",
      "1495/1495 [==============================] - 1s 691us/step - loss: 3233.6105 - mse: 3233.6111 - mae: 31.4686 - val_loss: 1478.8995 - val_mse: 1478.8998 - val_mae: 25.6609\n",
      "Epoch 47/80\n",
      "1495/1495 [==============================] - 1s 667us/step - loss: 3286.5088 - mse: 3286.5081 - mae: 32.6842 - val_loss: 1483.0364 - val_mse: 1483.0363 - val_mae: 25.4351\n",
      "Epoch 48/80\n",
      "1495/1495 [==============================] - 1s 655us/step - loss: 3158.7662 - mse: 3158.7651 - mae: 31.2643 - val_loss: 1479.4876 - val_mse: 1479.4877 - val_mae: 25.6374\n",
      "Epoch 49/80\n",
      "1495/1495 [==============================] - 1s 667us/step - loss: 3262.4248 - mse: 3262.4248 - mae: 31.6286 - val_loss: 1480.0011 - val_mse: 1480.0012 - val_mae: 25.4172\n",
      "Epoch 50/80\n",
      "1495/1495 [==============================] - 1s 624us/step - loss: 3258.9431 - mse: 3258.9431 - mae: 31.7754 - val_loss: 1476.5881 - val_mse: 1476.5880 - val_mae: 25.6972\n",
      "Epoch 51/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3353.4658 - mse: 3353.4658 - mae: 32.3389 - val_loss: 1476.6018 - val_mse: 1476.6019 - val_mae: 25.7644\n",
      "Epoch 52/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3245.5375 - mse: 3245.5374 - mae: 31.7352 - val_loss: 1478.2179 - val_mse: 1478.2181 - val_mae: 25.9260\n",
      "Epoch 53/80\n",
      "1495/1495 [==============================] - 1s 573us/step - loss: 3125.6516 - mse: 3125.6521 - mae: 30.6753 - val_loss: 1478.3255 - val_mse: 1478.3253 - val_mae: 25.9153\n",
      "Epoch 54/80\n",
      "1495/1495 [==============================] - 1s 578us/step - loss: 3239.5797 - mse: 3239.5796 - mae: 31.7102 - val_loss: 1481.3857 - val_mse: 1481.3857 - val_mae: 25.8569\n",
      "Epoch 55/80\n",
      "1495/1495 [==============================] - 1s 565us/step - loss: 3379.0605 - mse: 3379.0623 - mae: 32.2561 - val_loss: 1482.8829 - val_mse: 1482.8827 - val_mae: 25.6789\n",
      "Epoch 56/80\n",
      "1495/1495 [==============================] - 1s 522us/step - loss: 3296.5804 - mse: 3296.5815 - mae: 31.7052 - val_loss: 1482.2238 - val_mse: 1482.2238 - val_mae: 25.7043\n",
      "Epoch 57/80\n",
      "1495/1495 [==============================] - 1s 591us/step - loss: 3147.6632 - mse: 3147.6633 - mae: 31.1093 - val_loss: 1478.6038 - val_mse: 1478.6038 - val_mae: 25.8406\n",
      "Epoch 58/80\n",
      "1495/1495 [==============================] - 1s 665us/step - loss: 3228.2697 - mse: 3228.2695 - mae: 32.4055 - val_loss: 1480.4046 - val_mse: 1480.4045 - val_mae: 25.6092\n",
      "Epoch 59/80\n",
      "1495/1495 [==============================] - 1s 643us/step - loss: 3193.3854 - mse: 3193.3853 - mae: 31.3499 - val_loss: 1481.5579 - val_mse: 1481.5580 - val_mae: 25.5835\n",
      "Epoch 60/80\n",
      "1495/1495 [==============================] - 1s 610us/step - loss: 3150.0650 - mse: 3150.0645 - mae: 31.3705 - val_loss: 1479.8308 - val_mse: 1479.8306 - val_mae: 25.5307\n",
      "Epoch 61/80\n",
      "1495/1495 [==============================] - 1s 571us/step - loss: 3282.0736 - mse: 3282.0742 - mae: 31.8638 - val_loss: 1480.7061 - val_mse: 1480.7062 - val_mae: 25.7197\n",
      "Epoch 62/80\n",
      "1495/1495 [==============================] - 1s 440us/step - loss: 3329.0685 - mse: 3329.0686 - mae: 31.8864 - val_loss: 1486.1576 - val_mse: 1486.1576 - val_mae: 25.5583\n",
      "Epoch 63/80\n",
      "1495/1495 [==============================] - 1s 603us/step - loss: 3213.6172 - mse: 3213.6167 - mae: 32.0282 - val_loss: 1479.2256 - val_mse: 1479.2256 - val_mae: 25.9334\n",
      "Epoch 64/80\n",
      "1495/1495 [==============================] - 1s 538us/step - loss: 3265.6751 - mse: 3265.6748 - mae: 31.7093 - val_loss: 1480.7183 - val_mse: 1480.7183 - val_mae: 25.7588\n",
      "Epoch 65/80\n",
      "1495/1495 [==============================] - 1s 614us/step - loss: 3310.1937 - mse: 3310.1936 - mae: 31.3629 - val_loss: 1481.7417 - val_mse: 1481.7418 - val_mae: 25.8851\n",
      "Epoch 66/80\n",
      "1495/1495 [==============================] - 1s 589us/step - loss: 3332.9491 - mse: 3332.9485 - mae: 32.2599 - val_loss: 1484.8255 - val_mse: 1484.8256 - val_mae: 25.7050\n",
      "Epoch 67/80\n",
      "1495/1495 [==============================] - 1s 609us/step - loss: 3256.8402 - mse: 3256.8408 - mae: 31.6662 - val_loss: 1482.4988 - val_mse: 1482.4988 - val_mae: 25.7663\n",
      "Epoch 68/80\n",
      "1495/1495 [==============================] - 1s 592us/step - loss: 3127.6289 - mse: 3127.6299 - mae: 31.4313 - val_loss: 1485.3551 - val_mse: 1485.3551 - val_mae: 25.4778\n",
      "Epoch 69/80\n",
      "1495/1495 [==============================] - 1s 561us/step - loss: 3259.5072 - mse: 3259.5081 - mae: 31.3811 - val_loss: 1482.9529 - val_mse: 1482.9529 - val_mae: 25.5390\n",
      "Epoch 70/80\n",
      "1495/1495 [==============================] - 1s 552us/step - loss: 3233.5599 - mse: 3233.5603 - mae: 31.5414 - val_loss: 1484.4993 - val_mse: 1484.4994 - val_mae: 25.5697\n",
      "Epoch 71/80\n",
      "1495/1495 [==============================] - 1s 559us/step - loss: 3272.4614 - mse: 3272.4622 - mae: 32.3801 - val_loss: 1482.8490 - val_mse: 1482.8489 - val_mae: 25.6668\n",
      "Epoch 72/80\n",
      "1495/1495 [==============================] - 1s 576us/step - loss: 3193.3733 - mse: 3193.3740 - mae: 31.4877 - val_loss: 1480.1462 - val_mse: 1480.1462 - val_mae: 25.9112\n",
      "Epoch 73/80\n",
      "1495/1495 [==============================] - 1s 586us/step - loss: 3315.9937 - mse: 3315.9946 - mae: 31.8010 - val_loss: 1482.2123 - val_mse: 1482.2124 - val_mae: 25.6717\n",
      "Epoch 74/80\n",
      "1495/1495 [==============================] - 1s 567us/step - loss: 3178.4852 - mse: 3178.4844 - mae: 31.5918 - val_loss: 1483.3008 - val_mse: 1483.3008 - val_mae: 25.6787\n",
      "Epoch 75/80\n",
      "1495/1495 [==============================] - 1s 682us/step - loss: 3169.5379 - mse: 3169.5378 - mae: 31.4525 - val_loss: 1481.7593 - val_mse: 1481.7594 - val_mae: 25.7291\n",
      "Epoch 76/80\n",
      "1495/1495 [==============================] - 1s 616us/step - loss: 3207.4360 - mse: 3207.4358 - mae: 31.8397 - val_loss: 1480.8348 - val_mse: 1480.8347 - val_mae: 25.9417\n",
      "Epoch 77/80\n",
      "1495/1495 [==============================] - 1s 649us/step - loss: 3304.9243 - mse: 3304.9233 - mae: 31.9270 - val_loss: 1485.7372 - val_mse: 1485.7373 - val_mae: 25.4803\n",
      "Epoch 78/80\n",
      "1495/1495 [==============================] - 1s 601us/step - loss: 3216.4880 - mse: 3216.4883 - mae: 31.5798 - val_loss: 1484.4923 - val_mse: 1484.4922 - val_mae: 25.6572\n",
      "Epoch 79/80\n",
      "1495/1495 [==============================] - 1s 629us/step - loss: 3290.9397 - mse: 3290.9402 - mae: 31.9669 - val_loss: 1478.3638 - val_mse: 1478.3638 - val_mae: 25.8521\n",
      "Epoch 80/80\n",
      "1495/1495 [==============================] - 1s 607us/step - loss: 3224.9800 - mse: 3224.9802 - mae: 31.7375 - val_loss: 1478.2472 - val_mse: 1478.2473 - val_mae: 25.8759\n",
      "Train on 1992 samples, validate on 499 samples\n",
      "Epoch 1/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2906.3948 - mse: 2906.3948 - mae: 31.5709 - val_loss: 1065.3577 - val_mse: 1065.3577 - val_mae: 23.9116\n",
      "Epoch 2/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2923.2891 - mse: 2923.2883 - mae: 31.1939 - val_loss: 1061.5138 - val_mse: 1061.5140 - val_mae: 23.9876\n",
      "Epoch 3/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2954.8612 - mse: 2954.8613 - mae: 30.8602 - val_loss: 1063.5746 - val_mse: 1063.5746 - val_mae: 23.7610\n",
      "Epoch 4/80\n",
      "1992/1992 [==============================] - 1s 616us/step - loss: 2826.1500 - mse: 2826.1501 - mae: 30.5491 - val_loss: 1058.3437 - val_mse: 1058.3436 - val_mae: 24.1659\n",
      "Epoch 5/80\n",
      "1992/1992 [==============================] - 1s 599us/step - loss: 2943.7776 - mse: 2943.7781 - mae: 31.0079 - val_loss: 1055.5029 - val_mse: 1055.5028 - val_mae: 23.9538\n",
      "Epoch 6/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2978.9792 - mse: 2978.9792 - mae: 31.6129 - val_loss: 1055.5978 - val_mse: 1055.5977 - val_mae: 23.7582\n",
      "Epoch 7/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2946.6937 - mse: 2946.6938 - mae: 31.3066 - val_loss: 1054.0392 - val_mse: 1054.0391 - val_mae: 23.9776\n",
      "Epoch 8/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2898.8507 - mse: 2898.8501 - mae: 31.3325 - val_loss: 1052.9522 - val_mse: 1052.9523 - val_mae: 23.9217\n",
      "Epoch 9/80\n",
      "1992/1992 [==============================] - 1s 603us/step - loss: 2864.9362 - mse: 2864.9365 - mae: 31.1045 - val_loss: 1054.7580 - val_mse: 1054.7583 - val_mae: 23.6960\n",
      "Epoch 10/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2880.5872 - mse: 2880.5874 - mae: 30.7438 - val_loss: 1052.9018 - val_mse: 1052.9017 - val_mae: 23.6750\n",
      "Epoch 11/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2911.8173 - mse: 2911.8176 - mae: 31.1637 - val_loss: 1049.6712 - val_mse: 1049.6713 - val_mae: 23.9074\n",
      "Epoch 12/80\n",
      "1992/1992 [==============================] - 1s 588us/step - loss: 2901.3772 - mse: 2901.3772 - mae: 30.8564 - val_loss: 1053.1435 - val_mse: 1053.1436 - val_mae: 23.4501\n",
      "Epoch 13/80\n",
      "1992/1992 [==============================] - 1s 692us/step - loss: 2902.1028 - mse: 2902.1023 - mae: 31.0390 - val_loss: 1048.4278 - val_mse: 1048.4279 - val_mae: 23.7190\n",
      "Epoch 14/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2816.1434 - mse: 2816.1426 - mae: 30.7206 - val_loss: 1048.7633 - val_mse: 1048.7633 - val_mae: 23.5573\n",
      "Epoch 15/80\n",
      "1992/1992 [==============================] - 1s 638us/step - loss: 2887.5487 - mse: 2887.5486 - mae: 30.2850 - val_loss: 1045.4420 - val_mse: 1045.4419 - val_mae: 23.9700\n",
      "Epoch 16/80\n",
      "1992/1992 [==============================] - 1s 575us/step - loss: 2853.1255 - mse: 2853.1260 - mae: 30.4196 - val_loss: 1047.8157 - val_mse: 1047.8156 - val_mae: 23.7270\n",
      "Epoch 17/80\n",
      "1992/1992 [==============================] - 1s 532us/step - loss: 2896.1557 - mse: 2896.1558 - mae: 30.5299 - val_loss: 1046.5575 - val_mse: 1046.5575 - val_mae: 23.7287\n",
      "Epoch 18/80\n",
      "1992/1992 [==============================] - 1s 597us/step - loss: 2802.6972 - mse: 2802.6968 - mae: 30.7326 - val_loss: 1047.3500 - val_mse: 1047.3500 - val_mae: 23.7295\n",
      "Epoch 19/80\n",
      "1992/1992 [==============================] - 1s 552us/step - loss: 2935.6252 - mse: 2935.6255 - mae: 31.0175 - val_loss: 1047.3954 - val_mse: 1047.3954 - val_mae: 23.6324\n",
      "Epoch 20/80\n",
      "1992/1992 [==============================] - 1s 587us/step - loss: 2915.6464 - mse: 2915.6470 - mae: 30.9286 - val_loss: 1045.8710 - val_mse: 1045.8710 - val_mae: 23.6534\n",
      "Epoch 21/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2856.8567 - mse: 2856.8574 - mae: 31.0901 - val_loss: 1047.1967 - val_mse: 1047.1967 - val_mae: 23.4535\n",
      "Epoch 22/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2897.3170 - mse: 2897.3171 - mae: 30.6480 - val_loss: 1044.9872 - val_mse: 1044.9872 - val_mae: 23.6596\n",
      "Epoch 23/80\n",
      "1992/1992 [==============================] - 1s 556us/step - loss: 2860.8261 - mse: 2860.8271 - mae: 30.7759 - val_loss: 1042.9627 - val_mse: 1042.9626 - val_mae: 23.8274\n",
      "Epoch 24/80\n",
      "1992/1992 [==============================] - 1s 593us/step - loss: 2897.2824 - mse: 2897.2827 - mae: 31.0561 - val_loss: 1042.2362 - val_mse: 1042.2362 - val_mae: 23.9413\n",
      "Epoch 25/80\n",
      "1992/1992 [==============================] - 1s 625us/step - loss: 2901.0226 - mse: 2901.0217 - mae: 30.3114 - val_loss: 1041.3843 - val_mse: 1041.3844 - val_mae: 23.8065\n",
      "Epoch 26/80\n",
      "1992/1992 [==============================] - 1s 566us/step - loss: 2795.6139 - mse: 2795.6147 - mae: 30.2477 - val_loss: 1044.0173 - val_mse: 1044.0173 - val_mae: 23.4592\n",
      "Epoch 27/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2909.1809 - mse: 2909.1809 - mae: 30.6457 - val_loss: 1040.3547 - val_mse: 1040.3547 - val_mae: 24.0618\n",
      "Epoch 28/80\n",
      "1992/1992 [==============================] - 1s 554us/step - loss: 2817.9231 - mse: 2817.9233 - mae: 30.3576 - val_loss: 1041.5436 - val_mse: 1041.5436 - val_mae: 23.7391\n",
      "Epoch 29/80\n",
      "1992/1992 [==============================] - 1s 648us/step - loss: 2892.7999 - mse: 2892.7996 - mae: 30.7468 - val_loss: 1040.7893 - val_mse: 1040.7893 - val_mae: 23.7390\n",
      "Epoch 30/80\n",
      "1992/1992 [==============================] - 1s 658us/step - loss: 2796.2675 - mse: 2796.2683 - mae: 30.1129 - val_loss: 1039.9197 - val_mse: 1039.9196 - val_mae: 23.7757\n",
      "Epoch 31/80\n",
      "1992/1992 [==============================] - 1s 700us/step - loss: 2801.4480 - mse: 2801.4480 - mae: 29.9248 - val_loss: 1038.4021 - val_mse: 1038.4021 - val_mae: 23.8881\n",
      "Epoch 32/80\n",
      "1992/1992 [==============================] - 1s 647us/step - loss: 2762.2368 - mse: 2762.2363 - mae: 30.0065 - val_loss: 1038.6767 - val_mse: 1038.6769 - val_mae: 23.9989\n",
      "Epoch 33/80\n",
      "1992/1992 [==============================] - 1s 543us/step - loss: 2898.7529 - mse: 2898.7520 - mae: 30.9142 - val_loss: 1038.5857 - val_mse: 1038.5857 - val_mae: 23.8011\n",
      "Epoch 34/80\n",
      "1992/1992 [==============================] - 1s 601us/step - loss: 2855.2164 - mse: 2855.2161 - mae: 30.7010 - val_loss: 1040.6098 - val_mse: 1040.6099 - val_mae: 23.4483\n",
      "Epoch 35/80\n",
      "1992/1992 [==============================] - 1s 660us/step - loss: 2848.9597 - mse: 2848.9595 - mae: 30.3996 - val_loss: 1037.5316 - val_mse: 1037.5315 - val_mae: 23.9227\n",
      "Epoch 36/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2845.5973 - mse: 2845.5969 - mae: 30.3185 - val_loss: 1037.2122 - val_mse: 1037.2123 - val_mae: 24.0145\n",
      "Epoch 37/80\n",
      "1992/1992 [==============================] - 1s 632us/step - loss: 2859.2400 - mse: 2859.2407 - mae: 30.5480 - val_loss: 1037.9863 - val_mse: 1037.9862 - val_mae: 23.5590\n",
      "Epoch 38/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2939.2313 - mse: 2939.2305 - mae: 30.7075 - val_loss: 1036.3193 - val_mse: 1036.3192 - val_mae: 23.7969\n",
      "Epoch 39/80\n",
      "1992/1992 [==============================] - 1s 670us/step - loss: 2892.1958 - mse: 2892.1956 - mae: 31.1544 - val_loss: 1036.8186 - val_mse: 1036.8187 - val_mae: 23.9117\n",
      "Epoch 40/80\n",
      "1992/1992 [==============================] - 1s 561us/step - loss: 2799.6222 - mse: 2799.6226 - mae: 30.2430 - val_loss: 1036.6784 - val_mse: 1036.6783 - val_mae: 23.7699\n",
      "Epoch 41/80\n",
      "1992/1992 [==============================] - 1s 591us/step - loss: 2836.7975 - mse: 2836.7969 - mae: 30.1558 - val_loss: 1036.4807 - val_mse: 1036.4806 - val_mae: 23.7602\n",
      "Epoch 42/80\n",
      "1992/1992 [==============================] - 1s 598us/step - loss: 2795.8665 - mse: 2795.8660 - mae: 30.2685 - val_loss: 1034.9110 - val_mse: 1034.9110 - val_mae: 23.8237\n",
      "Epoch 43/80\n",
      "1992/1992 [==============================] - 1s 548us/step - loss: 2872.2435 - mse: 2872.2432 - mae: 30.5653 - val_loss: 1035.4243 - val_mse: 1035.4244 - val_mae: 23.7164\n",
      "Epoch 44/80\n",
      "1992/1992 [==============================] - 1s 610us/step - loss: 2846.1884 - mse: 2846.1882 - mae: 30.6222 - val_loss: 1036.0843 - val_mse: 1036.0844 - val_mae: 23.6826\n",
      "Epoch 45/80\n",
      "1992/1992 [==============================] - 1s 571us/step - loss: 2817.5102 - mse: 2817.5110 - mae: 30.2688 - val_loss: 1037.0368 - val_mse: 1037.0366 - val_mae: 23.5536\n",
      "Epoch 46/80\n",
      "1992/1992 [==============================] - 1s 614us/step - loss: 2811.2127 - mse: 2811.2131 - mae: 30.2832 - val_loss: 1036.2460 - val_mse: 1036.2460 - val_mae: 23.5681\n",
      "Epoch 47/80\n",
      "1992/1992 [==============================] - 1s 574us/step - loss: 2817.4067 - mse: 2817.4060 - mae: 29.5278 - val_loss: 1035.2233 - val_mse: 1035.2233 - val_mae: 23.9464\n",
      "Epoch 48/80\n",
      "1992/1992 [==============================] - 1s 708us/step - loss: 2882.8494 - mse: 2882.8494 - mae: 30.7570 - val_loss: 1038.4488 - val_mse: 1038.4487 - val_mae: 23.3819\n",
      "Epoch 49/80\n",
      "1992/1992 [==============================] - 1s 611us/step - loss: 2861.8386 - mse: 2861.8379 - mae: 31.0053 - val_loss: 1036.8243 - val_mse: 1036.8242 - val_mae: 23.5257\n",
      "Epoch 50/80\n",
      "1992/1992 [==============================] - 1s 608us/step - loss: 2801.4573 - mse: 2801.4578 - mae: 30.3999 - val_loss: 1037.7108 - val_mse: 1037.7107 - val_mae: 23.4415\n",
      "Epoch 51/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2866.2078 - mse: 2866.2083 - mae: 30.7855 - val_loss: 1035.0574 - val_mse: 1035.0574 - val_mae: 23.7126\n",
      "Epoch 52/80\n",
      "1992/1992 [==============================] - 1s 586us/step - loss: 2795.2687 - mse: 2795.2683 - mae: 30.2102 - val_loss: 1035.1927 - val_mse: 1035.1926 - val_mae: 24.2073\n",
      "Epoch 53/80\n",
      "1992/1992 [==============================] - 1s 544us/step - loss: 2884.7514 - mse: 2884.7510 - mae: 30.6875 - val_loss: 1036.3142 - val_mse: 1036.3142 - val_mae: 23.4420\n",
      "Epoch 54/80\n",
      "1992/1992 [==============================] - 1s 590us/step - loss: 2811.4007 - mse: 2811.4004 - mae: 29.9907 - val_loss: 1033.3704 - val_mse: 1033.3705 - val_mae: 23.7760\n",
      "Epoch 55/80\n",
      "1992/1992 [==============================] - 1s 595us/step - loss: 2889.9822 - mse: 2889.9827 - mae: 30.5452 - val_loss: 1034.0270 - val_mse: 1034.0271 - val_mae: 23.5009\n",
      "Epoch 56/80\n",
      "1992/1992 [==============================] - 1s 646us/step - loss: 2832.0618 - mse: 2832.0613 - mae: 30.3500 - val_loss: 1032.3526 - val_mse: 1032.3525 - val_mae: 23.6368\n",
      "Epoch 57/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2791.1861 - mse: 2791.1863 - mae: 30.2031 - val_loss: 1032.0351 - val_mse: 1032.0350 - val_mae: 23.9652\n",
      "Epoch 58/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2868.1096 - mse: 2868.1094 - mae: 30.4308 - val_loss: 1031.4487 - val_mse: 1031.4487 - val_mae: 23.9520\n",
      "Epoch 59/80\n",
      "1992/1992 [==============================] - 1s 674us/step - loss: 2756.7452 - mse: 2756.7449 - mae: 30.2024 - val_loss: 1030.9279 - val_mse: 1030.9277 - val_mae: 23.8636\n",
      "Epoch 60/80\n",
      "1992/1992 [==============================] - 1s 623us/step - loss: 2808.8962 - mse: 2808.8960 - mae: 30.0495 - val_loss: 1030.6362 - val_mse: 1030.6361 - val_mae: 23.6821\n",
      "Epoch 61/80\n",
      "1992/1992 [==============================] - 1s 606us/step - loss: 2840.1947 - mse: 2840.1946 - mae: 30.4085 - val_loss: 1030.1645 - val_mse: 1030.1646 - val_mae: 23.5599\n",
      "Epoch 62/80\n",
      "1992/1992 [==============================] - 1s 700us/step - loss: 2857.3749 - mse: 2857.3752 - mae: 30.6677 - val_loss: 1030.9001 - val_mse: 1030.9000 - val_mae: 23.4232\n",
      "Epoch 63/80\n",
      "1992/1992 [==============================] - 1s 652us/step - loss: 2776.6293 - mse: 2776.6292 - mae: 30.3028 - val_loss: 1030.0799 - val_mse: 1030.0798 - val_mae: 23.5216\n",
      "Epoch 64/80\n",
      "1992/1992 [==============================] - 1s 602us/step - loss: 2831.0163 - mse: 2831.0171 - mae: 30.2896 - val_loss: 1032.0213 - val_mse: 1032.0212 - val_mae: 23.3882\n",
      "Epoch 65/80\n",
      "1992/1992 [==============================] - 1s 589us/step - loss: 2792.8508 - mse: 2792.8503 - mae: 30.1649 - val_loss: 1029.6994 - val_mse: 1029.6992 - val_mae: 23.8916\n",
      "Epoch 66/80\n",
      "1992/1992 [==============================] - 1s 594us/step - loss: 2855.6523 - mse: 2855.6519 - mae: 30.8681 - val_loss: 1029.6820 - val_mse: 1029.6819 - val_mae: 23.5255\n",
      "Epoch 67/80\n",
      "1992/1992 [==============================] - 1s 600us/step - loss: 2819.0121 - mse: 2819.0122 - mae: 30.4045 - val_loss: 1028.5161 - val_mse: 1028.5161 - val_mae: 23.7210\n",
      "Epoch 68/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2779.4709 - mse: 2779.4702 - mae: 30.1933 - val_loss: 1028.2942 - val_mse: 1028.2943 - val_mae: 23.8205\n",
      "Epoch 69/80\n",
      "1992/1992 [==============================] - 1s 562us/step - loss: 2834.0427 - mse: 2834.0425 - mae: 30.5710 - val_loss: 1029.3595 - val_mse: 1029.3596 - val_mae: 24.0534\n",
      "Epoch 70/80\n",
      "1992/1992 [==============================] - 1s 558us/step - loss: 2791.4211 - mse: 2791.4221 - mae: 30.1042 - val_loss: 1028.4780 - val_mse: 1028.4780 - val_mae: 23.8093\n",
      "Epoch 71/80\n",
      "1992/1992 [==============================] - 1s 572us/step - loss: 2840.6233 - mse: 2840.6228 - mae: 30.2829 - val_loss: 1031.0374 - val_mse: 1031.0374 - val_mae: 23.3415\n",
      "Epoch 72/80\n",
      "1992/1992 [==============================] - 1s 585us/step - loss: 2734.4518 - mse: 2734.4521 - mae: 29.5063 - val_loss: 1029.3518 - val_mse: 1029.3518 - val_mae: 23.7303\n",
      "Epoch 73/80\n",
      "1992/1992 [==============================] - 1s 613us/step - loss: 2834.8011 - mse: 2834.8020 - mae: 30.6258 - val_loss: 1029.1643 - val_mse: 1029.1642 - val_mae: 23.4368\n",
      "Epoch 74/80\n",
      "1992/1992 [==============================] - 1s 631us/step - loss: 2814.7158 - mse: 2814.7163 - mae: 30.0539 - val_loss: 1028.4233 - val_mse: 1028.4233 - val_mae: 23.4069\n",
      "Epoch 75/80\n",
      "1992/1992 [==============================] - 1s 705us/step - loss: 2826.7560 - mse: 2826.7559 - mae: 30.6519 - val_loss: 1026.9958 - val_mse: 1026.9957 - val_mae: 23.5370\n",
      "Epoch 76/80\n",
      "1992/1992 [==============================] - 1s 682us/step - loss: 2751.3188 - mse: 2751.3191 - mae: 29.9075 - val_loss: 1026.8319 - val_mse: 1026.8317 - val_mae: 23.9543\n",
      "Epoch 77/80\n",
      "1992/1992 [==============================] - 1s 615us/step - loss: 2859.2312 - mse: 2859.2324 - mae: 29.8907 - val_loss: 1026.2053 - val_mse: 1026.2052 - val_mae: 23.7364\n",
      "Epoch 78/80\n",
      "1992/1992 [==============================] - 1s 636us/step - loss: 2795.8861 - mse: 2795.8855 - mae: 30.0022 - val_loss: 1027.6457 - val_mse: 1027.6458 - val_mae: 23.4616\n",
      "Epoch 79/80\n",
      "1992/1992 [==============================] - 1s 635us/step - loss: 2803.5740 - mse: 2803.5735 - mae: 29.5855 - val_loss: 1025.8642 - val_mse: 1025.8641 - val_mae: 23.5875\n",
      "Epoch 80/80\n",
      "1992/1992 [==============================] - 1s 582us/step - loss: 2747.4527 - mse: 2747.4521 - mae: 30.2163 - val_loss: 1026.2995 - val_mse: 1026.2994 - val_mae: 23.3724\n",
      "Train on 2490 samples, validate on 623 samples\n",
      "Epoch 1/80\n",
      "2490/2490 [==============================] - 1s 564us/step - loss: 2569.2966 - mse: 2569.2959 - mae: 30.0043 - val_loss: 1476.5534 - val_mse: 1476.5532 - val_mae: 26.5321\n",
      "Epoch 2/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2488.2177 - mse: 2488.2180 - mae: 29.4020 - val_loss: 1473.9240 - val_mse: 1473.9241 - val_mae: 26.5918\n",
      "Epoch 3/80\n",
      "2490/2490 [==============================] - 2s 623us/step - loss: 2514.2119 - mse: 2514.2124 - mae: 29.3213 - val_loss: 1472.4300 - val_mse: 1472.4299 - val_mae: 26.5939\n",
      "Epoch 4/80\n",
      "2490/2490 [==============================] - 1s 595us/step - loss: 2560.2105 - mse: 2560.2104 - mae: 30.0203 - val_loss: 1470.1577 - val_mse: 1470.1577 - val_mae: 26.5893\n",
      "Epoch 5/80\n",
      "2490/2490 [==============================] - 1s 556us/step - loss: 2500.1970 - mse: 2500.1968 - mae: 29.3574 - val_loss: 1465.9922 - val_mse: 1465.9923 - val_mae: 26.6653\n",
      "Epoch 6/80\n",
      "2490/2490 [==============================] - 1s 568us/step - loss: 2578.1858 - mse: 2578.1865 - mae: 29.9897 - val_loss: 1465.3799 - val_mse: 1465.3799 - val_mae: 26.6255\n",
      "Epoch 7/80\n",
      "2490/2490 [==============================] - 2s 604us/step - loss: 2500.3271 - mse: 2500.3267 - mae: 29.7362 - val_loss: 1464.7241 - val_mse: 1464.7240 - val_mae: 26.5803\n",
      "Epoch 8/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2446.7889 - mse: 2446.7886 - mae: 29.2122 - val_loss: 1461.6483 - val_mse: 1461.6482 - val_mae: 26.6182\n",
      "Epoch 9/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2516.6425 - mse: 2516.6421 - mae: 29.2240 - val_loss: 1447.7284 - val_mse: 1447.7283 - val_mae: 26.9532\n",
      "Epoch 10/80\n",
      "2490/2490 [==============================] - 2s 629us/step - loss: 2582.1312 - mse: 2582.1318 - mae: 30.3534 - val_loss: 1482.0770 - val_mse: 1482.0770 - val_mae: 26.1908\n",
      "Epoch 11/80\n",
      "2490/2490 [==============================] - 2s 655us/step - loss: 2492.9729 - mse: 2492.9727 - mae: 29.0656 - val_loss: 1472.1547 - val_mse: 1472.1548 - val_mae: 26.3132\n",
      "Epoch 12/80\n",
      "2490/2490 [==============================] - 1s 538us/step - loss: 2512.4614 - mse: 2512.4609 - mae: 29.7574 - val_loss: 1463.3676 - val_mse: 1463.3677 - val_mae: 26.4408\n",
      "Epoch 13/80\n",
      "2490/2490 [==============================] - 1s 580us/step - loss: 2550.9306 - mse: 2550.9312 - mae: 29.8832 - val_loss: 1454.0551 - val_mse: 1454.0551 - val_mae: 26.5566\n",
      "Epoch 14/80\n",
      "2490/2490 [==============================] - 2s 608us/step - loss: 2484.3795 - mse: 2484.3792 - mae: 29.2684 - val_loss: 1451.7626 - val_mse: 1451.7626 - val_mae: 26.5607\n",
      "Epoch 15/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2482.7967 - mse: 2482.7969 - mae: 29.3607 - val_loss: 1449.7431 - val_mse: 1449.7430 - val_mae: 26.5715\n",
      "Epoch 16/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2512.8306 - mse: 2512.8308 - mae: 29.3728 - val_loss: 1439.9943 - val_mse: 1439.9941 - val_mae: 26.7372\n",
      "Epoch 17/80\n",
      "2490/2490 [==============================] - 2s 614us/step - loss: 2558.1606 - mse: 2558.1606 - mae: 29.5264 - val_loss: 1451.3492 - val_mse: 1451.3490 - val_mae: 26.5332\n",
      "Epoch 18/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2488.5952 - mse: 2488.5957 - mae: 29.3785 - val_loss: 1449.4105 - val_mse: 1449.4105 - val_mae: 26.5507\n",
      "Epoch 19/80\n",
      "2490/2490 [==============================] - 1s 587us/step - loss: 2515.3026 - mse: 2515.3037 - mae: 29.1535 - val_loss: 1447.9904 - val_mse: 1447.9905 - val_mae: 26.5129\n",
      "Epoch 20/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2547.6413 - mse: 2547.6416 - mae: 29.7126 - val_loss: 1454.5089 - val_mse: 1454.5092 - val_mae: 26.3560\n",
      "Epoch 21/80\n",
      "2490/2490 [==============================] - 1s 530us/step - loss: 2472.5576 - mse: 2472.5583 - mae: 28.9061 - val_loss: 1439.7782 - val_mse: 1439.7783 - val_mae: 26.6352\n",
      "Epoch 22/80\n",
      "2490/2490 [==============================] - 1s 493us/step - loss: 2539.3073 - mse: 2539.3071 - mae: 29.8309 - val_loss: 1450.3355 - val_mse: 1450.3357 - val_mae: 26.3743\n",
      "Epoch 23/80\n",
      "2490/2490 [==============================] - 1s 583us/step - loss: 2473.9378 - mse: 2473.9382 - mae: 29.0012 - val_loss: 1441.0578 - val_mse: 1441.0579 - val_mae: 26.5073\n",
      "Epoch 24/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2428.3011 - mse: 2428.3015 - mae: 28.8630 - val_loss: 1433.0565 - val_mse: 1433.0565 - val_mae: 26.6198\n",
      "Epoch 25/80\n",
      "2490/2490 [==============================] - 2s 616us/step - loss: 2464.0284 - mse: 2464.0298 - mae: 28.7921 - val_loss: 1439.1651 - val_mse: 1439.1652 - val_mae: 26.4260\n",
      "Epoch 26/80\n",
      "2490/2490 [==============================] - 2s 647us/step - loss: 2508.5805 - mse: 2508.5811 - mae: 29.2101 - val_loss: 1448.7652 - val_mse: 1448.7655 - val_mae: 26.2422\n",
      "Epoch 27/80\n",
      "2490/2490 [==============================] - 2s 612us/step - loss: 2440.2803 - mse: 2440.2800 - mae: 28.4146 - val_loss: 1434.5494 - val_mse: 1434.5494 - val_mae: 26.4359\n",
      "Epoch 28/80\n",
      "2490/2490 [==============================] - 2s 672us/step - loss: 2479.3706 - mse: 2479.3708 - mae: 28.9301 - val_loss: 1431.9614 - val_mse: 1431.9614 - val_mae: 26.4947\n",
      "Epoch 29/80\n",
      "2490/2490 [==============================] - 2s 610us/step - loss: 2485.6517 - mse: 2485.6519 - mae: 29.1994 - val_loss: 1427.7834 - val_mse: 1427.7833 - val_mae: 26.5411\n",
      "Epoch 30/80\n",
      "2490/2490 [==============================] - 1s 569us/step - loss: 2537.8882 - mse: 2537.8884 - mae: 29.5056 - val_loss: 1439.2681 - val_mse: 1439.2679 - val_mae: 26.2772\n",
      "Epoch 31/80\n",
      "2490/2490 [==============================] - ETA: 0s - loss: 2584.4511 - mse: 2584.4517 - mae: 29.70 - 2s 675us/step - loss: 2540.9307 - mse: 2540.9312 - mae: 29.5255 - val_loss: 1436.6144 - val_mse: 1436.6145 - val_mae: 26.3071\n",
      "Epoch 32/80\n",
      "2490/2490 [==============================] - 1s 490us/step - loss: 2401.8971 - mse: 2401.8965 - mae: 28.5525 - val_loss: 1432.6957 - val_mse: 1432.6958 - val_mae: 26.3876\n",
      "Epoch 33/80\n",
      "2490/2490 [==============================] - 2s 611us/step - loss: 2411.0338 - mse: 2411.0344 - mae: 28.8281 - val_loss: 1435.0989 - val_mse: 1435.0989 - val_mae: 26.3664\n",
      "Epoch 34/80\n",
      "2490/2490 [==============================] - 1s 548us/step - loss: 2468.2849 - mse: 2468.2844 - mae: 28.6573 - val_loss: 1435.5011 - val_mse: 1435.5011 - val_mae: 26.2995\n",
      "Epoch 35/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2483.0714 - mse: 2483.0708 - mae: 29.2586 - val_loss: 1427.3909 - val_mse: 1427.3910 - val_mae: 26.3859\n",
      "Epoch 36/80\n",
      "2490/2490 [==============================] - 2s 624us/step - loss: 2466.3385 - mse: 2466.3376 - mae: 29.3439 - val_loss: 1432.9295 - val_mse: 1432.9293 - val_mae: 26.2160\n",
      "Epoch 37/80\n",
      "2490/2490 [==============================] - 1s 529us/step - loss: 2487.0512 - mse: 2487.0503 - mae: 29.3441 - val_loss: 1419.0362 - val_mse: 1419.0363 - val_mae: 26.4909\n",
      "Epoch 38/80\n",
      "2490/2490 [==============================] - 1s 590us/step - loss: 2404.6046 - mse: 2404.6050 - mae: 29.0250 - val_loss: 1416.0179 - val_mse: 1416.0181 - val_mae: 26.5362\n",
      "Epoch 39/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2427.7991 - mse: 2427.7988 - mae: 29.1900 - val_loss: 1426.5534 - val_mse: 1426.5533 - val_mae: 26.2786\n",
      "Epoch 40/80\n",
      "2490/2490 [==============================] - 2s 607us/step - loss: 2491.6997 - mse: 2491.7004 - mae: 29.3282 - val_loss: 1430.6091 - val_mse: 1430.6090 - val_mae: 26.2072\n",
      "Epoch 41/80\n",
      "2490/2490 [==============================] - 2s 625us/step - loss: 2476.5771 - mse: 2476.5774 - mae: 29.2320 - val_loss: 1420.0150 - val_mse: 1420.0149 - val_mae: 26.4429\n",
      "Epoch 42/80\n",
      "2490/2490 [==============================] - 1s 557us/step - loss: 2496.9134 - mse: 2496.9136 - mae: 29.5721 - val_loss: 1437.5388 - val_mse: 1437.5387 - val_mae: 26.0814\n",
      "Epoch 43/80\n",
      "2490/2490 [==============================] - 1s 517us/step - loss: 2361.6897 - mse: 2361.6899 - mae: 28.6190 - val_loss: 1417.9488 - val_mse: 1417.9489 - val_mae: 26.4409\n",
      "Epoch 44/80\n",
      "2490/2490 [==============================] - 1s 565us/step - loss: 2521.4250 - mse: 2521.4248 - mae: 29.6180 - val_loss: 1434.5780 - val_mse: 1434.5779 - val_mae: 26.0919\n",
      "Epoch 45/80\n",
      "2490/2490 [==============================] - 1s 515us/step - loss: 2432.0524 - mse: 2432.0525 - mae: 28.8460 - val_loss: 1432.3472 - val_mse: 1432.3472 - val_mae: 26.1007\n",
      "Epoch 46/80\n",
      "2490/2490 [==============================] - 2s 662us/step - loss: 2438.6382 - mse: 2438.6384 - mae: 28.9790 - val_loss: 1412.0728 - val_mse: 1412.0728 - val_mae: 26.4510\n",
      "Epoch 47/80\n",
      "2490/2490 [==============================] - 2s 671us/step - loss: 2450.5992 - mse: 2450.5996 - mae: 28.5808 - val_loss: 1417.4474 - val_mse: 1417.4473 - val_mae: 26.3009\n",
      "Epoch 48/80\n",
      "2490/2490 [==============================] - 1s 597us/step - loss: 2497.3772 - mse: 2497.3777 - mae: 29.4088 - val_loss: 1425.7377 - val_mse: 1425.7375 - val_mae: 26.1350\n",
      "Epoch 49/80\n",
      "2490/2490 [==============================] - 2s 690us/step - loss: 2375.9866 - mse: 2375.9856 - mae: 28.9764 - val_loss: 1409.3292 - val_mse: 1409.3293 - val_mae: 26.4182\n",
      "Epoch 50/80\n",
      "2490/2490 [==============================] - 2s 633us/step - loss: 2403.2779 - mse: 2403.2783 - mae: 29.1431 - val_loss: 1411.2345 - val_mse: 1411.2344 - val_mae: 26.2930\n",
      "Epoch 51/80\n",
      "2490/2490 [==============================] - 2s 636us/step - loss: 2471.9346 - mse: 2471.9346 - mae: 29.2242 - val_loss: 1416.5645 - val_mse: 1416.5645 - val_mae: 26.1938\n",
      "Epoch 52/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2473.4020 - mse: 2473.4019 - mae: 29.0566 - val_loss: 1412.3122 - val_mse: 1412.3120 - val_mae: 26.2698\n",
      "Epoch 53/80\n",
      "2490/2490 [==============================] - 2s 646us/step - loss: 2507.1572 - mse: 2507.1560 - mae: 28.9380 - val_loss: 1420.1268 - val_mse: 1420.1267 - val_mae: 26.1222\n",
      "Epoch 54/80\n",
      "2490/2490 [==============================] - 2s 664us/step - loss: 2517.4753 - mse: 2517.4751 - mae: 29.6873 - val_loss: 1432.1043 - val_mse: 1432.1045 - val_mae: 25.9466\n",
      "Epoch 55/80\n",
      "2490/2490 [==============================] - 2s 660us/step - loss: 2432.8978 - mse: 2432.8987 - mae: 28.7808 - val_loss: 1418.8643 - val_mse: 1418.8645 - val_mae: 26.0975\n",
      "Epoch 56/80\n",
      "2490/2490 [==============================] - 2s 632us/step - loss: 2509.6787 - mse: 2509.6792 - mae: 29.6178 - val_loss: 1421.4446 - val_mse: 1421.4447 - val_mae: 26.0001\n",
      "Epoch 57/80\n",
      "2490/2490 [==============================] - 1s 534us/step - loss: 2384.0686 - mse: 2384.0698 - mae: 28.2258 - val_loss: 1404.3394 - val_mse: 1404.3394 - val_mae: 26.3222\n",
      "Epoch 58/80\n",
      "2490/2490 [==============================] - 1s 537us/step - loss: 2431.9766 - mse: 2431.9763 - mae: 28.9366 - val_loss: 1411.7600 - val_mse: 1411.7599 - val_mae: 26.0694\n",
      "Epoch 59/80\n",
      "2490/2490 [==============================] - 2s 650us/step - loss: 2446.9444 - mse: 2446.9436 - mae: 28.9389 - val_loss: 1409.4903 - val_mse: 1409.4904 - val_mae: 26.1044\n",
      "Epoch 60/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2473.4549 - mse: 2473.4543 - mae: 29.1441 - val_loss: 1407.0681 - val_mse: 1407.0679 - val_mae: 26.1242\n",
      "Epoch 61/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2479.1873 - mse: 2479.1870 - mae: 29.2544 - val_loss: 1407.6944 - val_mse: 1407.6943 - val_mae: 26.0906\n",
      "Epoch 62/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2506.3848 - mse: 2506.3853 - mae: 29.0857 - val_loss: 1418.9396 - val_mse: 1418.9395 - val_mae: 25.9324\n",
      "Epoch 63/80\n",
      "2490/2490 [==============================] - 2s 621us/step - loss: 2478.2815 - mse: 2478.2803 - mae: 29.0119 - val_loss: 1412.6130 - val_mse: 1412.6129 - val_mae: 26.0004\n",
      "Epoch 64/80\n",
      "2490/2490 [==============================] - 1s 582us/step - loss: 2407.3776 - mse: 2407.3782 - mae: 28.5948 - val_loss: 1404.6398 - val_mse: 1404.6400 - val_mae: 26.2065\n",
      "Epoch 65/80\n",
      "2490/2490 [==============================] - 1s 594us/step - loss: 2372.4637 - mse: 2372.4639 - mae: 28.5005 - val_loss: 1400.0839 - val_mse: 1400.0836 - val_mae: 26.3287\n",
      "Epoch 66/80\n",
      "2490/2490 [==============================] - 1s 525us/step - loss: 2390.5017 - mse: 2390.5017 - mae: 28.9461 - val_loss: 1412.2152 - val_mse: 1412.2152 - val_mae: 26.0276\n",
      "Epoch 67/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2445.1869 - mse: 2445.1868 - mae: 28.8183 - val_loss: 1404.5175 - val_mse: 1404.5175 - val_mae: 26.1340\n",
      "Epoch 68/80\n",
      "2490/2490 [==============================] - 1s 572us/step - loss: 2403.4512 - mse: 2403.4519 - mae: 28.3965 - val_loss: 1401.0758 - val_mse: 1401.0757 - val_mae: 26.1829\n",
      "Epoch 69/80\n",
      "2490/2490 [==============================] - 2s 641us/step - loss: 2420.4151 - mse: 2420.4143 - mae: 28.5700 - val_loss: 1414.1474 - val_mse: 1414.1473 - val_mae: 25.9086\n",
      "Epoch 70/80\n",
      "2490/2490 [==============================] - 1s 554us/step - loss: 2454.7581 - mse: 2454.7578 - mae: 29.0465 - val_loss: 1410.5755 - val_mse: 1410.5757 - val_mae: 25.9294\n",
      "Epoch 71/80\n",
      "2490/2490 [==============================] - 2s 643us/step - loss: 2494.7426 - mse: 2494.7424 - mae: 29.1610 - val_loss: 1412.2684 - val_mse: 1412.2683 - val_mae: 25.9509\n",
      "Epoch 72/80\n",
      "2490/2490 [==============================] - 1s 537us/step - loss: 2441.7289 - mse: 2441.7297 - mae: 29.0266 - val_loss: 1412.8160 - val_mse: 1412.8159 - val_mae: 25.9272\n",
      "Epoch 73/80\n",
      "2490/2490 [==============================] - 2s 617us/step - loss: 2499.6461 - mse: 2499.6458 - mae: 28.9623 - val_loss: 1405.4937 - val_mse: 1405.4935 - val_mae: 26.0944\n",
      "Epoch 74/80\n",
      "2490/2490 [==============================] - 2s 669us/step - loss: 2455.8931 - mse: 2455.8940 - mae: 29.0167 - val_loss: 1417.5799 - val_mse: 1417.5801 - val_mae: 25.8365\n",
      "Epoch 75/80\n",
      "2490/2490 [==============================] - 2s 628us/step - loss: 2414.9986 - mse: 2414.9988 - mae: 28.5249 - val_loss: 1415.7023 - val_mse: 1415.7023 - val_mae: 25.8749\n",
      "Epoch 76/80\n",
      "2490/2490 [==============================] - 2s 654us/step - loss: 2443.1401 - mse: 2443.1401 - mae: 29.0224 - val_loss: 1405.6272 - val_mse: 1405.6272 - val_mae: 26.0689\n",
      "Epoch 77/80\n",
      "2490/2490 [==============================] - 1s 599us/step - loss: 2430.3229 - mse: 2430.3228 - mae: 28.7065 - val_loss: 1397.7041 - val_mse: 1397.7041 - val_mae: 26.2661\n",
      "Epoch 78/80\n",
      "2490/2490 [==============================] - 1s 586us/step - loss: 2440.6590 - mse: 2440.6589 - mae: 28.9777 - val_loss: 1398.1480 - val_mse: 1398.1479 - val_mae: 26.1955\n",
      "Epoch 79/80\n",
      "2490/2490 [==============================] - 1s 598us/step - loss: 2411.4529 - mse: 2411.4534 - mae: 28.8561 - val_loss: 1407.6978 - val_mse: 1407.6979 - val_mae: 25.9581\n",
      "Epoch 80/80\n",
      "2490/2490 [==============================] - 1s 575us/step - loss: 2417.2490 - mse: 2417.2490 - mae: 29.0606 - val_loss: 1409.9473 - val_mse: 1409.9474 - val_mae: 25.8700\n",
      "Train on 2988 samples, validate on 747 samples\n",
      "Epoch 1/80\n",
      "2988/2988 [==============================] - 2s 671us/step - loss: 2327.1381 - mse: 2327.1394 - mae: 28.8261 - val_loss: 3647.0609 - val_mse: 3647.0598 - val_mae: 23.2462\n",
      "Epoch 2/80\n",
      "2988/2988 [==============================] - 2s 662us/step - loss: 2344.9378 - mse: 2344.9382 - mae: 28.9821 - val_loss: 3649.4318 - val_mse: 3649.4316 - val_mae: 23.8914\n",
      "Epoch 3/80\n",
      "2988/2988 [==============================] - 2s 644us/step - loss: 2320.0434 - mse: 2320.0435 - mae: 28.8450 - val_loss: 3649.0737 - val_mse: 3649.0747 - val_mae: 23.6468\n",
      "Epoch 4/80\n",
      "2988/2988 [==============================] - 2s 650us/step - loss: 2343.9442 - mse: 2343.9436 - mae: 29.1428 - val_loss: 3650.1556 - val_mse: 3650.1560 - val_mae: 23.7329\n",
      "Epoch 5/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2285.7429 - mse: 2285.7422 - mae: 29.0688 - val_loss: 3651.9583 - val_mse: 3651.9585 - val_mae: 23.7799\n",
      "Epoch 6/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2346.0773 - mse: 2346.0776 - mae: 29.4357 - val_loss: 3652.5582 - val_mse: 3652.5576 - val_mae: 23.8189\n",
      "Epoch 7/80\n",
      "2988/2988 [==============================] - 2s 672us/step - loss: 2340.2762 - mse: 2340.2769 - mae: 29.2782 - val_loss: 3651.8355 - val_mse: 3651.8347 - val_mae: 23.6067\n",
      "Epoch 8/80\n",
      "2988/2988 [==============================] - 2s 610us/step - loss: 2340.8407 - mse: 2340.8406 - mae: 29.3873 - val_loss: 3651.7941 - val_mse: 3651.7944 - val_mae: 23.5983\n",
      "Epoch 9/80\n",
      "2988/2988 [==============================] - 2s 633us/step - loss: 2289.5022 - mse: 2289.5029 - mae: 28.3832 - val_loss: 3653.3036 - val_mse: 3653.3032 - val_mae: 24.0150\n",
      "Epoch 10/80\n",
      "2988/2988 [==============================] - 2s 580us/step - loss: 2328.0576 - mse: 2328.0574 - mae: 29.0044 - val_loss: 3652.4279 - val_mse: 3652.4280 - val_mae: 23.7843\n",
      "Epoch 11/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2295.4390 - mse: 2295.4390 - mae: 28.9256 - val_loss: 3653.0965 - val_mse: 3653.0964 - val_mae: 23.8973\n",
      "Epoch 12/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2296.7853 - mse: 2296.7852 - mae: 29.2151 - val_loss: 3652.5456 - val_mse: 3652.5452 - val_mae: 23.3182\n",
      "Epoch 13/80\n",
      "2988/2988 [==============================] - 2s 555us/step - loss: 2311.4817 - mse: 2311.4827 - mae: 29.1518 - val_loss: 3652.8151 - val_mse: 3652.8152 - val_mae: 23.5762\n",
      "Epoch 14/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2369.5928 - mse: 2369.5935 - mae: 29.3341 - val_loss: 3653.7791 - val_mse: 3653.7791 - val_mae: 23.6167\n",
      "Epoch 15/80\n",
      "2988/2988 [==============================] - 2s 582us/step - loss: 2289.6115 - mse: 2289.6111 - mae: 28.7839 - val_loss: 3653.9939 - val_mse: 3653.9934 - val_mae: 23.5815\n",
      "Epoch 16/80\n",
      "2988/2988 [==============================] - 2s 613us/step - loss: 2328.9412 - mse: 2328.9407 - mae: 28.8459 - val_loss: 3658.5334 - val_mse: 3658.5334 - val_mae: 24.2054\n",
      "Epoch 17/80\n",
      "2988/2988 [==============================] - 2s 591us/step - loss: 2311.4631 - mse: 2311.4624 - mae: 29.1889 - val_loss: 3656.5956 - val_mse: 3656.5955 - val_mae: 23.1240\n",
      "Epoch 18/80\n",
      "2988/2988 [==============================] - 2s 533us/step - loss: 2300.2159 - mse: 2300.2163 - mae: 28.8631 - val_loss: 3655.0261 - val_mse: 3655.0264 - val_mae: 23.5429\n",
      "Epoch 19/80\n",
      "2988/2988 [==============================] - 2s 652us/step - loss: 2271.0507 - mse: 2271.0505 - mae: 28.5400 - val_loss: 3657.7874 - val_mse: 3657.7871 - val_mae: 24.0250\n",
      "Epoch 20/80\n",
      "2988/2988 [==============================] - 2s 626us/step - loss: 2352.3965 - mse: 2352.3967 - mae: 29.0806 - val_loss: 3656.3180 - val_mse: 3656.3186 - val_mae: 23.6870\n",
      "Epoch 21/80\n",
      "2988/2988 [==============================] - 2s 561us/step - loss: 2316.7566 - mse: 2316.7568 - mae: 29.1794 - val_loss: 3658.5441 - val_mse: 3658.5442 - val_mae: 23.7869\n",
      "Epoch 22/80\n",
      "2988/2988 [==============================] - 2s 545us/step - loss: 2293.7803 - mse: 2293.7798 - mae: 28.7175 - val_loss: 3658.8057 - val_mse: 3658.8062 - val_mae: 23.8450\n",
      "Epoch 23/80\n",
      "2988/2988 [==============================] - 2s 604us/step - loss: 2335.7778 - mse: 2335.7778 - mae: 28.9764 - val_loss: 3657.5789 - val_mse: 3657.5793 - val_mae: 23.4752\n",
      "Epoch 24/80\n",
      "2988/2988 [==============================] - 2s 616us/step - loss: 2304.6515 - mse: 2304.6521 - mae: 28.7569 - val_loss: 3657.4198 - val_mse: 3657.4189 - val_mae: 23.3983\n",
      "Epoch 25/80\n",
      "2988/2988 [==============================] - 2s 556us/step - loss: 2321.7640 - mse: 2321.7639 - mae: 28.9213 - val_loss: 3657.2948 - val_mse: 3657.2949 - val_mae: 23.6389\n",
      "Epoch 26/80\n",
      "2988/2988 [==============================] - 2s 559us/step - loss: 2362.8001 - mse: 2362.8003 - mae: 29.1019 - val_loss: 3656.2567 - val_mse: 3656.2566 - val_mae: 23.6541\n",
      "Epoch 27/80\n",
      "2988/2988 [==============================] - 2s 631us/step - loss: 2359.3641 - mse: 2359.3645 - mae: 29.1380 - val_loss: 3658.2641 - val_mse: 3658.2642 - val_mae: 23.9684\n",
      "Epoch 28/80\n",
      "2988/2988 [==============================] - 2s 600us/step - loss: 2313.4892 - mse: 2313.4885 - mae: 28.6516 - val_loss: 3658.3538 - val_mse: 3658.3535 - val_mae: 23.7506\n",
      "Epoch 29/80\n",
      "2988/2988 [==============================] - 2s 609us/step - loss: 2322.1120 - mse: 2322.1118 - mae: 29.2834 - val_loss: 3658.5521 - val_mse: 3658.5522 - val_mae: 23.4773\n",
      "Epoch 30/80\n",
      "2988/2988 [==============================] - 2s 618us/step - loss: 2310.4306 - mse: 2310.4294 - mae: 29.0431 - val_loss: 3657.3380 - val_mse: 3657.3374 - val_mae: 23.0100\n",
      "Epoch 31/80\n",
      "2988/2988 [==============================] - 2s 588us/step - loss: 2299.2200 - mse: 2299.2195 - mae: 28.6775 - val_loss: 3660.8051 - val_mse: 3660.8042 - val_mae: 23.8511\n",
      "Epoch 32/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2284.6409 - mse: 2284.6406 - mae: 28.8506 - val_loss: 3660.8279 - val_mse: 3660.8279 - val_mae: 23.8150\n",
      "Epoch 33/80\n",
      "2988/2988 [==============================] - 2s 625us/step - loss: 2298.2473 - mse: 2298.2471 - mae: 28.9958 - val_loss: 3662.9903 - val_mse: 3662.9907 - val_mae: 24.1365\n",
      "Epoch 34/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2347.3327 - mse: 2347.3323 - mae: 28.8990 - val_loss: 3660.9815 - val_mse: 3660.9817 - val_mae: 23.5852\n",
      "Epoch 35/80\n",
      "2988/2988 [==============================] - 2s 569us/step - loss: 2278.8395 - mse: 2278.8394 - mae: 29.0121 - val_loss: 3660.6485 - val_mse: 3660.6482 - val_mae: 23.7567\n",
      "Epoch 36/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2329.9805 - mse: 2329.9805 - mae: 29.0937 - val_loss: 3661.5084 - val_mse: 3661.5088 - val_mae: 23.8906\n",
      "Epoch 37/80\n",
      "2988/2988 [==============================] - 2s 587us/step - loss: 2329.4290 - mse: 2329.4290 - mae: 28.9614 - val_loss: 3662.2832 - val_mse: 3662.2834 - val_mae: 23.2631\n",
      "Epoch 38/80\n",
      "2988/2988 [==============================] - 2s 504us/step - loss: 2289.4812 - mse: 2289.4817 - mae: 28.7518 - val_loss: 3662.2270 - val_mse: 3662.2266 - val_mae: 23.5364\n",
      "Epoch 39/80\n",
      "2988/2988 [==============================] - 2s 570us/step - loss: 2266.8871 - mse: 2266.8872 - mae: 28.4902 - val_loss: 3665.6055 - val_mse: 3665.6055 - val_mae: 23.8811\n",
      "Epoch 40/80\n",
      "2988/2988 [==============================] - 2s 579us/step - loss: 2272.2071 - mse: 2272.2073 - mae: 28.4562 - val_loss: 3664.9190 - val_mse: 3664.9185 - val_mae: 23.7769\n",
      "Epoch 41/80\n",
      "2988/2988 [==============================] - 2s 593us/step - loss: 2282.3343 - mse: 2282.3340 - mae: 28.6565 - val_loss: 3663.5390 - val_mse: 3663.5381 - val_mae: 23.4612\n",
      "Epoch 42/80\n",
      "2988/2988 [==============================] - 2s 575us/step - loss: 2333.8077 - mse: 2333.8076 - mae: 28.8113 - val_loss: 3663.0166 - val_mse: 3663.0171 - val_mae: 23.1987\n",
      "Epoch 43/80\n",
      "2988/2988 [==============================] - 2s 640us/step - loss: 2285.1299 - mse: 2285.1301 - mae: 28.5050 - val_loss: 3666.9268 - val_mse: 3666.9263 - val_mae: 23.9046\n",
      "Epoch 44/80\n",
      "2988/2988 [==============================] - 2s 645us/step - loss: 2273.0014 - mse: 2273.0007 - mae: 28.9578 - val_loss: 3667.2953 - val_mse: 3667.2952 - val_mae: 23.6001\n",
      "Epoch 45/80\n",
      "2988/2988 [==============================] - 2s 602us/step - loss: 2291.0359 - mse: 2291.0364 - mae: 28.6910 - val_loss: 3668.1770 - val_mse: 3668.1775 - val_mae: 23.7374\n",
      "Epoch 46/80\n",
      "2988/2988 [==============================] - 2s 599us/step - loss: 2255.2124 - mse: 2255.2119 - mae: 28.5529 - val_loss: 3669.8535 - val_mse: 3669.8540 - val_mae: 23.6501\n",
      "Epoch 47/80\n",
      "2988/2988 [==============================] - 2s 667us/step - loss: 2274.4928 - mse: 2274.4934 - mae: 28.8906 - val_loss: 3668.6406 - val_mse: 3668.6406 - val_mae: 23.7570\n",
      "Epoch 48/80\n",
      "2988/2988 [==============================] - 2s 647us/step - loss: 2290.9513 - mse: 2290.9517 - mae: 28.7135 - val_loss: 3667.4293 - val_mse: 3667.4297 - val_mae: 23.5616\n",
      "Epoch 49/80\n",
      "2988/2988 [==============================] - 2s 672us/step - loss: 2238.3451 - mse: 2238.3455 - mae: 28.5134 - val_loss: 3668.5620 - val_mse: 3668.5625 - val_mae: 23.8212\n",
      "Epoch 50/80\n",
      "2988/2988 [==============================] - 2s 659us/step - loss: 2271.7674 - mse: 2271.7678 - mae: 29.0064 - val_loss: 3667.3849 - val_mse: 3667.3848 - val_mae: 23.3770\n",
      "Epoch 51/80\n",
      "2988/2988 [==============================] - 2s 707us/step - loss: 2291.6612 - mse: 2291.6611 - mae: 28.7674 - val_loss: 3666.8050 - val_mse: 3666.8049 - val_mae: 23.5388\n",
      "Epoch 52/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2283.3395 - mse: 2283.3401 - mae: 28.6167 - val_loss: 3665.7513 - val_mse: 3665.7517 - val_mae: 23.4755\n",
      "Epoch 53/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2289.3000 - mse: 2289.2998 - mae: 28.7919 - val_loss: 3666.8076 - val_mse: 3666.8076 - val_mae: 23.6811\n",
      "Epoch 54/80\n",
      "2988/2988 [==============================] - 2s 589us/step - loss: 2325.0546 - mse: 2325.0547 - mae: 28.8371 - val_loss: 3663.2844 - val_mse: 3663.2844 - val_mae: 23.6533\n",
      "Epoch 55/80\n",
      "2988/2988 [==============================] - 2s 601us/step - loss: 2279.7347 - mse: 2279.7339 - mae: 28.7082 - val_loss: 3664.6680 - val_mse: 3664.6687 - val_mae: 24.0157\n",
      "Epoch 56/80\n",
      "2988/2988 [==============================] - 2s 595us/step - loss: 2285.8491 - mse: 2285.8491 - mae: 28.5024 - val_loss: 3664.6177 - val_mse: 3664.6174 - val_mae: 23.8555\n",
      "Epoch 57/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2249.7751 - mse: 2249.7747 - mae: 28.8228 - val_loss: 3664.6078 - val_mse: 3664.6084 - val_mae: 23.5354\n",
      "Epoch 58/80\n",
      "2988/2988 [==============================] - 2s 530us/step - loss: 2269.9118 - mse: 2269.9119 - mae: 28.4881 - val_loss: 3666.7211 - val_mse: 3666.7209 - val_mae: 23.3082\n",
      "Epoch 59/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2312.9485 - mse: 2312.9487 - mae: 28.7754 - val_loss: 3668.4997 - val_mse: 3668.5007 - val_mae: 24.1229\n",
      "Epoch 60/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2295.8268 - mse: 2295.8274 - mae: 28.5484 - val_loss: 3666.1030 - val_mse: 3666.1030 - val_mae: 23.8945\n",
      "Epoch 61/80\n",
      "2988/2988 [==============================] - 2s 592us/step - loss: 2270.4094 - mse: 2270.4097 - mae: 28.8070 - val_loss: 3663.1963 - val_mse: 3663.1960 - val_mae: 23.3608\n",
      "Epoch 62/80\n",
      "2988/2988 [==============================] - 1s 490us/step - loss: 2333.1807 - mse: 2333.1799 - mae: 28.7331 - val_loss: 3662.1874 - val_mse: 3662.1865 - val_mae: 23.6014\n",
      "Epoch 63/80\n",
      "2988/2988 [==============================] - 2s 645us/step - loss: 2283.4373 - mse: 2283.4380 - mae: 28.8975 - val_loss: 3662.3893 - val_mse: 3662.3889 - val_mae: 23.6284\n",
      "Epoch 64/80\n",
      "2988/2988 [==============================] - 2s 594us/step - loss: 2308.7182 - mse: 2308.7185 - mae: 28.6667 - val_loss: 3664.0562 - val_mse: 3664.0562 - val_mae: 23.4608\n",
      "Epoch 65/80\n",
      "2988/2988 [==============================] - 2s 577us/step - loss: 2265.9835 - mse: 2265.9832 - mae: 28.7232 - val_loss: 3667.0455 - val_mse: 3667.0449 - val_mae: 23.9378\n",
      "Epoch 66/80\n",
      "2988/2988 [==============================] - 2s 562us/step - loss: 2225.8478 - mse: 2225.8472 - mae: 28.4326 - val_loss: 3665.5656 - val_mse: 3665.5657 - val_mae: 23.4657\n",
      "Epoch 67/80\n",
      "2988/2988 [==============================] - 2s 598us/step - loss: 2272.4991 - mse: 2272.4995 - mae: 28.2709 - val_loss: 3667.9498 - val_mse: 3667.9502 - val_mae: 23.7458\n",
      "Epoch 68/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2301.4375 - mse: 2301.4377 - mae: 28.6832 - val_loss: 3667.8292 - val_mse: 3667.8303 - val_mae: 23.7834\n",
      "Epoch 69/80\n",
      "2988/2988 [==============================] - 2s 586us/step - loss: 2266.4596 - mse: 2266.4600 - mae: 28.5292 - val_loss: 3666.0207 - val_mse: 3666.0200 - val_mae: 23.5626\n",
      "Epoch 70/80\n",
      "2988/2988 [==============================] - 2s 603us/step - loss: 2178.6878 - mse: 2178.6875 - mae: 28.0766 - val_loss: 3671.3077 - val_mse: 3671.3086 - val_mae: 23.8576\n",
      "Epoch 71/80\n",
      "2988/2988 [==============================] - 2s 660us/step - loss: 2305.6900 - mse: 2305.6902 - mae: 28.9101 - val_loss: 3668.2503 - val_mse: 3668.2502 - val_mae: 23.5282\n",
      "Epoch 72/80\n",
      "2988/2988 [==============================] - 2s 629us/step - loss: 2326.6799 - mse: 2326.6804 - mae: 28.6954 - val_loss: 3667.8542 - val_mse: 3667.8538 - val_mae: 23.5227\n",
      "Epoch 73/80\n",
      "2988/2988 [==============================] - 2s 611us/step - loss: 2301.0044 - mse: 2301.0042 - mae: 28.7512 - val_loss: 3665.9234 - val_mse: 3665.9233 - val_mae: 23.3243\n",
      "Epoch 74/80\n",
      "2988/2988 [==============================] - 2s 628us/step - loss: 2298.7197 - mse: 2298.7205 - mae: 28.7158 - val_loss: 3664.3927 - val_mse: 3664.3928 - val_mae: 23.6210\n",
      "Epoch 75/80\n",
      "2988/2988 [==============================] - 2s 624us/step - loss: 2299.2168 - mse: 2299.2175 - mae: 28.9266 - val_loss: 3663.1403 - val_mse: 3663.1406 - val_mae: 23.4345\n",
      "Epoch 76/80\n",
      "2988/2988 [==============================] - 2s 627us/step - loss: 2230.3679 - mse: 2230.3679 - mae: 28.3698 - val_loss: 3669.2323 - val_mse: 3669.2332 - val_mae: 24.0912\n",
      "Epoch 77/80\n",
      "2988/2988 [==============================] - 2s 549us/step - loss: 2271.0757 - mse: 2271.0764 - mae: 28.7158 - val_loss: 3672.4291 - val_mse: 3672.4290 - val_mae: 23.9015\n",
      "Epoch 78/80\n",
      "2988/2988 [==============================] - 2s 572us/step - loss: 2251.2178 - mse: 2251.2173 - mae: 28.2692 - val_loss: 3674.7667 - val_mse: 3674.7661 - val_mae: 24.2461\n",
      "Epoch 79/80\n",
      "2988/2988 [==============================] - 2s 641us/step - loss: 2270.5211 - mse: 2270.5210 - mae: 28.7126 - val_loss: 3668.8442 - val_mse: 3668.8440 - val_mae: 23.2150\n",
      "Epoch 80/80\n",
      "2988/2988 [==============================] - 2s 607us/step - loss: 2236.3187 - mse: 2236.3193 - mae: 28.2602 - val_loss: 3667.8986 - val_mse: 3667.8987 - val_mae: 23.1602\n",
      "Train on 3485 samples, validate on 872 samples\n",
      "Epoch 1/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2685.2028 - mse: 2685.2026 - mae: 28.2261 - val_loss: 2166.7121 - val_mse: 2166.7119 - val_mae: 26.4688\n",
      "Epoch 2/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2689.7490 - mse: 2689.7490 - mae: 28.3676 - val_loss: 2165.7812 - val_mse: 2165.7810 - val_mae: 26.3939\n",
      "Epoch 3/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2670.3803 - mse: 2670.3801 - mae: 27.9143 - val_loss: 2176.7489 - val_mse: 2176.7488 - val_mae: 26.0136\n",
      "Epoch 4/80\n",
      "3485/3485 [==============================] - 2s 605us/step - loss: 2684.9369 - mse: 2684.9368 - mae: 28.1137 - val_loss: 2157.8545 - val_mse: 2157.8542 - val_mae: 26.7344\n",
      "Epoch 5/80\n",
      "3485/3485 [==============================] - 2s 581us/step - loss: 2699.2076 - mse: 2699.2078 - mae: 28.5189 - val_loss: 2167.1826 - val_mse: 2167.1821 - val_mae: 26.4610\n",
      "Epoch 6/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2669.4158 - mse: 2669.4153 - mae: 28.3307 - val_loss: 2163.4665 - val_mse: 2163.4668 - val_mae: 26.5580\n",
      "Epoch 7/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2697.8161 - mse: 2697.8162 - mae: 28.5406 - val_loss: 2159.1531 - val_mse: 2159.1528 - val_mae: 26.5123\n",
      "Epoch 8/80\n",
      "3485/3485 [==============================] - 2s 645us/step - loss: 2606.1497 - mse: 2606.1501 - mae: 27.8646 - val_loss: 2162.1962 - val_mse: 2162.1965 - val_mae: 26.4944\n",
      "Epoch 9/80\n",
      "3485/3485 [==============================] - 2s 632us/step - loss: 2655.2916 - mse: 2655.2913 - mae: 28.3502 - val_loss: 2163.9832 - val_mse: 2163.9832 - val_mae: 26.2175\n",
      "Epoch 10/80\n",
      "3485/3485 [==============================] - 2s 549us/step - loss: 2654.6626 - mse: 2654.6619 - mae: 27.9938 - val_loss: 2156.7655 - val_mse: 2156.7656 - val_mae: 26.7083\n",
      "Epoch 11/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2710.8872 - mse: 2710.8877 - mae: 28.3792 - val_loss: 2170.8656 - val_mse: 2170.8652 - val_mae: 26.0561\n",
      "Epoch 12/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2669.8413 - mse: 2669.8416 - mae: 28.0136 - val_loss: 2168.8118 - val_mse: 2168.8118 - val_mae: 26.1095\n",
      "Epoch 13/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2672.7567 - mse: 2672.7573 - mae: 28.1486 - val_loss: 2176.3734 - val_mse: 2176.3735 - val_mae: 26.0564\n",
      "Epoch 14/80\n",
      "3485/3485 [==============================] - 2s 645us/step - loss: 2670.9295 - mse: 2670.9297 - mae: 27.9446 - val_loss: 2160.7042 - val_mse: 2160.7041 - val_mae: 26.5040\n",
      "Epoch 15/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2617.0287 - mse: 2617.0281 - mae: 27.6188 - val_loss: 2165.0719 - val_mse: 2165.0720 - val_mae: 26.5035\n",
      "Epoch 16/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2659.7595 - mse: 2659.7593 - mae: 28.0554 - val_loss: 2164.1852 - val_mse: 2164.1855 - val_mae: 26.5029\n",
      "Epoch 17/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2607.6035 - mse: 2607.6038 - mae: 28.0688 - val_loss: 2163.4620 - val_mse: 2163.4617 - val_mae: 26.4500\n",
      "Epoch 18/80\n",
      "3485/3485 [==============================] - 2s 565us/step - loss: 2674.6900 - mse: 2674.6897 - mae: 28.0465 - val_loss: 2172.8848 - val_mse: 2172.8845 - val_mae: 26.3763\n",
      "Epoch 19/80\n",
      "3485/3485 [==============================] - 2s 634us/step - loss: 2641.1043 - mse: 2641.1042 - mae: 28.0427 - val_loss: 2173.4356 - val_mse: 2173.4358 - val_mae: 26.5929\n",
      "Epoch 20/80\n",
      "3485/3485 [==============================] - 2s 624us/step - loss: 2659.7329 - mse: 2659.7332 - mae: 28.1998 - val_loss: 2180.0174 - val_mse: 2180.0176 - val_mae: 26.2020\n",
      "Epoch 21/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2649.1359 - mse: 2649.1362 - mae: 27.7139 - val_loss: 2166.6931 - val_mse: 2166.6929 - val_mae: 26.6985\n",
      "Epoch 22/80\n",
      "3485/3485 [==============================] - 2s 568us/step - loss: 2609.0040 - mse: 2609.0037 - mae: 27.8907 - val_loss: 2167.5840 - val_mse: 2167.5840 - val_mae: 26.6356\n",
      "Epoch 23/80\n",
      "3485/3485 [==============================] - 2s 663us/step - loss: 2626.6874 - mse: 2626.6875 - mae: 27.9320 - val_loss: 2159.7864 - val_mse: 2159.7864 - val_mae: 26.6782\n",
      "Epoch 24/80\n",
      "3485/3485 [==============================] - 2s 547us/step - loss: 2684.8076 - mse: 2684.8074 - mae: 28.0124 - val_loss: 2159.7289 - val_mse: 2159.7288 - val_mae: 26.3775\n",
      "Epoch 25/80\n",
      "3485/3485 [==============================] - 2s 586us/step - loss: 2612.2162 - mse: 2612.2170 - mae: 27.8009 - val_loss: 2165.3290 - val_mse: 2165.3286 - val_mae: 26.2288\n",
      "Epoch 26/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2675.5433 - mse: 2675.5435 - mae: 27.9047 - val_loss: 2164.7166 - val_mse: 2164.7163 - val_mae: 26.2294\n",
      "Epoch 27/80\n",
      "3485/3485 [==============================] - 2s 618us/step - loss: 2606.3167 - mse: 2606.3164 - mae: 28.0201 - val_loss: 2162.1668 - val_mse: 2162.1667 - val_mae: 26.3805\n",
      "Epoch 28/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2659.5465 - mse: 2659.5464 - mae: 28.2041 - val_loss: 2171.7353 - val_mse: 2171.7356 - val_mae: 26.2903\n",
      "Epoch 29/80\n",
      "3485/3485 [==============================] - 2s 589us/step - loss: 2640.3325 - mse: 2640.3328 - mae: 28.2466 - val_loss: 2159.6279 - val_mse: 2159.6279 - val_mae: 26.5536\n",
      "Epoch 30/80\n",
      "3485/3485 [==============================] - 2s 612us/step - loss: 2688.4130 - mse: 2688.4124 - mae: 28.1605 - val_loss: 2174.1803 - val_mse: 2174.1799 - val_mae: 26.3186\n",
      "Epoch 31/80\n",
      "3485/3485 [==============================] - 2s 604us/step - loss: 2646.4637 - mse: 2646.4646 - mae: 28.1191 - val_loss: 2165.1294 - val_mse: 2165.1294 - val_mae: 26.5370\n",
      "Epoch 32/80\n",
      "3485/3485 [==============================] - 2s 635us/step - loss: 2627.3050 - mse: 2627.3057 - mae: 28.1504 - val_loss: 2177.3151 - val_mse: 2177.3152 - val_mae: 26.1000\n",
      "Epoch 33/80\n",
      "3485/3485 [==============================] - 2s 614us/step - loss: 2635.8456 - mse: 2635.8450 - mae: 27.7231 - val_loss: 2159.3986 - val_mse: 2159.3987 - val_mae: 26.5210\n",
      "Epoch 34/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2647.2391 - mse: 2647.2395 - mae: 28.1278 - val_loss: 2166.8477 - val_mse: 2166.8477 - val_mae: 26.1080\n",
      "Epoch 35/80\n",
      "3485/3485 [==============================] - 2s 591us/step - loss: 2622.6647 - mse: 2622.6643 - mae: 27.8895 - val_loss: 2164.1205 - val_mse: 2164.1206 - val_mae: 26.3448\n",
      "Epoch 36/80\n",
      "3485/3485 [==============================] - 2s 548us/step - loss: 2646.4494 - mse: 2646.4502 - mae: 28.0089 - val_loss: 2156.5293 - val_mse: 2156.5295 - val_mae: 26.6673\n",
      "Epoch 37/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2637.3403 - mse: 2637.3403 - mae: 27.8093 - val_loss: 2159.0808 - val_mse: 2159.0808 - val_mae: 26.5787\n",
      "Epoch 38/80\n",
      "3485/3485 [==============================] - 2s 507us/step - loss: 2624.0550 - mse: 2624.0542 - mae: 27.9161 - val_loss: 2169.5905 - val_mse: 2169.5906 - val_mae: 26.1795\n",
      "Epoch 39/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2634.7725 - mse: 2634.7720 - mae: 27.8719 - val_loss: 2160.0649 - val_mse: 2160.0652 - val_mae: 26.4826\n",
      "Epoch 40/80\n",
      "3485/3485 [==============================] - 2s 562us/step - loss: 2684.6940 - mse: 2684.6941 - mae: 28.0662 - val_loss: 2169.8633 - val_mse: 2169.8638 - val_mae: 26.1949\n",
      "Epoch 41/80\n",
      "3485/3485 [==============================] - 2s 502us/step - loss: 2653.1289 - mse: 2653.1292 - mae: 27.8179 - val_loss: 2156.7518 - val_mse: 2156.7520 - val_mae: 26.4095\n",
      "Epoch 42/80\n",
      "3485/3485 [==============================] - 2s 689us/step - loss: 2647.8260 - mse: 2647.8262 - mae: 28.1894 - val_loss: 2165.6056 - val_mse: 2165.6055 - val_mae: 26.2340\n",
      "Epoch 43/80\n",
      "3485/3485 [==============================] - 2s 664us/step - loss: 2676.9220 - mse: 2676.9226 - mae: 27.8799 - val_loss: 2172.5227 - val_mse: 2172.5227 - val_mae: 26.3734\n",
      "Epoch 44/80\n",
      "3485/3485 [==============================] - 2s 540us/step - loss: 2686.2980 - mse: 2686.2988 - mae: 28.2623 - val_loss: 2160.7465 - val_mse: 2160.7463 - val_mae: 26.9244\n",
      "Epoch 45/80\n",
      "3485/3485 [==============================] - 2s 579us/step - loss: 2655.9207 - mse: 2655.9204 - mae: 27.7354 - val_loss: 2166.8716 - val_mse: 2166.8713 - val_mae: 26.5539\n",
      "Epoch 46/80\n",
      "3485/3485 [==============================] - 2s 592us/step - loss: 2616.7438 - mse: 2616.7432 - mae: 28.0853 - val_loss: 2159.8808 - val_mse: 2159.8806 - val_mae: 26.6894\n",
      "Epoch 47/80\n",
      "3485/3485 [==============================] - 2s 603us/step - loss: 2670.0298 - mse: 2670.0300 - mae: 28.0803 - val_loss: 2163.6020 - val_mse: 2163.6018 - val_mae: 26.5946\n",
      "Epoch 48/80\n",
      "3485/3485 [==============================] - 2s 658us/step - loss: 2632.4073 - mse: 2632.4075 - mae: 28.1706 - val_loss: 2159.6001 - val_mse: 2159.6001 - val_mae: 26.5799\n",
      "Epoch 49/80\n",
      "3485/3485 [==============================] - 2s 640us/step - loss: 2655.7530 - mse: 2655.7539 - mae: 28.0978 - val_loss: 2168.9061 - val_mse: 2168.9058 - val_mae: 26.1352\n",
      "Epoch 50/80\n",
      "3485/3485 [==============================] - 2s 623us/step - loss: 2643.7011 - mse: 2643.7009 - mae: 27.6892 - val_loss: 2166.4683 - val_mse: 2166.4685 - val_mae: 26.4158\n",
      "Epoch 51/80\n",
      "3485/3485 [==============================] - 2s 598us/step - loss: 2619.7662 - mse: 2619.7659 - mae: 27.9405 - val_loss: 2157.6654 - val_mse: 2157.6655 - val_mae: 26.5358\n",
      "Epoch 52/80\n",
      "3485/3485 [==============================] - 2s 528us/step - loss: 2625.7577 - mse: 2625.7578 - mae: 27.8440 - val_loss: 2155.3611 - val_mse: 2155.3608 - val_mae: 26.4090\n",
      "Epoch 53/80\n",
      "3485/3485 [==============================] - 2s 626us/step - loss: 2689.6178 - mse: 2689.6174 - mae: 28.0691 - val_loss: 2156.9518 - val_mse: 2156.9519 - val_mae: 26.4178\n",
      "Epoch 54/80\n",
      "3485/3485 [==============================] - 2s 622us/step - loss: 2648.8897 - mse: 2648.8889 - mae: 27.7380 - val_loss: 2159.5170 - val_mse: 2159.5168 - val_mae: 26.7080\n",
      "Epoch 55/80\n",
      "3485/3485 [==============================] - 2s 577us/step - loss: 2641.2241 - mse: 2641.2241 - mae: 28.0445 - val_loss: 2165.2105 - val_mse: 2165.2107 - val_mae: 26.2167\n",
      "Epoch 56/80\n",
      "3485/3485 [==============================] - 2s 655us/step - loss: 2644.4174 - mse: 2644.4175 - mae: 27.9389 - val_loss: 2166.6717 - val_mse: 2166.6716 - val_mae: 26.1452\n",
      "Epoch 57/80\n",
      "3485/3485 [==============================] - 2s 594us/step - loss: 2658.0754 - mse: 2658.0754 - mae: 27.8717 - val_loss: 2160.5068 - val_mse: 2160.5068 - val_mae: 26.4688\n",
      "Epoch 58/80\n",
      "3485/3485 [==============================] - 2s 636us/step - loss: 2592.8490 - mse: 2592.8489 - mae: 27.7622 - val_loss: 2156.0799 - val_mse: 2156.0801 - val_mae: 26.6062\n",
      "Epoch 59/80\n",
      "3485/3485 [==============================] - 2s 558us/step - loss: 2641.3241 - mse: 2641.3237 - mae: 28.1256 - val_loss: 2156.7354 - val_mse: 2156.7356 - val_mae: 26.4679\n",
      "Epoch 60/80\n",
      "3485/3485 [==============================] - 2s 633us/step - loss: 2606.0883 - mse: 2606.0889 - mae: 27.4824 - val_loss: 2158.0390 - val_mse: 2158.0388 - val_mae: 26.2687\n",
      "Epoch 61/80\n",
      "3485/3485 [==============================] - 2s 619us/step - loss: 2631.8706 - mse: 2631.8708 - mae: 27.8586 - val_loss: 2154.1201 - val_mse: 2154.1201 - val_mae: 26.5864\n",
      "Epoch 62/80\n",
      "3485/3485 [==============================] - 2s 627us/step - loss: 2642.2520 - mse: 2642.2524 - mae: 27.9104 - val_loss: 2160.6086 - val_mse: 2160.6084 - val_mae: 26.5820\n",
      "Epoch 63/80\n",
      "3485/3485 [==============================] - 2s 575us/step - loss: 2601.8385 - mse: 2601.8391 - mae: 27.9359 - val_loss: 2146.8075 - val_mse: 2146.8076 - val_mae: 26.7666\n",
      "Epoch 64/80\n",
      "3485/3485 [==============================] - 2s 629us/step - loss: 2668.0751 - mse: 2668.0750 - mae: 27.9653 - val_loss: 2151.0845 - val_mse: 2151.0845 - val_mae: 26.5330\n",
      "Epoch 65/80\n",
      "3485/3485 [==============================] - 2s 616us/step - loss: 2622.9875 - mse: 2622.9880 - mae: 27.5824 - val_loss: 2147.5256 - val_mse: 2147.5256 - val_mae: 26.5440\n",
      "Epoch 66/80\n",
      "3485/3485 [==============================] - 2s 542us/step - loss: 2632.6381 - mse: 2632.6382 - mae: 27.5694 - val_loss: 2152.6526 - val_mse: 2152.6523 - val_mae: 26.3513\n",
      "Epoch 67/80\n",
      "3485/3485 [==============================] - 2s 572us/step - loss: 2588.2777 - mse: 2588.2781 - mae: 27.3881 - val_loss: 2152.0766 - val_mse: 2152.0764 - val_mae: 26.3797\n",
      "Epoch 68/80\n",
      "3485/3485 [==============================] - 2s 585us/step - loss: 2608.6260 - mse: 2608.6257 - mae: 27.6359 - val_loss: 2143.7615 - val_mse: 2143.7612 - val_mae: 26.6011\n",
      "Epoch 69/80\n",
      "3485/3485 [==============================] - 2s 571us/step - loss: 2695.4210 - mse: 2695.4216 - mae: 27.9835 - val_loss: 2144.4458 - val_mse: 2144.4458 - val_mae: 26.7104\n",
      "Epoch 70/80\n",
      "3485/3485 [==============================] - 2s 590us/step - loss: 2654.7804 - mse: 2654.7800 - mae: 28.1006 - val_loss: 2151.9887 - val_mse: 2151.9883 - val_mae: 26.6150\n",
      "Epoch 71/80\n",
      "3485/3485 [==============================] - 2s 628us/step - loss: 2605.4190 - mse: 2605.4194 - mae: 27.6279 - val_loss: 2148.5092 - val_mse: 2148.5093 - val_mae: 26.5606\n",
      "Epoch 72/80\n",
      "3485/3485 [==============================] - 2s 569us/step - loss: 2667.3355 - mse: 2667.3352 - mae: 28.2766 - val_loss: 2155.1437 - val_mse: 2155.1436 - val_mae: 26.2685\n",
      "Epoch 73/80\n",
      "3485/3485 [==============================] - 2s 641us/step - loss: 2653.1971 - mse: 2653.1978 - mae: 27.7279 - val_loss: 2143.3195 - val_mse: 2143.3196 - val_mae: 26.6434\n",
      "Epoch 74/80\n",
      "3485/3485 [==============================] - 2s 613us/step - loss: 2653.0207 - mse: 2653.0203 - mae: 27.8795 - val_loss: 2148.8104 - val_mse: 2148.8105 - val_mae: 26.6064\n",
      "Epoch 75/80\n",
      "3485/3485 [==============================] - 2s 670us/step - loss: 2670.6524 - mse: 2670.6526 - mae: 28.0864 - val_loss: 2154.2133 - val_mse: 2154.2131 - val_mae: 26.4040\n",
      "Epoch 76/80\n",
      "3485/3485 [==============================] - 2s 510us/step - loss: 2637.2688 - mse: 2637.2683 - mae: 27.9588 - val_loss: 2142.9891 - val_mse: 2142.9890 - val_mae: 26.6964\n",
      "Epoch 77/80\n",
      "3485/3485 [==============================] - 1s 325us/step - loss: 2611.0396 - mse: 2611.0388 - mae: 28.0011 - val_loss: 2153.4683 - val_mse: 2153.4683 - val_mae: 26.4145\n",
      "Epoch 78/80\n",
      "3485/3485 [==============================] - 1s 239us/step - loss: 2674.0380 - mse: 2674.0391 - mae: 27.9565 - val_loss: 2150.4271 - val_mse: 2150.4270 - val_mae: 26.3050\n",
      "Epoch 79/80\n",
      "3485/3485 [==============================] - 1s 257us/step - loss: 2632.9275 - mse: 2632.9282 - mae: 27.9362 - val_loss: 2161.6535 - val_mse: 2161.6536 - val_mae: 26.4375\n",
      "Epoch 80/80\n",
      "3485/3485 [==============================] - 1s 278us/step - loss: 2604.8014 - mse: 2604.8018 - mae: 27.6248 - val_loss: 2150.7802 - val_mse: 2150.7800 - val_mae: 26.5169\n"
     ]
    }
   ],
   "source": [
    "# features list; order made according to Linear Regression FS\n",
    "features_list = ['APXP', \n",
    "                 'LOLP',  \n",
    "                 'In_gen',\n",
    "                 'Ren_R',\n",
    "                 'DA_imb_France', \n",
    "                 'Rene',\n",
    "                 'ratio_offers_vol',\n",
    "                 'DA_price_france',\n",
    "                 'TSDF',\n",
    "                 'dino_bin',\n",
    "                 'DA_margin',\n",
    "                 'Im_Pr']\n",
    "\n",
    "best_score = rmse_spike\n",
    "\n",
    "for i in features_list: \n",
    "    \n",
    "   # X_recovery = X_.copy()\n",
    "    X_recovery = X_\n",
    "    \n",
    "    X_ = pd.concat([X_, X.loc[:,i]], axis = 1)\n",
    "                    \n",
    "    X_.fillna(method = 'ffill', inplace = True)\n",
    "    y.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "    X_ = X_.astype('float64')\n",
    "    X_ = X_.round(20)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_, y, test_size = 0.15, shuffle = False)\n",
    "    \n",
    "    sc_X = MinMaxScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "\n",
    "    # possible debug\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "\n",
    "    def regressor_tunning(n_hidden = 5, \n",
    "                          n_neurons = 40, \n",
    "                          kernel_initializer = \"he_normal\",\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = n_neurons, input_dim = len(X_.columns)))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.3))\n",
    "        for layer in range(n_hidden):\n",
    "            model.add(Dense(units = n_neurons))\n",
    "            model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(rate = 0.3))\n",
    "        model.add(Dense(units = 1, activation = 'linear'))\n",
    "        optimizer = optimizers.Adamax(lr = 0.001)\n",
    "        model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "        return model\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits = splits)    \n",
    "    regressor = regressor_tunning()\n",
    "\n",
    "    # train model\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "        y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "        regressor.fit(X_train_split, y_train_split,  \n",
    "                             shuffle = False, \n",
    "                             validation_split = 0.2,\n",
    "                             batch_size = 20, \n",
    "                             epochs = epochs)\n",
    "\n",
    "    # make predictions and evaluate for all regions\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRICS EVALUATION (1) for the whole test set\n",
    "    # =============================================================================\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    # calculate metrics\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "\n",
    "    # append to list\n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRICS EVALUATION (2) on spike regions\n",
    "    # =============================================================================\n",
    "\n",
    "    # download spike indication binary set\n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "    # append ot lists\n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "\n",
    "    # =============================================================================\n",
    "    # METRIC EVALUATION (3) on normal regions\n",
    "    # =============================================================================\n",
    "\n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "    # append to list\n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "\n",
    "    # condition of improvement for FS\n",
    "    if best_score < rmse_spi[-1]:\n",
    "        X_ = X_recovery\n",
    "    else:\n",
    "        X_ = X_\n",
    "        best_score = rmse_spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PrevDay</th>\n",
       "      <td>30.859331</td>\n",
       "      <td>21.799345</td>\n",
       "      <td>71.579277</td>\n",
       "      <td>60.584728</td>\n",
       "      <td>18.356216</td>\n",
       "      <td>16.077741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APXP</th>\n",
       "      <td>30.301084</td>\n",
       "      <td>21.129774</td>\n",
       "      <td>69.999551</td>\n",
       "      <td>58.298618</td>\n",
       "      <td>18.186944</td>\n",
       "      <td>15.646642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOLP</th>\n",
       "      <td>30.058745</td>\n",
       "      <td>20.207957</td>\n",
       "      <td>70.889924</td>\n",
       "      <td>58.963571</td>\n",
       "      <td>17.189361</td>\n",
       "      <td>14.490745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In_gen</th>\n",
       "      <td>30.268033</td>\n",
       "      <td>21.076997</td>\n",
       "      <td>70.140771</td>\n",
       "      <td>58.516779</td>\n",
       "      <td>18.042957</td>\n",
       "      <td>15.553896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ren_R</th>\n",
       "      <td>30.546874</td>\n",
       "      <td>20.973736</td>\n",
       "      <td>69.971366</td>\n",
       "      <td>57.481208</td>\n",
       "      <td>18.668397</td>\n",
       "      <td>15.588169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_imb_France</th>\n",
       "      <td>30.410576</td>\n",
       "      <td>20.735486</td>\n",
       "      <td>70.109970</td>\n",
       "      <td>57.701602</td>\n",
       "      <td>18.333324</td>\n",
       "      <td>15.282260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rene</th>\n",
       "      <td>30.883907</td>\n",
       "      <td>21.845374</td>\n",
       "      <td>69.186281</td>\n",
       "      <td>56.949086</td>\n",
       "      <td>19.707463</td>\n",
       "      <td>16.666889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_offers_vol</th>\n",
       "      <td>31.161971</td>\n",
       "      <td>21.585353</td>\n",
       "      <td>71.456593</td>\n",
       "      <td>59.024224</td>\n",
       "      <td>19.002064</td>\n",
       "      <td>16.062387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_price_france</th>\n",
       "      <td>30.593621</td>\n",
       "      <td>21.112575</td>\n",
       "      <td>69.783611</td>\n",
       "      <td>57.464252</td>\n",
       "      <td>18.858940</td>\n",
       "      <td>15.749991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSDF</th>\n",
       "      <td>30.744812</td>\n",
       "      <td>22.015641</td>\n",
       "      <td>67.822606</td>\n",
       "      <td>55.600419</td>\n",
       "      <td>20.152152</td>\n",
       "      <td>17.061229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dino_bin</th>\n",
       "      <td>30.385651</td>\n",
       "      <td>21.266241</td>\n",
       "      <td>68.470726</td>\n",
       "      <td>56.030381</td>\n",
       "      <td>19.180291</td>\n",
       "      <td>16.137849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA_margin</th>\n",
       "      <td>30.443326</td>\n",
       "      <td>20.943970</td>\n",
       "      <td>69.803902</td>\n",
       "      <td>57.369903</td>\n",
       "      <td>18.566502</td>\n",
       "      <td>15.570431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Im_Pr</th>\n",
       "      <td>30.674139</td>\n",
       "      <td>21.750498</td>\n",
       "      <td>68.644176</td>\n",
       "      <td>56.297005</td>\n",
       "      <td>19.610934</td>\n",
       "      <td>16.654212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rmse_general  mae_general  rmse_spike  mae_spike  \\\n",
       "PrevDay              30.859331    21.799345   71.579277  60.584728   \n",
       "APXP                 30.301084    21.129774   69.999551  58.298618   \n",
       "LOLP                 30.058745    20.207957   70.889924  58.963571   \n",
       "In_gen               30.268033    21.076997   70.140771  58.516779   \n",
       "Ren_R                30.546874    20.973736   69.971366  57.481208   \n",
       "DA_imb_France        30.410576    20.735486   70.109970  57.701602   \n",
       "Rene                 30.883907    21.845374   69.186281  56.949086   \n",
       "ratio_offers_vol     31.161971    21.585353   71.456593  59.024224   \n",
       "DA_price_france      30.593621    21.112575   69.783611  57.464252   \n",
       "TSDF                 30.744812    22.015641   67.822606  55.600419   \n",
       "dino_bin             30.385651    21.266241   68.470726  56.030381   \n",
       "DA_margin            30.443326    20.943970   69.803902  57.369903   \n",
       "Im_Pr                30.674139    21.750498   68.644176  56.297005   \n",
       "\n",
       "                  rmse_normal  mae_normal  \n",
       "PrevDay             18.356216   16.077741  \n",
       "APXP                18.186944   15.646642  \n",
       "LOLP                17.189361   14.490745  \n",
       "In_gen              18.042957   15.553896  \n",
       "Ren_R               18.668397   15.588169  \n",
       "DA_imb_France       18.333324   15.282260  \n",
       "Rene                19.707463   16.666889  \n",
       "ratio_offers_vol    19.002064   16.062387  \n",
       "DA_price_france     18.858940   15.749991  \n",
       "TSDF                20.152152   17.061229  \n",
       "dino_bin            19.180291   16.137849  \n",
       "DA_margin           18.566502   15.570431  \n",
       "Im_Pr               19.610934   16.654212  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list_index =   ['PrevDay',\n",
    "                         'APXP', \n",
    "                         'LOLP',  \n",
    "                         'In_gen',\n",
    "                         'Ren_R',\n",
    "                         'DA_imb_France', \n",
    "                         'Rene',\n",
    "                         'ratio_offers_vol',\n",
    "                         'DA_price_france',\n",
    "                         'TSDF',\n",
    "                         'dino_bin',\n",
    "                         'DA_margin',\n",
    "                         'Im_Pr']\n",
    "\n",
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,}, index = features_list_index)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PrevDay', 'APXP', 'Ren_R', 'Rene', 'TSDF'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.82260572023111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
