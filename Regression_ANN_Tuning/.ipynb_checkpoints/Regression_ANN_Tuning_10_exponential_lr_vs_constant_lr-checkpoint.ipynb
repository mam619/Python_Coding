{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression ANN with best parameters\n",
    "    find the best approach for learning rate (exponential schedueling - or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict;\n",
    "from sklearn.preprocessing import MinMaxScaler;\n",
    "from sklearn import metrics;\n",
    "from sklearn.model_selection import TimeSeriesSplit;\n",
    "\n",
    "mae_cv = []\n",
    "mse_cv = []\n",
    "mae_gen = []\n",
    "mse_gen  =[]\n",
    "rmse_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "mse_nor = []\n",
    "mse_spi = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for lr = 0.001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4cc26ef102ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data_set_1_smaller.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# COMPLETE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# data\n",
    "data = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "\n",
    "# COMPLETE\n",
    "data = data.loc[data.index > 2018090000, :]\n",
    "    \n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "    \n",
    "# Divide features and labels\n",
    "X = data.iloc[:, 0:15]\n",
    "y = data.loc[:, 'Offers']\n",
    "\n",
    "X.fillna(method = 'ffill', inplace = True)\n",
    "y.fillna(method = 'ffill', inplace = True)\n",
    "    \n",
    "X = X.astype('float64')\n",
    "X = X.round(20)\n",
    "    \n",
    "# divide data into train and test with 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "# feature scaling\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "    \n",
    "import keras\n",
    "from keras.models import Sequential # to initialise the NN\n",
    "from keras.layers import Dense # to create layers\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "    \n",
    "# possible debug\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "    \n",
    "def regressor_tunning(n_hidden = 2, \n",
    "                      n_neurons = 30,  \n",
    "                      kernel_initializer = \"he_normal\",\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = n_neurons, input_dim = 15))        \n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dropout(rate = 0.1))        \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.1))\n",
    "    model.add(Dense(units = 1, activation = 'linear'))\n",
    "    optimizer = optimizers.Adamax(lr = 0.001)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 7)\n",
    "\n",
    "hist_list = pd.DataFrame()\n",
    "count = 1\n",
    "    \n",
    "regressor = regressor_tunning()\n",
    "    \n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "    y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "    hist = regressor.fit(X_train_split, y_train_split, batch_size = 15, epochs = 80)\n",
    "    hist_list = hist_list.append(hist.history, ignore_index = True)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for i in range(len(hist_list.mse)):\n",
    "    a.append(np.mean(hist_list.mse[i]))\n",
    "    b.append(np.mean(hist_list.mae[i]))\n",
    "\n",
    "mse_cv.append(np.mean(a))\n",
    "mae_cv.append(np.mean(b))\n",
    "\n",
    "# predict for X_test  \n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mse_error = mse(y_test, y_pred) # 1479.61335\n",
    "mae_error = mae(y_test, y_pred) # 23.1525\n",
    "\n",
    "rmse_gen.append(rmse_error)\n",
    "mse_gen.append(mse_error)\n",
    "mae_gen.append(mae_error)\n",
    "\n",
    "# =============================================================================\n",
    "# Metrics evaluation on spike regions\n",
    "# =============================================================================\n",
    "\n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "\n",
    "# smal adjustment\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mse_spike = mse(y_test_spike, y_pred_spike)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "rmse_spi.append(rmse_spike)\n",
    "mse_spi.append(mse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "\n",
    "# =============================================================================\n",
    "# Metric evaluation on normal regions\n",
    "# =============================================================================\n",
    "\n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mse_normal = mse(y_test_normal, y_pred_normal)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "rmse_nor.append(rmse_normal)\n",
    "mse_nor.append(mse_normal)\n",
    "mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_cv</th>\n",
       "      <th>mae_cv</th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.276041</td>\n",
       "      <td>28.98105</td>\n",
       "      <td>33.480437</td>\n",
       "      <td>25.72624</td>\n",
       "      <td>70.280266</td>\n",
       "      <td>57.429934</td>\n",
       "      <td>23.855083</td>\n",
       "      <td>21.144065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rmse_cv    mae_cv  rmse_general  mae_general  rmse_spike  mae_spike  \\\n",
       "0  55.276041  28.98105     33.480437     25.72624   70.280266  57.429934   \n",
       "\n",
       "   rmse_normal  mae_normal  \n",
       "0    23.855083   21.144065  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_cv = []\n",
    "for i in mse_cv:\n",
    "    rmse_cv.append(i ** 0.5)\n",
    "    \n",
    "results = pd.DataFrame({'rmse_cv':rmse_cv,\n",
    "              \n",
    "                        'mae_cv': mae_cv,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for exponential schedueling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python-cpu/lib/python3.7/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 17809.8309 - mse: 17809.8301 - mae: 114.9894\n",
      "Epoch 2/80\n",
      "591/591 [==============================] - 0s 147us/step - loss: 17643.1505 - mse: 17643.1543 - mae: 114.2578\n",
      "Epoch 3/80\n",
      "591/591 [==============================] - 0s 174us/step - loss: 17340.5621 - mse: 17340.5625 - mae: 112.9505\n",
      "Epoch 4/80\n",
      "591/591 [==============================] - 0s 198us/step - loss: 16972.6350 - mse: 16972.6367 - mae: 111.2634\n",
      "Epoch 5/80\n",
      "591/591 [==============================] - 0s 194us/step - loss: 16607.2175 - mse: 16607.2188 - mae: 109.6209\n",
      "Epoch 6/80\n",
      "591/591 [==============================] - 0s 157us/step - loss: 16082.2955 - mse: 16082.2979 - mae: 107.2934\n",
      "Epoch 7/80\n",
      "591/591 [==============================] - 0s 168us/step - loss: 15493.6103 - mse: 15493.6123 - mae: 104.4968\n",
      "Epoch 8/80\n",
      "591/591 [==============================] - 0s 170us/step - loss: 14807.4340 - mse: 14807.4336 - mae: 101.1935\n",
      "Epoch 9/80\n",
      "591/591 [==============================] - 0s 167us/step - loss: 14130.6506 - mse: 14130.6494 - mae: 97.7188\n",
      "Epoch 10/80\n",
      "591/591 [==============================] - 0s 196us/step - loss: 13523.0623 - mse: 13523.0625 - mae: 94.5377\n",
      "Epoch 11/80\n",
      "591/591 [==============================] - 0s 190us/step - loss: 12956.8412 - mse: 12956.8389 - mae: 91.4667\n",
      "Epoch 12/80\n",
      "591/591 [==============================] - 0s 175us/step - loss: 12572.1822 - mse: 12572.1816 - mae: 89.4465\n",
      "Epoch 13/80\n",
      "591/591 [==============================] - 0s 133us/step - loss: 12213.3570 - mse: 12213.3574 - mae: 87.5561\n",
      "Epoch 14/80\n",
      "591/591 [==============================] - 0s 188us/step - loss: 11917.5377 - mse: 11917.5371 - mae: 85.7766\n",
      "Epoch 15/80\n",
      "591/591 [==============================] - 0s 193us/step - loss: 11681.0651 - mse: 11681.0645 - mae: 84.0901\n",
      "Epoch 16/80\n",
      "591/591 [==============================] - 0s 188us/step - loss: 11389.5378 - mse: 11389.5371 - mae: 82.4406\n",
      "Epoch 17/80\n",
      "591/591 [==============================] - 0s 176us/step - loss: 11073.7541 - mse: 11073.7549 - mae: 80.6127\n",
      "Epoch 18/80\n",
      "591/591 [==============================] - 0s 175us/step - loss: 10848.1298 - mse: 10848.1309 - mae: 78.9819\n",
      "Epoch 19/80\n",
      "591/591 [==============================] - 0s 179us/step - loss: 10538.3661 - mse: 10538.3652 - mae: 77.4800\n",
      "Epoch 20/80\n",
      "591/591 [==============================] - 0s 177us/step - loss: 10472.1879 - mse: 10472.1885 - mae: 76.3632\n",
      "Epoch 21/80\n",
      "591/591 [==============================] - 0s 177us/step - loss: 10161.2641 - mse: 10161.2627 - mae: 74.7400\n",
      "Epoch 22/80\n",
      "591/591 [==============================] - 0s 177us/step - loss: 10055.7355 - mse: 10055.7373 - mae: 73.8220\n",
      "Epoch 23/80\n",
      "591/591 [==============================] - 0s 179us/step - loss: 9860.6086 - mse: 9860.6084 - mae: 72.6982\n",
      "Epoch 24/80\n",
      "591/591 [==============================] - 0s 264us/step - loss: 9837.3333 - mse: 9837.3340 - mae: 71.8996\n",
      "Epoch 25/80\n",
      "591/591 [==============================] - 0s 196us/step - loss: 9615.1929 - mse: 9615.1934 - mae: 70.8096\n",
      "Epoch 26/80\n",
      "591/591 [==============================] - 0s 179us/step - loss: 9563.1221 - mse: 9563.1211 - mae: 70.6135\n",
      "Epoch 27/80\n",
      "591/591 [==============================] - 0s 224us/step - loss: 9453.3879 - mse: 9453.3867 - mae: 69.7502\n",
      "Epoch 28/80\n",
      "591/591 [==============================] - 0s 204us/step - loss: 9357.6151 - mse: 9357.6143 - mae: 69.1820\n",
      "Epoch 29/80\n",
      "591/591 [==============================] - 0s 271us/step - loss: 9355.7410 - mse: 9355.7432 - mae: 68.6481\n",
      "Epoch 30/80\n",
      "591/591 [==============================] - 0s 216us/step - loss: 9237.8471 - mse: 9237.8467 - mae: 68.4964\n",
      "Epoch 31/80\n",
      "591/591 [==============================] - 0s 220us/step - loss: 9137.7558 - mse: 9137.7559 - mae: 67.3633\n",
      "Epoch 32/80\n",
      "591/591 [==============================] - 0s 173us/step - loss: 9152.1703 - mse: 9152.1709 - mae: 67.1863\n",
      "Epoch 33/80\n",
      "591/591 [==============================] - 0s 160us/step - loss: 9052.7464 - mse: 9052.7461 - mae: 66.4441\n",
      "Epoch 34/80\n",
      "591/591 [==============================] - 0s 160us/step - loss: 9061.3270 - mse: 9061.3271 - mae: 66.7491\n",
      "Epoch 35/80\n",
      "591/591 [==============================] - 0s 162us/step - loss: 9068.9562 - mse: 9068.9570 - mae: 66.5226\n",
      "Epoch 36/80\n",
      "591/591 [==============================] - 0s 202us/step - loss: 9019.4281 - mse: 9019.4268 - mae: 66.4311\n",
      "Epoch 37/80\n",
      "591/591 [==============================] - 0s 198us/step - loss: 8943.2561 - mse: 8943.2559 - mae: 66.1742\n",
      "Epoch 38/80\n",
      "591/591 [==============================] - 0s 159us/step - loss: 8954.8254 - mse: 8954.8262 - mae: 65.6354\n",
      "Epoch 39/80\n",
      "591/591 [==============================] - 0s 158us/step - loss: 8883.1880 - mse: 8883.1885 - mae: 65.4149\n",
      "Epoch 40/80\n",
      "591/591 [==============================] - 0s 159us/step - loss: 8953.4278 - mse: 8953.4287 - mae: 65.5000\n",
      "Epoch 41/80\n",
      "591/591 [==============================] - 0s 159us/step - loss: 8833.9314 - mse: 8833.9316 - mae: 65.1812\n",
      "Epoch 42/80\n",
      "591/591 [==============================] - 0s 163us/step - loss: 8913.6999 - mse: 8913.6992 - mae: 65.4638\n",
      "Epoch 43/80\n",
      "591/591 [==============================] - 0s 222us/step - loss: 8772.5549 - mse: 8772.5557 - mae: 64.9982\n",
      "Epoch 44/80\n",
      "591/591 [==============================] - 0s 212us/step - loss: 8833.9947 - mse: 8833.9971 - mae: 65.3005\n",
      "Epoch 45/80\n",
      "591/591 [==============================] - 0s 168us/step - loss: 8793.4216 - mse: 8793.4209 - mae: 64.5841\n",
      "Epoch 46/80\n",
      "591/591 [==============================] - 0s 163us/step - loss: 8783.7446 - mse: 8783.7451 - mae: 65.1743\n",
      "Epoch 47/80\n",
      "591/591 [==============================] - 0s 161us/step - loss: 8748.1179 - mse: 8748.1162 - mae: 64.5545\n",
      "Epoch 48/80\n",
      "591/591 [==============================] - 0s 164us/step - loss: 8711.9435 - mse: 8711.9424 - mae: 64.7605\n",
      "Epoch 49/80\n",
      "591/591 [==============================] - 0s 199us/step - loss: 8786.2733 - mse: 8786.2734 - mae: 64.5828\n",
      "Epoch 50/80\n",
      "591/591 [==============================] - 0s 214us/step - loss: 8815.7151 - mse: 8815.7158 - mae: 64.3006\n",
      "Epoch 51/80\n",
      "591/591 [==============================] - 0s 176us/step - loss: 8733.7863 - mse: 8733.7852 - mae: 64.5471\n",
      "Epoch 52/80\n",
      "591/591 [==============================] - 0s 161us/step - loss: 8818.5837 - mse: 8818.5840 - mae: 64.4374\n",
      "Epoch 53/80\n",
      "591/591 [==============================] - 0s 149us/step - loss: 8763.3515 - mse: 8763.3516 - mae: 63.9042\n",
      "Epoch 54/80\n",
      "591/591 [==============================] - 0s 150us/step - loss: 8708.6602 - mse: 8708.6602 - mae: 64.5408\n",
      "Epoch 55/80\n",
      "591/591 [==============================] - 0s 167us/step - loss: 8761.1061 - mse: 8761.1064 - mae: 64.1314\n",
      "Epoch 56/80\n",
      "591/591 [==============================] - 0s 162us/step - loss: 8718.7511 - mse: 8718.7500 - mae: 64.3492\n",
      "Epoch 57/80\n",
      "591/591 [==============================] - 0s 144us/step - loss: 8585.2629 - mse: 8585.2637 - mae: 63.9786\n",
      "Epoch 58/80\n",
      "591/591 [==============================] - 0s 142us/step - loss: 8711.6501 - mse: 8711.6504 - mae: 64.4958\n",
      "Epoch 59/80\n",
      "591/591 [==============================] - 0s 163us/step - loss: 8641.2635 - mse: 8641.2646 - mae: 64.0504\n",
      "Epoch 60/80\n",
      "591/591 [==============================] - 0s 164us/step - loss: 8663.1755 - mse: 8663.1758 - mae: 64.0608\n",
      "Epoch 61/80\n",
      "591/591 [==============================] - 0s 148us/step - loss: 8677.3721 - mse: 8677.3711 - mae: 64.0715\n",
      "Epoch 62/80\n",
      "591/591 [==============================] - 0s 147us/step - loss: 8730.9300 - mse: 8730.9307 - mae: 64.3864\n",
      "Epoch 63/80\n",
      "591/591 [==============================] - 0s 148us/step - loss: 8718.8145 - mse: 8718.8154 - mae: 64.1434\n",
      "Epoch 64/80\n",
      "591/591 [==============================] - 0s 166us/step - loss: 8714.2571 - mse: 8714.2559 - mae: 64.1058\n",
      "Epoch 65/80\n",
      "591/591 [==============================] - 0s 162us/step - loss: 8763.0966 - mse: 8763.0967 - mae: 64.2967\n",
      "Epoch 66/80\n",
      "591/591 [==============================] - 0s 145us/step - loss: 8811.4696 - mse: 8811.4697 - mae: 64.5759\n",
      "Epoch 67/80\n",
      "591/591 [==============================] - 0s 160us/step - loss: 8690.8533 - mse: 8690.8535 - mae: 64.1990\n",
      "Epoch 68/80\n",
      "591/591 [==============================] - 0s 164us/step - loss: 8647.9048 - mse: 8647.9043 - mae: 63.8690\n",
      "Epoch 69/80\n",
      "591/591 [==============================] - 0s 161us/step - loss: 8609.5923 - mse: 8609.5928 - mae: 63.5350\n",
      "Epoch 70/80\n",
      "591/591 [==============================] - 0s 144us/step - loss: 8622.8997 - mse: 8622.9004 - mae: 63.5413\n",
      "Epoch 71/80\n",
      "591/591 [==============================] - 0s 147us/step - loss: 8732.4063 - mse: 8732.4062 - mae: 64.6923\n",
      "Epoch 72/80\n",
      "591/591 [==============================] - 0s 161us/step - loss: 8759.3922 - mse: 8759.3896 - mae: 64.2069\n",
      "Epoch 73/80\n",
      "591/591 [==============================] - 0s 161us/step - loss: 8695.4988 - mse: 8695.4971 - mae: 64.1296\n",
      "Epoch 74/80\n",
      "591/591 [==============================] - 0s 137us/step - loss: 8686.8861 - mse: 8686.8867 - mae: 64.0133\n",
      "Epoch 75/80\n",
      "591/591 [==============================] - 0s 139us/step - loss: 8647.7327 - mse: 8647.7314 - mae: 64.1298\n",
      "Epoch 76/80\n",
      "591/591 [==============================] - 0s 164us/step - loss: 8683.2700 - mse: 8683.2695 - mae: 63.9042\n",
      "Epoch 77/80\n",
      "591/591 [==============================] - 0s 156us/step - loss: 8749.3537 - mse: 8749.3516 - mae: 64.3486\n",
      "Epoch 78/80\n",
      "591/591 [==============================] - 0s 144us/step - loss: 8726.3476 - mse: 8726.3477 - mae: 64.4151\n",
      "Epoch 79/80\n",
      "591/591 [==============================] - 0s 149us/step - loss: 8635.0727 - mse: 8635.0732 - mae: 64.0371\n",
      "Epoch 80/80\n",
      "591/591 [==============================] - 0s 141us/step - loss: 8799.1893 - mse: 8799.1895 - mae: 64.7302\n",
      "1\n",
      "Epoch 1/80\n",
      "1176/1176 [==============================] - 0s 158us/step - loss: 6255.5304 - mse: 6255.5312 - mae: 52.9086\n",
      "Epoch 2/80\n",
      "1176/1176 [==============================] - 0s 150us/step - loss: 4008.4879 - mse: 4008.4885 - mae: 32.6767\n",
      "Epoch 3/80\n",
      "1176/1176 [==============================] - 0s 139us/step - loss: 3508.2276 - mse: 3508.2280 - mae: 30.7023\n",
      "Epoch 4/80\n",
      "1176/1176 [==============================] - 0s 144us/step - loss: 3436.6515 - mse: 3436.6516 - mae: 29.9242\n",
      "Epoch 5/80\n",
      "1176/1176 [==============================] - 0s 153us/step - loss: 3525.6829 - mse: 3525.6826 - mae: 31.7767\n",
      "Epoch 6/80\n",
      "1176/1176 [==============================] - 0s 146us/step - loss: 3511.8831 - mse: 3511.8823 - mae: 30.5525\n",
      "Epoch 7/80\n",
      "1176/1176 [==============================] - 0s 143us/step - loss: 3481.4818 - mse: 3481.4810 - mae: 31.4540\n",
      "Epoch 8/80\n",
      "1176/1176 [==============================] - 0s 137us/step - loss: 3562.2246 - mse: 3562.2249 - mae: 31.8355\n",
      "Epoch 9/80\n",
      "1176/1176 [==============================] - 0s 141us/step - loss: 3634.5976 - mse: 3634.5974 - mae: 31.7520\n",
      "Epoch 10/80\n",
      "1176/1176 [==============================] - 0s 151us/step - loss: 3569.9759 - mse: 3569.9761 - mae: 31.5347\n",
      "Epoch 11/80\n",
      "1176/1176 [==============================] - 0s 162us/step - loss: 3565.0737 - mse: 3565.0732 - mae: 30.9047\n",
      "Epoch 12/80\n",
      "1176/1176 [==============================] - 0s 145us/step - loss: 3619.1026 - mse: 3619.1025 - mae: 31.8046\n",
      "Epoch 13/80\n",
      "1176/1176 [==============================] - 0s 147us/step - loss: 3529.3713 - mse: 3529.3716 - mae: 31.3464\n",
      "Epoch 14/80\n",
      "1176/1176 [==============================] - 0s 156us/step - loss: 3559.9193 - mse: 3559.9189 - mae: 31.8655\n",
      "Epoch 15/80\n",
      "1176/1176 [==============================] - 0s 150us/step - loss: 3606.6318 - mse: 3606.6318 - mae: 31.0680\n",
      "Epoch 16/80\n",
      "1176/1176 [==============================] - 0s 140us/step - loss: 3560.3428 - mse: 3560.3425 - mae: 30.7787\n",
      "Epoch 17/80\n",
      "1176/1176 [==============================] - 0s 154us/step - loss: 3617.4338 - mse: 3617.4336 - mae: 31.9382\n",
      "Epoch 18/80\n",
      "1176/1176 [==============================] - 0s 160us/step - loss: 3540.4860 - mse: 3540.4851 - mae: 31.4028\n",
      "Epoch 19/80\n",
      "1176/1176 [==============================] - 0s 140us/step - loss: 3568.1659 - mse: 3568.1658 - mae: 31.5975\n",
      "Epoch 20/80\n",
      "1176/1176 [==============================] - 0s 163us/step - loss: 3480.0931 - mse: 3480.0930 - mae: 31.3599\n",
      "Epoch 21/80\n",
      "1176/1176 [==============================] - 0s 163us/step - loss: 3606.7175 - mse: 3606.7173 - mae: 31.6069\n",
      "Epoch 22/80\n",
      "1176/1176 [==============================] - 0s 158us/step - loss: 3430.3144 - mse: 3430.3145 - mae: 30.6603\n",
      "Epoch 23/80\n",
      "1176/1176 [==============================] - 0s 157us/step - loss: 3497.3568 - mse: 3497.3567 - mae: 31.5408\n",
      "Epoch 24/80\n",
      "1176/1176 [==============================] - 0s 159us/step - loss: 3460.7722 - mse: 3460.7722 - mae: 30.7612\n",
      "Epoch 25/80\n",
      "1176/1176 [==============================] - 0s 151us/step - loss: 3476.3563 - mse: 3476.3567 - mae: 30.9665\n",
      "Epoch 26/80\n",
      "1176/1176 [==============================] - 0s 157us/step - loss: 3526.8768 - mse: 3526.8774 - mae: 30.9014\n",
      "Epoch 27/80\n",
      "1176/1176 [==============================] - 0s 142us/step - loss: 3490.7198 - mse: 3490.7192 - mae: 31.3634\n",
      "Epoch 28/80\n",
      "1176/1176 [==============================] - 0s 146us/step - loss: 3573.1190 - mse: 3573.1187 - mae: 31.0781\n",
      "Epoch 29/80\n",
      "1176/1176 [==============================] - 0s 160us/step - loss: 3555.5901 - mse: 3555.5906 - mae: 31.3004\n",
      "Epoch 30/80\n",
      "1176/1176 [==============================] - 0s 149us/step - loss: 3529.1301 - mse: 3529.1299 - mae: 31.2734\n",
      "Epoch 31/80\n",
      "1176/1176 [==============================] - 0s 158us/step - loss: 3692.0887 - mse: 3692.0894 - mae: 32.7750\n",
      "Epoch 32/80\n",
      "1176/1176 [==============================] - 0s 153us/step - loss: 3556.2174 - mse: 3556.2173 - mae: 31.8844\n",
      "Epoch 33/80\n",
      "1176/1176 [==============================] - 0s 155us/step - loss: 3529.9821 - mse: 3529.9822 - mae: 31.7444\n",
      "Epoch 34/80\n",
      "1176/1176 [==============================] - 0s 142us/step - loss: 3543.4379 - mse: 3543.4380 - mae: 31.8683\n",
      "Epoch 35/80\n",
      "1176/1176 [==============================] - 0s 162us/step - loss: 3501.4678 - mse: 3501.4680 - mae: 31.6387\n",
      "Epoch 36/80\n",
      "1176/1176 [==============================] - 0s 153us/step - loss: 3551.9796 - mse: 3551.9795 - mae: 31.6218\n",
      "Epoch 37/80\n",
      "1176/1176 [==============================] - 0s 184us/step - loss: 3490.6756 - mse: 3490.6753 - mae: 31.1680\n",
      "Epoch 38/80\n",
      "1176/1176 [==============================] - 0s 187us/step - loss: 3607.9999 - mse: 3607.9993 - mae: 31.9680\n",
      "Epoch 39/80\n",
      "1176/1176 [==============================] - 0s 127us/step - loss: 3485.7622 - mse: 3485.7617 - mae: 31.5395\n",
      "Epoch 40/80\n",
      "1176/1176 [==============================] - 0s 141us/step - loss: 3505.9655 - mse: 3505.9656 - mae: 31.5161\n",
      "Epoch 41/80\n",
      "1176/1176 [==============================] - 0s 164us/step - loss: 3601.3915 - mse: 3601.3916 - mae: 31.7113\n",
      "Epoch 42/80\n",
      "1176/1176 [==============================] - 0s 187us/step - loss: 3523.4530 - mse: 3523.4531 - mae: 32.0895\n",
      "Epoch 43/80\n",
      "1176/1176 [==============================] - 0s 182us/step - loss: 3596.6325 - mse: 3596.6323 - mae: 31.3126\n",
      "Epoch 44/80\n",
      "1176/1176 [==============================] - 0s 220us/step - loss: 3613.4498 - mse: 3613.4500 - mae: 32.5845\n",
      "Epoch 45/80\n",
      "1176/1176 [==============================] - 0s 140us/step - loss: 3566.0891 - mse: 3566.0891 - mae: 30.7951\n",
      "Epoch 46/80\n",
      "1176/1176 [==============================] - 0s 165us/step - loss: 3466.1791 - mse: 3466.1785 - mae: 30.8233\n",
      "Epoch 47/80\n",
      "1176/1176 [==============================] - 0s 154us/step - loss: 3573.5231 - mse: 3573.5239 - mae: 31.5007\n",
      "Epoch 48/80\n",
      "1176/1176 [==============================] - 0s 150us/step - loss: 3562.4269 - mse: 3562.4268 - mae: 30.8639\n",
      "Epoch 49/80\n",
      "1176/1176 [==============================] - 0s 167us/step - loss: 3379.7899 - mse: 3379.7903 - mae: 30.7252\n",
      "Epoch 50/80\n",
      "1176/1176 [==============================] - 0s 131us/step - loss: 3525.0860 - mse: 3525.0864 - mae: 31.3262\n",
      "Epoch 51/80\n",
      "1176/1176 [==============================] - 0s 187us/step - loss: 3502.4279 - mse: 3502.4277 - mae: 31.2203\n",
      "Epoch 52/80\n",
      "1176/1176 [==============================] - 0s 190us/step - loss: 3544.0821 - mse: 3544.0823 - mae: 31.3008\n",
      "Epoch 53/80\n",
      "1176/1176 [==============================] - 0s 187us/step - loss: 3468.4538 - mse: 3468.4543 - mae: 30.8784\n",
      "Epoch 54/80\n",
      "1176/1176 [==============================] - 0s 191us/step - loss: 3474.6596 - mse: 3474.6599 - mae: 31.3906\n",
      "Epoch 55/80\n",
      "1176/1176 [==============================] - 0s 192us/step - loss: 3477.0942 - mse: 3477.0942 - mae: 31.0059\n",
      "Epoch 56/80\n",
      "1176/1176 [==============================] - 0s 192us/step - loss: 3679.6690 - mse: 3679.6687 - mae: 31.7156\n",
      "Epoch 57/80\n",
      "1176/1176 [==============================] - 0s 191us/step - loss: 3570.8793 - mse: 3570.8796 - mae: 32.0518\n",
      "Epoch 58/80\n",
      "1176/1176 [==============================] - 0s 188us/step - loss: 3490.6849 - mse: 3490.6851 - mae: 31.3290\n",
      "Epoch 59/80\n",
      "1176/1176 [==============================] - 0s 162us/step - loss: 3637.3738 - mse: 3637.3743 - mae: 32.0118\n",
      "Epoch 60/80\n",
      "1176/1176 [==============================] - 0s 151us/step - loss: 3529.6725 - mse: 3529.6719 - mae: 31.7561\n",
      "Epoch 61/80\n",
      "1176/1176 [==============================] - 0s 152us/step - loss: 3456.2184 - mse: 3456.2190 - mae: 31.1681\n",
      "Epoch 62/80\n",
      "1176/1176 [==============================] - 0s 143us/step - loss: 3672.7193 - mse: 3672.7190 - mae: 31.7798\n",
      "Epoch 63/80\n",
      "1176/1176 [==============================] - 0s 146us/step - loss: 3580.1400 - mse: 3580.1404 - mae: 31.4427\n",
      "Epoch 64/80\n",
      "1176/1176 [==============================] - 0s 159us/step - loss: 3586.4889 - mse: 3586.4890 - mae: 32.1589\n",
      "Epoch 65/80\n",
      "1176/1176 [==============================] - 0s 162us/step - loss: 3566.2478 - mse: 3566.2471 - mae: 32.0922\n",
      "Epoch 66/80\n",
      "1176/1176 [==============================] - 0s 156us/step - loss: 3627.8893 - mse: 3627.8899 - mae: 32.3572\n",
      "Epoch 67/80\n",
      "1176/1176 [==============================] - 0s 151us/step - loss: 3607.6775 - mse: 3607.6768 - mae: 31.1707\n",
      "Epoch 68/80\n",
      "1176/1176 [==============================] - 0s 137us/step - loss: 3469.5891 - mse: 3469.5891 - mae: 31.7350\n",
      "Epoch 69/80\n",
      "1176/1176 [==============================] - 0s 135us/step - loss: 3521.3583 - mse: 3521.3584 - mae: 31.0603\n",
      "Epoch 70/80\n",
      "1176/1176 [==============================] - 0s 149us/step - loss: 3489.8543 - mse: 3489.8542 - mae: 31.4410\n",
      "Epoch 71/80\n",
      "1176/1176 [==============================] - 0s 157us/step - loss: 3646.0561 - mse: 3646.0569 - mae: 32.0336\n",
      "Epoch 72/80\n",
      "1176/1176 [==============================] - 0s 192us/step - loss: 3516.6133 - mse: 3516.6125 - mae: 31.1290\n",
      "Epoch 73/80\n",
      "1176/1176 [==============================] - 0s 192us/step - loss: 3570.3760 - mse: 3570.3767 - mae: 32.3086\n",
      "Epoch 74/80\n",
      "1176/1176 [==============================] - 0s 186us/step - loss: 3546.2403 - mse: 3546.2402 - mae: 31.2863\n",
      "Epoch 75/80\n",
      "1176/1176 [==============================] - 0s 185us/step - loss: 3646.3608 - mse: 3646.3611 - mae: 31.7303\n",
      "Epoch 76/80\n",
      "1176/1176 [==============================] - 0s 186us/step - loss: 3669.9961 - mse: 3669.9961 - mae: 32.1134\n",
      "Epoch 77/80\n",
      "1176/1176 [==============================] - 0s 175us/step - loss: 3631.2777 - mse: 3631.2771 - mae: 32.6556\n",
      "Epoch 78/80\n",
      "1176/1176 [==============================] - 0s 140us/step - loss: 3679.0942 - mse: 3679.0928 - mae: 31.8091\n",
      "Epoch 79/80\n",
      "1176/1176 [==============================] - 0s 146us/step - loss: 3497.0089 - mse: 3497.0085 - mae: 31.6891\n",
      "Epoch 80/80\n",
      "1176/1176 [==============================] - 0s 166us/step - loss: 3639.4299 - mse: 3639.4307 - mae: 31.8125\n",
      "2\n",
      "Epoch 1/80\n",
      "1761/1761 [==============================] - 0s 163us/step - loss: 2791.2813 - mse: 2791.2815 - mae: 29.5478\n",
      "Epoch 2/80\n",
      "1761/1761 [==============================] - 0s 140us/step - loss: 2842.6733 - mse: 2842.6736 - mae: 29.8615\n",
      "Epoch 3/80\n",
      "1761/1761 [==============================] - 0s 157us/step - loss: 2788.6840 - mse: 2788.6848 - mae: 29.2599\n",
      "Epoch 4/80\n",
      "1761/1761 [==============================] - 0s 159us/step - loss: 2841.7308 - mse: 2841.7305 - mae: 29.8243\n",
      "Epoch 5/80\n",
      "1761/1761 [==============================] - 0s 149us/step - loss: 2762.7744 - mse: 2762.7749 - mae: 29.1963\n",
      "Epoch 6/80\n",
      "1761/1761 [==============================] - 0s 157us/step - loss: 2789.8795 - mse: 2789.8794 - mae: 29.2621\n",
      "Epoch 7/80\n",
      "1761/1761 [==============================] - 0s 155us/step - loss: 2797.1120 - mse: 2797.1118 - mae: 28.8893\n",
      "Epoch 8/80\n",
      "1761/1761 [==============================] - 0s 146us/step - loss: 2879.8487 - mse: 2879.8484 - mae: 29.7866\n",
      "Epoch 9/80\n",
      "1761/1761 [==============================] - 0s 147us/step - loss: 2816.2257 - mse: 2816.2266 - mae: 29.6153\n",
      "Epoch 10/80\n",
      "1761/1761 [==============================] - 0s 152us/step - loss: 2834.9047 - mse: 2834.9048 - mae: 29.2282\n",
      "Epoch 11/80\n",
      "1761/1761 [==============================] - 0s 151us/step - loss: 2827.8321 - mse: 2827.8315 - mae: 29.5531\n",
      "Epoch 12/80\n",
      "1761/1761 [==============================] - 0s 150us/step - loss: 2819.5233 - mse: 2819.5232 - mae: 29.4569\n",
      "Epoch 13/80\n",
      "1761/1761 [==============================] - 0s 155us/step - loss: 2843.5849 - mse: 2843.5847 - mae: 29.2006\n",
      "Epoch 14/80\n",
      "1761/1761 [==============================] - 0s 140us/step - loss: 2820.6658 - mse: 2820.6655 - mae: 29.5726\n",
      "Epoch 15/80\n",
      "1761/1761 [==============================] - 0s 157us/step - loss: 2864.6151 - mse: 2864.6157 - mae: 29.4209\n",
      "Epoch 16/80\n",
      "1761/1761 [==============================] - 0s 152us/step - loss: 2795.3048 - mse: 2795.3052 - mae: 28.9522\n",
      "Epoch 17/80\n",
      "1761/1761 [==============================] - 0s 133us/step - loss: 2825.5326 - mse: 2825.5327 - mae: 29.6175\n",
      "Epoch 18/80\n",
      "1761/1761 [==============================] - 0s 149us/step - loss: 2712.5006 - mse: 2712.5005 - mae: 28.8821\n",
      "Epoch 19/80\n",
      "1761/1761 [==============================] - 0s 156us/step - loss: 2728.9077 - mse: 2728.9072 - mae: 29.0984\n",
      "Epoch 20/80\n",
      "1761/1761 [==============================] - 0s 159us/step - loss: 2802.3242 - mse: 2802.3247 - mae: 28.8377\n",
      "Epoch 21/80\n",
      "1761/1761 [==============================] - 0s 135us/step - loss: 2814.1253 - mse: 2814.1252 - mae: 29.1107\n",
      "Epoch 22/80\n",
      "1761/1761 [==============================] - 0s 145us/step - loss: 2738.3806 - mse: 2738.3811 - mae: 29.0505\n",
      "Epoch 23/80\n",
      "1761/1761 [==============================] - 0s 153us/step - loss: 2821.3700 - mse: 2821.3691 - mae: 29.5634\n",
      "Epoch 24/80\n",
      "1761/1761 [==============================] - 0s 144us/step - loss: 2741.1187 - mse: 2741.1184 - mae: 28.6673\n",
      "Epoch 25/80\n",
      "1761/1761 [==============================] - 0s 149us/step - loss: 2809.6239 - mse: 2809.6240 - mae: 29.2395\n",
      "Epoch 26/80\n",
      "1761/1761 [==============================] - 0s 143us/step - loss: 2816.6497 - mse: 2816.6509 - mae: 28.8444\n",
      "Epoch 27/80\n",
      "1761/1761 [==============================] - 0s 156us/step - loss: 2805.7914 - mse: 2805.7915 - mae: 29.4404\n",
      "Epoch 28/80\n",
      "1761/1761 [==============================] - 0s 149us/step - loss: 2762.8870 - mse: 2762.8879 - mae: 28.9602\n",
      "Epoch 29/80\n",
      "1761/1761 [==============================] - 0s 149us/step - loss: 2764.6570 - mse: 2764.6570 - mae: 28.9993\n",
      "Epoch 30/80\n",
      "1761/1761 [==============================] - 0s 146us/step - loss: 2826.7413 - mse: 2826.7415 - mae: 29.2738\n",
      "Epoch 31/80\n",
      "1761/1761 [==============================] - 0s 154us/step - loss: 2776.2844 - mse: 2776.2847 - mae: 29.2324\n",
      "Epoch 32/80\n",
      "1761/1761 [==============================] - 0s 158us/step - loss: 2932.2654 - mse: 2932.2651 - mae: 29.7614\n",
      "Epoch 33/80\n",
      "1761/1761 [==============================] - 0s 143us/step - loss: 2806.7274 - mse: 2806.7275 - mae: 29.1084\n",
      "Epoch 34/80\n",
      "1761/1761 [==============================] - 0s 150us/step - loss: 2800.7803 - mse: 2800.7791 - mae: 29.5564\n",
      "Epoch 35/80\n",
      "1761/1761 [==============================] - 0s 158us/step - loss: 2780.7504 - mse: 2780.7502 - mae: 28.8814\n",
      "Epoch 36/80\n",
      "1761/1761 [==============================] - 0s 159us/step - loss: 2757.1599 - mse: 2757.1597 - mae: 28.8737\n",
      "Epoch 37/80\n",
      "1761/1761 [==============================] - 0s 158us/step - loss: 2770.2001 - mse: 2770.2004 - mae: 28.4600\n",
      "Epoch 38/80\n",
      "1761/1761 [==============================] - 0s 151us/step - loss: 2829.3369 - mse: 2829.3369 - mae: 29.4906\n",
      "Epoch 39/80\n",
      "1761/1761 [==============================] - 0s 136us/step - loss: 2852.6164 - mse: 2852.6167 - mae: 29.7225\n",
      "Epoch 40/80\n",
      "1761/1761 [==============================] - 0s 156us/step - loss: 2784.6584 - mse: 2784.6582 - mae: 29.5815\n",
      "Epoch 41/80\n",
      "1761/1761 [==============================] - 0s 155us/step - loss: 2814.3617 - mse: 2814.3611 - mae: 29.8760\n",
      "Epoch 42/80\n",
      "1761/1761 [==============================] - 0s 150us/step - loss: 2751.5348 - mse: 2751.5344 - mae: 29.3206\n",
      "Epoch 43/80\n",
      "1761/1761 [==============================] - 0s 154us/step - loss: 2782.8317 - mse: 2782.8323 - mae: 28.7156\n",
      "Epoch 44/80\n",
      "1761/1761 [==============================] - 0s 149us/step - loss: 2776.0355 - mse: 2776.0352 - mae: 29.2598\n",
      "Epoch 45/80\n",
      "1761/1761 [==============================] - 0s 138us/step - loss: 2807.7618 - mse: 2807.7627 - mae: 28.9060\n",
      "Epoch 46/80\n",
      "1761/1761 [==============================] - 0s 140us/step - loss: 2847.4658 - mse: 2847.4668 - mae: 28.7417\n",
      "Epoch 47/80\n",
      "1761/1761 [==============================] - 0s 161us/step - loss: 2803.9224 - mse: 2803.9219 - mae: 29.2402\n",
      "Epoch 48/80\n",
      "1761/1761 [==============================] - 0s 166us/step - loss: 2854.8437 - mse: 2854.8438 - mae: 29.6236\n",
      "Epoch 49/80\n",
      "1761/1761 [==============================] - 0s 160us/step - loss: 2826.4513 - mse: 2826.4517 - mae: 29.8768\n",
      "Epoch 50/80\n",
      "1761/1761 [==============================] - 0s 151us/step - loss: 2782.6783 - mse: 2782.6782 - mae: 29.1443\n",
      "Epoch 51/80\n",
      "1761/1761 [==============================] - 0s 160us/step - loss: 2740.8422 - mse: 2740.8416 - mae: 29.3904\n",
      "Epoch 52/80\n",
      "1761/1761 [==============================] - 0s 161us/step - loss: 2820.3358 - mse: 2820.3359 - mae: 29.4097\n",
      "Epoch 53/80\n",
      "1761/1761 [==============================] - 0s 160us/step - loss: 2808.2231 - mse: 2808.2231 - mae: 29.9209\n",
      "Epoch 54/80\n",
      "1761/1761 [==============================] - 0s 151us/step - loss: 2933.9841 - mse: 2933.9829 - mae: 30.1606\n",
      "Epoch 55/80\n",
      "1761/1761 [==============================] - 0s 142us/step - loss: 2874.0856 - mse: 2874.0854 - mae: 29.6295\n",
      "Epoch 56/80\n",
      "1761/1761 [==============================] - 0s 155us/step - loss: 2880.1691 - mse: 2880.1702 - mae: 29.9293\n",
      "Epoch 57/80\n",
      "1761/1761 [==============================] - 0s 149us/step - loss: 2873.5807 - mse: 2873.5798 - mae: 29.1773\n",
      "Epoch 58/80\n",
      "1761/1761 [==============================] - 0s 153us/step - loss: 2882.6199 - mse: 2882.6204 - mae: 29.2107\n",
      "Epoch 59/80\n",
      "1761/1761 [==============================] - 0s 157us/step - loss: 2864.2946 - mse: 2864.2944 - mae: 29.8628\n",
      "Epoch 60/80\n",
      "1761/1761 [==============================] - 0s 139us/step - loss: 2845.7549 - mse: 2845.7546 - mae: 29.4325\n",
      "Epoch 61/80\n",
      "1761/1761 [==============================] - 0s 153us/step - loss: 2841.3787 - mse: 2841.3774 - mae: 29.4943\n",
      "Epoch 62/80\n",
      "1761/1761 [==============================] - 0s 148us/step - loss: 2806.8484 - mse: 2806.8474 - mae: 29.5045\n",
      "Epoch 63/80\n",
      "1761/1761 [==============================] - 0s 155us/step - loss: 2864.0042 - mse: 2864.0054 - mae: 29.4633\n",
      "Epoch 64/80\n",
      "1761/1761 [==============================] - 0s 162us/step - loss: 2817.2218 - mse: 2817.2212 - mae: 29.3874\n",
      "Epoch 65/80\n",
      "1761/1761 [==============================] - 0s 156us/step - loss: 2801.8074 - mse: 2801.8069 - mae: 28.7488\n",
      "Epoch 66/80\n",
      "1761/1761 [==============================] - 0s 161us/step - loss: 2849.0800 - mse: 2849.0801 - mae: 29.2802\n",
      "Epoch 67/80\n",
      "1761/1761 [==============================] - 0s 145us/step - loss: 2755.4598 - mse: 2755.4592 - mae: 29.1145\n",
      "Epoch 68/80\n",
      "1761/1761 [==============================] - 0s 150us/step - loss: 2748.0003 - mse: 2748.0002 - mae: 28.6512\n",
      "Epoch 69/80\n",
      "1761/1761 [==============================] - 0s 154us/step - loss: 2809.7370 - mse: 2809.7371 - mae: 29.4873\n",
      "Epoch 70/80\n",
      "1761/1761 [==============================] - 0s 157us/step - loss: 2803.9245 - mse: 2803.9253 - mae: 29.3525\n",
      "Epoch 71/80\n",
      "1761/1761 [==============================] - 0s 161us/step - loss: 2825.5863 - mse: 2825.5867 - mae: 29.3679\n",
      "Epoch 72/80\n",
      "1761/1761 [==============================] - 0s 151us/step - loss: 2776.3133 - mse: 2776.3130 - mae: 29.3054\n",
      "Epoch 73/80\n",
      "1761/1761 [==============================] - 0s 152us/step - loss: 2751.3689 - mse: 2751.3682 - mae: 28.7608\n",
      "Epoch 74/80\n",
      "1761/1761 [==============================] - 0s 154us/step - loss: 2834.6501 - mse: 2834.6509 - mae: 29.3015\n",
      "Epoch 75/80\n",
      "1761/1761 [==============================] - 0s 161us/step - loss: 2784.5145 - mse: 2784.5146 - mae: 29.1472\n",
      "Epoch 76/80\n",
      "1761/1761 [==============================] - 0s 159us/step - loss: 2764.7417 - mse: 2764.7415 - mae: 29.4234\n",
      "Epoch 77/80\n",
      "1761/1761 [==============================] - 0s 162us/step - loss: 2773.8113 - mse: 2773.8105 - mae: 29.0756\n",
      "Epoch 78/80\n",
      "1761/1761 [==============================] - 0s 142us/step - loss: 2764.8309 - mse: 2764.8308 - mae: 28.9196\n",
      "Epoch 79/80\n",
      "1761/1761 [==============================] - 0s 147us/step - loss: 2851.5139 - mse: 2851.5144 - mae: 29.6717\n",
      "Epoch 80/80\n",
      "1761/1761 [==============================] - 0s 154us/step - loss: 2815.5221 - mse: 2815.5220 - mae: 29.4977\n",
      "3\n",
      "Epoch 1/80\n",
      "2346/2346 [==============================] - 0s 150us/step - loss: 2515.9989 - mse: 2515.9993 - mae: 29.1108\n",
      "Epoch 2/80\n",
      "2346/2346 [==============================] - 0s 149us/step - loss: 2474.6418 - mse: 2474.6414 - mae: 28.9012\n",
      "Epoch 3/80\n",
      "2346/2346 [==============================] - 0s 163us/step - loss: 2516.2745 - mse: 2516.2742 - mae: 29.6055\n",
      "Epoch 4/80\n",
      "2346/2346 [==============================] - 0s 155us/step - loss: 2544.1562 - mse: 2544.1565 - mae: 29.7775\n",
      "Epoch 5/80\n",
      "2346/2346 [==============================] - 0s 161us/step - loss: 2538.0933 - mse: 2538.0938 - mae: 29.1605\n",
      "Epoch 6/80\n",
      "2346/2346 [==============================] - 0s 148us/step - loss: 2461.5547 - mse: 2461.5544 - mae: 29.2933\n",
      "Epoch 7/80\n",
      "2346/2346 [==============================] - 0s 153us/step - loss: 2519.0401 - mse: 2519.0403 - mae: 29.1245\n",
      "Epoch 8/80\n",
      "2346/2346 [==============================] - 0s 161us/step - loss: 2543.3040 - mse: 2543.3042 - mae: 29.7069\n",
      "Epoch 9/80\n",
      "2346/2346 [==============================] - 0s 157us/step - loss: 2469.2725 - mse: 2469.2722 - mae: 29.0215\n",
      "Epoch 10/80\n",
      "2346/2346 [==============================] - 0s 152us/step - loss: 2460.5777 - mse: 2460.5784 - mae: 29.1491\n",
      "Epoch 11/80\n",
      "2346/2346 [==============================] - 0s 159us/step - loss: 2493.4856 - mse: 2493.4858 - mae: 29.6055\n",
      "Epoch 12/80\n",
      "2346/2346 [==============================] - 0s 157us/step - loss: 2437.2090 - mse: 2437.2085 - mae: 29.1266\n",
      "Epoch 13/80\n",
      "2346/2346 [==============================] - 0s 146us/step - loss: 2472.8050 - mse: 2472.8049 - mae: 28.9891\n",
      "Epoch 14/80\n",
      "2346/2346 [==============================] - 0s 157us/step - loss: 2482.2764 - mse: 2482.2761 - mae: 29.0502\n",
      "Epoch 15/80\n",
      "2346/2346 [==============================] - 0s 155us/step - loss: 2459.3135 - mse: 2459.3147 - mae: 29.0483\n",
      "Epoch 16/80\n",
      "2346/2346 [==============================] - 0s 150us/step - loss: 2519.8764 - mse: 2519.8760 - mae: 29.6239\n",
      "Epoch 17/80\n",
      "2346/2346 [==============================] - 0s 149us/step - loss: 2579.1110 - mse: 2579.1113 - mae: 29.3932\n",
      "Epoch 18/80\n",
      "2346/2346 [==============================] - 0s 161us/step - loss: 2543.8154 - mse: 2543.8152 - mae: 29.1852\n",
      "Epoch 19/80\n",
      "2346/2346 [==============================] - 0s 157us/step - loss: 2494.3879 - mse: 2494.3884 - mae: 29.1214\n",
      "Epoch 20/80\n",
      "2346/2346 [==============================] - 0s 150us/step - loss: 2577.4076 - mse: 2577.4065 - mae: 29.5351\n",
      "Epoch 21/80\n",
      "2346/2346 [==============================] - 0s 153us/step - loss: 2515.6392 - mse: 2515.6392 - mae: 28.8840\n",
      "Epoch 22/80\n",
      "2346/2346 [==============================] - 0s 142us/step - loss: 2433.8991 - mse: 2433.8992 - mae: 28.9978\n",
      "Epoch 23/80\n",
      "2346/2346 [==============================] - 0s 159us/step - loss: 2514.8993 - mse: 2514.8994 - mae: 29.1543\n",
      "Epoch 24/80\n",
      "2346/2346 [==============================] - 0s 160us/step - loss: 2473.7570 - mse: 2473.7576 - mae: 28.9640\n",
      "Epoch 25/80\n",
      "2346/2346 [==============================] - 0s 156us/step - loss: 2491.3818 - mse: 2491.3813 - mae: 29.5674\n",
      "Epoch 26/80\n",
      "2346/2346 [==============================] - 0s 150us/step - loss: 2528.0853 - mse: 2528.0862 - mae: 29.2206\n",
      "Epoch 27/80\n",
      "2346/2346 [==============================] - 0s 145us/step - loss: 2508.0478 - mse: 2508.0479 - mae: 29.5848\n",
      "Epoch 28/80\n",
      "2346/2346 [==============================] - 0s 156us/step - loss: 2462.6343 - mse: 2462.6345 - mae: 29.5140\n",
      "Epoch 29/80\n",
      "2346/2346 [==============================] - 0s 151us/step - loss: 2491.2404 - mse: 2491.2407 - mae: 29.0274\n",
      "Epoch 30/80\n",
      "2346/2346 [==============================] - 0s 162us/step - loss: 2501.4008 - mse: 2501.4006 - mae: 29.1955\n",
      "Epoch 31/80\n",
      "2346/2346 [==============================] - 0s 158us/step - loss: 2485.0897 - mse: 2485.0898 - mae: 28.9929\n",
      "Epoch 32/80\n",
      "2346/2346 [==============================] - 0s 160us/step - loss: 2490.6144 - mse: 2490.6143 - mae: 29.0840\n",
      "Epoch 33/80\n",
      "2346/2346 [==============================] - 0s 163us/step - loss: 2463.7170 - mse: 2463.7166 - mae: 28.5334\n",
      "Epoch 34/80\n",
      "2346/2346 [==============================] - 0s 164us/step - loss: 2491.6652 - mse: 2491.6648 - mae: 29.1592\n",
      "Epoch 35/80\n",
      "2346/2346 [==============================] - 0s 160us/step - loss: 2488.4586 - mse: 2488.4578 - mae: 29.2889\n",
      "Epoch 36/80\n",
      "2346/2346 [==============================] - 0s 160us/step - loss: 2479.3803 - mse: 2479.3796 - mae: 29.1676\n",
      "Epoch 37/80\n",
      "2346/2346 [==============================] - 0s 160us/step - loss: 2485.3468 - mse: 2485.3472 - mae: 29.3401\n",
      "Epoch 38/80\n",
      "2346/2346 [==============================] - 0s 163us/step - loss: 2484.3810 - mse: 2484.3816 - mae: 29.0728\n",
      "Epoch 39/80\n",
      "2346/2346 [==============================] - 0s 150us/step - loss: 2476.4745 - mse: 2476.4746 - mae: 29.5643\n",
      "Epoch 40/80\n",
      "2346/2346 [==============================] - 0s 158us/step - loss: 2446.3190 - mse: 2446.3184 - mae: 29.0441\n",
      "Epoch 41/80\n",
      "2346/2346 [==============================] - 0s 145us/step - loss: 2500.8824 - mse: 2500.8813 - mae: 29.4178\n",
      "Epoch 42/80\n",
      "2346/2346 [==============================] - 0s 160us/step - loss: 2491.2702 - mse: 2491.2700 - mae: 29.3651\n",
      "Epoch 43/80\n",
      "2346/2346 [==============================] - 0s 163us/step - loss: 2533.9256 - mse: 2533.9263 - mae: 29.3251\n",
      "Epoch 44/80\n",
      "2346/2346 [==============================] - 0s 149us/step - loss: 2523.7842 - mse: 2523.7844 - mae: 29.2115\n",
      "Epoch 45/80\n",
      "2346/2346 [==============================] - 0s 152us/step - loss: 2514.9660 - mse: 2514.9658 - mae: 29.1395\n",
      "Epoch 46/80\n",
      "2346/2346 [==============================] - 0s 169us/step - loss: 2495.4946 - mse: 2495.4949 - mae: 29.6281\n",
      "Epoch 47/80\n",
      "2346/2346 [==============================] - 0s 151us/step - loss: 2497.5566 - mse: 2497.5569 - mae: 29.2758\n",
      "Epoch 48/80\n",
      "2346/2346 [==============================] - 0s 158us/step - loss: 2478.6283 - mse: 2478.6294 - mae: 29.1039\n",
      "Epoch 49/80\n",
      "2346/2346 [==============================] - 0s 156us/step - loss: 2507.7868 - mse: 2507.7866 - mae: 29.2109\n",
      "Epoch 50/80\n",
      "2346/2346 [==============================] - 0s 152us/step - loss: 2550.0814 - mse: 2550.0823 - mae: 29.3428\n",
      "Epoch 51/80\n",
      "2346/2346 [==============================] - 0s 149us/step - loss: 2496.7330 - mse: 2496.7332 - mae: 29.2531\n",
      "Epoch 52/80\n",
      "2346/2346 [==============================] - 0s 153us/step - loss: 2529.4277 - mse: 2529.4275 - mae: 29.6058\n",
      "Epoch 53/80\n",
      "2346/2346 [==============================] - 0s 159us/step - loss: 2528.1066 - mse: 2528.1072 - mae: 29.2734\n",
      "Epoch 54/80\n",
      "2346/2346 [==============================] - 0s 159us/step - loss: 2497.4291 - mse: 2497.4290 - mae: 29.0547\n",
      "Epoch 55/80\n",
      "2346/2346 [==============================] - 0s 159us/step - loss: 2463.9364 - mse: 2463.9358 - mae: 28.8010\n",
      "Epoch 56/80\n",
      "2346/2346 [==============================] - 0s 154us/step - loss: 2464.6048 - mse: 2464.6047 - mae: 28.8039\n",
      "Epoch 57/80\n",
      "2346/2346 [==============================] - 0s 166us/step - loss: 2487.1141 - mse: 2487.1147 - mae: 28.8413\n",
      "Epoch 58/80\n",
      "2346/2346 [==============================] - 0s 159us/step - loss: 2412.5951 - mse: 2412.5955 - mae: 28.3978\n",
      "Epoch 59/80\n",
      "2346/2346 [==============================] - 0s 150us/step - loss: 2544.5700 - mse: 2544.5701 - mae: 29.3825\n",
      "Epoch 60/80\n",
      "2346/2346 [==============================] - 0s 161us/step - loss: 2490.4744 - mse: 2490.4739 - mae: 29.2906\n",
      "Epoch 61/80\n",
      "2346/2346 [==============================] - 0s 158us/step - loss: 2481.4929 - mse: 2481.4924 - mae: 29.0314\n",
      "Epoch 62/80\n",
      "2346/2346 [==============================] - 0s 167us/step - loss: 2536.8508 - mse: 2536.8496 - mae: 29.0280\n",
      "Epoch 63/80\n",
      "2346/2346 [==============================] - 0s 181us/step - loss: 2535.0549 - mse: 2535.0547 - mae: 29.3252\n",
      "Epoch 64/80\n",
      "2346/2346 [==============================] - 0s 148us/step - loss: 2509.5885 - mse: 2509.5891 - mae: 29.1631\n",
      "Epoch 65/80\n",
      "2346/2346 [==============================] - 0s 157us/step - loss: 2517.8887 - mse: 2517.8884 - mae: 29.3740\n",
      "Epoch 66/80\n",
      "2346/2346 [==============================] - 0s 155us/step - loss: 2474.7564 - mse: 2474.7576 - mae: 29.3916\n",
      "Epoch 67/80\n",
      "2346/2346 [==============================] - 0s 160us/step - loss: 2524.3010 - mse: 2524.3013 - mae: 29.2779\n",
      "Epoch 68/80\n",
      "2346/2346 [==============================] - 0s 153us/step - loss: 2561.4633 - mse: 2561.4629 - mae: 29.4357\n",
      "Epoch 69/80\n",
      "2346/2346 [==============================] - 0s 156us/step - loss: 2492.7852 - mse: 2492.7859 - mae: 29.3276\n",
      "Epoch 70/80\n",
      "2346/2346 [==============================] - 0s 159us/step - loss: 2484.1838 - mse: 2484.1833 - mae: 29.0158\n",
      "Epoch 71/80\n",
      "2346/2346 [==============================] - 0s 163us/step - loss: 2518.5917 - mse: 2518.5916 - mae: 29.5578\n",
      "Epoch 72/80\n",
      "2346/2346 [==============================] - 0s 153us/step - loss: 2500.2531 - mse: 2500.2524 - mae: 28.8867\n",
      "Epoch 73/80\n",
      "2346/2346 [==============================] - 0s 152us/step - loss: 2498.9587 - mse: 2498.9587 - mae: 28.7488\n",
      "Epoch 74/80\n",
      "2346/2346 [==============================] - 0s 152us/step - loss: 2499.5785 - mse: 2499.5784 - mae: 29.2666\n",
      "Epoch 75/80\n",
      "2346/2346 [==============================] - 0s 152us/step - loss: 2507.6319 - mse: 2507.6304 - mae: 29.1504\n",
      "Epoch 76/80\n",
      "2346/2346 [==============================] - 0s 157us/step - loss: 2467.1847 - mse: 2467.1851 - mae: 29.3727\n",
      "Epoch 77/80\n",
      "2346/2346 [==============================] - 0s 162us/step - loss: 2497.1462 - mse: 2497.1465 - mae: 29.3519\n",
      "Epoch 78/80\n",
      "2346/2346 [==============================] - 0s 148us/step - loss: 2491.4024 - mse: 2491.4023 - mae: 29.5118\n",
      "Epoch 79/80\n",
      "2346/2346 [==============================] - 0s 166us/step - loss: 2449.9678 - mse: 2449.9685 - mae: 29.0740\n",
      "Epoch 80/80\n",
      "2346/2346 [==============================] - 0s 151us/step - loss: 2549.7852 - mse: 2549.7854 - mae: 29.6247\n",
      "4\n",
      "Epoch 1/80\n",
      "2931/2931 [==============================] - 0s 143us/step - loss: 2315.0474 - mse: 2315.0466 - mae: 29.1337\n",
      "Epoch 2/80\n",
      "2931/2931 [==============================] - 0s 135us/step - loss: 2304.3089 - mse: 2304.3086 - mae: 28.8859\n",
      "Epoch 3/80\n",
      "2931/2931 [==============================] - 0s 140us/step - loss: 2280.5345 - mse: 2280.5337 - mae: 28.6430\n",
      "Epoch 4/80\n",
      "2931/2931 [==============================] - 0s 149us/step - loss: 2298.7057 - mse: 2298.7058 - mae: 28.9333\n",
      "Epoch 5/80\n",
      "2931/2931 [==============================] - 0s 162us/step - loss: 2294.8713 - mse: 2294.8711 - mae: 29.0972\n",
      "Epoch 6/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2299.0702 - mse: 2299.0691 - mae: 28.7272\n",
      "Epoch 7/80\n",
      "2931/2931 [==============================] - 0s 152us/step - loss: 2274.5838 - mse: 2274.5835 - mae: 28.8380\n",
      "Epoch 8/80\n",
      "2931/2931 [==============================] - 0s 141us/step - loss: 2281.1881 - mse: 2281.1873 - mae: 28.9507\n",
      "Epoch 9/80\n",
      "2931/2931 [==============================] - 0s 156us/step - loss: 2232.5548 - mse: 2232.5542 - mae: 28.4657\n",
      "Epoch 10/80\n",
      "2931/2931 [==============================] - 1s 187us/step - loss: 2233.0973 - mse: 2233.0977 - mae: 28.7619\n",
      "Epoch 11/80\n",
      "2931/2931 [==============================] - 0s 158us/step - loss: 2269.8296 - mse: 2269.8296 - mae: 28.1800\n",
      "Epoch 12/80\n",
      "2931/2931 [==============================] - 1s 176us/step - loss: 2335.0664 - mse: 2335.0667 - mae: 29.2368\n",
      "Epoch 13/80\n",
      "2931/2931 [==============================] - 0s 161us/step - loss: 2271.4403 - mse: 2271.4402 - mae: 28.4908\n",
      "Epoch 14/80\n",
      "2931/2931 [==============================] - 0s 139us/step - loss: 2259.0300 - mse: 2259.0303 - mae: 28.7355\n",
      "Epoch 15/80\n",
      "2931/2931 [==============================] - 0s 143us/step - loss: 2304.9351 - mse: 2304.9351 - mae: 28.4534\n",
      "Epoch 16/80\n",
      "2931/2931 [==============================] - 0s 162us/step - loss: 2275.9584 - mse: 2275.9578 - mae: 28.8193\n",
      "Epoch 17/80\n",
      "2931/2931 [==============================] - 0s 149us/step - loss: 2255.8571 - mse: 2255.8569 - mae: 28.4642\n",
      "Epoch 18/80\n",
      "2931/2931 [==============================] - 0s 151us/step - loss: 2273.0493 - mse: 2273.0491 - mae: 28.7346\n",
      "Epoch 19/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2261.0023 - mse: 2261.0020 - mae: 28.4184\n",
      "Epoch 20/80\n",
      "2931/2931 [==============================] - 0s 151us/step - loss: 2274.4018 - mse: 2274.4023 - mae: 28.5438\n",
      "Epoch 21/80\n",
      "2931/2931 [==============================] - 0s 138us/step - loss: 2270.6472 - mse: 2270.6482 - mae: 28.7405\n",
      "Epoch 22/80\n",
      "2931/2931 [==============================] - 0s 150us/step - loss: 2244.3667 - mse: 2244.3665 - mae: 28.6867\n",
      "Epoch 23/80\n",
      "2931/2931 [==============================] - 0s 149us/step - loss: 2280.1330 - mse: 2280.1333 - mae: 28.5456\n",
      "Epoch 24/80\n",
      "2931/2931 [==============================] - 0s 153us/step - loss: 2274.4832 - mse: 2274.4834 - mae: 28.5428\n",
      "Epoch 25/80\n",
      "2931/2931 [==============================] - 0s 160us/step - loss: 2258.9748 - mse: 2258.9744 - mae: 28.5373\n",
      "Epoch 26/80\n",
      "2931/2931 [==============================] - 0s 149us/step - loss: 2267.0479 - mse: 2267.0483 - mae: 28.9264\n",
      "Epoch 27/80\n",
      "2931/2931 [==============================] - 0s 151us/step - loss: 2330.2717 - mse: 2330.2717 - mae: 28.8875\n",
      "Epoch 28/80\n",
      "2931/2931 [==============================] - 0s 162us/step - loss: 2262.7602 - mse: 2262.7598 - mae: 28.4905\n",
      "Epoch 29/80\n",
      "2931/2931 [==============================] - 0s 165us/step - loss: 2224.6998 - mse: 2224.7007 - mae: 28.3451\n",
      "Epoch 30/80\n",
      "2931/2931 [==============================] - 0s 150us/step - loss: 2329.2659 - mse: 2329.2664 - mae: 29.0305\n",
      "Epoch 31/80\n",
      "2931/2931 [==============================] - 0s 149us/step - loss: 2252.2422 - mse: 2252.2424 - mae: 28.6447\n",
      "Epoch 32/80\n",
      "2931/2931 [==============================] - 0s 153us/step - loss: 2268.4184 - mse: 2268.4187 - mae: 28.8626\n",
      "Epoch 33/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2281.1881 - mse: 2281.1877 - mae: 28.5182\n",
      "Epoch 34/80\n",
      "2931/2931 [==============================] - 0s 161us/step - loss: 2261.9881 - mse: 2261.9873 - mae: 28.4854\n",
      "Epoch 35/80\n",
      "2931/2931 [==============================] - 0s 152us/step - loss: 2289.8796 - mse: 2289.8799 - mae: 28.5640\n",
      "Epoch 36/80\n",
      "2931/2931 [==============================] - 0s 150us/step - loss: 2324.7104 - mse: 2324.7109 - mae: 28.6807\n",
      "Epoch 37/80\n",
      "2931/2931 [==============================] - 0s 161us/step - loss: 2321.5589 - mse: 2321.5586 - mae: 28.9538\n",
      "Epoch 38/80\n",
      "2931/2931 [==============================] - 0s 160us/step - loss: 2247.6260 - mse: 2247.6257 - mae: 28.6093\n",
      "Epoch 39/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2313.3329 - mse: 2313.3328 - mae: 28.8061\n",
      "Epoch 40/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2291.4550 - mse: 2291.4541 - mae: 28.9584\n",
      "Epoch 41/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2291.9544 - mse: 2291.9546 - mae: 28.9317\n",
      "Epoch 42/80\n",
      "2931/2931 [==============================] - 0s 151us/step - loss: 2285.0716 - mse: 2285.0710 - mae: 28.4163\n",
      "Epoch 43/80\n",
      "2931/2931 [==============================] - 0s 155us/step - loss: 2289.3823 - mse: 2289.3821 - mae: 28.9166\n",
      "Epoch 44/80\n",
      "2931/2931 [==============================] - 0s 158us/step - loss: 2263.1855 - mse: 2263.1855 - mae: 28.3944\n",
      "Epoch 45/80\n",
      "2931/2931 [==============================] - 0s 160us/step - loss: 2324.2450 - mse: 2324.2456 - mae: 28.6086\n",
      "Epoch 46/80\n",
      "2931/2931 [==============================] - 0s 158us/step - loss: 2316.4810 - mse: 2316.4805 - mae: 29.0279\n",
      "Epoch 47/80\n",
      "2931/2931 [==============================] - 0s 154us/step - loss: 2270.6181 - mse: 2270.6187 - mae: 29.2190\n",
      "Epoch 48/80\n",
      "2931/2931 [==============================] - 0s 148us/step - loss: 2222.6972 - mse: 2222.6968 - mae: 28.1380\n",
      "Epoch 49/80\n",
      "2931/2931 [==============================] - 0s 152us/step - loss: 2259.6747 - mse: 2259.6748 - mae: 28.4361\n",
      "Epoch 50/80\n",
      "2931/2931 [==============================] - 0s 143us/step - loss: 2261.7283 - mse: 2261.7285 - mae: 28.5673\n",
      "Epoch 51/80\n",
      "2931/2931 [==============================] - 0s 163us/step - loss: 2280.0627 - mse: 2280.0625 - mae: 28.7417\n",
      "Epoch 52/80\n",
      "2931/2931 [==============================] - 0s 153us/step - loss: 2312.8332 - mse: 2312.8330 - mae: 29.1813\n",
      "Epoch 53/80\n",
      "2931/2931 [==============================] - 0s 152us/step - loss: 2302.8060 - mse: 2302.8054 - mae: 28.5818\n",
      "Epoch 54/80\n",
      "2931/2931 [==============================] - 0s 151us/step - loss: 2224.7907 - mse: 2224.7903 - mae: 28.0416\n",
      "Epoch 55/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2240.0876 - mse: 2240.0872 - mae: 28.6366\n",
      "Epoch 56/80\n",
      "2931/2931 [==============================] - 0s 162us/step - loss: 2269.4116 - mse: 2269.4111 - mae: 28.4226\n",
      "Epoch 57/80\n",
      "2931/2931 [==============================] - 0s 148us/step - loss: 2212.5182 - mse: 2212.5183 - mae: 28.1873\n",
      "Epoch 58/80\n",
      "2931/2931 [==============================] - 0s 142us/step - loss: 2317.8409 - mse: 2317.8401 - mae: 28.7524\n",
      "Epoch 59/80\n",
      "2931/2931 [==============================] - 0s 153us/step - loss: 2291.8940 - mse: 2291.8938 - mae: 28.9912\n",
      "Epoch 60/80\n",
      "2931/2931 [==============================] - 0s 163us/step - loss: 2264.3897 - mse: 2264.3896 - mae: 28.2849\n",
      "Epoch 61/80\n",
      "2931/2931 [==============================] - 0s 154us/step - loss: 2291.5043 - mse: 2291.5049 - mae: 28.9017\n",
      "Epoch 62/80\n",
      "2931/2931 [==============================] - 0s 152us/step - loss: 2260.8192 - mse: 2260.8196 - mae: 28.2104\n",
      "Epoch 63/80\n",
      "2931/2931 [==============================] - 0s 159us/step - loss: 2327.0072 - mse: 2327.0071 - mae: 28.9825\n",
      "Epoch 64/80\n",
      "2931/2931 [==============================] - 0s 154us/step - loss: 2253.5879 - mse: 2253.5874 - mae: 28.2897\n",
      "Epoch 65/80\n",
      "2931/2931 [==============================] - 0s 153us/step - loss: 2240.1226 - mse: 2240.1226 - mae: 28.7222\n",
      "Epoch 66/80\n",
      "2931/2931 [==============================] - 0s 149us/step - loss: 2239.8708 - mse: 2239.8706 - mae: 28.4695\n",
      "Epoch 67/80\n",
      "2931/2931 [==============================] - 0s 156us/step - loss: 2268.6205 - mse: 2268.6201 - mae: 28.7895\n",
      "Epoch 68/80\n",
      "2931/2931 [==============================] - 0s 152us/step - loss: 2247.0884 - mse: 2247.0886 - mae: 28.6869\n",
      "Epoch 69/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2294.5665 - mse: 2294.5674 - mae: 29.1020\n",
      "Epoch 70/80\n",
      "2931/2931 [==============================] - 0s 152us/step - loss: 2245.0875 - mse: 2245.0869 - mae: 28.4506\n",
      "Epoch 71/80\n",
      "2931/2931 [==============================] - 0s 150us/step - loss: 2298.6617 - mse: 2298.6614 - mae: 28.6771\n",
      "Epoch 72/80\n",
      "2931/2931 [==============================] - 0s 153us/step - loss: 2256.5303 - mse: 2256.5305 - mae: 28.4433\n",
      "Epoch 73/80\n",
      "2931/2931 [==============================] - 0s 158us/step - loss: 2311.7569 - mse: 2311.7566 - mae: 29.2507\n",
      "Epoch 74/80\n",
      "2931/2931 [==============================] - 0s 147us/step - loss: 2241.2155 - mse: 2241.2156 - mae: 28.5774\n",
      "Epoch 75/80\n",
      "2931/2931 [==============================] - 0s 149us/step - loss: 2318.3495 - mse: 2318.3499 - mae: 28.6700\n",
      "Epoch 76/80\n",
      "2931/2931 [==============================] - 0s 144us/step - loss: 2254.0112 - mse: 2254.0120 - mae: 28.9378\n",
      "Epoch 77/80\n",
      "2931/2931 [==============================] - 0s 159us/step - loss: 2287.6604 - mse: 2287.6599 - mae: 28.8870\n",
      "Epoch 78/80\n",
      "2931/2931 [==============================] - 0s 165us/step - loss: 2295.8414 - mse: 2295.8418 - mae: 28.8145\n",
      "Epoch 79/80\n",
      "2931/2931 [==============================] - 0s 157us/step - loss: 2267.0725 - mse: 2267.0718 - mae: 28.5717\n",
      "Epoch 80/80\n",
      "2931/2931 [==============================] - 0s 156us/step - loss: 2307.6918 - mse: 2307.6921 - mae: 28.7774\n",
      "5\n",
      "Epoch 1/80\n",
      "3516/3516 [==============================] - 1s 151us/step - loss: 2747.0114 - mse: 2747.0117 - mae: 29.1680\n",
      "Epoch 2/80\n",
      "3516/3516 [==============================] - 1s 152us/step - loss: 2655.0808 - mse: 2655.0813 - mae: 28.5086\n",
      "Epoch 3/80\n",
      "3516/3516 [==============================] - 1s 148us/step - loss: 2689.9207 - mse: 2689.9219 - mae: 28.8010\n",
      "Epoch 4/80\n",
      "3516/3516 [==============================] - 1s 176us/step - loss: 2737.3647 - mse: 2737.3650 - mae: 28.7727\n",
      "Epoch 5/80\n",
      "3516/3516 [==============================] - 1s 174us/step - loss: 2715.4446 - mse: 2715.4453 - mae: 28.7263\n",
      "Epoch 6/80\n",
      "3516/3516 [==============================] - 1s 180us/step - loss: 2696.3421 - mse: 2696.3416 - mae: 28.7564\n",
      "Epoch 7/80\n",
      "3516/3516 [==============================] - 1s 179us/step - loss: 2670.8671 - mse: 2670.8669 - mae: 28.5162\n",
      "Epoch 8/80\n",
      "3516/3516 [==============================] - 1s 163us/step - loss: 2641.3211 - mse: 2641.3208 - mae: 28.2805\n",
      "Epoch 9/80\n",
      "3516/3516 [==============================] - 1s 163us/step - loss: 2642.2607 - mse: 2642.2625 - mae: 28.6574\n",
      "Epoch 10/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2627.0054 - mse: 2627.0063 - mae: 28.3774\n",
      "Epoch 11/80\n",
      "3516/3516 [==============================] - 1s 170us/step - loss: 2664.6136 - mse: 2664.6123 - mae: 28.6489\n",
      "Epoch 12/80\n",
      "3516/3516 [==============================] - 1s 178us/step - loss: 2678.8579 - mse: 2678.8582 - mae: 28.2597\n",
      "Epoch 13/80\n",
      "3516/3516 [==============================] - 1s 167us/step - loss: 2693.8425 - mse: 2693.8420 - mae: 28.5752\n",
      "Epoch 14/80\n",
      "3516/3516 [==============================] - 1s 152us/step - loss: 2665.4782 - mse: 2665.4788 - mae: 28.7016\n",
      "Epoch 15/80\n",
      "3516/3516 [==============================] - 1s 152us/step - loss: 2712.5812 - mse: 2712.5803 - mae: 28.5889\n",
      "Epoch 16/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2666.2567 - mse: 2666.2551 - mae: 28.4495\n",
      "Epoch 17/80\n",
      "3516/3516 [==============================] - 1s 143us/step - loss: 2659.7861 - mse: 2659.7864 - mae: 28.2989\n",
      "Epoch 18/80\n",
      "3516/3516 [==============================] - 1s 151us/step - loss: 2694.2318 - mse: 2694.2312 - mae: 28.6428\n",
      "Epoch 19/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2630.4245 - mse: 2630.4246 - mae: 28.3872\n",
      "Epoch 20/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2647.5181 - mse: 2647.5183 - mae: 28.4074\n",
      "Epoch 21/80\n",
      "3516/3516 [==============================] - 1s 145us/step - loss: 2670.7742 - mse: 2670.7747 - mae: 28.4960\n",
      "Epoch 22/80\n",
      "3516/3516 [==============================] - 1s 152us/step - loss: 2673.5991 - mse: 2673.6016 - mae: 28.4892\n",
      "Epoch 23/80\n",
      "3516/3516 [==============================] - 1s 151us/step - loss: 2717.6910 - mse: 2717.6902 - mae: 28.6151\n",
      "Epoch 24/80\n",
      "3516/3516 [==============================] - 1s 148us/step - loss: 2674.4227 - mse: 2674.4231 - mae: 28.4917\n",
      "Epoch 25/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2692.2760 - mse: 2692.2759 - mae: 28.9047\n",
      "Epoch 26/80\n",
      "3516/3516 [==============================] - 1s 154us/step - loss: 2667.1139 - mse: 2667.1138 - mae: 28.7744\n",
      "Epoch 27/80\n",
      "3516/3516 [==============================] - 1s 161us/step - loss: 2680.5712 - mse: 2680.5713 - mae: 28.7861\n",
      "Epoch 28/80\n",
      "3516/3516 [==============================] - 1s 159us/step - loss: 2618.2522 - mse: 2618.2532 - mae: 28.0554\n",
      "Epoch 29/80\n",
      "3516/3516 [==============================] - 1s 152us/step - loss: 2682.2870 - mse: 2682.2876 - mae: 28.5016\n",
      "Epoch 30/80\n",
      "3516/3516 [==============================] - 1s 155us/step - loss: 2641.3730 - mse: 2641.3728 - mae: 28.0408\n",
      "Epoch 31/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2656.3872 - mse: 2656.3865 - mae: 28.3754\n",
      "Epoch 32/80\n",
      "3516/3516 [==============================] - 1s 159us/step - loss: 2642.5800 - mse: 2642.5786 - mae: 28.2813\n",
      "Epoch 33/80\n",
      "3516/3516 [==============================] - 1s 144us/step - loss: 2645.3351 - mse: 2645.3350 - mae: 28.3029\n",
      "Epoch 34/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2681.2814 - mse: 2681.2815 - mae: 28.3464\n",
      "Epoch 35/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2692.9297 - mse: 2692.9299 - mae: 28.6321\n",
      "Epoch 36/80\n",
      "3516/3516 [==============================] - 1s 153us/step - loss: 2668.4148 - mse: 2668.4143 - mae: 28.5945\n",
      "Epoch 37/80\n",
      "3516/3516 [==============================] - 1s 155us/step - loss: 2674.8405 - mse: 2674.8401 - mae: 28.1682\n",
      "Epoch 38/80\n",
      "3516/3516 [==============================] - 1s 158us/step - loss: 2636.0385 - mse: 2636.0391 - mae: 28.3755\n",
      "Epoch 39/80\n",
      "3516/3516 [==============================] - 1s 157us/step - loss: 2691.1520 - mse: 2691.1514 - mae: 28.7870\n",
      "Epoch 40/80\n",
      "3516/3516 [==============================] - 1s 154us/step - loss: 2669.1829 - mse: 2669.1826 - mae: 28.5662\n",
      "Epoch 41/80\n",
      "3516/3516 [==============================] - 1s 151us/step - loss: 2651.1074 - mse: 2651.1069 - mae: 28.6665\n",
      "Epoch 42/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2651.4008 - mse: 2651.4014 - mae: 28.0940\n",
      "Epoch 43/80\n",
      "3516/3516 [==============================] - 1s 163us/step - loss: 2682.5904 - mse: 2682.5913 - mae: 28.2449\n",
      "Epoch 44/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2662.0795 - mse: 2662.0789 - mae: 28.4892\n",
      "Epoch 45/80\n",
      "3516/3516 [==============================] - 1s 152us/step - loss: 2660.7050 - mse: 2660.7048 - mae: 28.3142\n",
      "Epoch 46/80\n",
      "3516/3516 [==============================] - 1s 160us/step - loss: 2690.6212 - mse: 2690.6206 - mae: 28.8314\n",
      "Epoch 47/80\n",
      "3516/3516 [==============================] - 1s 149us/step - loss: 2634.9245 - mse: 2634.9243 - mae: 28.4443\n",
      "Epoch 48/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2677.9344 - mse: 2677.9341 - mae: 28.5713\n",
      "Epoch 49/80\n",
      "3516/3516 [==============================] - 1s 151us/step - loss: 2639.4137 - mse: 2639.4138 - mae: 28.5720\n",
      "Epoch 50/80\n",
      "3516/3516 [==============================] - 1s 160us/step - loss: 2640.7804 - mse: 2640.7800 - mae: 28.4535\n",
      "Epoch 51/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2704.4974 - mse: 2704.4961 - mae: 29.0800\n",
      "Epoch 52/80\n",
      "3516/3516 [==============================] - 1s 154us/step - loss: 2660.4419 - mse: 2660.4414 - mae: 28.6296\n",
      "Epoch 53/80\n",
      "3516/3516 [==============================] - 1s 154us/step - loss: 2689.0917 - mse: 2689.0918 - mae: 28.5020\n",
      "Epoch 54/80\n",
      "3516/3516 [==============================] - 1s 160us/step - loss: 2660.2647 - mse: 2660.2644 - mae: 28.2592\n",
      "Epoch 55/80\n",
      "3516/3516 [==============================] - 1s 154us/step - loss: 2643.4692 - mse: 2643.4700 - mae: 28.4045\n",
      "Epoch 56/80\n",
      "3516/3516 [==============================] - 1s 157us/step - loss: 2678.2789 - mse: 2678.2793 - mae: 28.5974\n",
      "Epoch 57/80\n",
      "3516/3516 [==============================] - 1s 155us/step - loss: 2652.1322 - mse: 2652.1323 - mae: 28.4540\n",
      "Epoch 58/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2652.2342 - mse: 2652.2346 - mae: 28.2279\n",
      "Epoch 59/80\n",
      "3516/3516 [==============================] - 1s 158us/step - loss: 2684.2619 - mse: 2684.2622 - mae: 28.5966\n",
      "Epoch 60/80\n",
      "3516/3516 [==============================] - 1s 154us/step - loss: 2631.3393 - mse: 2631.3398 - mae: 28.4392\n",
      "Epoch 61/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2696.5040 - mse: 2696.5042 - mae: 28.6336\n",
      "Epoch 62/80\n",
      "3516/3516 [==============================] - 1s 159us/step - loss: 2682.5302 - mse: 2682.5300 - mae: 28.5008\n",
      "Epoch 63/80\n",
      "3516/3516 [==============================] - 1s 150us/step - loss: 2658.4704 - mse: 2658.4707 - mae: 28.6087\n",
      "Epoch 64/80\n",
      "3516/3516 [==============================] - 1s 149us/step - loss: 2711.9177 - mse: 2711.9185 - mae: 29.0172\n",
      "Epoch 65/80\n",
      "3516/3516 [==============================] - 1s 148us/step - loss: 2636.4892 - mse: 2636.4895 - mae: 28.3148\n",
      "Epoch 66/80\n",
      "3516/3516 [==============================] - 1s 157us/step - loss: 2686.5144 - mse: 2686.5137 - mae: 28.8299\n",
      "Epoch 67/80\n",
      "3516/3516 [==============================] - 1s 153us/step - loss: 2662.3441 - mse: 2662.3447 - mae: 28.9268\n",
      "Epoch 68/80\n",
      "3516/3516 [==============================] - 1s 170us/step - loss: 2691.6205 - mse: 2691.6206 - mae: 28.7000\n",
      "Epoch 69/80\n",
      "3516/3516 [==============================] - 1s 158us/step - loss: 2676.3441 - mse: 2676.3433 - mae: 28.9720\n",
      "Epoch 70/80\n",
      "3516/3516 [==============================] - 1s 171us/step - loss: 2685.3125 - mse: 2685.3123 - mae: 28.8087\n",
      "Epoch 71/80\n",
      "3516/3516 [==============================] - 1s 172us/step - loss: 2678.2596 - mse: 2678.2598 - mae: 28.3588\n",
      "Epoch 72/80\n",
      "3516/3516 [==============================] - 1s 163us/step - loss: 2660.5253 - mse: 2660.5254 - mae: 28.4032\n",
      "Epoch 73/80\n",
      "3516/3516 [==============================] - 0s 134us/step - loss: 2676.7246 - mse: 2676.7253 - mae: 28.2363\n",
      "Epoch 74/80\n",
      "3516/3516 [==============================] - 0s 131us/step - loss: 2628.3355 - mse: 2628.3350 - mae: 28.2745\n",
      "Epoch 75/80\n",
      "3516/3516 [==============================] - 0s 137us/step - loss: 2685.6792 - mse: 2685.6804 - mae: 28.5386\n",
      "Epoch 76/80\n",
      "3516/3516 [==============================] - 1s 159us/step - loss: 2688.2418 - mse: 2688.2419 - mae: 28.6268\n",
      "Epoch 77/80\n",
      "3516/3516 [==============================] - 1s 159us/step - loss: 2689.5710 - mse: 2689.5708 - mae: 28.6075\n",
      "Epoch 78/80\n",
      "3516/3516 [==============================] - 1s 154us/step - loss: 2634.1706 - mse: 2634.1724 - mae: 28.5960\n",
      "Epoch 79/80\n",
      "3516/3516 [==============================] - 1s 156us/step - loss: 2619.3817 - mse: 2619.3804 - mae: 28.3320\n",
      "Epoch 80/80\n",
      "3516/3516 [==============================] - 1s 161us/step - loss: 2743.3050 - mse: 2743.3049 - mae: 28.8582\n",
      "6\n",
      "Epoch 1/80\n",
      "4101/4101 [==============================] - 1s 159us/step - loss: 2762.0293 - mse: 2762.0293 - mae: 29.5962\n",
      "Epoch 2/80\n",
      "4101/4101 [==============================] - 1s 160us/step - loss: 2798.9612 - mse: 2798.9612 - mae: 29.4990\n",
      "Epoch 3/80\n",
      "4101/4101 [==============================] - 1s 150us/step - loss: 2827.8275 - mse: 2827.8276 - mae: 29.6739\n",
      "Epoch 4/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2783.1383 - mse: 2783.1372 - mae: 29.9449\n",
      "Epoch 5/80\n",
      "4101/4101 [==============================] - 1s 152us/step - loss: 2810.6162 - mse: 2810.6160 - mae: 29.3768\n",
      "Epoch 6/80\n",
      "4101/4101 [==============================] - 1s 155us/step - loss: 2794.1579 - mse: 2794.1575 - mae: 29.4560\n",
      "Epoch 7/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2766.8340 - mse: 2766.8345 - mae: 29.3479\n",
      "Epoch 8/80\n",
      "4101/4101 [==============================] - 1s 147us/step - loss: 2772.8414 - mse: 2772.8416 - mae: 29.7198\n",
      "Epoch 9/80\n",
      "4101/4101 [==============================] - 1s 166us/step - loss: 2737.1211 - mse: 2737.1201 - mae: 29.2660\n",
      "Epoch 10/80\n",
      "4101/4101 [==============================] - 1s 164us/step - loss: 2782.5446 - mse: 2782.5449 - mae: 29.5738\n",
      "Epoch 11/80\n",
      "4101/4101 [==============================] - 1s 150us/step - loss: 2775.7836 - mse: 2775.7842 - mae: 29.4806\n",
      "Epoch 12/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2760.0800 - mse: 2760.0796 - mae: 29.2750\n",
      "Epoch 13/80\n",
      "4101/4101 [==============================] - 1s 150us/step - loss: 2756.5894 - mse: 2756.5908 - mae: 29.3845\n",
      "Epoch 14/80\n",
      "4101/4101 [==============================] - 1s 145us/step - loss: 2757.5100 - mse: 2757.5095 - mae: 29.1456\n",
      "Epoch 15/80\n",
      "4101/4101 [==============================] - 1s 145us/step - loss: 2786.2626 - mse: 2786.2627 - mae: 29.1677\n",
      "Epoch 16/80\n",
      "4101/4101 [==============================] - 1s 152us/step - loss: 2744.7427 - mse: 2744.7429 - mae: 29.3075\n",
      "Epoch 17/80\n",
      "4101/4101 [==============================] - 1s 155us/step - loss: 2767.9636 - mse: 2767.9631 - mae: 29.1966\n",
      "Epoch 18/80\n",
      "4101/4101 [==============================] - 1s 144us/step - loss: 2737.1228 - mse: 2737.1221 - mae: 29.2556\n",
      "Epoch 19/80\n",
      "4101/4101 [==============================] - 1s 143us/step - loss: 2728.2718 - mse: 2728.2712 - mae: 29.1094\n",
      "Epoch 20/80\n",
      "4101/4101 [==============================] - 1s 159us/step - loss: 2748.3610 - mse: 2748.3616 - mae: 29.2209\n",
      "Epoch 21/80\n",
      "4101/4101 [==============================] - 1s 148us/step - loss: 2741.5737 - mse: 2741.5735 - mae: 29.2618\n",
      "Epoch 22/80\n",
      "4101/4101 [==============================] - 1s 158us/step - loss: 2769.7390 - mse: 2769.7375 - mae: 29.2828\n",
      "Epoch 23/80\n",
      "4101/4101 [==============================] - 1s 149us/step - loss: 2752.8961 - mse: 2752.8960 - mae: 29.3048\n",
      "Epoch 24/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2779.2737 - mse: 2779.2732 - mae: 29.6420\n",
      "Epoch 25/80\n",
      "4101/4101 [==============================] - 1s 148us/step - loss: 2765.5592 - mse: 2765.5588 - mae: 29.3045\n",
      "Epoch 26/80\n",
      "4101/4101 [==============================] - 1s 147us/step - loss: 2705.1151 - mse: 2705.1150 - mae: 29.0237\n",
      "Epoch 27/80\n",
      "4101/4101 [==============================] - 1s 157us/step - loss: 2745.9498 - mse: 2745.9490 - mae: 29.3867\n",
      "Epoch 28/80\n",
      "4101/4101 [==============================] - 1s 154us/step - loss: 2739.9137 - mse: 2739.9136 - mae: 29.2136\n",
      "Epoch 29/80\n",
      "4101/4101 [==============================] - 1s 154us/step - loss: 2735.9300 - mse: 2735.9299 - mae: 29.1455\n",
      "Epoch 30/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2766.9885 - mse: 2766.9878 - mae: 29.3846\n",
      "Epoch 31/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2739.6788 - mse: 2739.6792 - mae: 29.1046\n",
      "Epoch 32/80\n",
      "4101/4101 [==============================] - 1s 158us/step - loss: 2746.8807 - mse: 2746.8818 - mae: 29.4817\n",
      "Epoch 33/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2805.8483 - mse: 2805.8494 - mae: 29.5522\n",
      "Epoch 34/80\n",
      "4101/4101 [==============================] - 1s 145us/step - loss: 2785.6957 - mse: 2785.6936 - mae: 29.3186\n",
      "Epoch 35/80\n",
      "4101/4101 [==============================] - 1s 156us/step - loss: 2755.5114 - mse: 2755.5110 - mae: 29.3915\n",
      "Epoch 36/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2771.0094 - mse: 2771.0107 - mae: 29.1176\n",
      "Epoch 37/80\n",
      "4101/4101 [==============================] - 1s 159us/step - loss: 2738.2170 - mse: 2738.2173 - mae: 29.0729\n",
      "Epoch 38/80\n",
      "4101/4101 [==============================] - 1s 165us/step - loss: 2759.8278 - mse: 2759.8279 - mae: 28.9833\n",
      "Epoch 39/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2762.7757 - mse: 2762.7754 - mae: 29.4552\n",
      "Epoch 40/80\n",
      "4101/4101 [==============================] - 1s 154us/step - loss: 2770.0780 - mse: 2770.0781 - mae: 29.8071\n",
      "Epoch 41/80\n",
      "4101/4101 [==============================] - 1s 186us/step - loss: 2756.8945 - mse: 2756.8943 - mae: 28.9895\n",
      "Epoch 42/80\n",
      "4101/4101 [==============================] - 1s 187us/step - loss: 2788.5274 - mse: 2788.5286 - mae: 29.2164\n",
      "Epoch 43/80\n",
      "4101/4101 [==============================] - 1s 164us/step - loss: 2745.4275 - mse: 2745.4270 - mae: 29.0018\n",
      "Epoch 44/80\n",
      "4101/4101 [==============================] - 1s 146us/step - loss: 2728.7029 - mse: 2728.7039 - mae: 29.0738\n",
      "Epoch 45/80\n",
      "4101/4101 [==============================] - 1s 155us/step - loss: 2757.0311 - mse: 2757.0310 - mae: 29.1142\n",
      "Epoch 46/80\n",
      "4101/4101 [==============================] - 1s 184us/step - loss: 2752.7739 - mse: 2752.7742 - mae: 29.2374\n",
      "Epoch 47/80\n",
      "4101/4101 [==============================] - 1s 166us/step - loss: 2758.4945 - mse: 2758.4944 - mae: 29.0566\n",
      "Epoch 48/80\n",
      "4101/4101 [==============================] - 1s 180us/step - loss: 2734.8367 - mse: 2734.8364 - mae: 28.8848\n",
      "Epoch 49/80\n",
      "4101/4101 [==============================] - 1s 204us/step - loss: 2765.9032 - mse: 2765.9028 - mae: 29.3997\n",
      "Epoch 50/80\n",
      "4101/4101 [==============================] - 1s 154us/step - loss: 2744.7041 - mse: 2744.7031 - mae: 29.0915\n",
      "Epoch 51/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2744.2713 - mse: 2744.2715 - mae: 29.1339\n",
      "Epoch 52/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2777.0532 - mse: 2777.0527 - mae: 29.3947\n",
      "Epoch 53/80\n",
      "4101/4101 [==============================] - 1s 147us/step - loss: 2768.2477 - mse: 2768.2468 - mae: 29.2128\n",
      "Epoch 54/80\n",
      "4101/4101 [==============================] - 1s 147us/step - loss: 2772.5834 - mse: 2772.5828 - mae: 28.8160\n",
      "Epoch 55/80\n",
      "4101/4101 [==============================] - 1s 149us/step - loss: 2769.5224 - mse: 2769.5225 - mae: 29.5473\n",
      "Epoch 56/80\n",
      "4101/4101 [==============================] - 1s 154us/step - loss: 2713.3905 - mse: 2713.3909 - mae: 29.0700\n",
      "Epoch 57/80\n",
      "4101/4101 [==============================] - 1s 148us/step - loss: 2742.4809 - mse: 2742.4802 - mae: 28.8791\n",
      "Epoch 58/80\n",
      "4101/4101 [==============================] - 1s 150us/step - loss: 2752.7145 - mse: 2752.7144 - mae: 29.3779\n",
      "Epoch 59/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2738.3206 - mse: 2738.3188 - mae: 29.2439\n",
      "Epoch 60/80\n",
      "4101/4101 [==============================] - 1s 154us/step - loss: 2725.6681 - mse: 2725.6689 - mae: 28.6489\n",
      "Epoch 61/80\n",
      "4101/4101 [==============================] - 1s 148us/step - loss: 2737.6253 - mse: 2737.6243 - mae: 29.1781\n",
      "Epoch 62/80\n",
      "4101/4101 [==============================] - 1s 158us/step - loss: 2747.2127 - mse: 2747.2126 - mae: 29.0175\n",
      "Epoch 63/80\n",
      "4101/4101 [==============================] - 1s 158us/step - loss: 2714.2027 - mse: 2714.2029 - mae: 28.9781\n",
      "Epoch 64/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2768.9547 - mse: 2768.9553 - mae: 29.2073\n",
      "Epoch 65/80\n",
      "4101/4101 [==============================] - 1s 153us/step - loss: 2776.5626 - mse: 2776.5615 - mae: 29.2513\n",
      "Epoch 66/80\n",
      "4101/4101 [==============================] - 1s 154us/step - loss: 2773.4531 - mse: 2773.4526 - mae: 29.4743\n",
      "Epoch 67/80\n",
      "4101/4101 [==============================] - 1s 148us/step - loss: 2756.1353 - mse: 2756.1357 - mae: 29.0207\n",
      "Epoch 68/80\n",
      "4101/4101 [==============================] - 1s 155us/step - loss: 2740.6840 - mse: 2740.6831 - mae: 29.3309\n",
      "Epoch 69/80\n",
      "4101/4101 [==============================] - 1s 150us/step - loss: 2727.8849 - mse: 2727.8850 - mae: 29.2129\n",
      "Epoch 70/80\n",
      "4101/4101 [==============================] - 1s 150us/step - loss: 2739.6374 - mse: 2739.6367 - mae: 29.0707\n",
      "Epoch 71/80\n",
      "4101/4101 [==============================] - 1s 148us/step - loss: 2774.2731 - mse: 2774.2744 - mae: 29.3365\n",
      "Epoch 72/80\n",
      "4101/4101 [==============================] - 1s 148us/step - loss: 2781.9428 - mse: 2781.9436 - mae: 29.3097\n",
      "Epoch 73/80\n",
      "4101/4101 [==============================] - 1s 137us/step - loss: 2761.1190 - mse: 2761.1191 - mae: 29.3017\n",
      "Epoch 74/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2733.9589 - mse: 2733.9585 - mae: 29.1849\n",
      "Epoch 75/80\n",
      "4101/4101 [==============================] - 1s 146us/step - loss: 2798.8159 - mse: 2798.8167 - mae: 29.3411\n",
      "Epoch 76/80\n",
      "4101/4101 [==============================] - 1s 156us/step - loss: 2734.0383 - mse: 2734.0386 - mae: 29.3136\n",
      "Epoch 77/80\n",
      "4101/4101 [==============================] - 1s 149us/step - loss: 2711.4951 - mse: 2711.4949 - mae: 29.0301\n",
      "Epoch 78/80\n",
      "4101/4101 [==============================] - 1s 150us/step - loss: 2723.7903 - mse: 2723.7891 - mae: 29.0340\n",
      "Epoch 79/80\n",
      "4101/4101 [==============================] - 1s 151us/step - loss: 2757.5386 - mse: 2757.5381 - mae: 29.0604\n",
      "Epoch 80/80\n",
      "4101/4101 [==============================] - 1s 152us/step - loss: 2719.8434 - mse: 2719.8418 - mae: 28.8121\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# data\n",
    "data = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "data = data.loc[data.index > 2018090000, :]\n",
    "    \n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "    \n",
    "# Divide features and labels\n",
    "X = data.iloc[:, 0:15]\n",
    "y = data.loc[:, 'Offers']\n",
    "\n",
    "X.fillna(X.mean(), inplace = True)\n",
    "y.fillna(y.mean(), inplace = True)\n",
    "    \n",
    "X = X.astype('float64')\n",
    "X = X.round(20)\n",
    "    \n",
    "# divide data into train and test with 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "# feature scaling\n",
    "sc_X = MinMaxScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "    \n",
    "import keras\n",
    "from keras.models import Sequential # to initialise the NN\n",
    "from keras.layers import Dense # to create layers\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "    \n",
    "# possible debug\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "    \n",
    "def regressor_tunning(n_hidden = 2, \n",
    "                      n_neurons = 30,  \n",
    "                      kernel_initializer = \"he_normal\",\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = n_neurons, input_dim = 15))        \n",
    "    model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dropout(rate = 0.1))        \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(keras.layers.LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(rate = 0.1))\n",
    "    model.add(Dense(units = 1, activation = 'linear'))\n",
    "    optimizer = optimizers.Adamax(lr = 0.001)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.001 * 0.1 ** (epoch / 20)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 7)\n",
    "    \n",
    "hist_list = pd.DataFrame()\n",
    "count = 1\n",
    "    \n",
    "regressor = regressor_tunning()\n",
    "    \n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_split, X_test_split = X_train[train_index], X_train[test_index]\n",
    "    y_train_split, y_test_split = y_train[train_index], y_train[test_index]\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "    hist = regressor.fit(X_train_split, y_train_split, batch_size = 15, epochs = 80, callbacks = [lr_scheduler])\n",
    "    hist_list = hist_list.append(hist.history, ignore_index = True)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for i in range(len(hist_list.mse)):\n",
    "    a.append(np.mean(hist_list.mse[i]))\n",
    "    b.append(np.mean(hist_list.mae[i]))\n",
    "\n",
    "mse_cv.append(np.mean(a))\n",
    "mae_cv.append(np.mean(b))\n",
    "\n",
    "# predict for X_test  \n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mse_error = mse(y_test, y_pred) # 1479.61335\n",
    "mae_error = mae(y_test, y_pred) # 23.1525\n",
    "\n",
    "rmse_gen.append(rmse_error)\n",
    "mse_gen.append(mse_error)\n",
    "mae_gen.append(mae_error)\n",
    "\n",
    "# =============================================================================\n",
    "# Metrics evaluation on spike regions\n",
    "# =============================================================================\n",
    "\n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "\n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "\n",
    "# smal adjustment\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "\n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mse_spike = mse(y_test_spike, y_pred_spike)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "\n",
    "rmse_spi.append(rmse_spike)\n",
    "mse_spi.append(mse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "\n",
    "# =============================================================================\n",
    "# Metric evaluation on normal regions\n",
    "# =============================================================================\n",
    "\n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "\n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "\n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "\n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mse_normal = mse(y_test_normal, y_pred_normal)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "\n",
    "rmse_nor.append(rmse_normal)\n",
    "mse_nor.append(mse_normal)\n",
    "mae_nor.append(mae_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_cv</th>\n",
       "      <th>mae_cv</th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr = 0.001</th>\n",
       "      <td>55.276041</td>\n",
       "      <td>28.981050</td>\n",
       "      <td>33.480437</td>\n",
       "      <td>25.72624</td>\n",
       "      <td>70.280266</td>\n",
       "      <td>57.429934</td>\n",
       "      <td>23.855083</td>\n",
       "      <td>21.144065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp schedueling</th>\n",
       "      <td>61.821680</td>\n",
       "      <td>35.702667</td>\n",
       "      <td>33.925013</td>\n",
       "      <td>26.66175</td>\n",
       "      <td>71.837222</td>\n",
       "      <td>59.189063</td>\n",
       "      <td>23.903608</td>\n",
       "      <td>21.960537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rmse_cv     mae_cv  rmse_general  mae_general  rmse_spike  \\\n",
       "lr = 0.001       55.276041  28.981050     33.480437     25.72624   70.280266   \n",
       "exp schedueling  61.821680  35.702667     33.925013     26.66175   71.837222   \n",
       "\n",
       "                 mae_spike  rmse_normal  mae_normal  \n",
       "lr = 0.001       57.429934    23.855083   21.144065  \n",
       "exp schedueling  59.189063    23.903608   21.960537  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_cv = []\n",
    "for i in mse_cv:\n",
    "    rmse_cv.append(i ** 0.5)\n",
    "    \n",
    "results = pd.DataFrame({'rmse_cv':rmse_cv,\n",
    "              \n",
    "                        'mae_cv': mae_cv,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor}, index = ['lr = 0.001', 'exp schedueling'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col0 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col2 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col3 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col4 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col5 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col6 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col7 {\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rmse_cv</th>        <th class=\"col_heading level0 col1\" >mae_cv</th>        <th class=\"col_heading level0 col2\" >rmse_general</th>        <th class=\"col_heading level0 col3\" >mae_general</th>        <th class=\"col_heading level0 col4\" >rmse_spike</th>        <th class=\"col_heading level0 col5\" >mae_spike</th>        <th class=\"col_heading level0 col6\" >rmse_normal</th>        <th class=\"col_heading level0 col7\" >mae_normal</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5level0_row0\" class=\"row_heading level0 row0\" >lr = 0.001</th>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col0\" class=\"data row0 col0\" >55.276041</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col1\" class=\"data row0 col1\" >28.981050</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col2\" class=\"data row0 col2\" >33.480437</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col3\" class=\"data row0 col3\" >25.726240</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col4\" class=\"data row0 col4\" >70.280266</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col5\" class=\"data row0 col5\" >57.429934</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col6\" class=\"data row0 col6\" >23.855083</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row0_col7\" class=\"data row0 col7\" >21.144065</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5level0_row1\" class=\"row_heading level0 row1\" >exp schedueling</th>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col0\" class=\"data row1 col0\" >61.821680</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col1\" class=\"data row1 col1\" >35.702667</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col2\" class=\"data row1 col2\" >33.925013</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col3\" class=\"data row1 col3\" >26.661750</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col4\" class=\"data row1 col4\" >71.837222</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col5\" class=\"data row1 col5\" >59.189063</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col6\" class=\"data row1 col6\" >23.903608</td>\n",
       "                        <td id=\"T_3da1de8c_c464_11ea_b95d_4ded42f9afe5row1_col7\" class=\"data row1 col7\" >21.960537</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fd44c47fd50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
